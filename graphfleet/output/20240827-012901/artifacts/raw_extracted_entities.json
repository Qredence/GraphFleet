{"id":"0c932f7def033fa2b1bf210fbb771e7d","chunk":"From Local to Global: A Graph RAG Approach to\nQuery-Focused Summarization\nDarren Edge1\u2020Ha Trinh1\u2020Newman Cheng2Joshua Bradley2Alex Chao3\nApurva Mody3Steven Truitt2\nJonathan Larson1\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n@microsoft.com\n\u2020These authors contributed equally to this work\nAbstract\nThe use of retrieval-augmented generation (RAG) to retrieve relevant informa-\ntion from an external knowledge source enables large language models (LLMs)\nto answer questions over private and\/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas \u201cWhat are the main themes in the dataset?\u201d, since this is inherently a query-\nfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\nRAG systems. To combine the strengths of these contrasting methods, we propose\na Graph RAG approach to question answering over private text corpora that scales\nwith both the generality of user questions and the quantity of source text to be in-\ndexed. Our approach uses an LLM to build a graph-based text index in two stages:\nfirst to derive an entity knowledge graph from the source documents, then to pre-\ngenerate community summaries for all groups of closely-related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that Graph RAG leads to substantial improvements over a na \u00a8\u0131ve RAG\nbaseline for both the comprehensiveness and diversity of generated answers. An\nopen-source, Python-based implementation of both global and local Graph RAG\napproaches is forthcoming at https:\/\/aka .ms\/graphrag .\n1 Introduction\nHuman endeavors across a range of domains rely on our ability to read and reason about large\ncollections of documents, often reaching conclusions that go beyond anything stated in the source\ntexts themselves. With the emergence of large language models (LLMs), we are already witnessing\nattempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\ncrosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\nPreprint. Under review.arXiv:2404.16130v1  [cs.CL]  24 Apr 2024Source Documents\nText Chunkstext extraction\nand chunking\nElement Instancesdomain-tailored\nsummarization\nElement Summariesdomain-tailored\nsummarization\nGraph Communitiescommunity\ndetectionCommunity Summaries\ndomain-tailored\nsummarizationCommunity Answers\nquery-focused\nsummarizationGlobal Answer\nquery-focused\nsummarization\nIndexing Time Query Time Pipeline Stage\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\nindex spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\nbeen detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\nCommunity detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into\ngroups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both index-\ning time and query time. The \u201cglobal answer\u201d to a given query is produced using a final round of\nquery-focused summarization over all community summaries reporting relevance to that query.\n\u201ca motivated, continuous effort to understand connections (which can be among people, places, and\nevents) in order to anticipate their trajectories and act effectively \u201d (Klein et al., 2006a). Supporting\nhuman-led sensemaking over entire text corpora, however, needs a way for people to both apply and\nrefine their mental model of the data (Klein et al., 2006b) by asking questions of a global nature.\nRetrieval-augmented generation (RAG, Lewis et al., 2020) is an established approach to answering\nuser questions over entire datasets, but it is designed for situations where these answers are contained\nlocally within regions of text whose retrieval provides sufficient grounding for the generation task.\nInstead, a more appropriate task framing is query-focused summarization (QFS, Dang, 2006), and in\nparticular, query-focused abstractive summarization that generates natural language summaries and\nnot just concatenated excerpts (Baumel et al., 2018; Laskar et al., 2020; Yao et al., 2017) . In recent\nyears, however, such distinctions between summarization tasks that are abstractive versus extractive,\ngeneric versus query-focused, and single-document versus multi-document, have become less rele-\nvant. While early applications of the transformer architecture showed substantial improvements on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al.,","chunk_id":"0c932f7def033fa2b1bf210fbb771e7d","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"DARREN EDGE","type":"PERSON","description":"Darren Edge is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"HA TRINH","type":"PERSON","description":"Ha Trinh is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"NEWMAN CHENG","type":"PERSON","description":"Newman Cheng is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"JOSHUA BRADLEY","type":"PERSON","description":"Joshua Bradley is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"ALEX CHAO","type":"PERSON","description":"Alex Chao is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"APURVA MODY","type":"PERSON","description":"Apurva Mody is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"STEVEN TRUITT","type":"PERSON","description":"Steven Truitt is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"JONATHAN LARSON","type":"PERSON","description":"Jonathan Larson is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT RESEARCH","type":"ORGANIZATION","description":"Microsoft Research is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES","type":"ORGANIZATION","description":"Microsoft Strategic Missions and Technologies is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"MICROSOFT OFFICE OF THE CTO","type":"ORGANIZATION","description":"Microsoft Office of the CTO is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is an approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"RETRIEVAL-AUGMENTED GENERATION (RAG)","type":"TECHNOLOGY","description":"Retrieval-augmented generation (RAG) is a method that retrieves relevant information from an external knowledge source to enable large language models (LLMs) to answer questions over private and\/or previously unseen document collections","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"QUERY-FOCUSED SUMMARIZATION (QFS)","type":"TECHNOLOGY","description":"Query-focused summarization (QFS) is a task that generates natural language summaries based on specific user queries, rather than just retrieving relevant text excerpts","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LARGE LANGUAGE MODELS (LLMS)","type":"TECHNOLOGY","description":"Large language models (LLMs) are advanced AI models capable of understanding and generating human-like text, used in various applications including question answering and summarization","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LEIDEN","type":"TECHNOLOGY","description":"Leiden is a community detection algorithm used to partition graph indexes into groups of elements that can be summarized in parallel","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"KLEIN","type":"PERSON","description":"Klein is an author referenced in the context of sensemaking and understanding connections among people, places, and events","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LEWIS","type":"PERSON","description":"Lewis is an author referenced in the context of retrieval-augmented generation (RAG)","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"DANG","type":"PERSON","description":"Dang is an author referenced in the context of query-focused summarization (QFS)","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"BAUMEL","type":"PERSON","description":"Baumel is an author referenced in the context of query-focused abstractive summarization","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LASKAR","type":"PERSON","description":"Laskar is an author referenced in the context of query-focused abstractive summarization","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"YAO","type":"PERSON","description":"Yao is an author referenced in the context of query-focused abstractive summarization","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"GOODWIN","type":"PERSON","description":"Goodwin is an author referenced in the context of transformer architecture improvements in summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LIU","type":"PERSON","description":"Liu is an author referenced in the context of transformer architecture improvements in summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LAPATA","type":"PERSON","description":"Lapata is an author referenced in the context of transformer architecture improvements in summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"GPT","type":"TECHNOLOGY","description":"GPT is a large language model referenced in the context of modern LLMs trivializing various summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"LLAMA","type":"TECHNOLOGY","description":"Llama is a large language model referenced in the context of modern LLMs trivializing various summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"GEMINI","type":"TECHNOLOGY","description":"Gemini is a large language model referenced in the context of modern LLMs trivializing various summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"SENSEMAKING","type":"","description":"\nSensemaking is defined as a motivated, continuous effort to understand connections among people, places, and events in order to anticipate their trajectories and act effectively","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"CONCEPT"},{"name":"TRANSFORMER ARCHITECTURE","type":"","description":"\nTransformer architecture is a type of neural network architecture that has shown substantial improvements in various summarization tasks","source_id":"0c932f7def033fa2b1bf210fbb771e7d","entity_type":"TECHNOLOGY"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is the company affiliated with the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"RANADE","type":"PERSON","description":"Ranade is an author referenced in the context of intelligence analysis and sensemaking","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"JOSHI","type":"PERSON","description":"Joshi is an author referenced in the context of intelligence analysis and sensemaking","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"KLEIN ET AL.","type":"PERSON","description":"Klein et al. are authors referenced in the context of sensemaking and understanding connections among people, places, and events","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"TRAAG ET AL.","type":"PERSON","description":"Traag et al. are authors referenced in the context of community detection using the Leiden algorithm","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"ACHIAM ET AL.","type":"PERSON","description":"Achiam et al. are authors referenced in the context of the GPT large language model","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"BROWN ET AL.","type":"PERSON","description":"Brown et al. are authors referenced in the context of the GPT large language model","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"TOUVRON ET AL.","type":"PERSON","description":"Touvron et al. are authors referenced in the context of the Llama large language model","source_id":"0c932f7def033fa2b1bf210fbb771e7d"},{"name":"ANIL ET AL.","type":"PERSON","description":"Anil et al. are authors referenced in the context of the Gemini large language model","source_id":"0c932f7def033fa2b1bf210fbb771e7d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DARREN EDGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Darren Edge is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"HA TRINH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ha Trinh is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"NEWMAN CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Newman Cheng is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"JOSHUA BRADLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Bradley is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"ALEX CHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Chao is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"APURVA MODY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Apurva Mody is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"STEVEN TRUITT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Truitt is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"JONATHAN LARSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Larson is one of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT RESEARCH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Research is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Strategic Missions and Technologies is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Office of the CTO is one of the affiliations of the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is an approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval-augmented generation (RAG) is a method that retrieves relevant information from an external knowledge source to enable large language models (LLMs) to answer questions over private and\/or previously unseen document collections<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Query-focused summarization (QFS) is a task that generates natural language summaries based on specific user queries, rather than just retrieving relevant text excerpts<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Large language models (LLMs) are advanced AI models capable of understanding and generating human-like text, used in various applications including question answering and summarization<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LEIDEN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Leiden is a community detection algorithm used to partition graph indexes into groups of elements that can be summarized in parallel<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"KLEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Klein is an author referenced in the context of sensemaking and understanding connections among people, places, and events<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis is an author referenced in the context of retrieval-augmented generation (RAG)<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"DANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dang is an author referenced in the context of query-focused summarization (QFS)<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"BAUMEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baumel is an author referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LASKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Laskar is an author referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"GOODWIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goodwin is an author referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LAPATA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lapata is an author referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT is a large language model referenced in the context of modern LLMs trivializing various summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"LLAMA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama is a large language model referenced in the context of modern LLMs trivializing various summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gemini is a large language model referenced in the context of modern LLMs trivializing various summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"SENSEMAKING\">      <data key=\"d0\" \/>      <data key=\"d1\">Sensemaking is defined as a motivated, continuous effort to understand connections among people, places, and events in order to anticipate their trajectories and act effectively<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d0\" \/>      <data key=\"d1\">Transformer architecture is a type of neural network architecture that has shown substantial improvements in various summarization tasks<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is the company affiliated with the authors of the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"RANADE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ranade is an author referenced in the context of intelligence analysis and sensemaking<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshi is an author referenced in the context of intelligence analysis and sensemaking<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"KLEIN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Klein et al. are authors referenced in the context of sensemaking and understanding connections among people, places, and events<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"TRAAG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Traag et al. are authors referenced in the context of community detection using the Leiden algorithm<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"ACHIAM ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Achiam et al. are authors referenced in the context of the GPT large language model<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"BROWN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown et al. are authors referenced in the context of the GPT large language model<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"TOUVRON ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron et al. are authors referenced in the context of the Llama large language model<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <node id=\"ANIL ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anil et al. are authors referenced in the context of the Gemini large language model<\/data>      <data key=\"d2\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/node>    <edge source=\"DARREN EDGE\" target=\"HA TRINH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Ha Trinh co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"NEWMAN CHENG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Newman Cheng co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"JOSHUA BRADLEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Joshua Bradley co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"ALEX CHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Alex Chao co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"APURVA MODY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Apurva Mody co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Darren Edge and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"DARREN EDGE\" target=\"MICROSOFT RESEARCH\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Darren Edge is affiliated with Microsoft Research<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"NEWMAN CHENG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Newman Cheng co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"JOSHUA BRADLEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Joshua Bradley co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"ALEX CHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Alex Chao co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"APURVA MODY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Apurva Mody co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ha Trinh and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"HA TRINH\" target=\"MICROSOFT RESEARCH\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Ha Trinh is affiliated with Microsoft Research<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"JOSHUA BRADLEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Newman Cheng and Joshua Bradley co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"ALEX CHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Newman Cheng and Alex Chao co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"APURVA MODY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Newman Cheng and Apurva Mody co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Newman Cheng and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Newman Cheng and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"NEWMAN CHENG\" target=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Newman Cheng is affiliated with Microsoft Strategic Missions and Technologies<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JOSHUA BRADLEY\" target=\"ALEX CHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joshua Bradley and Alex Chao co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JOSHUA BRADLEY\" target=\"APURVA MODY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joshua Bradley and Apurva Mody co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JOSHUA BRADLEY\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joshua Bradley and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JOSHUA BRADLEY\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Joshua Bradley and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JOSHUA BRADLEY\" target=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Joshua Bradley is affiliated with Microsoft Strategic Missions and Technologies<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"ALEX CHAO\" target=\"APURVA MODY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Alex Chao and Apurva Mody co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"ALEX CHAO\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Alex Chao and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"ALEX CHAO\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Alex Chao and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"ALEX CHAO\" target=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Alex Chao is affiliated with Microsoft Office of the CTO<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"APURVA MODY\" target=\"STEVEN TRUITT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Apurva Mody and Steven Truitt co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"APURVA MODY\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Apurva Mody and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"APURVA MODY\" target=\"MICROSOFT OFFICE OF THE CTO\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Apurva Mody is affiliated with Microsoft Office of the CTO<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"STEVEN TRUITT\" target=\"JONATHAN LARSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Steven Truitt and Jonathan Larson co-authored the paper titled \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"STEVEN TRUITT\" target=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Steven Truitt is affiliated with Microsoft Strategic Missions and Technologies<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"JONATHAN LARSON\" target=\"MICROSOFT RESEARCH\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Jonathan Larson is affiliated with Microsoft Research<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT RESEARCH\" target=\"MICROSOFT\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Microsoft Research is a division of Microsoft<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES\" target=\"MICROSOFT\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Microsoft Strategic Missions and Technologies is a division of Microsoft<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"MICROSOFT OFFICE OF THE CTO\" target=\"MICROSOFT\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Microsoft Office of the CTO is a division of Microsoft<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Graph RAG is proposed as an improvement over traditional RAG for answering global questions over large text corpora<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Graph RAG combines the strengths of RAG and QFS to answer global questions over large text corpora<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"LARGE LANGUAGE MODELS (LLMS)\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Graph RAG uses large language models (LLMs) to build a graph-based text index<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"LEIDEN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Leiden is used in the Graph RAG approach for community detection<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\" target=\"QUERY-FOCUSED SUMMARIZATION (QFS)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">RAG and QFS are contrasting methods for answering questions over text corpora, with RAG focusing on retrieval and QFS on summarization<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"RETRIEVAL-AUGMENTED GENERATION (RAG)\" target=\"LEWIS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Lewis is referenced in the context of retrieval-augmented generation (RAG)<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"DANG\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Dang is referenced in the context of query-focused summarization (QFS)<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"BAUMEL\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Baumel is referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"LASKAR\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Laskar is referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"QUERY-FOCUSED SUMMARIZATION (QFS)\" target=\"YAO\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yao is referenced in the context of query-focused abstractive summarization<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"GPT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">GPT is a large language model referenced in the context of modern LLMs<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"LLAMA\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Llama is a large language model referenced in the context of modern LLMs<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LARGE LANGUAGE MODELS (LLMS)\" target=\"GEMINI\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Gemini is a large language model referenced in the context of modern LLMs<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LEIDEN\" target=\"TRAAG ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Traag et al. are referenced in the context of community detection using the Leiden algorithm<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"KLEIN\" target=\"SENSEMAKING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Klein is referenced in the context of sensemaking and understanding connections among people, places, and events<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GOODWIN\" target=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Goodwin is referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LIU\" target=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Liu is referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LAPATA\" target=\"TRANSFORMER ARCHITECTURE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Lapata is referenced in the context of transformer architecture improvements in summarization tasks<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GPT\" target=\"ACHIAM ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Achiam et al. are referenced in the context of the GPT large language model<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GPT\" target=\"BROWN ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Brown et al. are referenced in the context of the GPT large language model<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"LLAMA\" target=\"TOUVRON ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Touvron et al. are referenced in the context of the Llama large language model<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"GEMINI\" target=\"ANIL ET AL.\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Anil et al. are referenced in the context of the Gemini large language model<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"SENSEMAKING\" target=\"KLEIN ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Klein et al. are referenced in the context of sensemaking and understanding connections among people, places, and events<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>    <edge source=\"RANADE\" target=\"JOSHI\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Ranade and Joshi are co-authors referenced in the context of intelligence analysis and sensemaking<\/data>      <data key=\"d6\">0c932f7def033fa2b1bf210fbb771e7d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"64476a39d7d8b87b399e3bd3cead79c7","chunk":" on\nthe state-of-the-art for all such summarization tasks (Goodwin et al., 2020; Laskar et al., 2022; Liu\nand Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\net al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\nall of which can use in-context learning to summarize any content provided in their context window.\nThe challenge remains, however, for query-focused abstractive summarization over an entire corpus.\nSuch volumes of text can greatly exceed the limits of LLM context windows, and the expansion of\nsuch windows may not be enough given that information can be \u201clost in the middle\u201d of longer\ncontexts (Kuratov et al., 2024; Liu et al., 2023). In addition, although the direct retrieval of text\nchunks in na \u00a8\u0131ve RAG is likely inadequate for QFS tasks, it is possible that an alternative form of\npre-indexing could support a new RAG approach specifically targeting global summarization.\nIn this paper, we present a Graph RAG approach based on global summarization of an LLM-derived\nknowledge graph (Figure 1). In contrast with related work that exploits the structured retrieval\nand traversal affordances of graph indexes (subsection 4.2), we focus on a previously unexplored\nquality of graphs in this context: their inherent modularity (Newman, 2006) and the ability of com-\nmunity detection algorithms to partition graphs into modular communities of closely-related nodes\n(e.g., Louvain, Blondel et al., 2008; Leiden, Traag et al., 2019). LLM-generated summaries of these\n20 1 2 30100002000030000\nNumber of gleanings performedEntity references detected600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 2: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and gleanings for our generic entity extraction prompt with gpt-4-turbo .\ncommunity descriptions provide complete coverage of the underlying graph index and the input doc-\numents it represents. Query-focused summarization of an entire corpus is then made possible using\na map-reduce approach: first using each community summary to answer the query independently\nand in parallel, then summarizing all relevant partial answers into a final global answer.\nTo evaluate this approach, we used an LLM to generate a diverse set of activity-centered sense-\nmaking questions from short descriptions of two representative real-world datasets, containing pod-\ncast transcripts and news articles respectively. For the target qualities of comprehensiveness, diver-\nsity, and empowerment (defined in subsection 3.4) that develop understanding of broad issues and\nthemes, we both explore the impact of varying the the hierarchical level of community summaries\nused to answer queries, as well as compare to na \u00a8\u0131ve RAG and global map-reduce summarization\nof source texts. We show that all global approaches outperform na \u00a8\u0131ve RAG on comprehensiveness\nand diversity, and that Graph RAG with intermediate- and low-level community summaries shows\nfavorable performance over source text summarization on these same metrics, at lower token costs.\n2 Graph RAG Approach & Pipeline\nWe now unpack the high-level data flow of the Graph RAG approach (Figure 1) and pipeline, de-\nscribing key design parameters, techniques, and implementation details for each step.\n2.1 Source Documents \u2192Text Chunks\nA fundamental design decision is the granularity with which input texts extracted from source doc-\numents should be split into text chunks for processing. In the following step, each of these chunks\nwill be passed to a set of LLM prompts designed to extract the various elements of a graph index.\nLonger text chunks require fewer LLM calls for such extraction, but suffer from the recall degrada-\ntion of longer LLM context windows (Kuratov et al., 2024; Liu et al., 2023). This behavior can be\nobserved in Figure 2 in the case of a single extraction round (i.e., with zero gleanings): on a sample\ndataset (HotPotQA, Yang et al., 2018), using a chunk size of 600 token extracted almost twice as\nmany entity references as when using a chunk size of 2400. While more references are generally\nbetter, any extraction process needs to balance recall and precision for the target activity.\n2.2 Text Chunks \u2192Element Instances\nThe baseline requirement for this step is to identify and extract instances of graph nodes and edges\nfrom each chunk of source text. We do this using a multipart LLM prompt that first identifies all\nentities in the text, including their name, type, and description, before identifying all relationships\nbetween clearly-related entities, including the source and target entities and a description of their\nrelationship. Both kinds of element instance are output in a single list of delimited tuples.\nThe primary opportunity to tailor this prompt to the domain of the document corpus lies in the\nchoice of few-shot examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our","chunk_id":"64476a39d7d8b87b399e3bd3cead79c7","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GOODWIN","type":"PERSON","description":"Goodwin is one of the authors mentioned in the text, associated with summarization tasks in 2020","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LASKAR","type":"PERSON","description":"Laskar is one of the authors mentioned in the text, associated with summarization tasks in 2022","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LIU","type":"PERSON","description":"Liu is one of the authors mentioned in the text, associated with summarization tasks in 2019\nLiu is one of the authors mentioned in the text, associated with the issue of information loss in longer contexts in 2023","source_id":"64476a39d7d8b87b399e3bd3cead79c7","entity_type":"PERSON"},{"name":"LAPATA","type":"PERSON","description":"Lapata is one of the authors mentioned in the text, associated with summarization tasks in 2019","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GPT","type":"TECHNOLOGY","description":"GPT is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ACHIAM","type":"PERSON","description":"Achiam is one of the authors mentioned in the text, associated with the GPT series in 2023","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"BROWN","type":"PERSON","description":"Brown is one of the authors mentioned in the text, associated with the GPT series in 2020","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LLAMA","type":"TECHNOLOGY","description":"Llama is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TOUVRON","type":"PERSON","description":"Touvron is one of the authors mentioned in the text, associated with the Llama series in 2023","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GEMINI","type":"TECHNOLOGY","description":"Gemini is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ANIL","type":"PERSON","description":"Anil is one of the authors mentioned in the text, associated with the Gemini series in 2023","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"KURATOV","type":"PERSON","description":"Kuratov is one of the authors mentioned in the text, associated with the issue of information loss in longer contexts in 2024","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"RAG","type":"TECHNOLOGY","description":"RAG (Retrieval-Augmented Generation) is a method mentioned in the text, used for query-focused summarization tasks","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is an approach based on global summarization of an LLM-derived knowledge graph, focusing on the modularity of graphs and community detection algorithms","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"NEWMAN","type":"PERSON","description":"Newman is an author mentioned in the text, associated with the concept of graph modularity in 2006","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LOUVAIN","type":"TECHNOLOGY","description":"Louvain is a community detection algorithm mentioned in the text, used to partition graphs into modular communities of closely-related nodes","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"BLONDEL","type":"PERSON","description":"Blondel is one of the authors mentioned in the text, associated with the Louvain algorithm in 2008","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LEIDEN","type":"TECHNOLOGY","description":"Leiden is a community detection algorithm mentioned in the text, used to partition graphs into modular communities of closely-related nodes","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TRAAG","type":"PERSON","description":"Traag is one of the authors mentioned in the text, associated with the Leiden algorithm in 2019","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset mentioned in the text, used to evaluate the entity extraction prompt with gpt-4-turbo","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"YANG","type":"PERSON","description":"Yang is one of the authors mentioned in the text, associated with the HotPotQA dataset in 2018","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"MAP-REDUCE","type":"TECHNOLOGY","description":"Map-Reduce is an approach mentioned in the text, used for query-focused summarization of an entire corpus by summarizing community summaries in parallel","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"PODCAST TRANSCRIPTS","type":"DATASET","description":"Podcast transcripts are one of the real-world datasets used to generate activity-centered sense-making questions","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"NEWS ARTICLES","type":"DATASET","description":"News articles are one of the real-world datasets used to generate activity-centered sense-making questions","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"COMPREHENSIVENESS","type":"QUALITY","description":"Comprehensiveness is one of the target qualities for evaluating the Graph RAG approach, focusing on the thoroughness of the summaries","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"DIVERSITY","type":"QUALITY","description":"Diversity is one of the target qualities for evaluating the Graph RAG approach, focusing on the variety of perspectives in the summaries","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"EMPOWERMENT","type":"QUALITY","description":"Empowerment is one of the target qualities for evaluating the Graph RAG approach, focusing on the ability to develop understanding of broad issues and themes","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"GRAPH RAG APPROACH","type":"TECHNOLOGY","description":"Graph RAG Approach is a method that involves the high-level data flow and pipeline for global summarization using graph indexes","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"TEXT CHUNKS","type":"DATA","description":"Text chunks are segments of input texts extracted from source documents, used for processing in the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ELEMENT INSTANCES","type":"DATA","description":"Element instances are instances of graph nodes and edges extracted from text chunks in the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"NAMED ENTITIES","type":"DATA","description":"Named entities are a broad class of entities like people, places, and organizations, extracted using LLM prompts","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"FEW-SHOT EXAMPLES","type":"TECHNIQUE","description":"Few-shot examples are used in LLM prompts for in-context learning, tailored to the domain of the document corpus","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"IN-CONTEXT LEARNING","type":"TECHNOLOGY","description":"In-context learning is a capability of modern LLMs, including GPT, Llama, and Gemini, to summarize any content provided in their context window","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"PRE-INDEXING","type":"TECHNOLOGY","description":"Pre-indexing is an alternative form of indexing mentioned as a potential support for a new RAG approach targeting global summarization","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"KNOWLEDGE GRAPH","type":"TECHNOLOGY","description":"A knowledge graph is a structured representation of knowledge, used in the Graph RAG approach for global summarization","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"COMMUNITY DETECTION ALGORITHMS","type":"TECHNOLOGY","description":"Community detection algorithms are used to partition graphs into modular communities of closely-related nodes, such as Louvain and Leiden","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"ACTIVITY-CENTERED SENSE-MAKING QUESTIONS","type":"DATA","description":"Activity-centered sense-making questions are generated from short descriptions of datasets to evaluate the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"SOURCE DOCUMENTS","type":"DATA","description":"Source documents are the original texts from which input texts are extracted and split into text chunks for processing in the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"LLM PROMPTS","type":"TECHNOLOGY","description":"LLM prompts are used to extract various elements of a graph index from text chunks in the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"RECALL","type":"QUALITY","description":"Recall is a quality metric that needs to be balanced with precision in the extraction process of the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"PRECISION","type":"QUALITY","description":"Precision is a quality metric that needs to be balanced with recall in the extraction process of the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"DOMAIN-SPECIFIC KNOWLEDGE","type":"DATA","description":"Domain-specific knowledge refers to specialized knowledge in fields like science, medicine, and law, which can benefit from tailored few-shot examples in LLM prompts","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"COVARIATES","type":"DATA","description":"Covariates are additional variables that can be associated with extracted node instances in the Graph RAG approach","source_id":"64476a39d7d8b87b399e3bd3cead79c7"},{"name":"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION","type":"","description":"","source_id":"64476a39d7d8b87b399e3bd3cead79c7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GOODWIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goodwin is one of the authors mentioned in the text, associated with summarization tasks in 2020<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LASKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Laskar is one of the authors mentioned in the text, associated with summarization tasks in 2022<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is one of the authors mentioned in the text, associated with summarization tasks in 2019Liu is one of the authors mentioned in the text, associated with the issue of information loss in longer contexts in 2023<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAPATA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lapata is one of the authors mentioned in the text, associated with summarization tasks in 2019<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ACHIAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Achiam is one of the authors mentioned in the text, associated with the GPT series in 2023<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown is one of the authors mentioned in the text, associated with the GPT series in 2020<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LLAMA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TOUVRON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron is one of the authors mentioned in the text, associated with the Llama series in 2023<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gemini is a series of Large Language Models (LLMs) mentioned in the text, capable of using in-context learning to summarize content<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ANIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anil is one of the authors mentioned in the text, associated with the Gemini series in 2023<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"KURATOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov is one of the authors mentioned in the text, associated with the issue of information loss in longer contexts in 2024<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is a method mentioned in the text, used for query-focused summarization tasks<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is an approach based on global summarization of an LLM-derived knowledge graph, focusing on the modularity of graphs and community detection algorithms<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"NEWMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Newman is an author mentioned in the text, associated with the concept of graph modularity in 2006<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LOUVAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Louvain is a community detection algorithm mentioned in the text, used to partition graphs into modular communities of closely-related nodes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"BLONDEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Blondel is one of the authors mentioned in the text, associated with the Louvain algorithm in 2008<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LEIDEN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Leiden is a community detection algorithm mentioned in the text, used to partition graphs into modular communities of closely-related nodes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TRAAG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Traag is one of the authors mentioned in the text, associated with the Leiden algorithm in 2019<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset mentioned in the text, used to evaluate the entity extraction prompt with gpt-4-turbo<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is one of the authors mentioned in the text, associated with the HotPotQA dataset in 2018<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"MAP-REDUCE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Map-Reduce is an approach mentioned in the text, used for query-focused summarization of an entire corpus by summarizing community summaries in parallel<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Podcast transcripts are one of the real-world datasets used to generate activity-centered sense-making questions<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">News articles are one of the real-world datasets used to generate activity-centered sense-making questions<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">QUALITY<\/data>      <data key=\"d1\">Comprehensiveness is one of the target qualities for evaluating the Graph RAG approach, focusing on the thoroughness of the summaries<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">QUALITY<\/data>      <data key=\"d1\">Diversity is one of the target qualities for evaluating the Graph RAG approach, focusing on the variety of perspectives in the summaries<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">QUALITY<\/data>      <data key=\"d1\">Empowerment is one of the target qualities for evaluating the Graph RAG approach, focusing on the ability to develop understanding of broad issues and themes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"GRAPH RAG APPROACH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG Approach is a method that involves the high-level data flow and pipeline for global summarization using graph indexes<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Text chunks are segments of input texts extracted from source documents, used for processing in the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ELEMENT INSTANCES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Element instances are instances of graph nodes and edges extracted from text chunks in the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"NAMED ENTITIES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Named entities are a broad class of entities like people, places, and organizations, extracted using LLM prompts<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"FEW-SHOT EXAMPLES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Few-shot examples are used in LLM prompts for in-context learning, tailored to the domain of the document corpus<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">In-context learning is a capability of modern LLMs, including GPT, Llama, and Gemini, to summarize any content provided in their context window<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"PRE-INDEXING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Pre-indexing is an alternative form of indexing mentioned as a potential support for a new RAG approach targeting global summarization<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A knowledge graph is a structured representation of knowledge, used in the Graph RAG approach for global summarization<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Community detection algorithms are used to partition graphs into modular communities of closely-related nodes, such as Louvain and Leiden<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"ACTIVITY-CENTERED SENSE-MAKING QUESTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Activity-centered sense-making questions are generated from short descriptions of datasets to evaluate the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"SOURCE DOCUMENTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Source documents are the original texts from which input texts are extracted and split into text chunks for processing in the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"LLM PROMPTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM prompts are used to extract various elements of a graph index from text chunks in the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"RECALL\">      <data key=\"d0\">QUALITY<\/data>      <data key=\"d1\">Recall is a quality metric that needs to be balanced with precision in the extraction process of the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"PRECISION\">      <data key=\"d0\">QUALITY<\/data>      <data key=\"d1\">Precision is a quality metric that needs to be balanced with recall in the extraction process of the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"DOMAIN-SPECIFIC KNOWLEDGE\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Domain-specific knowledge refers to specialized knowledge in fields like science, medicine, and law, which can benefit from tailored few-shot examples in LLM prompts<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Covariates are additional variables that can be associated with extracted node instances in the Graph RAG approach<\/data>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <node id=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/node>    <edge source=\"GOODWIN\" target=\"LASKAR\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Goodwin and Laskar are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GOODWIN\" target=\"LIU\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Goodwin and Liu are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GOODWIN\" target=\"LAPATA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Goodwin and Lapata are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LASKAR\" target=\"LIU\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Laskar and Liu are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LASKAR\" target=\"LAPATA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Laskar and Lapata are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LIU\" target=\"LAPATA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Liu and Lapata are both authors associated with summarization tasks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LIU\" target=\"KURATOV\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Kuratov and Liu are both authors associated with the issue of information loss in longer contexts<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GPT\" target=\"ACHIAM\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Achiam is one of the authors associated with the GPT series<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GPT\" target=\"BROWN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Brown is one of the authors associated with the GPT series<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GPT\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">GPT series of LLMs are capable of using in-context learning to summarize content<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LLAMA\" target=\"TOUVRON\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Touvron is one of the authors associated with the Llama series<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LLAMA\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Llama series of LLMs are capable of using in-context learning to summarize content<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GEMINI\" target=\"ANIL\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Anil is one of the authors associated with the Gemini series<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GEMINI\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Gemini series of LLMs are capable of using in-context learning to summarize content<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"RAG\" target=\"QUERY-FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Query-focused abstractive summarization is a task that RAG methods aim to address<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"RAG\" target=\"PRE-INDEXING\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Pre-indexing is mentioned as a potential support for a new RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MAP-REDUCE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Graph RAG uses a map-reduce approach for query-focused summarization<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KNOWLEDGE GRAPH\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Graph RAG approach is based on the global summarization of an LLM-derived knowledge graph<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Graph RAG approach uses community detection algorithms to partition graphs into modular communities<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ACTIVITY-CENTERED SENSE-MAKING QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Activity-centered sense-making questions are used to evaluate the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LOUVAIN\" target=\"BLONDEL\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Blondel is one of the authors associated with the Louvain algorithm<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"LEIDEN\" target=\"TRAAG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Traag is one of the authors associated with the Leiden algorithm<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"YANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yang is one of the authors associated with the HotPotQA dataset<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"PODCAST TRANSCRIPTS\" target=\"NEWS ARTICLES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both podcast transcripts and news articles are real-world datasets used to generate activity-centered sense-making questions<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"DIVERSITY\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Comprehensiveness and diversity are target qualities for evaluating the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"EMPOWERMENT\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Comprehensiveness and empowerment are target qualities for evaluating the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"DIVERSITY\" target=\"EMPOWERMENT\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Diversity and empowerment are target qualities for evaluating the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"TEXT CHUNKS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Text chunks are segments of input texts used in the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"GRAPH RAG APPROACH\" target=\"ELEMENT INSTANCES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Element instances are extracted from text chunks in the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"TEXT CHUNKS\" target=\"SOURCE DOCUMENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Text chunks are segments of input texts extracted from source documents<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"NAMED ENTITIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Named entities are a type of element instance extracted using LLM prompts<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"LLM PROMPTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LLM prompts are used to extract element instances from text chunks<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"COVARIATES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Covariates can be associated with extracted node instances in the Graph RAG approach<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"NAMED ENTITIES\" target=\"FEW-SHOT EXAMPLES\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Few-shot examples are used in LLM prompts to extract named entities<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"FEW-SHOT EXAMPLES\" target=\"DOMAIN-SPECIFIC KNOWLEDGE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Few-shot examples tailored to domain-specific knowledge can improve the extraction process<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>    <edge source=\"RECALL\" target=\"PRECISION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Recall and precision are quality metrics that need to be balanced in the extraction process<\/data>      <data key=\"d6\">64476a39d7d8b87b399e3bd3cead79c7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"e66ed885a08f92cc69f4895302c33047","chunk":" examples provided to the LLM for in-context learning (Brown et al., 2020).\n3For example, while our default prompt extracting the broad class of \u201cnamed entities\u201d like people,\nplaces, and organizations is generally applicable, domains with specialized knowledge (e.g., science,\nmedicine, law) will benefit from few-shot examples specialized to those domains. We also support\na secondary extraction prompt for any additional covariates we would like to associate with the\nextracted node instances. Our default covariate prompt aims to extract claims linked to detected\nentities, including the subject, object, type, description, source text span, and start and end dates.\nTo balance the needs of efficiency and quality, we use multiple rounds of \u201cgleanings\u201d, up to a\nspecified maximum, to encourage the LLM to detect any additional entities it may have missed\non prior extraction rounds. This is a multi-stage process in which we first ask the LLM to assess\nwhether all entities were extracted, using a logit bias of 100 to force a yes\/no decision. If the LLM\nresponds that entities were missed, then a continuation indicating that \u201cMANY entities were missed\nin the last extraction\u201d encourages the LLM to glean these missing entities. This approach allows us\nto use larger chunk sizes without a drop in quality (Figure 2) or the forced introduction of noise.\n2.3 Element Instances \u2192Element Summaries\nThe use of an LLM to \u201cextract\u201d descriptions of entities, relationships, and claims represented in\nsource texts is already a form of abstractive summarization, relying on the LLM to create inde-\npendently meaningful summaries of concepts that may be implied but not stated by the text itself\n(e.g., the presence of implied relationships). To convert all such instance-level summaries into sin-\ngle blocks of descriptive text for each graph element (i.e., entity node, relationship edge, and claim\ncovariate) requires a further round of LLM summarization over matching groups of instances.\nA potential concern at this stage is that the LLM may not consistently extract references to the\nsame entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes\nin the entity graph. However, since all closely-related \u201ccommunities\u201d of entities will be detected\nand summarized in the following step, and given that LLMs can understand the common entity\nbehind multiple name variations, our overall approach is resilient to such variations provided there\nis sufficient connectivity from all variations to a shared set of closely-related entities.\nOverall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure\nis aligned with both the capabilities of LLMs and the needs of global, query-focused summarization.\nThese qualities also differentiate our graph index from typical knowledge graphs, which rely on\nconcise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.\n2.4 Element Summaries \u2192Graph Communities\nThe index created in the previous step can be modelled as an homogeneous undirected weighted\ngraph in which entity nodes are connected by relationship edges, with edge weights representing the\nnormalized counts of detected relationship instances. Given such a graph, a variety of community\ndetection algorithms may be used to partition the graph into communities of nodes with stronger\nconnections to one another than to the other nodes in the graph (e.g., see the surveys by Fortu-\nnato, 2010 and Jin et al., 2021). In our pipeline, we use Leiden (Traag et al., 2019) on account of\nits ability to recover hierarchical community structure of large-scale graphs efficiently (Figure 3).\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually-exclusive, collective-exhaustive way, enabling divide-and-conquer global summarization.\n2.5 Graph Communities \u2192Community Summaries\nThe next step is to create report-like summaries of each community in the Leiden hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful in\ntheir own right as a way to understand the global structure and semantics of the dataset, and may\nthemselves be used to make sense of a corpus in the absence of a question. For example, a user\nmay scan through community summaries at one level looking for general themes of interest, then\nfollow links to the reports at the lower level that provide more details for each of the subtopics. Here,\nhowever, we focus on their utility as part of a graph-based index used for answering global queries.\nCommunity summaries are generated in the following way:\n4(a) Root communities at level 0 (b) Sub-communities at level 1\nFigure 3: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\nof hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covari","chunk_id":"e66ed885a08f92cc69f4895302c33047","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LLM-DEBATE","type":"TECHNOLOGY","description":"LLM-Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"INTELLIGENT GO-EXPLORE","type":"TECHNOLOGY","description":"Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on open-endedness and AI-GAs","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author who has worked on open-endedness and AI-GAs","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author who has worked on open-endedness and AI-GAs","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM stands for Large Language Model, a type of artificial intelligence model used for natural language processing tasks","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"BROWN","type":"PERSON","description":"Brown is an author who contributed to the research on in-context learning for LLMs in 2020","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"FEW-SHOT EXAMPLES","type":"TECHNIQUE","description":"Few-shot examples are specialized examples used to train or prompt LLMs in domains with specialized knowledge like science, medicine, and law","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CLAIMS","type":"CONCEPT","description":"Claims are statements linked to detected entities, including attributes like subject, object, type, description, source text span, and start and end dates","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LOGIT BIAS","type":"TECHNIQUE","description":"Logit bias is a technique used to force a yes\/no decision in LLMs during the entity extraction process","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ELEMENT INSTANCES","type":"CONCEPT","description":"Element instances are descriptions of entities, relationships, and claims extracted from source texts by LLMs","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ELEMENT SUMMARIES","type":"CONCEPT","description":"Element summaries are single blocks of descriptive text for each graph element, created by further summarizing instance-level summaries","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GRAPH COMMUNITIES","type":"CONCEPT","description":"Graph communities are groups of nodes in a graph that have stronger connections to one another than to other nodes, detected using community detection algorithms","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LEIDEN","type":"TECHNIQUE","description":"Leiden is a community detection algorithm used to recover hierarchical community structure of large-scale graphs efficiently","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"FORTUNATO","type":"PERSON","description":"Fortunato is an author who has conducted surveys on community detection algorithms","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"JIN","type":"PERSON","description":"Jin is an author who has conducted surveys on community detection algorithms","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"TRAAG","type":"PERSON","description":"Traag is an author who contributed to the development of the Leiden algorithm in 2019","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"MULTIHOP-RAG","type":"DATASET","description":"MultiHop-RAG is a dataset used for indexing and graph community detection","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"TANG","type":"PERSON","description":"Tang is an author who contributed to the MultiHop-RAG dataset in 2024","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"YANG","type":"PERSON","description":"Yang is an author who contributed to the MultiHop-RAG dataset in 2024","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"OPENORD","type":"TECHNIQUE","description":"OpenORD is a node layout algorithm used for visualizing graph communities","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TECHNIQUE"},{"name":"FORCE ATLAS 2","type":"TECHNIQUE","description":"Force Atlas 2 is a node layout algorithm used for visualizing graph communities","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"TECHNIQUE"},{"name":"MARTIN","type":"PERSON","description":"Martin is an author who contributed to the development of the OpenORD algorithm in 2011","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"JACOMY","type":"PERSON","description":"Jacomy is an author who contributed to the development of the Force Atlas 2 algorithm in 2014","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LEAF-LEVEL COMMUNITIES","type":"CONCEPT","description":"Leaf-level communities are the most granular level of graph communities, prioritized and iteratively added to the LLM context window until the token limit is reached","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"NODE","type":"CONCEPT","description":"A node represents an entity in a graph, with attributes like degree indicating its prominence","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"EDGE","type":"CONCEPT","description":"An edge represents a relationship between two nodes in a graph, with weights indicating the strength of the relationship","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"COVARIATE","type":"CONCEPT","description":"A covariate is an additional variable associated with nodes and edges in a graph, used for more detailed analysis","source_id":"e66ed885a08f92cc69f4895302c33047","entity_type":"CONCEPT"},{"name":"EXTRACTION PROMPT","type":"TECHNIQUE","description":"An extraction prompt is used to extract specific types of information, such as named entities or covariates, from text","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"NAMED ENTITIES","type":"CONCEPT","description":"Named entities are specific types of entities like people, places, and organizations that are extracted from text","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"COVARIATE PROMPT","type":"TECHNIQUE","description":"A covariate prompt is used to extract additional variables associated with detected entities, such as claims","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GLEANINGS","type":"TECHNIQUE","description":"Gleanings are multiple rounds of extraction used to detect any additional entities that may have been missed in prior rounds","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"LOGIT BIAS OF 100","type":"TECHNIQUE","description":"A logit bias of 100 is used to force a yes\/no decision in LLMs during the entity extraction process","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"YES\/NO DECISION","type":"TECHNIQUE","description":"A yes\/no decision is a binary choice used to determine if all entities were extracted","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CONTINUATION","type":"TECHNIQUE","description":"A continuation is a follow-up prompt used to encourage the LLM to glean missing entities","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CHUNK SIZES","type":"TECHNIQUE","description":"Chunk sizes refer to the amount of text processed in each round of extraction to balance efficiency and quality","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"NOISE","type":"CONCEPT","description":"Noise refers to irrelevant or extraneous information that can be introduced during the extraction process","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ABSTRACTIVE SUMMARIZATION","type":"TECHNIQUE","description":"Abstractive summarization is a method where the LLM creates independently meaningful summaries of concepts implied but not stated by the text","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"INSTANCE-LEVEL SUMMARIES","type":"CONCEPT","description":"Instance-level summaries are initial summaries of entities, relationships, and claims extracted from text","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"DUPLICATE ENTITY ELEMENTS","type":"CONCEPT","description":"Duplicate entity elements are multiple references to the same entity in different formats, which can result in duplicate nodes in the entity graph","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ENTITY GRAPH","type":"CONCEPT","description":"An entity graph is a graph structure where nodes represent entities and edges represent relationships between them","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"CLOSELY-RELATED COMMUNITIES","type":"CONCEPT","description":"Closely-related communities are groups of entities with strong connections to each other, detected and summarized to handle variations in entity names","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"GLOBAL, QUERY-FOCUSED SUMMARIZATION","type":"TECHNIQUE","description":"Global, query-focused summarization is a method that uses rich descriptive text for nodes in a graph to answer global queries","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"KNOWLEDGE GRAPHS","type":"CONCEPT","description":"Knowledge graphs are graph structures that rely on concise and consistent knowledge triples for reasoning tasks","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"COMMUNITY DETECTION ALGORITHMS","type":"TECHNIQUE","description":"Community detection algorithms are used to partition a graph into communities of nodes with stronger connections to each other","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"HOMOGENEOUS UNDIRECTED WEIGHTED GRAPH","type":"CONCEPT","description":"A homogeneous undirected weighted graph is a type of graph where nodes are connected by edges with weights representing the strength of relationships","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"EDGE WEIGHTS","type":"CONCEPT","description":"Edge weights are values that represent the normalized counts of detected relationship instances between nodes in a graph","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"HIERARCHICAL COMMUNITY STRUCTURE","type":"CONCEPT","description":"Hierarchical community structure is a multi-level organization of graph communities, where each level provides a partition of the nodes","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"HIERARCHICAL PARTITION","type":"CONCEPT","description":"A hierarchical partition is a division of graph nodes into mutually-exclusive, collectively-exhaustive communities at different levels","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"DIVIDE-AND-CONQUER GLOBAL SUMMARIZATION","type":"TECHNIQUE","description":"Divide-and-conquer global summarization is a method that uses hierarchical community structure to enable efficient summarization of large datasets","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"REPORT-LIKE SUMMARIES","type":"TECHNIQUE","description":"Report-like summaries are detailed summaries of each community in a graph, useful for understanding the global structure and semantics of the dataset","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"ROOT COMMUNITIES","type":"CONCEPT","description":"Root communities are the top-level communities in a hierarchical community structure, corresponding to the partition with maximum modularity","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"SUB-COMMUNITIES","type":"CONCEPT","description":"Sub-communities are lower-level communities within root communities, revealing internal structure","source_id":"e66ed885a08f92cc69f4895302c33047"},{"name":"BROWN ET AL.","type":"","description":"","source_id":"e66ed885a08f92cc69f4895302c33047"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM-Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM stands for Large Language Model, a type of artificial intelligence model used for natural language processing tasks<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown is an author who contributed to the research on in-context learning for LLMs in 2020<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"FEW-SHOT EXAMPLES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Few-shot examples are specialized examples used to train or prompt LLMs in domains with specialized knowledge like science, medicine, and law<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CLAIMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Claims are statements linked to detected entities, including attributes like subject, object, type, description, source text span, and start and end dates<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LOGIT BIAS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Logit bias is a technique used to force a yes\/no decision in LLMs during the entity extraction process<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ELEMENT INSTANCES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Element instances are descriptions of entities, relationships, and claims extracted from source texts by LLMs<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ELEMENT SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Element summaries are single blocks of descriptive text for each graph element, created by further summarizing instance-level summaries<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GRAPH COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Graph communities are groups of nodes in a graph that have stronger connections to one another than to other nodes, detected using community detection algorithms<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LEIDEN\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Leiden is a community detection algorithm used to recover hierarchical community structure of large-scale graphs efficiently<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"FORTUNATO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fortunato is an author who has conducted surveys on community detection algorithms<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jin is an author who has conducted surveys on community detection algorithms<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"TRAAG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Traag is an author who contributed to the development of the Leiden algorithm in 2019<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MultiHop-RAG is a dataset used for indexing and graph community detection<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tang is an author who contributed to the MultiHop-RAG dataset in 2024<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who contributed to the MultiHop-RAG dataset in 2024<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"OPENORD\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">OpenORD is a node layout algorithm used for visualizing graph communities<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"FORCE ATLAS 2\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Force Atlas 2 is a node layout algorithm used for visualizing graph communities<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin is an author who contributed to the development of the OpenORD algorithm in 2011<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"JACOMY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacomy is an author who contributed to the development of the Force Atlas 2 algorithm in 2014<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Leaf-level communities are the most granular level of graph communities, prioritized and iteratively added to the LLM context window until the token limit is reached<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"NODE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A node represents an entity in a graph, with attributes like degree indicating its prominence<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EDGE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">An edge represents a relationship between two nodes in a graph, with weights indicating the strength of the relationship<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"COVARIATE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A covariate is an additional variable associated with nodes and edges in a graph, used for more detailed analysis<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"EXTRACTION PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">An extraction prompt is used to extract specific types of information, such as named entities or covariates, from text<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"NAMED ENTITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Named entities are specific types of entities like people, places, and organizations that are extracted from text<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"COVARIATE PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A covariate prompt is used to extract additional variables associated with detected entities, such as claims<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GLEANINGS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Gleanings are multiple rounds of extraction used to detect any additional entities that may have been missed in prior rounds<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"LOGIT BIAS OF 100\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A logit bias of 100 is used to force a yes\/no decision in LLMs during the entity extraction process<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"YES\/NO DECISION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A yes\/no decision is a binary choice used to determine if all entities were extracted<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CONTINUATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">A continuation is a follow-up prompt used to encourage the LLM to glean missing entities<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CHUNK SIZES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chunk sizes refer to the amount of text processed in each round of extraction to balance efficiency and quality<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"NOISE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Noise refers to irrelevant or extraneous information that can be introduced during the extraction process<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Abstractive summarization is a method where the LLM creates independently meaningful summaries of concepts implied but not stated by the text<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"INSTANCE-LEVEL SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Instance-level summaries are initial summaries of entities, relationships, and claims extracted from text<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"DUPLICATE ENTITY ELEMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Duplicate entity elements are multiple references to the same entity in different formats, which can result in duplicate nodes in the entity graph<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ENTITY GRAPH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">An entity graph is a graph structure where nodes represent entities and edges represent relationships between them<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"CLOSELY-RELATED COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Closely-related communities are groups of entities with strong connections to each other, detected and summarized to handle variations in entity names<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"GLOBAL, QUERY-FOCUSED SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Global, query-focused summarization is a method that uses rich descriptive text for nodes in a graph to answer global queries<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPHS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Knowledge graphs are graph structures that rely on concise and consistent knowledge triples for reasoning tasks<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Community detection algorithms are used to partition a graph into communities of nodes with stronger connections to each other<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"HOMOGENEOUS UNDIRECTED WEIGHTED GRAPH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A homogeneous undirected weighted graph is a type of graph where nodes are connected by edges with weights representing the strength of relationships<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"EDGE WEIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Edge weights are values that represent the normalized counts of detected relationship instances between nodes in a graph<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"HIERARCHICAL COMMUNITY STRUCTURE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hierarchical community structure is a multi-level organization of graph communities, where each level provides a partition of the nodes<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"HIERARCHICAL PARTITION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A hierarchical partition is a division of graph nodes into mutually-exclusive, collectively-exhaustive communities at different levels<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"DIVIDE-AND-CONQUER GLOBAL SUMMARIZATION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Divide-and-conquer global summarization is a method that uses hierarchical community structure to enable efficient summarization of large datasets<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"REPORT-LIKE SUMMARIES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Report-like summaries are detailed summaries of each community in a graph, useful for understanding the global structure and semantics of the dataset<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"ROOT COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Root communities are the top-level communities in a hierarchical community structure, corresponding to the partition with maximum modularity<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"SUB-COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Sub-communities are lower-level communities within root communities, revealing internal structure<\/data>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <node id=\"BROWN ET AL.\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/node>    <edge source=\"LLM-DEBATE\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both LLM-Debate and Quality-Diversity are systems designed to leverage diverse perspectives to find better answers<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"META AGENT SEARCH\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Meta Agent Search uses LLM-Debate as one of the initial seeds in the archive<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Quality-Diversity is a simplified version of Intelligent Go-Explore<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"META AGENT SEARCH\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Meta Agent Search uses Quality-Diversity as one of the initial seeds in the archive<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"INTELLIGENT GO-EXPLORE\" target=\"META AGENT SEARCH\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Meta Agent Search uses Intelligent Go-Explore as one of the initial seeds in the archive<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"LEHMAN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Lehman have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"STANLEY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"WANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"STANLEY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Lehman and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"WANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Lehman and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"WANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Stanley and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"BROWN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown contributed to the research on in-context learning for LLMs in 2020<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"FEW-SHOT EXAMPLES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Few-shot examples are used to train or prompt LLMs in specialized domains<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CLAIMS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Claims are extracted by LLMs and linked to detected entities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"LOGIT BIAS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Logit bias is used to force a yes\/no decision in LLMs during the entity extraction process<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ELEMENT INSTANCES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Element instances are descriptions of entities, relationships, and claims extracted from source texts by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"BROWN ET AL.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown et al. contributed to the research on in-context learning for LLMs in 2020<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"EXTRACTION PROMPT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">An extraction prompt is used to extract specific types of information from text using LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"NAMED ENTITIES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Named entities are extracted from text by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"COVARIATE PROMPT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">A covariate prompt is used to extract additional variables associated with detected entities by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"GLEANINGS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Gleanings are multiple rounds of extraction used by LLMs to detect any additional entities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"LOGIT BIAS OF 100\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">A logit bias of 100 is used to force a yes\/no decision in LLMs during the entity extraction process<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"YES\/NO DECISION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">A yes\/no decision is used to determine if all entities were extracted by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CONTINUATION\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">A continuation is used to encourage LLMs to glean missing entities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CHUNK SIZES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Chunk sizes refer to the amount of text processed by LLMs in each round of extraction<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"NOISE\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Noise can be introduced during the extraction process by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Abstractive summarization is a method used by LLMs to create meaningful summaries of concepts<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"INSTANCE-LEVEL SUMMARIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Instance-level summaries are initial summaries created by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"DUPLICATE ENTITY ELEMENTS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Duplicate entity elements can result from inconsistent extraction by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ENTITY GRAPH\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">An entity graph is created by LLMs to represent entities and their relationships<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CLOSELY-RELATED COMMUNITIES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Closely-related communities are detected and summarized by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"GLOBAL, QUERY-FOCUSED SUMMARIZATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Global, query-focused summarization is a method used by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"KNOWLEDGE GRAPHS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Knowledge graphs are created and used by LLMs for reasoning tasks<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"COMMUNITY DETECTION ALGORITHMS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Community detection algorithms are used by LLMs to partition graphs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"HOMOGENEOUS UNDIRECTED WEIGHTED GRAPH\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">A homogeneous undirected weighted graph is used by LLMs to represent entities and relationships<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"EDGE WEIGHTS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Edge weights are used by LLMs to represent the strength of relationships<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"HIERARCHICAL COMMUNITY STRUCTURE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Hierarchical community structure is detected by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"HIERARCHICAL PARTITION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">A hierarchical partition is created by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"DIVIDE-AND-CONQUER GLOBAL SUMMARIZATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Divide-and-conquer global summarization is a method used by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"REPORT-LIKE SUMMARIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Report-like summaries are created by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"ROOT COMMUNITIES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Root communities are detected by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"SUB-COMMUNITIES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Sub-communities are detected by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"OPENORD\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">OpenORD is used by LLMs for visualizing graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"FORCE ATLAS 2\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Force Atlas 2 is used by LLMs for visualizing graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Leaf-level communities are detected and summarized by LLMs<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"NODE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Nodes are used by LLMs to represent entities in a graph<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"EDGE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Edges are used by LLMs to represent relationships between nodes<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LLM\" target=\"COVARIATE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Covariates are used by LLMs for more detailed analysis<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ELEMENT INSTANCES\" target=\"ELEMENT SUMMARIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Element summaries are created by further summarizing instance-level summaries<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"ELEMENT SUMMARIES\" target=\"GRAPH COMMUNITIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Graph communities are created by grouping element summaries<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"LEIDEN\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Leiden is used to detect graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"FORTUNATO\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Fortunato has conducted surveys on community detection algorithms<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"JIN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Jin has conducted surveys on community detection algorithms<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"MULTIHOP-RAG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">MultiHop-RAG is a dataset used for indexing and graph community detection<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"OPENORD\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">OpenORD is used for visualizing graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"FORCE ATLAS 2\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Force Atlas 2 is used for visualizing graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Leaf-level communities are the most granular level of graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"NODE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Nodes represent entities in graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"EDGE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Edges represent relationships between nodes in graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"GRAPH COMMUNITIES\" target=\"COVARIATE\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Covariates are additional variables associated with nodes and edges in graph communities<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"LEIDEN\" target=\"TRAAG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Traag contributed to the development of the Leiden algorithm in 2019<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"MULTIHOP-RAG\" target=\"TANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Tang contributed to the MultiHop-RAG dataset in 2024<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"MULTIHOP-RAG\" target=\"YANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yang contributed to the MultiHop-RAG dataset in 2024<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"OPENORD\" target=\"MARTIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Martin contributed to the development of the OpenORD algorithm in 2011<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>    <edge source=\"FORCE ATLAS 2\" target=\"JACOMY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jacomy contributed to the development of the Force Atlas 2 algorithm in 2014<\/data>      <data key=\"d6\">e66ed885a08f92cc69f4895302c33047<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4930fce6da868f894757a9da465807ba","chunk":" which reveals internal structure within these root-level communities.\n\u2022Leaf-level communities . The element summaries of a leaf-level community (nodes, edges,\ncovariates) are prioritized and then iteratively added to the LLM context window until\nthe token limit is reached. The prioritization is as follows: for each community edge in\ndecreasing order of combined source and target node degree (i.e., overall prominance), add\ndescriptions of the source node, target node, linked covariates, and the edge itself.\n\u2022Higher-level communities . If all element summaries fit within the token limit of the con-\ntext window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until fit within the context window is achieved.\n2.6 Community Summaries \u2192Community Answers \u2192Global Answer\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 3).\nFor a given community level, the global answer to any user query is generated as follows:\n\u2022Prepare community summaries . Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n\u2022Map community answers . Generate intermediate answers in parallel, one for each chunk.\nThe LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\nerated answer is in answering the target question. Answers with score 0 are filtered out.\n\u2022Reduce to global answer . Intermediate community answers are sorted in descending order\nof helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n5Dataset Example activity framing and generation of global sensemaking questions\nPodcast\ntranscriptsUser : A tech journalist looking for insights and trends in the tech industry\nTask: Understanding how tech leaders view the role of policy and regulation\nQuestions :\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\nNews\narticlesUser : Educator incorporating current affairs into curricula\nTask: Teaching about health and wellness\nQuestions :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n3 Evaluation\n3.1 Datasets\nWe selected two datasets in the one million token range, each equivalent to about 10 novels of text\nand representative of the kind of corpora that users may encounter in their real world activities:\n\u2022Podcast transcripts . Compiled transcripts of podcast conversations between Kevin Scott,\nMicrosoft CTO, and other technology leaders (Behind the Tech, Scott, 2024). Size: 1669\n\u00d7600-token text chunks, with 100-token overlaps between chunks ( \u223c1 million tokens).\n\u2022News articles . Benchmark dataset comprising news articles published from September\n2013 to December 2023 in a range of categories, including entertainment, business, sports,\ntechnology, health, and science (MultiHop-RAG; Tang and Yang, 2024). Size: 3197 \u00d7\n600-token text chunks, with 100-token overlaps between chunks ( \u223c1.7 million tokens).\n3.2 Queries\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthe associated question sets target explicit fact retrieval rather than summarization for the purpose\nof data sensemaking, i.e., the process though which people inspect, engage with, and contextualize\ndata within the broader scope of real-world activities (Koesten et al., 2021). Similarly, methods for\nextracting latent summarization queries from source texts also exist (Xu and Lapata, 2021), but such\nextracted questions can target details that betray prior knowledge of the texts.\nTo evaluate the effectiveness of RAG systems for more global sensemaking tasks, we need questions\nthat convey only a high-level understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper","chunk_id":"4930fce6da868f894757a9da465807ba","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"LEAF-LEVEL COMMUNITIES","type":"CONCEPT","description":"Leaf-level communities are the smallest units within a hierarchical community structure, where element summaries of nodes, edges, and covariates are prioritized and added to the LLM context window until the token limit is reached","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"HIGHER-LEVEL COMMUNITIES","type":"CONCEPT","description":"Higher-level communities are larger units within a hierarchical community structure, where element summaries are summarized and ranked to fit within the context window if they exceed the token limit","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COMMUNITY SUMMARIES","type":"CONCEPT","description":"Community summaries are generated descriptions of elements within a community, used to answer user queries in a multi-stage process","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"GLOBAL ANSWER","type":"CONCEPT","description":"The global answer is the final response generated from community summaries to answer a user query, ensuring a balance of summary detail and scope","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"PODCAST TRANSCRIPTS","type":"DATASET","description":"Podcast transcripts are compiled records of podcast conversations, used as a dataset for evaluating sensemaking questions","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"NEWS ARTICLES","type":"DATASET","description":"News articles are a benchmark dataset comprising various categories of news published over a decade, used for evaluating sensemaking questions","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"KEVIN SCOTT","type":"PERSON","description":"Kevin Scott is the CTO of Microsoft and a participant in the podcast conversations compiled in the podcast transcripts dataset","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"MULTIHOP-RAG","type":"DATASET","description":"MultiHop-RAG is a benchmark dataset of news articles used for evaluating sensemaking questions, published by Tang and Yang in 2024","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a benchmark dataset for open-domain question answering, targeting explicit fact retrieval","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"MT-BENCH","type":"DATASET","description":"MT-Bench is a benchmark dataset for open-domain question answering, targeting explicit fact retrieval","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"KOESTEN","type":"PERSON","description":"Koesten is an author who has worked on the process of data sensemaking","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"XU","type":"PERSON","description":"Xu is an author who has worked on methods for extracting latent summarization queries from source texts","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"LAPATA","type":"PERSON","description":"Lapata is an author who has worked on methods for extracting latent summarization queries from source texts","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"TANG","type":"PERSON","description":"Tang is an author who has worked on the MultiHop-RAG dataset","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"YANG","type":"PERSON","description":"Yang is an author who has worked on the MultiHop-RAG dataset and HotPotQA","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"DATA SENSEMAKING","type":"","description":"\nData sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities","source_id":"4930fce6da868f894757a9da465807ba","entity_type":"CONCEPT"},{"name":"NODES","type":"CONCEPT","description":"Nodes are elements within a community structure that are described and prioritized in the context window based on their degree of prominence","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"EDGES","type":"CONCEPT","description":"Edges are connections between nodes within a community structure, described and prioritized in the context window based on their degree of prominence","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COVARIATES","type":"CONCEPT","description":"Covariates are linked attributes or variables associated with nodes and edges within a community structure, described and prioritized in the context window","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COMMUNITY ANSWERS","type":"CONCEPT","description":"Community answers are intermediate responses generated from community summaries, scored for helpfulness, and used to form the global answer","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"USER","type":"CONCEPT","description":"A user is an individual or entity interacting with the system to generate queries and receive answers based on community summaries","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"TECH JOURNALIST","type":"PERSON","description":"A tech journalist is a user looking for insights and trends in the tech industry, particularly regarding tech policy and regulation","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"EDUCATOR","type":"PERSON","description":"An educator is a user incorporating current affairs into curricula, particularly focusing on health and wellness","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"BEHIND THE TECH","type":"DATASET","description":"Behind the Tech is a podcast series featuring conversations between Kevin Scott and other technology leaders, compiled into the podcast transcripts dataset","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"SCOTT","type":"PERSON","description":"Scott is an author who compiled the podcast transcripts dataset, featuring conversations with technology leaders","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"SENSEMAKING QUESTIONS","type":"CONCEPT","description":"Sensemaking questions are queries generated to evaluate the effectiveness of RAG systems for global understanding of dataset contents","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"RAG SYSTEMS","type":"CONCEPT","description":"RAG systems are retrieval-augmented generation systems used for answering sensemaking questions by summarizing large datasets","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"PRIVACY LAWS","type":"CONCEPT","description":"Privacy laws are regulations discussed by guests in the podcast transcripts, focusing on their impact on technology development","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"INNOVATION","type":"CONCEPT","description":"Innovation is a concept discussed by guests in the podcast transcripts, particularly in relation to ethical considerations and policy","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"ETHICAL CONSIDERATIONS","type":"CONCEPT","description":"Ethical considerations are discussed by guests in the podcast transcripts, particularly in relation to innovation and policy","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"POLICIES","type":"CONCEPT","description":"Policies are regulations and guidelines discussed by guests in the podcast transcripts, with suggestions for changes","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"COLLABORATIONS","type":"CONCEPT","description":"Collaborations between tech companies and governments are discussed by guests in the podcast transcripts","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"HEALTH EDUCATION","type":"CONCEPT","description":"Health education is a topic discussed in news articles, focusing on integrating current health topics into curricula","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"PREVENTIVE MEDICINE","type":"CONCEPT","description":"Preventive medicine is a concept addressed in news articles, focusing on wellness and health education","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"PUBLIC HEALTH","type":"CONCEPT","description":"Public health is a priority discussed in news articles, providing insights for health education","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"HEALTH LITERACY","type":"CONCEPT","description":"Health literacy is a concept highlighted in news articles, emphasizing its importance in health education","source_id":"4930fce6da868f894757a9da465807ba"},{"name":"ROOT-LEVEL COMMUNITIES","type":"","description":"","source_id":"4930fce6da868f894757a9da465807ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LEAF-LEVEL COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Leaf-level communities are the smallest units within a hierarchical community structure, where element summaries of nodes, edges, and covariates are prioritized and added to the LLM context window until the token limit is reached<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"HIGHER-LEVEL COMMUNITIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Higher-level communities are larger units within a hierarchical community structure, where element summaries are summarized and ranked to fit within the context window if they exceed the token limit<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Community summaries are generated descriptions of elements within a community, used to answer user queries in a multi-stage process<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"GLOBAL ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The global answer is the final response generated from community summaries to answer a user query, ensuring a balance of summary detail and scope<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Podcast transcripts are compiled records of podcast conversations, used as a dataset for evaluating sensemaking questions<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">News articles are a benchmark dataset comprising various categories of news published over a decade, used for evaluating sensemaking questions<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"KEVIN SCOTT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin Scott is the CTO of Microsoft and a participant in the podcast conversations compiled in the podcast transcripts dataset<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MultiHop-RAG is a benchmark dataset of news articles used for evaluating sensemaking questions, published by Tang and Yang in 2024<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a benchmark dataset for open-domain question answering, targeting explicit fact retrieval<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MT-Bench is a benchmark dataset for open-domain question answering, targeting explicit fact retrieval<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"KOESTEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Koesten is an author who has worked on the process of data sensemaking<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is an author who has worked on methods for extracting latent summarization queries from source texts<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"LAPATA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lapata is an author who has worked on methods for extracting latent summarization queries from source texts<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tang is an author who has worked on the MultiHop-RAG dataset<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who has worked on the MultiHop-RAG dataset and HotPotQA<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"DATA SENSEMAKING\">      <data key=\"d0\" \/>      <data key=\"d1\">Data sensemaking is the process through which people inspect, engage with, and contextualize data within the broader scope of real-world activities<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"NODES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Nodes are elements within a community structure that are described and prioritized in the context window based on their degree of prominence<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"EDGES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Edges are connections between nodes within a community structure, described and prioritized in the context window based on their degree of prominence<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COVARIATES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Covariates are linked attributes or variables associated with nodes and edges within a community structure, described and prioritized in the context window<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COMMUNITY ANSWERS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Community answers are intermediate responses generated from community summaries, scored for helpfulness, and used to form the global answer<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A user is an individual or entity interacting with the system to generate queries and receive answers based on community summaries<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"TECH JOURNALIST\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A tech journalist is a user looking for insights and trends in the tech industry, particularly regarding tech policy and regulation<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"EDUCATOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An educator is a user incorporating current affairs into curricula, particularly focusing on health and wellness<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"BEHIND THE TECH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Behind the Tech is a podcast series featuring conversations between Kevin Scott and other technology leaders, compiled into the podcast transcripts dataset<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"SCOTT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Scott is an author who compiled the podcast transcripts dataset, featuring conversations with technology leaders<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"SENSEMAKING QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Sensemaking questions are queries generated to evaluate the effectiveness of RAG systems for global understanding of dataset contents<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"RAG SYSTEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">RAG systems are retrieval-augmented generation systems used for answering sensemaking questions by summarizing large datasets<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"PRIVACY LAWS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Privacy laws are regulations discussed by guests in the podcast transcripts, focusing on their impact on technology development<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"INNOVATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Innovation is a concept discussed by guests in the podcast transcripts, particularly in relation to ethical considerations and policy<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"ETHICAL CONSIDERATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Ethical considerations are discussed by guests in the podcast transcripts, particularly in relation to innovation and policy<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"POLICIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Policies are regulations and guidelines discussed by guests in the podcast transcripts, with suggestions for changes<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"COLLABORATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Collaborations between tech companies and governments are discussed by guests in the podcast transcripts<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"HEALTH EDUCATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Health education is a topic discussed in news articles, focusing on integrating current health topics into curricula<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"PREVENTIVE MEDICINE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Preventive medicine is a concept addressed in news articles, focusing on wellness and health education<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"PUBLIC HEALTH\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Public health is a priority discussed in news articles, providing insights for health education<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"HEALTH LITERACY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Health literacy is a concept highlighted in news articles, emphasizing its importance in health education<\/data>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <node id=\"ROOT-LEVEL COMMUNITIES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4930fce6da868f894757a9da465807ba<\/data>    <\/node>    <edge source=\"LEAF-LEVEL COMMUNITIES\" target=\"HIGHER-LEVEL COMMUNITIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Leaf-level communities and higher-level communities are both part of a hierarchical community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"LEAF-LEVEL COMMUNITIES\" target=\"ROOT-LEVEL COMMUNITIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Root-level communities and leaf-level communities are part of a hierarchical community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HIGHER-LEVEL COMMUNITIES\" target=\"ROOT-LEVEL COMMUNITIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Root-level communities and higher-level communities are part of a hierarchical community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"GLOBAL ANSWER\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Community summaries are used to generate the global answer in a multi-stage process<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"COMMUNITY ANSWERS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Community summaries are used to generate community answers<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"GLOBAL ANSWER\" target=\"COMMUNITY ANSWERS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Community answers are used to generate the global answer<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"PODCAST TRANSCRIPTS\" target=\"KEVIN SCOTT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Kevin Scott is a participant in the podcast conversations compiled in the podcast transcripts dataset<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"PODCAST TRANSCRIPTS\" target=\"BEHIND THE TECH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Behind the Tech is the podcast series compiled into the podcast transcripts dataset<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NEWS ARTICLES\" target=\"MULTIHOP-RAG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">MultiHop-RAG is a benchmark dataset of news articles<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"KEVIN SCOTT\" target=\"SCOTT\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Scott and Kevin Scott are the same person, who compiled the podcast transcripts dataset<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"MT-BENCH\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">HotPotQA and MT-Bench are both benchmark datasets for open-domain question answering<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"KOESTEN\" target=\"DATA SENSEMAKING\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Koesten has worked on the process of data sensemaking<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"XU\" target=\"LAPATA\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Xu and Lapata have both worked on methods for extracting latent summarization queries from source texts<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"TANG\" target=\"YANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Tang and Yang have both worked on the MultiHop-RAG dataset<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"DATA SENSEMAKING\" target=\"SENSEMAKING QUESTIONS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Sensemaking questions are part of the data sensemaking process<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NODES\" target=\"EDGES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Nodes and edges are elements within a community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"NODES\" target=\"COVARIATES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Nodes and covariates are elements within a community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"EDGES\" target=\"COVARIATES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Edges and covariates are elements within a community structure<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"USER\" target=\"TECH JOURNALIST\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">A tech journalist is a type of user<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"USER\" target=\"EDUCATOR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">An educator is a type of user<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"SENSEMAKING QUESTIONS\" target=\"RAG SYSTEMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Sensemaking questions are used to evaluate the effectiveness of RAG systems<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"PRIVACY LAWS\" target=\"INNOVATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Privacy laws and innovation are concepts discussed in relation to each other in the podcast transcripts<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"INNOVATION\" target=\"ETHICAL CONSIDERATIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Innovation and ethical considerations are concepts discussed in relation to each other in the podcast transcripts<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"POLICIES\" target=\"COLLABORATIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Policies and collaborations are concepts discussed in relation to each other in the podcast transcripts<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HEALTH EDUCATION\" target=\"PREVENTIVE MEDICINE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Health education and preventive medicine are concepts discussed in relation to each other in news articles<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HEALTH EDUCATION\" target=\"PUBLIC HEALTH\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Public health and health education are concepts discussed in relation to each other in news articles<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>    <edge source=\"HEALTH EDUCATION\" target=\"HEALTH LITERACY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Health literacy and health education are concepts discussed in relation to each other in news articles<\/data>      <data key=\"d6\">4930fce6da868f894757a9da465807ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"26b2dad01a219bc034ac7d6a32d07582","chunk":" understanding of dataset contents, and not the details of specific texts.\nWe used an activity-centered approach to automate the generation of such questions: given a short\ndescription of a dataset, we asked the LLM to identify Npotential users and Ntasks per user,\nthen for each (user, task) combination, we asked the LLM to generate Nquestions that require\nunderstanding of the entire corpus. For our evaluation, a value of N= 5 resulted in 125 test questions\nper dataset. Table 1 shows example questions for each of the two evaluation datasets.\n63.3 Conditions\nWe compare six different conditions in our analysis, including Graph RAG using four levels of graph\ncommunities ( C0,C1,C2,C3), a text summarization method applying our map-reduce approach\ndirectly to source texts ( TS), and a na \u00a8\u0131ve \u201csemantic search\u201d RAG approach ( SS):\n\u2022CO. Uses root-level community summaries (fewest in number) to answer user queries.\n\u2022C1. Uses high-level community summaries to answer queries. These are sub-communities\nof C0, if present, otherwise C0 communities projected down.\n\u2022C2. Uses intermediate-level community summaries to answer queries. These are sub-\ncommunities of C1, if present, otherwise C1 communities projected down.\n\u2022C3. Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected down.\n\u2022TS. The same method as in subsection 2.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\u2022SS. An implementation of na \u00a8\u0131ve RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\nThe graph index supporting conditions C0-C3was created using our generic prompts for entity and\nrelationship extraction only, with entity types and few-shot examples tailored to the domain of the\ndata. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\nPodcast dataset and 0 gleanings for the News dataset.\n3.4 Metrics\nLLMs have been shown to be good evaluators of natural language generation, achieving state-of-\nthe-art or competitive results compared against human judgements (Wang et al., 2023a; Zheng et al.,\n2024). While this approach can generate reference-based metrics when gold standard answers are\nknown, it is also capable of measuring the qualities of generated texts (e.g., fluency) in a reference-\nfree style (Wang et al., 2023a) as well as in head-to-head comparison of competing outputs (LLM-\nas-a-judge, Zheng et al., 2024). LLMs have also shown promise at evaluating the performance of\nconventional RAG systems, automatically evaluating qualities like context relevance, faithfulness,\nand answer relevance (RAGAS, Es et al., 2023).\nGiven the multi-stage nature of our Graph RAG mechanism, the multiple conditions we wanted to\ncompare, and the lack of gold standard answers to our activity-based sensemaking questions, we\ndecided to adopt a head-to-head comparison approach using an LLM evaluator. We selected three\ntarget metrics capturing qualities that are desirable for sensemaking activities, as well as a control\nmetric (directness) used as a indicator of validity. Since directness is effectively in opposition to\ncomprehensiveness and diversity, we would not expect any method to win across all four metrics.\nOur head-to-head measures computed using an LLM evaluator are as follows:\n\u2022Comprehensiveness . How much detail does the answer provide to cover all aspects and\ndetails of the question?\n\u2022Diversity . How varied and rich is the answer in providing different perspectives and insights\non the question?\n\u2022Empowerment . How well does the answer help the reader understand and make informed\njudgements about the topic?\n\u2022Directness . How specifically and clearly does the answer address the question?\nFor our evaluation, the LLM is provided with the question, target metric, and a pair of answers, and\nasked to assess which answer is better according to the metric, as well as why. It returns the winner\nif one exists, otherwise a tie if they are fundamentally similar and the differences are negligible.\nTo account for the stochasticity of LLMs, we run each comparison five times and use mean scores.\nTable 2 shows an example of LLM-generated assessment.\n7Question Which public figures are repeatedly mentioned across various entertainment articles?\nGraph\nRAGOverview of Prominent Public Figures in Entertainment\nThe entertainment industry is vast and diverse, encompassing film, television, music, sports, and\ndigital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\nActors and Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural","chunk_id":"26b2dad01a219bc034ac7d6a32d07582","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"DATASET","type":"DATA","description":"A dataset is a collection of data, often used for analysis or training machine learning models","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM stands for Large Language Model, a type of artificial intelligence model designed to understand and generate human language","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"USER","type":"PERSON","description":"A user is an individual who interacts with a system or dataset to perform tasks or gain information","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TASK","type":"ACTIVITY","description":"A task is an activity or piece of work that a user performs, often involving interaction with a dataset or system","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"QUESTION","type":"INFORMATION","description":"A question is an inquiry made to gain information or test knowledge, often generated to evaluate understanding of a dataset","source_id":"26b2dad01a219bc034ac7d6a32d07582","entity_type":"INFORMATION"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is a method that uses graph communities to answer user queries, with different levels of community summaries","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TEXT SUMMARIZATION (TS)","type":"TECHNOLOGY","description":"Text Summarization (TS) is a method that applies a map-reduce approach directly to source texts to create summaries","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"SEMANTIC SEARCH (SS)","type":"TECHNOLOGY","description":"Semantic Search (SS) is a naive RAG approach where text chunks are retrieved and added to the context window until a token limit is reached","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"CONTEXT WINDOW","type":"TECHNOLOGY","description":"A context window is a segment of text used by a model to generate answers or perform tasks, with a specified token limit","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"PODCAST DATASET","type":"DATA","description":"The Podcast dataset is a collection of podcast data used for analysis and evaluation in the study","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"NEWS DATASET","type":"DATA","description":"The News dataset is a collection of news articles used for analysis and evaluation in the study","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"Comprehensiveness is a metric that measures how much detail an answer provides to cover all aspects and details of a question","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"DIVERSITY","type":"METRIC","description":"Diversity is a metric that measures how varied and rich an answer is in providing different perspectives and insights on a question","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"EMPOWERMENT","type":"METRIC","description":"Empowerment is a metric that measures how well an answer helps the reader understand and make informed judgments about a topic","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"DIRECTNESS","type":"METRIC","description":"Directness is a metric that measures how specifically and clearly an answer addresses a question","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"LLM EVALUATOR","type":"TECHNOLOGY","description":"An LLM evaluator is a Large Language Model used to assess the quality of generated answers based on specific metrics","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"RAGAS","type":"TECHNOLOGY","description":"RAGAS is a system that automatically evaluates qualities like context relevance, faithfulness, and answer relevance in RAG systems","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has contributed to research on evaluating natural language generation and RAG systems","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ZHENG","type":"PERSON","description":"Zheng is an author who has contributed to research on evaluating natural language generation and RAG systems","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"PUBLIC FIGURE","type":"PERSON","description":"A public figure is an individual who is well-known and often mentioned in various entertainment articles due to their significant contributions and influence","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ENTERTAINMENT INDUSTRY","type":"INDUSTRY","description":"The entertainment industry encompasses film, television, music, sports, and digital media, and includes various public figures who influence cultural narratives and trends","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ACTIVITY-CENTERED APPROACH","type":"TECHNOLOGY","description":"An activity-centered approach is a method used to automate the generation of questions based on a short description of a dataset","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"N","type":"INFORMATION","description":"N is a variable representing the number of potential users, tasks per user, or questions generated per (user, task) combination","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TABLE 1","type":"INFORMATION","description":"Table 1 shows example questions for each of the two evaluation datasets","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"CONDITION","type":"INFORMATION","description":"A condition is a specific setup or method used in the analysis, such as Graph RAG, text summarization, or semantic search","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C0","type":"INFORMATION","description":"C0 is a condition that uses root-level community summaries to answer user queries","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C1","type":"INFORMATION","description":"C1 is a condition that uses high-level community summaries to answer user queries, which are sub-communities of C0","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C2","type":"INFORMATION","description":"C2 is a condition that uses intermediate-level community summaries to answer user queries, which are sub-communities of C1","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"C3","type":"INFORMATION","description":"C3 is a condition that uses low-level community summaries to answer user queries, which are sub-communities of C2","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"SUBSECTION 2.6","type":"INFORMATION","description":"Subsection 2.6 describes the method used for text summarization, where source texts are shuffled and chunked for the map-reduce summarization stages","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"CONTEXT INFORMATION","type":"INFORMATION","description":"Context information refers to the types of data used in the context window for answer generation","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"GRAPH INDEX","type":"TECHNOLOGY","description":"A graph index is a structure created using generic prompts for entity and relationship extraction, tailored to the domain of the data","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"CONTEXT WINDOW SIZE","type":"INFORMATION","description":"Context window size is the number of tokens used in the context window for the graph indexing process","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"GLEANING","type":"TECHNOLOGY","description":"Gleaning is a process used in the graph indexing method to extract relevant information from the dataset","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"METRIC","type":"INFORMATION","description":"A metric is a standard of measurement used to evaluate the quality of generated answers, such as comprehensiveness, diversity, empowerment, and directness","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"HEAD-TO-HEAD COMPARISON","type":"TECHNOLOGY","description":"Head-to-head comparison is an approach where an LLM evaluator assesses pairs of answers based on specific metrics to determine which is better","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"TABLE 2","type":"INFORMATION","description":"Table 2 shows an example of LLM-generated assessment","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ANSWER","type":"INFORMATION","description":"An answer is a response generated to address a question, evaluated based on metrics like comprehensiveness, diversity, empowerment, and directness","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"PROMINENT PUBLIC FIGURES","type":"PERSON","description":"Prominent public figures are key individuals repeatedly mentioned in various entertainment articles due to their significant contributions and influence","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ACTORS AND DIRECTORS","type":"PERSON","description":"Actors and directors are public figures in the entertainment industry known for their work in film and television","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"MUSICIANS AND EXECUTIVES","type":"PERSON","description":"Musicians and executives are public figures in the entertainment industry known for their contributions to music and the business side of the industry","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"ATHLETES AND COACHES","type":"PERSON","description":"Athletes and coaches are public figures in the entertainment industry known for their involvement in sports","source_id":"26b2dad01a219bc034ac7d6a32d07582"},{"name":"INFLUENCERS AND ENTREPRENEURS","type":"PERSON","description":"Influencers and entrepreneurs are public figures in the entertainment industry known for their impact on digital media and business ventures","source_id":"26b2dad01a219bc034ac7d6a32d07582"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A dataset is a collection of data, often used for analysis or training machine learning models<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM stands for Large Language Model, a type of artificial intelligence model designed to understand and generate human language<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A user is an individual who interacts with a system or dataset to perform tasks or gain information<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">A task is an activity or piece of work that a user performs, often involving interaction with a dataset or system<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">A question is an inquiry made to gain information or test knowledge, often generated to evaluate understanding of a dataset<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>      <data key=\"d3\">INFORMATION<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is a method that uses graph communities to answer user queries, with different levels of community summaries<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TEXT SUMMARIZATION (TS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Text Summarization (TS) is a method that applies a map-reduce approach directly to source texts to create summaries<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"SEMANTIC SEARCH (SS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Semantic Search (SS) is a naive RAG approach where text chunks are retrieved and added to the context window until a token limit is reached<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"CONTEXT WINDOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A context window is a segment of text used by a model to generate answers or perform tasks, with a specified token limit<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The Podcast dataset is a collection of podcast data used for analysis and evaluation in the study<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The News dataset is a collection of news articles used for analysis and evaluation in the study<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Comprehensiveness is a metric that measures how much detail an answer provides to cover all aspects and details of a question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Diversity is a metric that measures how varied and rich an answer is in providing different perspectives and insights on a question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Empowerment is a metric that measures how well an answer helps the reader understand and make informed judgments about a topic<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Directness is a metric that measures how specifically and clearly an answer addresses a question<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"LLM EVALUATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An LLM evaluator is a Large Language Model used to assess the quality of generated answers based on specific metrics<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"RAGAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAGAS is a system that automatically evaluates qualities like context relevance, faithfulness, and answer relevance in RAG systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has contributed to research on evaluating natural language generation and RAG systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is an author who has contributed to research on evaluating natural language generation and RAG systems<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"PUBLIC FIGURE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A public figure is an individual who is well-known and often mentioned in various entertainment articles due to their significant contributions and influence<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ENTERTAINMENT INDUSTRY\">      <data key=\"d0\">INDUSTRY<\/data>      <data key=\"d1\">The entertainment industry encompasses film, television, music, sports, and digital media, and includes various public figures who influence cultural narratives and trends<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ACTIVITY-CENTERED APPROACH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An activity-centered approach is a method used to automate the generation of questions based on a short description of a dataset<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"N\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">N is a variable representing the number of potential users, tasks per user, or questions generated per (user, task) combination<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TABLE 1\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">Table 1 shows example questions for each of the two evaluation datasets<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"CONDITION\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">A condition is a specific setup or method used in the analysis, such as Graph RAG, text summarization, or semantic search<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">C0 is a condition that uses root-level community summaries to answer user queries<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C1\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">C1 is a condition that uses high-level community summaries to answer user queries, which are sub-communities of C0<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C2\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">C2 is a condition that uses intermediate-level community summaries to answer user queries, which are sub-communities of C1<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">C3 is a condition that uses low-level community summaries to answer user queries, which are sub-communities of C2<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"SUBSECTION 2.6\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">Subsection 2.6 describes the method used for text summarization, where source texts are shuffled and chunked for the map-reduce summarization stages<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"CONTEXT INFORMATION\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">Context information refers to the types of data used in the context window for answer generation<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"GRAPH INDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A graph index is a structure created using generic prompts for entity and relationship extraction, tailored to the domain of the data<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"CONTEXT WINDOW SIZE\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">Context window size is the number of tokens used in the context window for the graph indexing process<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"GLEANING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gleaning is a process used in the graph indexing method to extract relevant information from the dataset<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"METRIC\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">A metric is a standard of measurement used to evaluate the quality of generated answers, such as comprehensiveness, diversity, empowerment, and directness<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"HEAD-TO-HEAD COMPARISON\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Head-to-head comparison is an approach where an LLM evaluator assesses pairs of answers based on specific metrics to determine which is better<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"TABLE 2\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">Table 2 shows an example of LLM-generated assessment<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">INFORMATION<\/data>      <data key=\"d1\">An answer is a response generated to address a question, evaluated based on metrics like comprehensiveness, diversity, empowerment, and directness<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"PROMINENT PUBLIC FIGURES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prominent public figures are key individuals repeatedly mentioned in various entertainment articles due to their significant contributions and influence<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ACTORS AND DIRECTORS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Actors and directors are public figures in the entertainment industry known for their work in film and television<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"MUSICIANS AND EXECUTIVES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Musicians and executives are public figures in the entertainment industry known for their contributions to music and the business side of the industry<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"ATHLETES AND COACHES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Athletes and coaches are public figures in the entertainment industry known for their involvement in sports<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <node id=\"INFLUENCERS AND ENTREPRENEURS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Influencers and entrepreneurs are public figures in the entertainment industry known for their impact on digital media and business ventures<\/data>      <data key=\"d2\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/node>    <edge source=\"DATASET\" target=\"USER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Users interact with datasets to perform tasks or gain information<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DATASET\" target=\"ACTIVITY-CENTERED APPROACH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The activity-centered approach is used to automate the generation of questions based on a short description of a dataset<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DATASET\" target=\"TABLE 1\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Table 1 shows example questions for each of the two evaluation datasets<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"USER\" target=\"TASK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Users perform tasks that often involve interaction with datasets or systems<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TASK\" target=\"QUESTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Tasks often involve generating questions to evaluate understanding of a dataset<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"N\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">N represents the number of questions generated per (user, task) combination<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"ANSWER\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Questions are inquiries made to gain information, and answers are responses generated to address those questions<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"TEXT SUMMARIZATION (TS)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Graph RAG and Text Summarization (TS) are methods used to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SEMANTIC SEARCH (SS)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Graph RAG and Semantic Search (SS) are methods used to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CONDITION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Graph RAG is one of the conditions compared in the analysis<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION (TS)\" target=\"SEMANTIC SEARCH (SS)\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Text Summarization (TS) and Semantic Search (SS) are methods used to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION (TS)\" target=\"CONDITION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Text Summarization (TS) is one of the conditions compared in the analysis<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION (TS)\" target=\"SUBSECTION 2.6\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Subsection 2.6 describes the method used for text summarization<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"SEMANTIC SEARCH (SS)\" target=\"CONDITION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Semantic Search (SS) is one of the conditions compared in the analysis<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW\" target=\"CONTEXT INFORMATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Context information is used in the context window for answer generation<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW\" target=\"CONTEXT WINDOW SIZE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Context window size is the number of tokens used in the context window<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PODCAST DATASET\" target=\"NEWS DATASET\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both the Podcast dataset and the News dataset are used for analysis and evaluation in the study<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"DIVERSITY\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Comprehensiveness and Diversity are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"EMPOWERMENT\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Comprehensiveness and Empowerment are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"DIRECTNESS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Comprehensiveness and Directness are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"COMPREHENSIVENESS\" target=\"METRIC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Comprehensiveness is one of the metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DIVERSITY\" target=\"EMPOWERMENT\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Diversity and Empowerment are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DIVERSITY\" target=\"DIRECTNESS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Diversity and Directness are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DIVERSITY\" target=\"METRIC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Diversity is one of the metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EMPOWERMENT\" target=\"DIRECTNESS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Empowerment and Directness are metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"EMPOWERMENT\" target=\"METRIC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Empowerment is one of the metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"DIRECTNESS\" target=\"METRIC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Directness is one of the metrics used to evaluate the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM EVALUATOR\" target=\"RAGAS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Both LLM Evaluator and RAGAS are systems used to assess the quality of generated answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM EVALUATOR\" target=\"HEAD-TO-HEAD COMPARISON\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Head-to-head comparison is an approach where an LLM evaluator assesses pairs of answers<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"LLM EVALUATOR\" target=\"TABLE 2\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Table 2 shows an example of LLM-generated assessment<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"WANG\" target=\"ZHENG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Wang and Zheng have both contributed to research on evaluating natural language generation and RAG systems<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURE\" target=\"ENTERTAINMENT INDUSTRY\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Public figures are individuals who are well-known in the entertainment industry due to their significant contributions and influence<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"ENTERTAINMENT INDUSTRY\" target=\"PROMINENT PUBLIC FIGURES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prominent public figures are key individuals in the entertainment industry<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONDITION\" target=\"C0\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">C0 is a condition that uses root-level community summaries to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONDITION\" target=\"C1\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">C1 is a condition that uses high-level community summaries to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONDITION\" target=\"C2\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">C2 is a condition that uses intermediate-level community summaries to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONDITION\" target=\"C3\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">C3 is a condition that uses low-level community summaries to answer user queries<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"CONDITION\" target=\"GRAPH INDEX\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The graph index supports conditions C0-C3<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"GRAPH INDEX\" target=\"GLEANING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Gleaning is a process used in the graph indexing method<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PROMINENT PUBLIC FIGURES\" target=\"ACTORS AND DIRECTORS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Actors and directors are types of prominent public figures<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PROMINENT PUBLIC FIGURES\" target=\"MUSICIANS AND EXECUTIVES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Musicians and executives are types of prominent public figures<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PROMINENT PUBLIC FIGURES\" target=\"ATHLETES AND COACHES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Athletes and coaches are types of prominent public figures<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>    <edge source=\"PROMINENT PUBLIC FIGURES\" target=\"INFLUENCERS AND ENTREPRENEURS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Influencers and entrepreneurs are types of prominent public figures<\/data>      <data key=\"d6\">26b2dad01a219bc034ac7d6a32d07582<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c8e8019de153e439d6a79dcf209b943b","chunk":" Directors [...]Public Figures in Controversy [...]Musicians and Executives [...]\nAthletes and Coaches [...]Influencers and Entrepreneurs [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\nNa\u00a8\u0131ve\nRAGPublic figures who are repeatedly mentioned across various entertainment articles include Taylor\nSwift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\nTaylor Swift [...]Travis Kelce [...]Britney Spears [...]Justin Timberlake [...]\nThese figures are frequently covered due to their high-profile status and the public\u2019s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\nLLM\nDecisionComprehensiveness: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more comprehensive and detailed list of public figures from a\nwider range of entertainment sectors, including film, television, music, sports, gaming, and digital\nmedia. It also includes specific examples of their contributions and the impact they have on their\nrespective fields, as well as mentions of controversies and their implications. Answer 2, while\ndetailed in its coverage of a few individuals, is limited to a smaller number of public figures and\nfocuses primarily on their personal lives and relationships rather than a broad spectrum of their\nprofessional influence across the entertainment industry.\nDiversity: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a more varied and rich response by covering a wide range of\npublic figures from different sectors of the entertainment industry, including film, television, music,\nsports, gaming, and digital media. It offers insights into the contributions and influence of these\nfigures, as well as controversies and their impact on public discourse. The answer also cites specific\ndata sources for each mentioned figure, indicating a diverse range of evidence to support the claims.\nIn contrast, Answer 2 focuses on a smaller group of public figures, primarily from the music industry\nand sports, and relies heavily on a single source for data, which makes it less diverse in perspectives\nand insights.\nEmpowerment: Winner=1 (Graph RAG)\nAnswer 1 is better because it provides a comprehensive and structured overview of public figures\nacross various sectors of the entertainment industry, including film, television, music, sports, and\ndigital media. It lists multiple individuals, providing specific examples of their contributions and the\ncontext in which they are mentioned in entertainment articles, along with references to data reports\nfor each claim. This approach helps the reader understand the breadth of the topic and make informed\njudgments without being misled. In contrast, Answer 2 focuses on a smaller group of public figures\nand primarily discusses their personal lives and relationships, which may not provide as broad an\nunderstanding of the topic. While Answer 2 also cites sources, it does not match the depth and variety\nof Answer 1.\nDirectness: Winner=2 (Na \u00a8\u0131ve RAG)\nAnswer 2 is better because it directly lists specific public figures who are repeatedly mentioned\nacross various entertainment articles, such as Taylor Swift, Travis Kelce, Britney Spears, and Justin\nTimberlake, and provides concise explanations for their frequent mentions. Answer 1, while\ncomprehensive, includes a lot of detailed information about various figures in different sectors of\nentertainment, which, while informative, does not directly answer the question with the same level of\nconciseness and specificity as Answer 2.\nTable 2: Example question for the News article dataset, with generated answers from Graph RAG\n(C2) and Na \u00a8\u0131ve RAG, as well as LLM-generated assessments.\n8Podcast transcripts\n501728252221\n835050484344\n725050535049\n755247505250\n785750485052\n795651504850SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness501823251919\n825050504346\n775050504644\n755050504445\n815754565048\n815456555250SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504257524951\n585059555251\n434150494748\n484551504950\n514853515051\n494952504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505665606060\n445055525152\n354550474848\n404853505050\n404952505050\n404852505050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nNews articles\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC","chunk_id":"c8e8019de153e439d6a79dcf209b943b","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"PUBLIC FIGURES","type":"GROUP","description":"Public figures are individuals who are well-known in society and often covered in media, including those involved in controversies","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"MUSICIANS","type":"GROUP","description":"Musicians are individuals who create and perform music","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"EXECUTIVES","type":"GROUP","description":"Executives are individuals who hold high-level management positions in organizations, often within the entertainment industry","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ATHLETES","type":"GROUP","description":"Athletes are individuals who compete in sports","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"COACHES","type":"GROUP","description":"Coaches are individuals who train and guide athletes or sports teams","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"INFLUENCERS","type":"GROUP","description":"Influencers are individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ENTREPRENEURS","type":"GROUP","description":"Entrepreneurs are individuals who create and manage businesses, often within the entertainment and digital media sectors","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"TAYLOR SWIFT","type":"PERSON","description":"Taylor Swift is a musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"TRAVIS KELCE","type":"PERSON","description":"Travis Kelce is an athlete and public figure frequently mentioned in entertainment articles for his professional achievements and personal life","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"BRITNEY SPEARS","type":"PERSON","description":"Britney Spears is a musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"JUSTIN TIMBERLAKE","type":"PERSON","description":"Justin Timberlake is a musician and public figure frequently mentioned in entertainment articles for his professional achievements and personal life","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ENTERTAINMENT ARTICLES","type":"DOCUMENT","description":"Entertainment articles are written pieces that cover various aspects of the entertainment industry, including the activities and lives of public figures","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"MEDIA COVERAGE","type":"ACTIVITY","description":"Media coverage refers to the reporting and discussion of events, activities, and individuals in the media","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PUBLIC INTEREST","type":"CONCEPT","description":"Public interest refers to the level of attention and concern that the general public has towards certain topics, events, or individuals","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"CULTURAL NARRATIVES","type":"CONCEPT","description":"Cultural narratives are the stories and ideas that shape and reflect the values, beliefs, and experiences of a culture","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"DIGITAL MEDIA","type":"MEDIUM","description":"Digital media refers to content that is created, distributed, and consumed through digital platforms, including social media, websites, and streaming services","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"SOCIAL DISCUSSIONS","type":"ACTIVITY","description":"Social discussions are conversations and debates that occur within society, often influenced by media coverage and public figures","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PUBLIC DISCOURSE","type":"ACTIVITY","description":"Public discourse refers to the exchange of ideas and opinions in the public sphere, often involving media, public figures, and the general public","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ENTERTAINMENT","type":"CONCEPT","description":"Entertainment encompasses various forms of media and activities designed to amuse or engage an audience, including film, television, music, sports, and digital media","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"FILM","type":"MEDIUM","description":"Film is a medium of entertainment that involves the production and screening of movies","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"TELEVISION","type":"MEDIUM","description":"Television is a medium of entertainment that involves the broadcasting of programs, series, and news","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"MUSIC","type":"MEDIUM","description":"Music is a medium of entertainment that involves the creation and performance of songs and instrumental pieces","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"SPORTS","type":"MEDIUM","description":"Sports are competitive physical activities that entertain and engage audiences","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"TREND","type":"CONCEPT","description":"A trend is a general direction in which something is developing or changing, often influenced by public figures and media","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"CULTURAL LANDSCAPE","type":"CONCEPT","description":"The cultural landscape refers to the cultural features and social dynamics of a society, shaped by public figures and media","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PROFESSIONAL ACHIEVEMENTS","type":"CONCEPT","description":"Professional achievements refer to the accomplishments and successes individuals attain in their careers","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PERSONAL LIVES","type":"CONCEPT","description":"Personal lives refer to the private aspects of individuals' lives, often covered in media when involving public figures","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"ECONOMIC IMPACTS","type":"CONCEPT","description":"Economic impacts refer to the financial effects that public figures and their activities have on the economy","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"MEDIA","type":"MEDIUM","description":"Media refers to the various channels of communication, including print, digital, and broadcast, that disseminate information to the public","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"DATA SOURCES","type":"CONCEPT","description":"Data sources are the origins of information used to support claims and reports in media coverage","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"PODCAST TRANSCRIPTS","type":"DOCUMENT","description":"Podcast transcripts are written records of spoken content from podcasts, often used as data sources in media coverage","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"NEWS ARTICLES","type":"DOCUMENT","description":"News articles are written pieces that report on current events and topics, including those related to public figures and entertainment","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is a system used to generate comprehensive and detailed lists of public figures and their contributions across various entertainment sectors","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"NA\u00cfVE RAG","type":"TECHNOLOGY","description":"Na\u00efve RAG is a system used to generate lists of public figures, focusing on their personal lives and relationships","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM refers to Large Language Models used to generate assessments and responses in various contexts","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"DECISION","type":"ACTIVITY","description":"Decision refers to the process of choosing between different options or answers, often involving assessments and comparisons","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"COMPREHENSIVENESS","type":"CONCEPT","description":"Comprehensiveness refers to the extent to which information is complete and covers all relevant aspects of a topic","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"DIVERSITY","type":"CONCEPT","description":"Diversity refers to the inclusion of a wide range of different perspectives, sectors, and individuals in media coverage","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"EMPOWERMENT","type":"CONCEPT","description":"Empowerment refers to the process of providing individuals with the information and tools they need to make informed decisions","source_id":"c8e8019de153e439d6a79dcf209b943b"},{"name":"DIRECTNESS","type":"CONCEPT","description":"Directness refers to the clarity and conciseness with which information is presented, directly addressing the topic or question at hand","source_id":"c8e8019de153e439d6a79dcf209b943b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PUBLIC FIGURES\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Public figures are individuals who are well-known in society and often covered in media, including those involved in controversies<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"MUSICIANS\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Musicians are individuals who create and perform music<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"EXECUTIVES\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Executives are individuals who hold high-level management positions in organizations, often within the entertainment industry<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ATHLETES\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Athletes are individuals who compete in sports<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"COACHES\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Coaches are individuals who train and guide athletes or sports teams<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"INFLUENCERS\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Influencers are individuals who have the power to affect the purchasing decisions of others because of their authority, knowledge, position, or relationship with their audience<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ENTREPRENEURS\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Entrepreneurs are individuals who create and manage businesses, often within the entertainment and digital media sectors<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"TAYLOR SWIFT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Taylor Swift is a musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"TRAVIS KELCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Travis Kelce is an athlete and public figure frequently mentioned in entertainment articles for his professional achievements and personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"BRITNEY SPEARS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Britney Spears is a musician and public figure frequently mentioned in entertainment articles for her professional achievements and personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"JUSTIN TIMBERLAKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Justin Timberlake is a musician and public figure frequently mentioned in entertainment articles for his professional achievements and personal life<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ENTERTAINMENT ARTICLES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Entertainment articles are written pieces that cover various aspects of the entertainment industry, including the activities and lives of public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"MEDIA COVERAGE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Media coverage refers to the reporting and discussion of events, activities, and individuals in the media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PUBLIC INTEREST\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Public interest refers to the level of attention and concern that the general public has towards certain topics, events, or individuals<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"CULTURAL NARRATIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Cultural narratives are the stories and ideas that shape and reflect the values, beliefs, and experiences of a culture<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"DIGITAL MEDIA\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Digital media refers to content that is created, distributed, and consumed through digital platforms, including social media, websites, and streaming services<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"SOCIAL DISCUSSIONS\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Social discussions are conversations and debates that occur within society, often influenced by media coverage and public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PUBLIC DISCOURSE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Public discourse refers to the exchange of ideas and opinions in the public sphere, often involving media, public figures, and the general public<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ENTERTAINMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Entertainment encompasses various forms of media and activities designed to amuse or engage an audience, including film, television, music, sports, and digital media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"FILM\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Film is a medium of entertainment that involves the production and screening of movies<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"TELEVISION\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Television is a medium of entertainment that involves the broadcasting of programs, series, and news<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"MUSIC\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Music is a medium of entertainment that involves the creation and performance of songs and instrumental pieces<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"SPORTS\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Sports are competitive physical activities that entertain and engage audiences<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"TREND\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A trend is a general direction in which something is developing or changing, often influenced by public figures and media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"CULTURAL LANDSCAPE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The cultural landscape refers to the cultural features and social dynamics of a society, shaped by public figures and media<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Professional achievements refer to the accomplishments and successes individuals attain in their careers<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PERSONAL LIVES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Personal lives refer to the private aspects of individuals' lives, often covered in media when involving public figures<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"ECONOMIC IMPACTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Economic impacts refer to the financial effects that public figures and their activities have on the economy<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"MEDIA\">      <data key=\"d0\">MEDIUM<\/data>      <data key=\"d1\">Media refers to the various channels of communication, including print, digital, and broadcast, that disseminate information to the public<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"DATA SOURCES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data sources are the origins of information used to support claims and reports in media coverage<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"PODCAST TRANSCRIPTS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Podcast transcripts are written records of spoken content from podcasts, often used as data sources in media coverage<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"NEWS ARTICLES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">News articles are written pieces that report on current events and topics, including those related to public figures and entertainment<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is a system used to generate comprehensive and detailed lists of public figures and their contributions across various entertainment sectors<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Na&#239;ve RAG is a system used to generate lists of public figures, focusing on their personal lives and relationships<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM refers to Large Language Models used to generate assessments and responses in various contexts<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"DECISION\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Decision refers to the process of choosing between different options or answers, often involving assessments and comparisons<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Comprehensiveness refers to the extent to which information is complete and covers all relevant aspects of a topic<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Diversity refers to the inclusion of a wide range of different perspectives, sectors, and individuals in media coverage<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Empowerment refers to the process of providing individuals with the information and tools they need to make informed decisions<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Directness refers to the clarity and conciseness with which information is presented, directly addressing the topic or question at hand<\/data>      <data key=\"d2\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/node>    <edge source=\"PUBLIC FIGURES\" target=\"MEDIA COVERAGE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Public figures are often the subject of media coverage<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"PUBLIC INTEREST\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Public figures attract public interest<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"CULTURAL NARRATIVES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Public figures help shape cultural narratives<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"SOCIAL DISCUSSIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Public figures often become central figures in social discussions<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"PUBLIC DISCOURSE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Public figures influence public discourse<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"DIGITAL MEDIA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Public figures use digital media to reach their audience<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ENTERTAINMENT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Public figures play significant roles in various aspects of entertainment<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"ECONOMIC IMPACTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Public figures have significant economic impacts<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"GRAPH RAG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Graph RAG generates comprehensive lists of public figures<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"PUBLIC FIGURES\" target=\"NA&#207;VE RAG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Na&#239;ve RAG generates lists of public figures<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TAYLOR SWIFT\" target=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Taylor Swift is known for her professional achievements in music<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TAYLOR SWIFT\" target=\"PERSONAL LIVES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Taylor Swift's personal life is frequently covered in media<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TRAVIS KELCE\" target=\"ENTERTAINMENT ARTICLES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Travis Kelce is frequently mentioned in entertainment articles<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TRAVIS KELCE\" target=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Travis Kelce is known for his professional achievements in sports<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"TRAVIS KELCE\" target=\"PERSONAL LIVES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Travis Kelce's personal life is frequently covered in media<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"BRITNEY SPEARS\" target=\"ENTERTAINMENT ARTICLES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Britney Spears is frequently mentioned in entertainment articles<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"BRITNEY SPEARS\" target=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Britney Spears is known for her professional achievements in music<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"BRITNEY SPEARS\" target=\"PERSONAL LIVES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Britney Spears's personal life is frequently covered in media<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"JUSTIN TIMBERLAKE\" target=\"ENTERTAINMENT ARTICLES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Justin Timberlake is frequently mentioned in entertainment articles<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"JUSTIN TIMBERLAKE\" target=\"PROFESSIONAL ACHIEVEMENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Justin Timberlake is known for his professional achievements in music<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"JUSTIN TIMBERLAKE\" target=\"PERSONAL LIVES\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Justin Timberlake's personal life is frequently covered in media<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"ENTERTAINMENT ARTICLES\" target=\"MEDIA COVERAGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Entertainment articles are a form of media coverage<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"ENTERTAINMENT ARTICLES\" target=\"MEDIA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Media includes entertainment articles<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"MEDIA COVERAGE\" target=\"PUBLIC INTEREST\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Media coverage influences public interest<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"MEDIA COVERAGE\" target=\"DIGITAL MEDIA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Digital media is a platform for media coverage<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"CULTURAL NARRATIVES\" target=\"ENTERTAINMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Entertainment helps shape cultural narratives<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"SOCIAL DISCUSSIONS\" target=\"PUBLIC DISCOURSE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Social discussions contribute to public discourse<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"ENTERTAINMENT\" target=\"TREND\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Entertainment drives trends in society<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"ENTERTAINMENT\" target=\"CULTURAL LANDSCAPE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Entertainment influences the broader cultural landscape<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"MEDIA\" target=\"NEWS ARTICLES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Media includes news articles<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"MEDIA\" target=\"PODCAST TRANSCRIPTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Media includes podcast transcripts<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"MEDIA\" target=\"DATA SOURCES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Data sources provide information for media coverage<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"LLM\" target=\"DECISION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LLM is used to generate assessments and decisions<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"DECISION\" target=\"COMPREHENSIVENESS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Decisions involve assessing comprehensiveness<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"DECISION\" target=\"DIVERSITY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Decisions involve assessing diversity<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"DECISION\" target=\"EMPOWERMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Decisions involve assessing empowerment<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>    <edge source=\"DECISION\" target=\"DIRECTNESS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Decisions involve assessing directness<\/data>      <data key=\"d5\">c8e8019de153e439d6a79dcf209b943b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ede7063998065122cf7a7152979c1909","chunk":"\n502028252121\n805044413836\n725650525452\n755948505855\n796246425059\n796448454150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nComprehensiveness503338352931\n675053454440\n624750404141\n655560505050\n715659505051\n696059504950SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDiversity504757495050\n535058505048\n434250424544\n515058505251\n505055485050\n505256495050SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nEmpowerment505459555554\n465055535252\n414550484847\n454752504949\n454852515049\n464853515150SS\nTS\nC0\nC1\nC2\nC3SSTSC0C1C2C3\nDirectness\nFigure 4: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na \u00a8\u0131ve RAG\non comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n3.5 Configuration\nThe effect of context window size on any particular task is unclear, especially for models like\ngpt-4-turbo with a large context size of 128k tokens. Given the potential for information to\nbe \u201clost in the middle\u201d of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to ex-\nplore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\npowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n3.6 Results\nThe indexing process resulted in a graph consisting of 8564 nodes and 20691 edges for the Podcast\ndataset, and a larger graph of 15754 nodes and 19520 edges for the News dataset. Table 3 shows the\nnumber of community summaries at different levels of each graph community hierarchy.\nGlobal approaches vs. na \u00a8\u0131ve RAG . As shown in Figure 4, global approaches consistently out-\nperformed the na \u00a8\u0131ve RAG ( SS) approach in both comprehensiveness and diversity metrics across\ndatasets. Specifically, global approaches achieved comprehensiveness win rates between 72-83%\nfor Podcast transcripts and 72-80% for News articles, while diversity win rates ranged from 75-82%\nand 62-71% respectively. Our use of directness as a validity test also achieved the expected results,\ni.e., that na \u00a8\u0131ve RAG produces the most direct responses across all comparisons.\n9Podcast Transcripts News Articles\nC0 C1 C2 C3 TS C0 C1 C2 C3 TS\nUnits 34 367 969 1310 1669 55 555 1797 2142 3197\nTokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n% Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\nTable 3: Number of context units (community summaries for C0-C3 and text chunks for TS), corre-\nsponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries ( C0) require dramatically fewer tokens per query (9x-43x).\nCommunity summaries vs. source texts. When comparing community summaries to source texts\nusing Graph RAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens.","chunk_id":"ede7063998065122cf7a7152979c1909","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"INTELLIGENT GO-EXPLORE","type":"TECHNOLOGY","description":"Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on open-endedness and AI-GAs","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author who has worked on open-endedness and AI-GAs","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author who has worked on open-endedness and AI-GAs","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is a method that uses a graph-based approach to improve comprehensiveness and diversity in responses","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"NA\u00cfVE RAG","type":"TECHNOLOGY","description":"Na\u00efve RAG is a simpler approach that does not use a graph index and is used as a baseline in comparisons","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"PODCAST DATASET","type":"DATASET","description":"The Podcast dataset is a collection of podcast transcripts used in the study","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"DATASET"},{"name":"NEWS DATASET","type":"DATASET","description":"The News dataset is a collection of news articles used in the study","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"DATASET"},{"name":"CONTEXT WINDOW SIZE","type":"PARAMETER","description":"Context window size refers to the number of tokens considered in a single context window, tested at sizes 8k, 16k, 32k, and 64k","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PARAMETER"},{"name":"COMPREHENSIVENESS","type":"METRIC","description":"Comprehensiveness is a metric used to evaluate the completeness of answers","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"DIVERSITY","type":"METRIC","description":"Diversity is a metric used to evaluate the variety of answers","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"EMPOWERMENT","type":"METRIC","description":"Empowerment is a metric used to evaluate the degree to which answers enable further action or understanding","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"DIRECTNESS","type":"METRIC","description":"Directness is a metric used to evaluate the straightforwardness of answers","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"METRIC"},{"name":"KURATOV","type":"PERSON","description":"Kuratov is an author who has worked on the effects of context window size","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has worked on the effects of context window size","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"PERSON"},{"name":"COMMUNITY SUMMARIES","type":"TECHNOLOGY","description":"Community summaries are summaries generated at different levels of a graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"MAP-REDUCE SUMMARIZATION","type":"TECHNOLOGY","description":"Map-reduce summarization is a resource-intensive approach requiring a high number of context tokens","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"ROOT-LEVEL COMMUNITY SUMMARIES","type":"TECHNOLOGY","description":"Root-level community summaries are summaries generated at the root level of a graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"INTERMEDIATE-LEVEL SUMMARIES","type":"TECHNOLOGY","description":"Intermediate-level summaries are summaries generated at intermediate levels of a graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"LOW-LEVEL COMMUNITY SUMMARIES","type":"TECHNOLOGY","description":"Low-level community summaries are summaries generated at low levels of a graph community hierarchy","source_id":"ede7063998065122cf7a7152979c1909","entity_type":"TECHNOLOGY"},{"name":"LLM-DEBATE","type":"","description":"","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"SS","type":"PARAMETER","description":"SS is a baseline condition used in the study","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"TS","type":"PARAMETER","description":"TS is a global text summarization approach without a graph index","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C0","type":"PARAMETER","description":"C0 is a condition used in the study, representing root-level community summaries","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C1","type":"PARAMETER","description":"C1 is a condition used in the study, representing intermediate-level community summaries","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C2","type":"PARAMETER","description":"C2 is a condition used in the study, representing low-level community summaries","source_id":"ede7063998065122cf7a7152979c1909"},{"name":"C3","type":"PARAMETER","description":"C3 is a condition used in the study, representing the lowest level of community summaries","source_id":"ede7063998065122cf7a7152979c1909"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is a method that uses a graph-based approach to improve comprehensiveness and diversity in responses<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Na&#239;ve RAG is a simpler approach that does not use a graph index and is used as a baseline in comparisons<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"PODCAST DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Podcast dataset is a collection of podcast transcripts used in the study<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"NEWS DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The News dataset is a collection of news articles used in the study<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"CONTEXT WINDOW SIZE\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Context window size refers to the number of tokens considered in a single context window, tested at sizes 8k, 16k, 32k, and 64k<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"COMPREHENSIVENESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Comprehensiveness is a metric used to evaluate the completeness of answers<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIVERSITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Diversity is a metric used to evaluate the variety of answers<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Empowerment is a metric used to evaluate the degree to which answers enable further action or understanding<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"DIRECTNESS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Directness is a metric used to evaluate the straightforwardness of answers<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"KURATOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov is an author who has worked on the effects of context window size<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has worked on the effects of context window size<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Community summaries are summaries generated at different levels of a graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Map-reduce summarization is a resource-intensive approach requiring a high number of context tokens<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ROOT-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Root-level community summaries are summaries generated at the root level of a graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"INTERMEDIATE-LEVEL SUMMARIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Intermediate-level summaries are summaries generated at intermediate levels of a graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Low-level community summaries are summaries generated at low levels of a graph community hierarchy<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"SS\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">SS is a baseline condition used in the study<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"TS\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">TS is a global text summarization approach without a graph index<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C0\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">C0 is a condition used in the study, representing root-level community summaries<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C1\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">C1 is a condition used in the study, representing intermediate-level community summaries<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C2\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">C2 is a condition used in the study, representing low-level community summaries<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <node id=\"C3\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">C3 is a condition used in the study, representing the lowest level of community summaries<\/data>      <data key=\"d2\">ede7063998065122cf7a7152979c1909<\/data>    <\/node>    <edge source=\"QUALITY-DIVERSITY\" target=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Quality-Diversity is a simplified version of Intelligent Go-Explore<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"META AGENT SEARCH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Quality-Diversity as one of the initial seeds in the archive<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"INTELLIGENT GO-EXPLORE\" target=\"META AGENT SEARCH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Intelligent Go-Explore as one of the initial seeds in the archive<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LLM-DEBATE\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses LLM-Debate as one of the initial seeds in the archive<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"LEHMAN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Lehman have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"STANLEY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"STANLEY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lehman and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lehman and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Stanley and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"NA&#207;VE RAG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Graph RAG outperformed Na&#239;ve RAG in comprehensiveness and diversity metrics<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"PODCAST DATASET\" target=\"NEWS DATASET\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Both the Podcast dataset and the News dataset were used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW SIZE\" target=\"COMPREHENSIVENESS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Context window size affects the comprehensiveness of answers<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW SIZE\" target=\"DIVERSITY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Context window size affects the diversity of answers<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"CONTEXT WINDOW SIZE\" target=\"EMPOWERMENT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Context window size affects the empowerment of answers<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"KURATOV\" target=\"LIU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Kuratov and Liu have both worked on the effects of context window size<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"MAP-REDUCE SUMMARIZATION\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Community summaries require fewer context tokens compared to map-reduce summarization<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"ROOT-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Root-level community summaries are a type of community summaries<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"INTERMEDIATE-LEVEL SUMMARIES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Intermediate-level summaries are a type of community summaries<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"COMMUNITY SUMMARIES\" target=\"LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Low-level community summaries are a type of community summaries<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"SS\" target=\"TS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">SS and TS are both baseline conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C0\" target=\"C1\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">C0 and C1 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C0\" target=\"C2\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">C0 and C2 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C0\" target=\"C3\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">C0 and C3 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C1\" target=\"C2\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">C1 and C2 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C1\" target=\"C3\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">C1 and C3 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>    <edge source=\"C2\" target=\"C3\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">C2 and C3 are both conditions used in the study<\/data>      <data key=\"d6\">ede7063998065122cf7a7152979c1909<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"edab4014b8f55e5b25bd7f396314be1f","chunk":" win rates of 57% and 64%, respectively. Diversity win rates were 57% for\nPodcast intermediate-level summaries and 60% for News low-level community summaries. Table 3\nalso illustrates the scalability advantages of Graph RAG compared to source text summarization: for\nlow-level community summaries ( C3), Graph RAG required 26-33% fewer context tokens, while\nfor root-level community summaries ( C0), it required over 97% fewer tokens. For a modest drop in\nperformance compared with other global methods, root-level Graph RAG offers a highly efficient\nmethod for the iterative question answering that characterizes sensemaking activity, while retaining\nadvantages in comprehensiveness (72% win rate) and diversity (62% win rate) over na \u00a8\u0131ve RAG.\nEmpowerment . Empowerment comparisons showed mixed results for both global approaches versus\nna\u00a8\u0131ve RAG ( SS) and Graph RAG approaches versus source text summarization ( TS). Ad-hoc LLM\nuse to analyze LLM reasoning for this measure indicated that the ability to provide specific exam-\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the Graph RAG index.\n4 Related Work\n4.1 RAG Approaches and Systems\nWhen using LLMs, RAG involves first retrieving relevant information from external data sources,\nthen adding this information to the context window of the LLM along with the original query (Ram\net al., 2023). Na \u00a8\u0131ve RAG approaches (Gao et al., 2023) do this by converting documents to text,\nsplitting text into chunks, and embedding these chunks into a vector space in which similar positions\nrepresent similar semantics. Queries are then embedded into the same vector space, with the text\nchunks of the nearest kvectors used as context. More advanced variations exist, but all solve the\nproblem of what to do when an external dataset of interest exceeds the LLM\u2019s context window.\nAdvanced RAG systems include pre-retrieval, retrieval, post-retrieval strategies designed to over-\ncome the drawbacks of Na \u00a8\u0131ve RAG, while Modular RAG systems include patterns for iterative and\ndynamic cycles of interleaved retrieval and generation (Gao et al., 2023). Our implementation of\nGraph RAG incorporates multiple concepts related to other systems. For example, our community\nsummaries are a kind of self-memory (Selfmem, Cheng et al., 2024) for generation-augmented re-\ntrieval (GAR, Mao et al., 2020) that facilitates future generation cycles, while our parallel generation\nof community answers from these summaries is a kind of iterative (Iter-RetGen, Shao et al., 2023)\nor federated (FeB4RAG, Wang et al., 2024) retrieval-generation strategy. Other systems have also\ncombined these concepts for multi-document summarization (CAiRE-COVID, Su et al., 2020) and\nmulti-hop question answering (ITRG, Feng et al., 2023; IR-CoT, Trivedi et al., 2022; DSP, Khattab\net al., 2022). Our use of a hierarchical index and summarization also bears resemblance to further\napproaches, such as generating a hierarchical index of text chunks by clustering the vectors of text\nembeddings (RAPTOR, Sarthi et al., 2024) or generating a \u201ctree of clarifications\u201d to answer mul-\ntiple interpretations of ambiguous questions (Kim et al., 2023). However, none of these iterative or\nhierarchical approaches use the kind of self-generated graph index that enables Graph RAG.\n104.2 Graphs and LLMs\nUse of graphs in connection with LLMs and RAG is a developing research area, with multiple\ndirections already established. These include using LLMs for knowledge graph creation (Tra-\njanoska et al., 2023) and completion (Yao et al., 2023), as well as for the extraction of causal\ngraphs (Ban et al., 2023; Zhang et al., 2024) from source texts. They also include forms of ad-\nvanced RAG (Gao et al., 2023) where the index is a knowledge graph (KAPING, Baek et al., 2023),\nwhere subsets of the graph structure (G-Retriever, He et al., 2024) or derived graph metrics (Graph-\nToolFormer, Zhang, 2023) are the objects of enquiry, where narrative outputs are strongly grounded\nin the facts of retrieved subgraphs (SURGE, Kang et al., 2023), where retrieved event-plot sub-\ngraphs are serialized using narrative templates (FABULA, Ranade and Joshi, 2023), and where the\nsystem supports both creation and traversal of text-relationship graphs for multi-hop question an-\nswering (Wang et al., 2023b). In terms of open-source software, a variety a graph databases are\nsupported by both the LangChain (LangChain, 2024) and LlamaIndex (LlamaIndex, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization","chunk_id":"edab4014b8f55e5b25bd7f396314be1f","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is a system that offers scalability advantages in summarization by requiring fewer context tokens and providing an efficient method for iterative question answering while retaining comprehensiveness and diversity","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"PODCAST INTERMEDIATE-LEVEL SUMMARIES","type":"DOCUMENT","description":"Podcast intermediate-level summaries are summaries of podcast content at an intermediate level of detail, with a diversity win rate of 57%","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NEWS LOW-LEVEL COMMUNITY SUMMARIES","type":"DOCUMENT","description":"News low-level community summaries are summaries of news content at a low level of detail, with a diversity win rate of 60%","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ROOT-LEVEL COMMUNITY SUMMARIES","type":"DOCUMENT","description":"Root-level community summaries are summaries of content at the root level, requiring over 97% fewer tokens compared to other methods","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NA\u00cfVE RAG","type":"TECHNOLOGY","description":"Na\u00efve RAG is a basic retrieval-augmented generation approach that converts documents to text, splits text into chunks, and embeds these chunks into a vector space for context retrieval","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"MODULAR RAG","type":"TECHNOLOGY","description":"Modular RAG is a system that includes patterns for iterative and dynamic cycles of interleaved retrieval and generation to overcome the drawbacks of Na\u00efve RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SELF-MEMORY (SELFMEM)","type":"TECHNOLOGY","description":"Self-memory (Selfmem) is a concept related to generation-augmented retrieval that facilitates future generation cycles","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GENERATION-AUGMENTED RETRIEVAL (GAR)","type":"TECHNOLOGY","description":"Generation-augmented retrieval (GAR) is a system that combines retrieval and generation to enhance the retrieval process","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)","type":"TECHNOLOGY","description":"Iterative retrieval-generation (Iter-RetGen) is a strategy that involves iterative cycles of retrieval and generation","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"FEDERATED RETRIEVAL-GENERATION (FEB4RAG)","type":"TECHNOLOGY","description":"Federated retrieval-generation (FeB4RAG) is a strategy that involves federated cycles of retrieval and generation","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"CAIRE-COVID","type":"TECHNOLOGY","description":"CAiRE-COVID is a system that combines multiple concepts for multi-document summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"ITRG","type":"TECHNOLOGY","description":"ITRG is a system for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"IR-COT","type":"TECHNOLOGY","description":"IR-CoT is a system for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"DSP","type":"TECHNOLOGY","description":"DSP is a system for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RAPTOR","type":"TECHNOLOGY","description":"RAPTOR is a system that generates a hierarchical index of text chunks by clustering the vectors of text embeddings","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TREE OF CLARIFICATIONS","type":"TECHNOLOGY","description":"Tree of clarifications is a system that generates a hierarchical structure to answer multiple interpretations of ambiguous questions","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"TECHNOLOGY"},{"name":"KAPING","type":"TECHNOLOGY","description":"KAPING is an advanced RAG system where the index is a knowledge graph","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"G-RETRIEVER","type":"TECHNOLOGY","description":"G-Retriever is a system where subsets of the graph structure are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"GRAPH-TOOLFORMER","type":"TECHNOLOGY","description":"Graph-ToolFormer is a system where derived graph metrics are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SURGE","type":"TECHNOLOGY","description":"SURGE is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"FABULA","type":"TECHNOLOGY","description":"FABULA is a system where retrieved event-plot subgraphs are serialized using narrative templates","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"LANGCHAIN","type":"SOFTWARE","description":"LangChain is a library that supports a variety of graph databases\nLangChain is an organization that supports a variety of graph databases","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"ORGANIZATION"},{"name":"LLAMAINDEX","type":"SOFTWARE","description":"LlamaIndex is a library that supports a variety of graph databases\nLlamaIndex is an organization that supports a variety of graph databases","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"ORGANIZATION"},{"name":"NEO4J","type":"SOFTWARE","description":"Neo4J is a graph database format supported by NaLLM\nNeo4J is an organization that supports the NaLLM system for creating and reasoning over knowledge graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"ORGANIZATION"},{"name":"NALLM","type":"SOFTWARE","description":"NaLLM is a system that can create and reason over knowledge graphs in Neo4J format","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"NEBULA-GRAPH","type":"SOFTWARE","description":"Nebula-Graph is a graph database format supported by GraphRAG","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"GRAPHRAG","type":"SOFTWARE","description":"GraphRAG is a system that can create and reason over knowledge graphs in Nebula-Graph format","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"SOFTWARE"},{"name":"RAM","type":"PERSON","description":"Ram is an author who has worked on RAG approaches and systems","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"GAO","type":"PERSON","description":"Gao is an author who has worked on Na\u00efve RAG and Modular RAG systems","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"CHENG","type":"PERSON","description":"Cheng is an author who has worked on Self-memory (Selfmem) for generation-augmented retrieval","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"MAO","type":"PERSON","description":"Mao is an author who has worked on generation-augmented retrieval (GAR)","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"SHAO","type":"PERSON","description":"Shao is an author who has worked on iterative retrieval-generation (Iter-RetGen)","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on federated retrieval-generation (FeB4RAG) and multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"SU","type":"PERSON","description":"Su is an author who has worked on CAiRE-COVID for multi-document summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"FENG","type":"PERSON","description":"Feng is an author who has worked on ITRG for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"TRIVEDI","type":"PERSON","description":"Trivedi is an author who has worked on IR-CoT for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"KHATTAB","type":"PERSON","description":"Khattab is an author who has worked on DSP for multi-hop question answering","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"SARTHI","type":"PERSON","description":"Sarthi is an author who has worked on RAPTOR for generating a hierarchical index of text chunks","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"KIM","type":"PERSON","description":"Kim is an author who has worked on the tree of clarifications for answering multiple interpretations of ambiguous questions","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"BAEK","type":"PERSON","description":"Baek is an author who has worked on KAPING where the index is a knowledge graph","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"HE","type":"PERSON","description":"He is an author who has worked on G-Retriever where subsets of the graph structure are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author who has worked on Graph-ToolFormer where derived graph metrics are the objects of enquiry","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"KANG","type":"PERSON","description":"Kang is an author who has worked on SURGE where narrative outputs are grounded in the facts of retrieved subgraphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"RANADE","type":"PERSON","description":"Ranade is an author who has worked on FABULA where retrieved event-plot subgraphs are serialized using narrative templates","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"JOSHI","type":"PERSON","description":"Joshi is an author who has worked on FABULA where retrieved event-plot subgraphs are serialized using narrative templates","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"TRAJANOSKA","type":"PERSON","description":"Trajanoska is an author who has worked on using LLMs for knowledge graph creation","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"YAO","type":"PERSON","description":"Yao is an author who has worked on using LLMs for knowledge graph completion","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"BAN","type":"PERSON","description":"Ban is an author who has worked on the extraction of causal graphs from source texts","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"PERSON"},{"name":"NEBULAGRAPH","type":"ORGANIZATION","description":"NebulaGraph is an organization that supports the GraphRAG system for creating and reasoning over knowledge graphs","source_id":"edab4014b8f55e5b25bd7f396314be1f","entity_type":"ORGANIZATION"},{"name":"TABLE 3","type":"DOCUMENT","description":"Table 3 illustrates the scalability advantages of Graph RAG compared to source text summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SENSEMAKING ACTIVITY","type":"ACTIVITY","description":"Sensemaking activity involves iterative question answering and is characterized by the use of root-level Graph RAG for efficiency","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"EMPOWERMENT","type":"CONCEPT","description":"Empowerment comparisons showed mixed results for both global approaches versus na\u00efve RAG and Graph RAG approaches versus source text summarization","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"AD-HOC LLM USE","type":"ACTIVITY","description":"Ad-hoc LLM use involves analyzing LLM reasoning to provide specific examples, quotes, and citations to help users reach an informed understanding","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TUNING ELEMENT EXTRACTION PROMPTS","type":"ACTIVITY","description":"Tuning element extraction prompts may help retain more details in the Graph RAG index","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RAG","type":"TECHNOLOGY","description":"RAG (Retrieval-Augmented Generation) involves retrieving relevant information from external data sources and adding it to the context window of the LLM along with the original query","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"EXTERNAL DATA SOURCES","type":"DATA","description":"External data sources are used in RAG to retrieve relevant information for the context window of the LLM","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"CONTEXT WINDOW","type":"CONCEPT","description":"The context window is the part of the LLM where relevant information and the original query are added in RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"DOCUMENTS","type":"DATA","description":"Documents are converted to text, split into chunks, and embedded into a vector space in na\u00efve RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TEXT CHUNKS","type":"DATA","description":"Text chunks are the split parts of documents that are embedded into a vector space in na\u00efve RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"VECTOR SPACE","type":"CONCEPT","description":"Vector space is where text chunks are embedded, and similar positions represent similar semantics in na\u00efve RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"QUERIES","type":"DATA","description":"Queries are embedded into the same vector space as text chunks, with the nearest vectors used as context in na\u00efve RAG approaches","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"PRE-RETRIEVAL STRATEGIES","type":"TECHNOLOGY","description":"Pre-retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na\u00efve RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"RETRIEVAL STRATEGIES","type":"TECHNOLOGY","description":"Retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na\u00efve RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"POST-RETRIEVAL STRATEGIES","type":"TECHNOLOGY","description":"Post-retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na\u00efve RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"COMMUNITY SUMMARIES","type":"DOCUMENT","description":"Community summaries are a kind of self-memory for generation-augmented retrieval in Graph RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"COMMUNITY ANSWERS","type":"DOCUMENT","description":"Community answers are generated in parallel from community summaries in Graph RAG","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"HIERARCHICAL INDEX","type":"CONCEPT","description":"Hierarchical index is used in Graph RAG and other systems to organize text chunks by clustering the vectors of text embeddings","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"TEXT EMBEDDINGS","type":"DATA","description":"Text embeddings are the vector representations of text chunks used in hierarchical indexing","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"KNOWLEDGE GRAPH","type":"DATA","description":"Knowledge graph is used as an index in advanced RAG systems like KAPING","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"SUBGRAPHS","type":"DATA","description":"Subgraphs are subsets of the graph structure used in systems like G-Retriever and SURGE","source_id":"edab4014b8f55e5b25bd7f396314be1f"},{"name":"NARRATIVE TEMPLATES","type":"TECHNOLOGY","description":"Narrative templates are used in systems like FABULA to serialize retrieved event-plot subgraphs","source_id":"edab4014b8f55e5b25bd7f396314be1f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is a system that offers scalability advantages in summarization by requiring fewer context tokens and providing an efficient method for iterative question answering while retaining comprehensiveness and diversity<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"PODCAST INTERMEDIATE-LEVEL SUMMARIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Podcast intermediate-level summaries are summaries of podcast content at an intermediate level of detail, with a diversity win rate of 57%<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NEWS LOW-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">News low-level community summaries are summaries of news content at a low level of detail, with a diversity win rate of 60%<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ROOT-LEVEL COMMUNITY SUMMARIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Root-level community summaries are summaries of content at the root level, requiring over 97% fewer tokens compared to other methods<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NA&#207;VE RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Na&#239;ve RAG is a basic retrieval-augmented generation approach that converts documents to text, splits text into chunks, and embeds these chunks into a vector space for context retrieval<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"MODULAR RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Modular RAG is a system that includes patterns for iterative and dynamic cycles of interleaved retrieval and generation to overcome the drawbacks of Na&#239;ve RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SELF-MEMORY (SELFMEM)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-memory (Selfmem) is a concept related to generation-augmented retrieval that facilitates future generation cycles<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GENERATION-AUGMENTED RETRIEVAL (GAR)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Generation-augmented retrieval (GAR) is a system that combines retrieval and generation to enhance the retrieval process<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Iterative retrieval-generation (Iter-RetGen) is a strategy that involves iterative cycles of retrieval and generation<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"FEDERATED RETRIEVAL-GENERATION (FEB4RAG)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Federated retrieval-generation (FeB4RAG) is a strategy that involves federated cycles of retrieval and generation<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"CAIRE-COVID\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CAiRE-COVID is a system that combines multiple concepts for multi-document summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"ITRG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ITRG is a system for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"IR-COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">IR-CoT is a system for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"DSP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DSP is a system for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RAPTOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAPTOR is a system that generates a hierarchical index of text chunks by clustering the vectors of text embeddings<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TREE OF CLARIFICATIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tree of clarifications is a system that generates a hierarchical structure to answer multiple interpretations of ambiguous questions<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"KAPING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">KAPING is an advanced RAG system where the index is a knowledge graph<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"G-RETRIEVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">G-Retriever is a system where subsets of the graph structure are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"GRAPH-TOOLFORMER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph-ToolFormer is a system where derived graph metrics are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SURGE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">SURGE is a system where narrative outputs are strongly grounded in the facts of retrieved subgraphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"FABULA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FABULA is a system where retrieved event-plot subgraphs are serialized using narrative templates<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">LangChain is a library that supports a variety of graph databasesLangChain is an organization that supports a variety of graph databases<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"LLAMAINDEX\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">LlamaIndex is a library that supports a variety of graph databasesLlamaIndex is an organization that supports a variety of graph databases<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">Neo4J is a graph database format supported by NaLLMNeo4J is an organization that supports the NaLLM system for creating and reasoning over knowledge graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NALLM\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">NaLLM is a system that can create and reason over knowledge graphs in Neo4J format<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"NEBULA-GRAPH\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">Nebula-Graph is a graph database format supported by GraphRAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"GRAPHRAG\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">GraphRAG is a system that can create and reason over knowledge graphs in Nebula-Graph format<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">SOFTWARE<\/data>    <\/node>    <node id=\"RAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ram is an author who has worked on RAG approaches and systems<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao is an author who has worked on Na&#239;ve RAG and Modular RAG systems<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng is an author who has worked on Self-memory (Selfmem) for generation-augmented retrieval<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mao is an author who has worked on generation-augmented retrieval (GAR)<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shao is an author who has worked on iterative retrieval-generation (Iter-RetGen)<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on federated retrieval-generation (FeB4RAG) and multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Su is an author who has worked on CAiRE-COVID for multi-document summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng is an author who has worked on ITRG for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRIVEDI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trivedi is an author who has worked on IR-CoT for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHATTAB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khattab is an author who has worked on DSP for multi-hop question answering<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SARTHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarthi is an author who has worked on RAPTOR for generating a hierarchical index of text chunks<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kim is an author who has worked on the tree of clarifications for answering multiple interpretations of ambiguous questions<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAEK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baek is an author who has worked on KAPING where the index is a knowledge graph<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He is an author who has worked on G-Retriever where subsets of the graph structure are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author who has worked on Graph-ToolFormer where derived graph metrics are the objects of enquiry<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kang is an author who has worked on SURGE where narrative outputs are grounded in the facts of retrieved subgraphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RANADE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ranade is an author who has worked on FABULA where retrieved event-plot subgraphs are serialized using narrative templates<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshi is an author who has worked on FABULA where retrieved event-plot subgraphs are serialized using narrative templates<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAJANOSKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trajanoska is an author who has worked on using LLMs for knowledge graph creation<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on using LLMs for knowledge graph completion<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ban is an author who has worked on the extraction of causal graphs from source texts<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">NebulaGraph is an organization that supports the GraphRAG system for creating and reasoning over knowledge graphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"TABLE 3\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Table 3 illustrates the scalability advantages of Graph RAG compared to source text summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SENSEMAKING ACTIVITY\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Sensemaking activity involves iterative question answering and is characterized by the use of root-level Graph RAG for efficiency<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"EMPOWERMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Empowerment comparisons showed mixed results for both global approaches versus na&#239;ve RAG and Graph RAG approaches versus source text summarization<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"AD-HOC LLM USE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Ad-hoc LLM use involves analyzing LLM reasoning to provide specific examples, quotes, and citations to help users reach an informed understanding<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TUNING ELEMENT EXTRACTION PROMPTS\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Tuning element extraction prompts may help retain more details in the Graph RAG index<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) involves retrieving relevant information from external data sources and adding it to the context window of the LLM along with the original query<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"EXTERNAL DATA SOURCES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">External data sources are used in RAG to retrieve relevant information for the context window of the LLM<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"CONTEXT WINDOW\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The context window is the part of the LLM where relevant information and the original query are added in RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"DOCUMENTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Documents are converted to text, split into chunks, and embedded into a vector space in na&#239;ve RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TEXT CHUNKS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Text chunks are the split parts of documents that are embedded into a vector space in na&#239;ve RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"VECTOR SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Vector space is where text chunks are embedded, and similar positions represent similar semantics in na&#239;ve RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"QUERIES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Queries are embedded into the same vector space as text chunks, with the nearest vectors used as context in na&#239;ve RAG approaches<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"PRE-RETRIEVAL STRATEGIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Pre-retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na&#239;ve RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"RETRIEVAL STRATEGIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na&#239;ve RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"POST-RETRIEVAL STRATEGIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Post-retrieval strategies are part of advanced RAG systems designed to overcome the drawbacks of na&#239;ve RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"COMMUNITY SUMMARIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Community summaries are a kind of self-memory for generation-augmented retrieval in Graph RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"COMMUNITY ANSWERS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Community answers are generated in parallel from community summaries in Graph RAG<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"HIERARCHICAL INDEX\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hierarchical index is used in Graph RAG and other systems to organize text chunks by clustering the vectors of text embeddings<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"TEXT EMBEDDINGS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Text embeddings are the vector representations of text chunks used in hierarchical indexing<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Knowledge graph is used as an index in advanced RAG systems like KAPING<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"SUBGRAPHS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Subgraphs are subsets of the graph structure used in systems like G-Retriever and SURGE<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <node id=\"NARRATIVE TEMPLATES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Narrative templates are used in systems like FABULA to serialize retrieved event-plot subgraphs<\/data>      <data key=\"d2\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/node>    <edge source=\"GRAPH RAG\" target=\"NA&#207;VE RAG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Graph RAG offers scalability advantages and efficiency over Na&#239;ve RAG<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"MODULAR RAG\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Graph RAG incorporates multiple concepts related to Modular RAG systems<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SELF-MEMORY (SELFMEM)\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Graph RAG's community summaries are a kind of self-memory (Selfmem) for generation-augmented retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GENERATION-AUGMENTED RETRIEVAL (GAR)\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Graph RAG's community summaries are related to generation-augmented retrieval (GAR)<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ITERATIVE RETRIEVAL-GENERATION (ITER-RETGEN)\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Graph RAG's parallel generation of community answers is a kind of iterative retrieval-generation (Iter-RetGen) strategy<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"FEDERATED RETRIEVAL-GENERATION (FEB4RAG)\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Graph RAG's parallel generation of community answers is a kind of federated retrieval-generation (FeB4RAG) strategy<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"CAIRE-COVID\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG combines concepts similar to those used in CAiRE-COVID for multi-document summarization<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"ITRG\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG combines concepts similar to those used in ITRG for multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"IR-COT\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG combines concepts similar to those used in IR-CoT for multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"DSP\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG combines concepts similar to those used in DSP for multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"RAPTOR\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG's hierarchical index and summarization are similar to the approach used in RAPTOR<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"TREE OF CLARIFICATIONS\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG's hierarchical index and summarization are similar to the approach used in the tree of clarifications<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"KAPING\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG is related to KAPING where the index is a knowledge graph<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"G-RETRIEVER\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG is related to G-Retriever where subsets of the graph structure are the objects of enquiry<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"GRAPH-TOOLFORMER\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG is related to Graph-ToolFormer where derived graph metrics are the objects of enquiry<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"SURGE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG is related to SURGE where narrative outputs are grounded in the facts of retrieved subgraphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"GRAPH RAG\" target=\"FABULA\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Graph RAG is related to FABULA where retrieved event-plot subgraphs are serialized using narrative templates<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"LLAMAINDEX\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Both LangChain and LlamaIndex support a variety of graph databases<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"NEO4J\" target=\"NALLM\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Neo4J supports the NaLLM system for creating and reasoning over knowledge graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"NEBULA-GRAPH\" target=\"GRAPHRAG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Nebula-Graph supports the GraphRAG system for creating and reasoning over knowledge graphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"RAM\" target=\"GAO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Ram and Gao have both worked on RAG approaches and systems<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"CHENG\" target=\"MAO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Cheng and Mao have both worked on generation-augmented retrieval<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SHAO\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Shao and Wang have both worked on iterative and federated retrieval-generation strategies<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SU\" target=\"FENG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Su and Feng have both worked on multi-document summarization and multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"TRIVEDI\" target=\"KHATTAB\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Trivedi and Khattab have both worked on multi-hop question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"SARTHI\" target=\"KIM\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Sarthi and Kim have both worked on hierarchical approaches for text summarization and question answering<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"BAEK\" target=\"HE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Baek and He have both worked on advanced RAG systems<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"ZHANG\" target=\"KANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Zhang and Kang have both worked on graph-based RAG systems<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"ZHANG\" target=\"BAN\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Ban and Zhang have both worked on the extraction of causal graphs from source texts<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"RANADE\" target=\"JOSHI\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Ranade and Joshi have both worked on narrative templates for event-plot subgraphs<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>    <edge source=\"TRAJANOSKA\" target=\"YAO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Trajanoska and Yao have both worked on using LLMs for knowledge graph creation and completion<\/data>      <data key=\"d6\">edab4014b8f55e5b25bd7f396314be1f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ac21ebe9a9d70d691c717f961d3f10c8","chunk":"Index, 2024) libraries,\nwhile a more general class of graph-based RAG applications is also emerging, including systems that\ncan create and reason over knowledge graphs in both Neo4J (NaLLM, Neo4J, 2024) and Nebula-\nGraph (GraphRAG, NebulaGraph, 2024) formats. Unlike our Graph RAG approach, however, none\nof these systems use the natural modularity of graphs to partition data for global summarization.\n5 Discussion\nLimitations of evaluation approach . Our evaluation to date has only examined a certain class of\nsensemaking questions for two corpora in the region of 1 million tokens. More work is needed\nto understand how performance varies across different ranges of question types, data types, and\ndataset sizes, as well as to validate our sensemaking questions and target metrics with end users.\nComparison of fabrication rates, e.g., using approaches like SelfCheckGPT (Manakul et al., 2023),\nwould also improve on the current analysis.\nTrade-offs of building a graph index . We consistently observed Graph RAG achieve the best head-\nto-head results against other methods, but in many cases the graph-free approach to global summa-\nrization of source texts performed competitively. The real-world decision about whether to invest in\nbuilding a graph index depends on multiple factors, including the compute budget, expected number\nof lifetime queries per dataset, and value obtained from other aspects of the graph index (including\nthe generic community summaries and the use of other graph-related RAG approaches).\nFuture work . The graph index, rich text annotations, and hierarchical community structure support-\ning the current Graph RAG approach offer many possibilities for refinement and adaptation. This\nincludes RAG approaches that operate in a more local manner, via embedding-based matching of\nuser queries and graph annotations, as well as the possibility of hybrid RAG schemes that combine\nembedding-based matching against community reports before employing our map-reduce summa-\nrization mechanisms. This \u201croll-up\u201d operation could also be extended across more levels of the\ncommunity hierarchy, as well as implemented as a more exploratory \u201cdrill down\u201d mechanism that\nfollows the information scent contained in higher-level community summaries.\n6 Conclusion\nWe have presented a global approach to Graph RAG, combining knowledge graph generation,\nretrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human\nsensemaking over entire text corpora. Initial evaluations show substantial improvements over a\nna\u00a8\u0131ve RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable\ncomparisons to a global but graph-free approach using map-reduce source text summarization. For\nsituations requiring many global queries over the same dataset, summaries of root-level communi-\nties in the entity-based graph index provide a data index that is both superior to na \u00a8\u0131ve RAG and\nachieves competitive performance to other global methods at a fraction of the token cost.\nAn open-source, Python-based implementation of both global and local Graph RAG approaches is\nforthcoming at https:\/\/aka .ms\/graphrag .\n11Acknowledgements\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern \u00b4andez, Amber Hoak, Andr \u00b4es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M \u00b4onica Carva-\njal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\ntenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 .\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 .\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136 .\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbia","chunk_id":"ac21ebe9a9d70d691c717f961d3f10c8","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"INDEX","type":"TECHNOLOGY","description":"Index is a library mentioned in the text, likely used for graph-based RAG applications","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GRAPH-BASED RAG APPLICATIONS","type":"TECHNOLOGY","description":"Graph-based RAG applications are systems that create and reason over knowledge graphs in various formats","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NEO4J","type":"TECHNOLOGY","description":"Neo4J is a format for knowledge graphs used in graph-based RAG applications","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NALLM","type":"TECHNOLOGY","description":"NaLLM is a system that uses Neo4J format for knowledge graphs","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NEBULAGRAPH","type":"TECHNOLOGY","description":"NebulaGraph is a format for knowledge graphs used in graph-based RAG applications","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GRAPHRAG","type":"TECHNOLOGY","description":"GraphRAG is a system that uses NebulaGraph format for knowledge graphs","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GRAPH RAG","type":"TECHNOLOGY","description":"Graph RAG is an approach that uses the natural modularity of graphs to partition data for global summarization","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SELFCHECKGPT","type":"TECHNOLOGY","description":"SelfCheckGPT is a system used for comparing fabrication rates","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ALONSO GUEVARA FERN\u00c1NDEZ","type":"PERSON","description":"Alonso Guevara Fern\u00e1ndez is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"AMBER HOAK","type":"PERSON","description":"Amber Hoak is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ANDR\u00c9S MORALES ESQUIVEL","type":"PERSON","description":"Andr\u00e9s Morales Esquivel is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"BEN CUTLER","type":"PERSON","description":"Ben Cutler is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"BILLIE RINALDI","type":"PERSON","description":"Billie Rinaldi is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRIS SANCHEZ","type":"PERSON","description":"Chris Sanchez is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRIS TREVINO","type":"PERSON","description":"Chris Trevino is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"CHRISTINE CAGGIANO","type":"PERSON","description":"Christine Caggiano is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DAVID TITTSWORTH","type":"PERSON","description":"David Tittsworth is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DAYENNE DE SOUZA","type":"PERSON","description":"Dayenne de Souza is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DOUGLAS ORBAKER","type":"PERSON","description":"Douglas Orbaker is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ED CLARK","type":"PERSON","description":"Ed Clark is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GABRIEL NIEVES-PONCE","type":"PERSON","description":"Gabriel Nieves-Ponce is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GAUDY BLANCO MENESES","type":"PERSON","description":"Gaudy Blanco Meneses is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATE LYTVYNETS","type":"PERSON","description":"Kate Lytvynets is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KATY SMITH","type":"PERSON","description":"Katy Smith is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"M\u00d3NICA CARVAJAL","type":"PERSON","description":"M\u00f3nica Carvajal is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"NATHAN EVANS","type":"PERSON","description":"Nathan Evans is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RICHARD ORTEGA","type":"PERSON","description":"Richard Ortega is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RODRIGO RACANICCI","type":"PERSON","description":"Rodrigo Racanicci is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SARAH SMITH","type":"PERSON","description":"Sarah Smith is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SHANE SOLOMON","type":"PERSON","description":"Shane Solomon is one of the contributors to the work mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SAMUEL ADLER","type":"PERSON","description":"Samuel Adler is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SHIVANI AGARWAL","type":"PERSON","description":"Shivani Agarwal is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"LAILA AHMAD","type":"PERSON","description":"Laila Ahmad is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ILYA AKKAYA","type":"PERSON","description":"Ilya Akkaya is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"FERNANDO L. ALEMAN","type":"PERSON","description":"Fernando L. Aleman is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"DANIEL ALMEIDA","type":"PERSON","description":"Daniel Almeida is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JULIA ALTENSCHMIDT","type":"PERSON","description":"Julia Altenschmidt is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SAM ALTMAN","type":"PERSON","description":"Sam Altman is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SAMEER ANADKAT","type":"PERSON","description":"Sameer Anadkat is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RISHI ANIL","type":"PERSON","description":"Rishi Anil is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"SEBASTIAN BORGEAUD","type":"PERSON","description":"Sebastian Borgeaud is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"YONGJUN WU","type":"PERSON","description":"Yongjun Wu is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JEAN-BAPTISTE ALAYRAC","type":"PERSON","description":"Jean-Baptiste Alayrac is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JUNYU YU","type":"PERSON","description":"Junyu Yu is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"RADU SORICUT","type":"PERSON","description":"Radu Soricut is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JOHAN SCHALKWYK","type":"PERSON","description":"Johan Schalkwyk is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ANDREW M. DAI","type":"PERSON","description":"Andrew M. Dai is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ANDREAS HAUTH","type":"PERSON","description":"Andreas Hauth is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JUNSU BAEK","type":"PERSON","description":"Junsu Baek is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ALFIAN F. AJI","type":"PERSON","description":"Alfian F. Aji is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ALI SAFFARI","type":"PERSON","description":"Ali Saffari is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"TAKASHI BAN","type":"PERSON","description":"Takashi Ban is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"LI CHEN","type":"PERSON","description":"Li Chen is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"XIAOWEI WANG","type":"PERSON","description":"Xiaowei Wang is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"HONG CHEN","type":"PERSON","description":"Hong Chen is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"TAL BAUMEL","type":"PERSON","description":"Tal Baumel is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"MICHAL EYAL","type":"PERSON","description":"Michal Eyal is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"MICHAEL ELHADAD","type":"PERSON","description":"Michael Elhadad is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"VINCENT D. BLONDEL","type":"PERSON","description":"Vincent D. Blondel is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JEAN-LOUP GUILLAUME","type":"PERSON","description":"Jean-Loup Guillaume is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"R\u00c9MY LAMBIOTTE","type":"PERSON","description":"R\u00e9my Lambiotte is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"ETIENNE LEFEBVRE","type":"PERSON","description":"Etienne Lefebvre is one of the authors of a referenced paper","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GPT-4 TECHNICAL REPORT","type":"DOCUMENT","description":"The GPT-4 technical report is a document published by various authors on arXiv in 2023","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"GEMINI","type":"TECHNOLOGY","description":"Gemini is a family of highly capable multimodal models mentioned in the text","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING","type":"TECHNOLOGY","description":"Knowledge-augmented language model prompting is a technique for zero-shot knowledge graph question answering","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION","type":"TECHNOLOGY","description":"Query focused abstractive summarization is a technique that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"FAST UNFOLDING OF COMMUNITIES IN LARGE NETWORKS","type":"TECHNOLOGY","description":"Fast unfolding of communities in large networks is a method for community detection in large networks","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"},{"name":"JOSH ACHIAM","type":"","description":"","source_id":"ac21ebe9a9d70d691c717f961d3f10c8"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"INDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Index is a library mentioned in the text, likely used for graph-based RAG applications<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GRAPH-BASED RAG APPLICATIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph-based RAG applications are systems that create and reason over knowledge graphs in various formats<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neo4J is a format for knowledge graphs used in graph-based RAG applications<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NALLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NaLLM is a system that uses Neo4J format for knowledge graphs<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NebulaGraph is a format for knowledge graphs used in graph-based RAG applications<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GRAPHRAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GraphRAG is a system that uses NebulaGraph format for knowledge graphs<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GRAPH RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph RAG is an approach that uses the natural modularity of graphs to partition data for global summarization<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SELFCHECKGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">SelfCheckGPT is a system used for comparing fabrication rates<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ALONSO GUEVARA FERN&#193;NDEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alonso Guevara Fern&#225;ndez is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"AMBER HOAK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amber Hoak is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andr&#233;s Morales Esquivel is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"BEN CUTLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ben Cutler is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"BILLIE RINALDI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Billie Rinaldi is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRIS SANCHEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Sanchez is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRIS TREVINO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Trevino is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"CHRISTINE CAGGIANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christine Caggiano is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DAVID TITTSWORTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Tittsworth is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DAYENNE DE SOUZA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dayenne de Souza is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DOUGLAS ORBAKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Douglas Orbaker is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ED CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Clark is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GABRIEL NIEVES-PONCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Nieves-Ponce is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GAUDY BLANCO MENESES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gaudy Blanco Meneses is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATE LYTVYNETS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kate Lytvynets is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KATY SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katy Smith is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"M&#211;NICA CARVAJAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M&#243;nica Carvajal is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"NATHAN EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Evans is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RICHARD ORTEGA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard Ortega is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RODRIGO RACANICCI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rodrigo Racanicci is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SARAH SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Smith is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SHANE SOLOMON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shane Solomon is one of the contributors to the work mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SAMUEL ADLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel Adler is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SHIVANI AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shivani Agarwal is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"LAILA AHMAD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Laila Ahmad is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ILYA AKKAYA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Akkaya is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"FERNANDO L. ALEMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando L. Aleman is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"DANIEL ALMEIDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Almeida is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JULIA ALTENSCHMIDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julia Altenschmidt is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SAM ALTMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam Altman is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SAMEER ANADKAT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Anadkat is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RISHI ANIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rishi Anil is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"SEBASTIAN BORGEAUD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Borgeaud is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"YONGJUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yongjun Wu is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JEAN-BAPTISTE ALAYRAC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jean-Baptiste Alayrac is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JUNYU YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junyu Yu is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"RADU SORICUT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Radu Soricut is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JOHAN SCHALKWYK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Johan Schalkwyk is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ANDREW M. DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew M. Dai is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ANDREAS HAUTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andreas Hauth is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JUNSU BAEK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junsu Baek is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ALFIAN F. AJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alfian F. Aji is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ALI SAFFARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ali Saffari is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"TAKASHI BAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Takashi Ban is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"LI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Chen is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"XIAOWEI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaowei Wang is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"HONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hong Chen is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"TAL BAUMEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tal Baumel is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"MICHAL EYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michal Eyal is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"MICHAEL ELHADAD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Elhadad is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"VINCENT D. BLONDEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vincent D. Blondel is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JEAN-LOUP GUILLAUME\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jean-Loup Guillaume is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"R&#201;MY LAMBIOTTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R&#233;my Lambiotte is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"ETIENNE LEFEBVRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Etienne Lefebvre is one of the authors of a referenced paper<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The GPT-4 technical report is a document published by various authors on arXiv in 2023<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"GEMINI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gemini is a family of highly capable multimodal models mentioned in the text<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Knowledge-augmented language model prompting is a technique for zero-shot knowledge graph question answering<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Query focused abstractive summarization is a technique that incorporates query relevance, multi-document coverage, and summary length constraints into seq2seq models<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"FAST UNFOLDING OF COMMUNITIES IN LARGE NETWORKS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Fast unfolding of communities in large networks is a method for community detection in large networks<\/data>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <node id=\"JOSH ACHIAM\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/node>    <edge source=\"GRAPH-BASED RAG APPLICATIONS\" target=\"NEO4J\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Neo4J is a format used in graph-based RAG applications<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH-BASED RAG APPLICATIONS\" target=\"NEBULAGRAPH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">NebulaGraph is a format used in graph-based RAG applications<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH-BASED RAG APPLICATIONS\" target=\"GRAPH RAG\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Graph RAG is an approach within the broader category of graph-based RAG applications<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GRAPH-BASED RAG APPLICATIONS\" target=\"FAST UNFOLDING OF COMMUNITIES IN LARGE NETWORKS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Fast unfolding of communities in large networks is a method that could be used in graph-based RAG applications<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"NEO4J\" target=\"NALLM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">NaLLM uses the Neo4J format for knowledge graphs<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"NEBULAGRAPH\" target=\"GRAPHRAG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GraphRAG uses the NebulaGraph format for knowledge graphs<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SELFCHECKGPT\" target=\"META AGENT SEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">SelfCheckGPT is mentioned as a system that could improve the current analysis in the context of Meta Agent Search<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"AMBER HOAK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Amber Hoak both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Andr&#233;s Morales Esquivel both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"BEN CUTLER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Ben Cutler both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"BILLIE RINALDI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Billie Rinaldi both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"CHRIS SANCHEZ\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Chris Sanchez both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"CHRIS TREVINO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Chris Trevino both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"CHRISTINE CAGGIANO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Christine Caggiano both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"DAVID TITTSWORTH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and David Tittsworth both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"DAYENNE DE SOUZA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Dayenne de Souza both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"DOUGLAS ORBAKER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Douglas Orbaker both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"ED CLARK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Ed Clark both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"GABRIEL NIEVES-PONCE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Gabriel Nieves-Ponce both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"GAUDY BLANCO MENESES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Gaudy Blanco Meneses both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"KATE LYTVYNETS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Kate Lytvynets both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"KATY SMITH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Katy Smith both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"M&#211;NICA CARVAJAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and M&#243;nica Carvajal both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"NATHAN EVANS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Nathan Evans both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"RICHARD ORTEGA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Richard Ortega both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"RODRIGO RACANICCI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Rodrigo Racanicci both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"SARAH SMITH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Sarah Smith both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ALONSO GUEVARA FERN&#193;NDEZ\" target=\"SHANE SOLOMON\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Alonso Guevara Fern&#225;ndez and Shane Solomon both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"ANDR&#201;S MORALES ESQUIVEL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Andr&#233;s Morales Esquivel both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"BEN CUTLER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Ben Cutler both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"BILLIE RINALDI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Billie Rinaldi both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"CHRIS SANCHEZ\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Chris Sanchez both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"CHRIS TREVINO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Chris Trevino both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"CHRISTINE CAGGIANO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Christine Caggiano both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"DAVID TITTSWORTH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and David Tittsworth both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"DAYENNE DE SOUZA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Dayenne de Souza both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"DOUGLAS ORBAKER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Douglas Orbaker both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"ED CLARK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Ed Clark both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"GABRIEL NIEVES-PONCE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Gabriel Nieves-Ponce both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"GAUDY BLANCO MENESES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Gaudy Blanco Meneses both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"KATE LYTVYNETS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Kate Lytvynets both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"KATY SMITH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Katy Smith both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"M&#211;NICA CARVAJAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and M&#243;nica Carvajal both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"NATHAN EVANS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Nathan Evans both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"RICHARD ORTEGA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Richard Ortega both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"RODRIGO RACANICCI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Rodrigo Racanicci both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"SARAH SMITH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Sarah Smith both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"AMBER HOAK\" target=\"SHANE SOLOMON\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Amber Hoak and Shane Solomon both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"BEN CUTLER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Ben Cutler both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"BILLIE RINALDI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Billie Rinaldi both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"CHRIS SANCHEZ\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Chris Sanchez both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"CHRIS TREVINO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Chris Trevino both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"CHRISTINE CAGGIANO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Christine Caggiano both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"DAVID TITTSWORTH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and David Tittsworth both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"DAYENNE DE SOUZA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Dayenne de Souza both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"DOUGLAS ORBAKER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Douglas Orbaker both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"ED CLARK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Ed Clark both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"GABRIEL NIEVES-PONCE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Gabriel Nieves-Ponce both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"GAUDY BLANCO MENESES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and Gaudy Blanco Meneses both contributed to the work mentioned in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ANDR&#201;S MORALES ESQUIVEL\" target=\"KATE LYTVYNETS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Andr&#233;s Morales Esquivel and(\"entity\"<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SAMUEL ADLER\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Samuel Adler co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SHIVANI AGARWAL\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Shivani Agarwal co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"LAILA AHMAD\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Laila Ahmad co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"ILYA AKKAYA\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Ilya Akkaya co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"FERNANDO L. ALEMAN\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Fernando L. Aleman co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"DANIEL ALMEIDA\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Daniel Almeida co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"JULIA ALTENSCHMIDT\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Julia Altenschmidt co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SAM ALTMAN\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Sam Altman co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"SAMEER ANADKAT\" target=\"JOSH ACHIAM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Josh Achiam and Sameer Anadkat co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"SEBASTIAN BORGEAUD\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Sebastian Borgeaud co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"YONGJUN WU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Yongjun Wu co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"JEAN-BAPTISTE ALAYRAC\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Jean-Baptiste Alayrac co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"JUNYU YU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Junyu Yu co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"RADU SORICUT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Radu Soricut co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"JOHAN SCHALKWYK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Johan Schalkwyk co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"ANDREW M. DAI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Andrew M. Dai co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"RISHI ANIL\" target=\"ANDREAS HAUTH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rishi Anil and Andreas Hauth co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"JUNSU BAEK\" target=\"ALFIAN F. AJI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Junsu Baek and Alfian F. Aji co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"JUNSU BAEK\" target=\"ALI SAFFARI\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Junsu Baek and Ali Saffari co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"TAKASHI BAN\" target=\"LI CHEN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Takashi Ban and Li Chen co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"TAKASHI BAN\" target=\"XIAOWEI WANG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Takashi Ban and Xiaowei Wang co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"TAKASHI BAN\" target=\"HONG CHEN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Takashi Ban and Hong Chen co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"TAL BAUMEL\" target=\"MICHAL EYAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Tal Baumel and Michal Eyal co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"TAL BAUMEL\" target=\"MICHAEL ELHADAD\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Tal Baumel and Michael Elhadad co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"VINCENT D. BLONDEL\" target=\"JEAN-LOUP GUILLAUME\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Vincent D. Blondel and Jean-Loup Guillaume co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"VINCENT D. BLONDEL\" target=\"R&#201;MY LAMBIOTTE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Vincent D. Blondel and R&#233;my Lambiotte co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"VINCENT D. BLONDEL\" target=\"ETIENNE LEFEBVRE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Vincent D. Blondel and Etienne Lefebvre co-authored a referenced paper<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"GPT-4 TECHNICAL REPORT\" target=\"GEMINI\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The GPT-4 technical report and Gemini are both mentioned as significant technologies in the text<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>    <edge source=\"KNOWLEDGE-AUGMENTED LANGUAGE MODEL PROMPTING\" target=\"QUERY FOCUSED ABSTRACTIVE SUMMARIZATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Knowledge-augmented language model prompting and Query focused abstractive summarization are techniques related to language models<\/data>      <data key=\"d5\">ac21ebe9a9d70d691c717f961d3f10c8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"aa79049289e6532592eec17b9e76adfb","chunk":" summary length constraints into seq2seq\nmodels. arXiv preprint arXiv:1801.07704 .\nBlondel, V . D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. Journal of statistical mechanics: theory and experiment ,\n2008(10):P10008.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in\nneural information processing systems , 33:1877\u20131901.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrieval-\naugmented text generation with self-memory. Advances in Neural Information Processing Sys-\ntems, 36.\nDang, H. T. (2006). Duc 2005: Evaluation of question-focused summarization systems. In Proceed-\nings of the Workshop on Task-Focused Summarization and Question Answering , pages 48\u201355.\nEs, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of\nretrieval augmented generation. arXiv preprint arXiv:2309.15217 .\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. (2023). Retrieval-generation synergy augmented\nlarge language models. arXiv preprint arXiv:2310.05149 .\nFortunato, S. (2010). Community detection in graphs. Physics reports , 486(3-5):75\u2013174.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., and Wang, H. (2023). Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 .\nGoodwin, T. R., Savery, M. E., and Demner-Fushman, D. (2020). Flight of the pegasus? comparing\ntransformers on few-shot and zero-shot multi-document abstractive summarization. In Proceed-\nings of COLING. International Conference on Computational Linguistics , volume 2020, page\n5640. NIH Public Access.\nHe, X., Tian, Y ., Sun, Y ., Chawla, N. V ., Laurent, T., LeCun, Y ., Bresson, X., and Hooi, B. (2024).\nG-retriever: Retrieval-augmented generation for textual graph understanding and question an-\nswering. arXiv preprint arXiv:2402.07630 .\n12Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. PLoS ONE\n9(6): e98679. https:\/\/doi.org\/10.1371\/journal.pone.0098679 .\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y ., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. IEEE Transactions\non Knowledge and Data Engineering , 35(2):1149\u20131170.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846 .\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024 .\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu-\nous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696 .\nKlein, G., Moon, B., and Hoffman, R. R. (2006a). Making sense of sensemaking 1: Alternative\nperspectives. IEEE intelligent systems , 21(4):70\u201373.\nKlein, G., Moon, B., and Hoffman, R. R. (2006b). Making sense of sensemaking 2: A macrocogni-\ntive model. IEEE Intelligent systems , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (202","chunk_id":"aa79049289e6532592eec17b9e76adfb","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"BLONDEL, V. D.","type":"PERSON","description":"Blondel, V. D. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GUILLAUME, J.-L.","type":"PERSON","description":"Guillaume, J.-L. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LAMIOTTE, R.","type":"PERSON","description":"Lambiotte, R. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LEFEBVRE, E.","type":"PERSON","description":"Lefebvre, E. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BROWN, T.","type":"PERSON","description":"Brown, T. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"MANN, B.","type":"PERSON","description":"Mann, B. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RYDER, N.","type":"PERSON","description":"Ryder, N. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUBBIAH, M.","type":"PERSON","description":"Subbiah, M. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KAPLAN, J. D.","type":"PERSON","description":"Kaplan, J. D. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DHARIWAL, P.","type":"PERSON","description":"Dhariwal, P. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"NEELAKANTAN, A.","type":"PERSON","description":"Neelakantan, A. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SHYAM, P.","type":"PERSON","description":"Shyam, P. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SASTRY, G.","type":"PERSON","description":"Sastry, G. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ASKELL, A.","type":"PERSON","description":"Askell, A. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHENG, X.","type":"PERSON","description":"Cheng, X. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LUO, D.","type":"PERSON","description":"Luo, D. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHEN, X.","type":"PERSON","description":"Chen, X. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIU, L.","type":"PERSON","description":"Liu, L. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZHAO, D.","type":"PERSON","description":"Zhao, D. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024\nZhao, D. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb","entity_type":"PERSON"},{"name":"YAN, R.","type":"PERSON","description":"Yan, R. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DANG, H. T.","type":"PERSON","description":"Dang, H. T. is the author of the paper titled \"Duc 2005: Evaluation of question-focused summarization systems\" published in the Proceedings of the Workshop on Task-Focused Summarization and Question Answering in 2006","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ES, S.","type":"PERSON","description":"Es, S. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JAMES, J.","type":"PERSON","description":"James, J. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ESPINOSA-ANKE, L.","type":"PERSON","description":"Espinosa-Anke, L. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SCHOCKAERT, S.","type":"PERSON","description":"Schockaert, S. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FENG, Z.","type":"PERSON","description":"Feng, Z. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FENG, X.","type":"PERSON","description":"Feng, X. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"YANG, M.","type":"PERSON","description":"Yang, M. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"QIN, B.","type":"PERSON","description":"Qin, B. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FORTUNATO, S.","type":"PERSON","description":"Fortunato, S. is the author of the paper titled \"Community detection in graphs\" published in Physics Reports in 2010","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GAO, Y.","type":"PERSON","description":"Gao, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"XIONG, Y.","type":"PERSON","description":"Xiong, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GAO, X.","type":"PERSON","description":"Gao, X. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIA, K.","type":"PERSON","description":"Jia, K. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PAN, J.","type":"PERSON","description":"Pan, J. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BI, Y.","type":"PERSON","description":"Bi, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DAI, Y.","type":"PERSON","description":"Dai, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUN, J.","type":"PERSON","description":"Sun, J. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"WANG, H.","type":"PERSON","description":"Wang, H. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GOODWIN, T. R.","type":"PERSON","description":"Goodwin, T. R. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SAVERY, M. E.","type":"PERSON","description":"Savery, M. E. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DEMNER-FUSHMAN, D.","type":"PERSON","description":"Demner-Fushman, D. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HE, X.","type":"PERSON","description":"He, X. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"TIAN, Y.","type":"PERSON","description":"Tian, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"SUN, Y.","type":"PERSON","description":"Sun, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"CHAWLA, N. V.","type":"PERSON","description":"Chawla, N. V. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LAURENT, T.","type":"PERSON","description":"Laurent, T. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LECUN, Y.","type":"PERSON","description":"LeCun, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BRESSON, X.","type":"PERSON","description":"Bresson, X. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HOOI, B.","type":"PERSON","description":"Hooi, B. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JACOMY, M.","type":"PERSON","description":"Jacomy, M. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"VENTURINI, T.","type":"PERSON","description":"Venturini, T. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HEYMANN, S.","type":"PERSON","description":"Heymann, S. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BASTIAN, M.","type":"PERSON","description":"Bastian, M. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIN, D.","type":"PERSON","description":"Jin, D. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"YU, Z.","type":"PERSON","description":"Yu, Z. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JIAO, P.","type":"PERSON","description":"Jiao, P. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PAN, S.","type":"PERSON","description":"Pan, S. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HE, D.","type":"PERSON","description":"He, D. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"WU, J.","type":"PERSON","description":"Wu, J. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PHILIP, S. Y.","type":"PERSON","description":"Philip, S. Y. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ZHANG, W.","type":"PERSON","description":"Zhang, W. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KANG, M.","type":"PERSON","description":"Kang, M. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KWAK, J. M.","type":"PERSON","description":"Kwak, J. M. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"BAEK, J.","type":"PERSON","description":"Baek, J. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"HWANG, S. J.","type":"PERSON","description":"Hwang, S. J. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KHATTAB, O.","type":"PERSON","description":"Khattab, O.(\"entity\"","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"JOURNAL OF STATISTICAL MECHANICS: THEORY AND EXPERIMENT","type":"JOURNAL","description":"The Journal of Statistical Mechanics: Theory and Experiment is a scientific journal where the paper \"Fast unfolding of communities in large networks\" was published in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS","type":"CONFERENCE","description":"Advances in Neural Information Processing Systems (NeurIPS) is a conference where the paper \"Language models are few-shot learners\" was published in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PROCEEDINGS OF THE WORKSHOP ON TASK-FOCUSED SUMMARIZATION AND QUESTION ANSWERING","type":"CONFERENCE","description":"The Proceedings of the Workshop on Task-Focused Summarization and Question Answering is a conference where the paper \"Duc 2005: Evaluation of question-focused summarization systems\" was published in 2006","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PHYSICS REPORTS","type":"JOURNAL","description":"Physics Reports is a scientific journal where the paper \"Community detection in graphs\" was published in 2010","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PROCEEDINGS OF COLING","type":"CONFERENCE","description":"The Proceedings of COLING is a conference where the paper \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" was published in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"PLOS ONE","type":"JOURNAL","description":"PLoS ONE is a scientific journal where the paper \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" was published in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING","type":"JOURNAL","description":"IEEE Transactions on Knowledge and Data Engineering is a scientific journal where the paper \"A survey of community detection approaches: From statistical modeling to deep learning\" was published in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DUC 2005","type":"EVENT","description":"DUC 2005 is an event where the evaluation of question-focused summarization systems was conducted","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"G-RETRIEVER","type":"TECHNOLOGY","description":"G-Retriever is a system for retrieval-augmented generation for textual graph understanding and question answering","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RAGAS","type":"TECHNOLOGY","description":"Ragas is a system for automated evaluation of retrieval augmented generation","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-GENERATION SYNERGY","type":"TECHNOLOGY","description":"Retrieval-Generation Synergy is a method for augmenting large language models","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-AUGMENTED GENERATION","type":"TECHNOLOGY","description":"Retrieval-Augmented Generation is a method for enhancing large language models by integrating retrieval mechanisms","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FORCEATLAS2","type":"TECHNOLOGY","description":"ForceAtlas2 is a continuous graph layout algorithm designed for network visualization in the Gephi software","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"GEPHI","type":"SOFTWARE","description":"Gephi is a software designed for network visualization","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIFT YOURSELF UP","type":"TECHNOLOGY","description":"Lift Yourself Up is a method for retrieval-augmented text generation with self-memory","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LANGUAGE MODELS ARE FEW-SHOT LEARNERS","type":"DOCUMENT","description":"The paper \"Language models are few-shot learners\" discusses the capabilities of language models in few-shot learning and was published in Advances in Neural Information Processing Systems in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FAST UNFOLDING OF COMMUNITIES IN LARGE NETWORKS","type":"DOCUMENT","description":"The paper \"Fast unfolding of communities in large networks\" discusses methods for community detection in large networks and was published in the Journal of Statistical Mechanics: Theory and Experiment in 2008","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"LIFT YOURSELF UP: RETRIEVAL-AUGMENTED TEXT GENERATION WITH SELF-MEMORY","type":"DOCUMENT","description":"The paper \"Lift yourself up: Retrieval-augmented text generation with self-memory\" discusses methods for enhancing text generation and was published in Advances in Neural Information Processing Systems in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"DUC 2005: EVALUATION OF QUESTION-FOCUSED SUMMARIZATION SYSTEMS","type":"DOCUMENT","description":"The paper \"Duc 2005: Evaluation of question-focused summarization systems\" discusses the evaluation of summarization systems and was published in the Proceedings of the Workshop on Task-Focused Summarization and Question Answering in 2006","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RAGAS: AUTOMATED EVALUATION OF RETRIEVAL AUGMENTED GENERATION","type":"DOCUMENT","description":"The paper \"Ragas: Automated evaluation of retrieval augmented generation\" discusses methods for evaluating retrieval-augmented generation and was published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS","type":"DOCUMENT","description":"The paper \"Retrieval-generation synergy augmented large language models\" discusses methods for enhancing large language models and was published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"COMMUNITY DETECTION IN GRAPHS","type":"DOCUMENT","description":"The paper \"Community detection in graphs\" discusses methods for detecting communities in graphs and was published in Physics Reports in 2010","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"RETRIEVAL-AUGMENTED GENERATION FOR LARGE LANGUAGE MODELS: A SURVEY","type":"DOCUMENT","description":"The paper \"Retrieval-augmented generation for large language models: A survey\" discusses various methods for enhancing large language models and was published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FLIGHT OF THE PEGASUS? COMPARING TRANSFORMERS ON FEW-SHOT AND ZERO-SHOT MULTI-DOCUMENT ABSTRACTIVE SUMMARIZATION","type":"DOCUMENT","description":"The paper \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" discusses the performance of transformers in summarization tasks and was published in the Proceedings of COLING in 2020","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"G-RETRIEVER: RETRIEVAL-AUGMENTED GENERATION FOR TEXTUAL GRAPH UNDERSTANDING AND QUESTION ANSWERING","type":"DOCUMENT","description":"The paper \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" discusses methods for enhancing text generation and was published as an arXiv preprint in 2024","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"FORCEATLAS2, A CONTINUOUS GRAPH LAYOUT ALGORITHM FOR HANDY NETWORK VISUALIZATION DESIGNED FOR THE GEPHI SOFTWARE","type":"DOCUMENT","description":"The paper \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" discusses methods for network visualization and was published in PLoS ONE in 2014","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"A SURVEY OF COMMUNITY DETECTION APPROACHES: FROM STATISTICAL MODELING TO DEEP LEARNING","type":"DOCUMENT","description":"The paper \"A survey of community detection approaches: From statistical modeling to deep learning\" discusses various methods for community detection and was published in IEEE Transactions on Knowledge and Data Engineering in 2021","source_id":"aa79049289e6532592eec17b9e76adfb"},{"name":"KNOWLEDGE GRAPH-AUGMENTED LANGUAGE MODELS FOR KNOWLEDGE-GROUNDED DIALOGUE GENERATION","type":"DOCUMENT","description":"The paper \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" discusses methods for enhancing dialogue generation and was published as an arXiv preprint in 2023","source_id":"aa79049289e6532592eec17b9e76adfb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"BLONDEL, V. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Blondel, V. D. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GUILLAUME, J.-L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillaume, J.-L. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LAMIOTTE, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lambiotte, R. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LEFEBVRE, E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lefebvre, E. is one of the authors of the paper titled \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BROWN, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown, T. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"MANN, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mann, B. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RYDER, N.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryder, N. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUBBIAH, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Subbiah, M. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KAPLAN, J. D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kaplan, J. D. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DHARIWAL, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dhariwal, P. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"NEELAKANTAN, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neelakantan, A. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SHYAM, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shyam, P. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SASTRY, G.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sastry, G. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ASKELL, A.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Askell, A. is one of the authors of the paper titled \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHENG, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng, X. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LUO, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luo, D. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHEN, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen, X. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIU, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu, L. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZHAO, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhao, D. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024Zhao, D. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAN, R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yan, R. is one of the authors of the paper titled \"Lift yourself up: Retrieval-augmented text generation with self-memory\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DANG, H. T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dang, H. T. is the author of the paper titled \"Duc 2005: Evaluation of question-focused summarization systems\" published in the Proceedings of the Workshop on Task-Focused Summarization and Question Answering in 2006<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ES, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Es, S. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JAMES, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James, J. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ESPINOSA-ANKE, L.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Espinosa-Anke, L. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SCHOCKAERT, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schockaert, S. is one of the authors of the paper titled \"Ragas: Automated evaluation of retrieval augmented generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FENG, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng, Z. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FENG, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng, X. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"YANG, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang, M. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"QIN, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qin, B. is one of the authors of the paper titled \"Retrieval-generation synergy augmented large language models\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FORTUNATO, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fortunato, S. is the author of the paper titled \"Community detection in graphs\" published in Physics Reports in 2010<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GAO, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"XIONG, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiong, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GAO, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao, X. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIA, K.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jia, K. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PAN, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pan, J. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BI, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bi, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DAI, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dai, Y. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUN, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sun, J. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"WANG, H.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang, H. is one of the authors of the paper titled \"Retrieval-augmented generation for large language models: A survey\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GOODWIN, T. R.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goodwin, T. R. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SAVERY, M. E.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Savery, M. E. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DEMNER-FUSHMAN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Demner-Fushman, D. is one of the authors of the paper titled \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" published in the Proceedings of COLING in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HE, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He, X. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"TIAN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tian, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"SUN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sun, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"CHAWLA, N. V.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chawla, N. V. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LAURENT, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Laurent, T. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LECUN, Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">LeCun, Y. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BRESSON, X.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bresson, X. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HOOI, B.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hooi, B. is one of the authors of the paper titled \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JACOMY, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacomy, M. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"VENTURINI, T.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Venturini, T. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HEYMANN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heymann, S. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BASTIAN, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bastian, M. is one of the authors of the paper titled \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" published in PLoS ONE in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIN, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jin, D. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"YU, Z.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu, Z. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JIAO, P.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiao, P. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PAN, S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pan, S. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HE, D.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He, D. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"WU, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wu, J. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PHILIP, S. Y.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philip, S. Y. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ZHANG, W.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang, W. is one of the authors of the paper titled \"A survey of community detection approaches: From statistical modeling to deep learning\" published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KANG, M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kang, M. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KWAK, J. M.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kwak, J. M. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"BAEK, J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baek, J. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"HWANG, S. J.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hwang, S. J. is one of the authors of the paper titled \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KHATTAB, O.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khattab, O.(\"entity\"<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"JOURNAL OF STATISTICAL MECHANICS: THEORY AND EXPERIMENT\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">The Journal of Statistical Mechanics: Theory and Experiment is a scientific journal where the paper \"Fast unfolding of communities in large networks\" was published in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">Advances in Neural Information Processing Systems (NeurIPS) is a conference where the paper \"Language models are few-shot learners\" was published in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PROCEEDINGS OF THE WORKSHOP ON TASK-FOCUSED SUMMARIZATION AND QUESTION ANSWERING\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The Proceedings of the Workshop on Task-Focused Summarization and Question Answering is a conference where the paper \"Duc 2005: Evaluation of question-focused summarization systems\" was published in 2006<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PHYSICS REPORTS\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Physics Reports is a scientific journal where the paper \"Community detection in graphs\" was published in 2010<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PROCEEDINGS OF COLING\">      <data key=\"d0\">CONFERENCE<\/data>      <data key=\"d1\">The Proceedings of COLING is a conference where the paper \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" was published in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"PLOS ONE\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">PLoS ONE is a scientific journal where the paper \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" was published in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">IEEE Transactions on Knowledge and Data Engineering is a scientific journal where the paper \"A survey of community detection approaches: From statistical modeling to deep learning\" was published in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DUC 2005\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">DUC 2005 is an event where the evaluation of question-focused summarization systems was conducted<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"G-RETRIEVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">G-Retriever is a system for retrieval-augmented generation for textual graph understanding and question answering<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RAGAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Ragas is a system for automated evaluation of retrieval augmented generation<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval-Generation Synergy is a method for augmenting large language models<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Retrieval-Augmented Generation is a method for enhancing large language models by integrating retrieval mechanisms<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FORCEATLAS2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ForceAtlas2 is a continuous graph layout algorithm designed for network visualization in the Gephi software<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"GEPHI\">      <data key=\"d0\">SOFTWARE<\/data>      <data key=\"d1\">Gephi is a software designed for network visualization<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIFT YOURSELF UP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Lift Yourself Up is a method for retrieval-augmented text generation with self-memory<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LANGUAGE MODELS ARE FEW-SHOT LEARNERS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Language models are few-shot learners\" discusses the capabilities of language models in few-shot learning and was published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FAST UNFOLDING OF COMMUNITIES IN LARGE NETWORKS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Fast unfolding of communities in large networks\" discusses methods for community detection in large networks and was published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"LIFT YOURSELF UP: RETRIEVAL-AUGMENTED TEXT GENERATION WITH SELF-MEMORY\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Lift yourself up: Retrieval-augmented text generation with self-memory\" discusses methods for enhancing text generation and was published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"DUC 2005: EVALUATION OF QUESTION-FOCUSED SUMMARIZATION SYSTEMS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Duc 2005: Evaluation of question-focused summarization systems\" discusses the evaluation of summarization systems and was published in the Proceedings of the Workshop on Task-Focused Summarization and Question Answering in 2006<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RAGAS: AUTOMATED EVALUATION OF RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Ragas: Automated evaluation of retrieval augmented generation\" discusses methods for evaluating retrieval-augmented generation and was published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Retrieval-generation synergy augmented large language models\" discusses methods for enhancing large language models and was published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"COMMUNITY DETECTION IN GRAPHS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Community detection in graphs\" discusses methods for detecting communities in graphs and was published in Physics Reports in 2010<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"RETRIEVAL-AUGMENTED GENERATION FOR LARGE LANGUAGE MODELS: A SURVEY\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Retrieval-augmented generation for large language models: A survey\" discusses various methods for enhancing large language models and was published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FLIGHT OF THE PEGASUS? COMPARING TRANSFORMERS ON FEW-SHOT AND ZERO-SHOT MULTI-DOCUMENT ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization\" discusses the performance of transformers in summarization tasks and was published in the Proceedings of COLING in 2020<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"G-RETRIEVER: RETRIEVAL-AUGMENTED GENERATION FOR TEXTUAL GRAPH UNDERSTANDING AND QUESTION ANSWERING\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\" discusses methods for enhancing text generation and was published as an arXiv preprint in 2024<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"FORCEATLAS2, A CONTINUOUS GRAPH LAYOUT ALGORITHM FOR HANDY NETWORK VISUALIZATION DESIGNED FOR THE GEPHI SOFTWARE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software\" discusses methods for network visualization and was published in PLoS ONE in 2014<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"A SURVEY OF COMMUNITY DETECTION APPROACHES: FROM STATISTICAL MODELING TO DEEP LEARNING\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"A survey of community detection approaches: From statistical modeling to deep learning\" discusses various methods for community detection and was published in IEEE Transactions on Knowledge and Data Engineering in 2021<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <node id=\"KNOWLEDGE GRAPH-AUGMENTED LANGUAGE MODELS FOR KNOWLEDGE-GROUNDED DIALOGUE GENERATION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Knowledge graph-augmented language models for knowledge-grounded dialogue generation\" discusses methods for enhancing dialogue generation and was published as an arXiv preprint in 2023<\/data>      <data key=\"d2\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/node>    <edge source=\"BLONDEL, V. D.\" target=\"GUILLAUME, J.-L.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Blondel, V. D. and Guillaume, J.-L. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BLONDEL, V. D.\" target=\"LAMIOTTE, R.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Blondel, V. D. and Lambiotte, R. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BLONDEL, V. D.\" target=\"LEFEBVRE, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Blondel, V. D. and Lefebvre, E. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"GUILLAUME, J.-L.\" target=\"LAMIOTTE, R.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Guillaume, J.-L. and Lambiotte, R. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"GUILLAUME, J.-L.\" target=\"LEFEBVRE, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Guillaume, J.-L. and Lefebvre, E. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"LAMIOTTE, R.\" target=\"LEFEBVRE, E.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lambiotte, R. and Lefebvre, E. co-authored the paper \"Fast unfolding of communities in large networks\" published in the Journal of Statistical Mechanics: Theory and Experiment in 2008<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"MANN, B.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Mann, B. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"RYDER, N.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Ryder, N. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"SUBBIAH, M.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Subbiah, M. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"KAPLAN, J. D.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Kaplan, J. D. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"DHARIWAL, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Dhariwal, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"NEELAKANTAN, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Neelakantan, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"SHYAM, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Shyam, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"SASTRY, G.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Sastry, G. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"BROWN, T.\" target=\"ASKELL, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Brown, T. and Askell, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"RYDER, N.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Ryder, N. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"SUBBIAH, M.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Subbiah, M. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"KAPLAN, J. D.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Kaplan, J. D. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"DHARIWAL, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Dhariwal, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"NEELAKANTAN, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Neelakantan, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"SHYAM, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Shyam, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"SASTRY, G.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Sastry, G. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"MANN, B.\" target=\"ASKELL, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mann, B. and Askell, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"SUBBIAH, M.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Subbiah, M. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"KAPLAN, J. D.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Kaplan, J. D. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"DHARIWAL, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Dhariwal, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"NEELAKANTAN, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Neelakantan, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"SHYAM, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Shyam, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"SASTRY, G.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Sastry, G. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"RYDER, N.\" target=\"ASKELL, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ryder, N. and Askell, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SUBBIAH, M.\" target=\"KAPLAN, J. D.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Subbiah, M. and Kaplan, J. D. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SUBBIAH, M.\" target=\"DHARIWAL, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Subbiah, M. and Dhariwal, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SUBBIAH, M.\" target=\"NEELAKANTAN, A.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Subbiah, M. and Neelakantan, A. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SUBBIAH, M.\" target=\"SHYAM, P.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Subbiah, M. and Shyam, P. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>    <edge source=\"SUBBIAH, M.\" target=\"SASTRY, G.\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Subbiah, M. and Sastry, G. co-authored the paper \"Language models are few-shot learners\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d6\">aa79049289e6532592eec17b9e76adfb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"df50c95dff7da074cbb2f68e88686f88","chunk":" , 21(5):88\u201392.\nKoesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets\u2013understanding data\nsensemaking behaviours. International journal of human-computer studies , 146:102562.\nKuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\nof needles in a 11m haystack: Recurrent memory finds what llms miss.\nLangChain (2024). Langchain graphs. https:\/\/python .langchain .com\/docs\/use cases\/graph\/.\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In Advances in\nArtificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13\u201315, 2020, Proceedings 33 , pages 342\u2013348. Springer.\nLaskar, M. T. R., Hoque, E., and Huang, J. X. (2022). Domain adaptation with pre-trained transform-\ners for query-focused abstractive text summarization. Computational Linguistics , 48(2):279\u2013320.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K \u00a8uttler, H., Lewis, M., Yih,\nW.-t., Rockt \u00a8aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems , 33:9459\u20139474.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\nLiu, Y . and Lapata, M. (2019). Hierarchical transformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164 .\nLlamaIndex (2024). LlamaIndex Knowledge Graph Index. https:\/\/docs .llamaindex .ai\/en\/stable\/\nexamples\/index structs\/knowledge graph\/KnowledgeGraphDemo .html.\nManakul, P., Liusie, A., and Gales, M. J. (2023). Selfcheckgpt: Zero-resource black-box hallucina-\ntion detection for generative large language models. arXiv preprint arXiv:2303.08896 .\nMao, Y ., He, P., Liu, X., Shen, Y ., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented\nretrieval for open-domain question answering. arXiv preprint arXiv:2009.08553 .\nMartin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\nfor large graph layout. SPIE Conference on Visualization and Data Analysis (VDA) .\nMicrosoft (2023). The impact of large language models on scientific discovery: a preliminary study\nusing gpt-4.\n13NebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented genera-\ntion with llm based on knowledge graphs. https:\/\/www .nebula-graph .io\/posts\/graph-RAG.\nNeo4J (2024). Project NaLLM. https:\/\/github .com\/neo4j\/NaLLM.\nNewman, M. E. (2006). Modularity and community structure in networks. Proceedings of the\nnational academy of sciences , 103(23):8577\u20138582.\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY . (2023). In-context retrieval-augmented language models. Transactions of the Association for\nComputational Linguistics , 11:1316\u20131331.\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. arXiv preprint arXiv:2310.13848 .\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 .\nScott, K. (2024). Behind the Tech. https:\/\/www .microsoft .com\/en-us\/behind-the-tech.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization","chunk_id":"df50c95dff7da074cbb2f68e88686f88","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":1200,"entities":[{"name":"KOESTEN","type":"PERSON","description":"Koesten is one of the authors of the paper titled \"Talking datasets\u2013understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GREGORY","type":"PERSON","description":"Gregory is one of the authors of the paper titled \"Talking datasets\u2013understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GROTH","type":"PERSON","description":"Groth is one of the authors of the paper titled \"Talking datasets\u2013understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SIMPERL","type":"PERSON","description":"Simperl is one of the authors of the paper titled \"Talking datasets\u2013understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"KURATOV","type":"PERSON","description":"Kuratov is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BULATOV","type":"PERSON","description":"Bulatov is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"ANOKHIN","type":"PERSON","description":"Anokhin is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SOROKIN","type":"PERSON","description":"Sorokin is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BURTSEV","type":"PERSON","description":"Burtsev is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is a technology that provides tools for creating and managing graphs, as mentioned in the 2024 document \"Langchain graphs\"","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LASKAR","type":"PERSON","description":"Laskar is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"HOQUE","type":"PERSON","description":"Hoque is one of the authors of the paper titled \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022\nHoque is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HUANG","type":"PERSON","description":"Huang is one of the authors of the paper titled \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022\nHuang is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020\nHuang is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LEWIS","type":"PERSON","description":"Lewis is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"PEREZ","type":"PERSON","description":"Perez is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"PIKTUS","type":"PERSON","description":"Piktus is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"PETRONI","type":"PERSON","description":"Petroni is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"KARPUKHIN","type":"PERSON","description":"Karpukhin is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GOYAL","type":"PERSON","description":"Goyal is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"KUTTLER","type":"PERSON","description":"Kuttler is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"YIH","type":"PERSON","description":"Yih is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"ROCKTASCHEL","type":"PERSON","description":"Rocktaschel is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LIU","type":"PERSON","description":"Liu is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023\nLiu is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020\nLiu is one of the authors of the paper titled \"Hierarchical transformers for multi-document summarization\" published on arXiv in 2019","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"LIN","type":"PERSON","description":"Lin is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"HEWITT","type":"PERSON","description":"Hewitt is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"PARANJAPE","type":"PERSON","description":"Paranjape is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BEVILACQUA","type":"PERSON","description":"Bevilacqua is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LIANG","type":"PERSON","description":"Liang is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LLAMAINDEX","type":"TECHNOLOGY","description":"LlamaIndex is a technology that provides tools for creating and managing knowledge graphs, as mentioned in the 2024 document \"LlamaIndex Knowledge Graph Index\"","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"MANAKUL","type":"PERSON","description":"Manakul is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LIUSIE","type":"PERSON","description":"Liusie is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GALES","type":"PERSON","description":"Gales is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"MAO","type":"PERSON","description":"Mao is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HE","type":"PERSON","description":"He is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"SHEN","type":"PERSON","description":"Shen is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020\nShen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"GAO","type":"PERSON","description":"Gao is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"HAN","type":"PERSON","description":"Han is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"CHEN","type":"PERSON","description":"Chen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023\nChen is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88","entity_type":"PERSON"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is the organization that conducted the study titled \"The impact of large language models on scientific discovery: a preliminary study using GPT-4\" in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"NEBULAGRAPH","type":"TECHNOLOGY","description":"NebulaGraph is a technology that launched the industry-first graph RAG: Retrieval-augmented generation with LLM based on knowledge graphs, as mentioned in the 2024 document","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"NEO4J","type":"TECHNOLOGY","description":"Neo4J is a technology that developed Project NaLLM, as mentioned in the 2024 document","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"NEWMAN","type":"PERSON","description":"Newman is the author of the paper titled \"Modularity and community structure in networks\" published in the Proceedings of the National Academy of Sciences in 2006","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"RAM","type":"PERSON","description":"Ram is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LEVINE","type":"PERSON","description":"Levine is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"DALMEDIGOS","type":"PERSON","description":"Dalmedigos is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"MUHLGAY","type":"PERSON","description":"Muhlgay is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SHASHUA","type":"PERSON","description":"Shashua is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LEYTON-BROWN","type":"PERSON","description":"Leyton-Brown is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SHOHAM","type":"PERSON","description":"Shoham is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"RANADE","type":"PERSON","description":"Ranade is one of the authors of the paper titled \"Fabula: Intelligence report generation using retrieval-augmented narrative construction\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"JOSHI","type":"PERSON","description":"Joshi is one of the authors of the paper titled \"Fabula: Intelligence report generation using retrieval-augmented narrative construction\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SARTHI","type":"PERSON","description":"Sarthi is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"ABDULLAH","type":"PERSON","description":"Abdullah is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"TULI","type":"PERSON","description":"Tuli is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"KHANNA","type":"PERSON","description":"Khanna is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GOLDIE","type":"PERSON","description":"Goldie is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"MANNING","type":"PERSON","description":"Manning is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SCOTT","type":"PERSON","description":"Scott is the author of the document titled \"Behind the Tech\" published in 2024","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SHAO","type":"PERSON","description":"Shao is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"GONG","type":"PERSON","description":"Gong is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"DUAN","type":"PERSON","description":"Duan is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"LAPATA","type":"PERSON","description":"Lapata is one of the authors of the paper titled \"Hierarchical transformers for multi-document summarization\" published on arXiv in 2019","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SU","type":"PERSON","description":"Su is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"XU","type":"PERSON","description":"Xu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"YU","type":"PERSON","description":"Yu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"SIDDIQUE","type":"PERSON","description":"Siddique is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"BAREZI","type":"PERSON","description":"Barezi is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"},{"name":"FUNG","type":"PERSON","description":"Fung is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020","source_id":"df50c95dff7da074cbb2f68e88686f88"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"KOESTEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Koesten is one of the authors of the paper titled \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GREGORY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gregory is one of the authors of the paper titled \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GROTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Groth is one of the authors of the paper titled \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SIMPERL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simperl is one of the authors of the paper titled \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"KURATOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuratov is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BULATOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bulatov is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"ANOKHIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anokhin is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SOROKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sorokin is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BURTSEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Burtsev is one of the authors of the paper titled \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is a technology that provides tools for creating and managing graphs, as mentioned in the 2024 document \"Langchain graphs\"<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LASKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Laskar is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"HOQUE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hoque is one of the authors of the paper titled \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022Hoque is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang is one of the authors of the paper titled \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022Huang is one of the authors of the paper titled \"Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models\" published in the Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence in 2020Huang is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"PEREZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Perez is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"PIKTUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piktus is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"PETRONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Petroni is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"KARPUKHIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karpukhin is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GOYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goyal is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"KUTTLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuttler is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"YIH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yih is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"ROCKTASCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rocktaschel is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023Liu is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020Liu is one of the authors of the paper titled \"Hierarchical transformers for multi-document summarization\" published on arXiv in 2019<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lin is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"HEWITT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hewitt is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"PARANJAPE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Paranjape is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BEVILACQUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bevilacqua is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liang is one of the authors of the paper titled \"Lost in the middle: How language models use long contexts\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LLAMAINDEX\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LlamaIndex is a technology that provides tools for creating and managing knowledge graphs, as mentioned in the 2024 document \"LlamaIndex Knowledge Graph Index\"<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"MANAKUL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Manakul is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LIUSIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liusie is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GALES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gales is one of the authors of the paper titled \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mao is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shen is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020Shen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Han is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023Chen is one of the authors of the paper titled \"Generation-augmented retrieval for open-domain question answering\" published on arXiv in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is the organization that conducted the study titled \"The impact of large language models on scientific discovery: a preliminary study using GPT-4\" in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"NEBULAGRAPH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NebulaGraph is a technology that launched the industry-first graph RAG: Retrieval-augmented generation with LLM based on knowledge graphs, as mentioned in the 2024 document<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"NEO4J\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neo4J is a technology that developed Project NaLLM, as mentioned in the 2024 document<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"NEWMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Newman is the author of the paper titled \"Modularity and community structure in networks\" published in the Proceedings of the National Academy of Sciences in 2006<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"RAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ram is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Levine is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"DALMEDIGOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dalmedigos is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"MUHLGAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Muhlgay is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SHASHUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shashua is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LEYTON-BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Leyton-Brown is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SHOHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shoham is one of the authors of the paper titled \"In-context retrieval-augmented language models\" published in the Transactions of the Association for Computational Linguistics in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"RANADE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ranade is one of the authors of the paper titled \"Fabula: Intelligence report generation using retrieval-augmented narrative construction\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshi is one of the authors of the paper titled \"Fabula: Intelligence report generation using retrieval-augmented narrative construction\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SARTHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarthi is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"ABDULLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abdullah is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"TULI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tuli is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"KHANNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khanna is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GOLDIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Goldie is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"MANNING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Manning is one of the authors of the paper titled \"Raptor: Recursive abstractive processing for tree-organized retrieval\" published on arXiv in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SCOTT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Scott is the author of the document titled \"Behind the Tech\" published in 2024<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shao is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"GONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gong is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Duan is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published on arXiv in 2023<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"LAPATA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lapata is one of the authors of the paper titled \"Hierarchical transformers for multi-document summarization\" published on arXiv in 2019<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Su is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"SIDDIQUE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siddique is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"BAREZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barezi is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <node id=\"FUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fung is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d2\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/node>    <edge source=\"KOESTEN\" target=\"GREGORY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Koesten and Gregory co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KOESTEN\" target=\"GROTH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Koesten and Groth co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KOESTEN\" target=\"SIMPERL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Koesten and Simperl co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GREGORY\" target=\"GROTH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Gregory and Groth co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GREGORY\" target=\"SIMPERL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Gregory and Simperl co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"GROTH\" target=\"SIMPERL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Groth and Simperl co-authored the paper \"Talking datasets&#8211;understanding data sensemaking behaviours\" published in the International Journal of Human-Computer Studies in 2021<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV\" target=\"BULATOV\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kuratov and Bulatov co-authored the paper \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV\" target=\"ANOKHIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kuratov and Anokhin co-authored the paper \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV\" target=\"SOROKIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kuratov and Sorokin co-authored the paper \"In search of needles in a 11m haystack: Recurrent memory finds what LLMs miss\" published in 2024<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"KURATOV\" target=\"BURTSEV\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Kuratov and Burtsev co-authored the paper \"In search of needles in a 11m haystack: Recurrent memory finds what(\"entity\"<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LASKAR\" target=\"HOQUE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Laskar and Hoque co-authored the paper \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LASKAR\" target=\"HUANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Laskar and Huang co-authored the paper \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"HOQUE\" target=\"HUANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Hoque and Huang co-authored the paper \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\" published in Computational Linguistics in 2022<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"LIU\" target=\"LAPATA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Liu and Lapata co-authored the paper \"Hierarchical transformers for multi-document summarization\" published on arXiv in 2019<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SU\" target=\"XU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su and Xu co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SU\" target=\"YU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su and Yu co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SU\" target=\"SIDDIQUE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su and Siddique co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SU\" target=\"BAREZI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su and Barezi co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SU\" target=\"FUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Su and Fung co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"XU\" target=\"YU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu and Yu co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"XU\" target=\"SIDDIQUE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu and Siddique co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"XU\" target=\"BAREZI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu and Barezi co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"XU\" target=\"FUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu and Fung co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"YU\" target=\"SIDDIQUE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yu and Siddique co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"YU\" target=\"BAREZI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yu and Barezi co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"YU\" target=\"FUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yu and Fung co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SIDDIQUE\" target=\"BAREZI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Siddique and Barezi co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"SIDDIQUE\" target=\"FUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Siddique and Fung co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>    <edge source=\"BAREZI\" target=\"FUNG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Barezi and Fung co-authored the paper \"Caire-covid: A question answering and query-focused multi-document summarization\" published in 2020<\/data>      <data key=\"d6\">df50c95dff7da074cbb2f68e88686f88<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","chunk":"., Duan, N., and Chen, W. (2023). Enhancing retrieval-\naugmented large language models with iterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294 .\nSu, D., Xu, Y ., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A ques-\ntion answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. arXiv preprint arXiv:2005.03975 .\nTang, Y . and Yang, Y . (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. arXiv preprint arXiv:2401.15391 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 .\nTraag, V . A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. Scientific Reports , 9(1).\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. ArXiv , abs\/2305.04676.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint\narXiv:2212.10509 .\nWang, J., Liang, Y ., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 .\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. arXiv preprint arXiv:2402.11891 .\nWang, Y ., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\nXu, Y . and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint\narXiv:2106.00104 .\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) .\nYao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\nand Information Systems , 53:297\u2013336.\n14Yao, L., Peng, J., Mao, C., and Luo, Y . (2023). Exploring large language models for knowledge\ngraph completion.\nZhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116 .\nZhang, Y ., Zhang, Y ., Gan, Y ., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\naugmented generation based large language models. arXiv preprint arXiv:2402.15301 .\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems , 36.\n15","chunk_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","document_ids":["0668cddc5f873265ba50da5a0a06edad"],"n_tokens":987,"entities":[{"name":"DUAN","type":"PERSON","description":"Duan is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CHEN","type":"PERSON","description":"Chen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SU","type":"PERSON","description":"Su is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XU","type":"PERSON","description":"Xu is one of the authors of the paper titled \"Text summarization with latent queries\" published in 2021\nXu is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023\nXu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"YU","type":"PERSON","description":"Yu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SIDDIQUE","type":"PERSON","description":"Siddique is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BAREZI","type":"PERSON","description":"Barezi is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"FUNG","type":"PERSON","description":"Fung is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TANG","type":"PERSON","description":"Tang is one of the authors of the paper titled \"MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"YANG","type":"PERSON","description":"Yang is one of the authors of the paper titled \"MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries\" published in 2024\nYang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TOUVRON","type":"PERSON","description":"Touvron is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MARTIN","type":"PERSON","description":"Martin is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"STONE","type":"PERSON","description":"Stone is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ALBERT","type":"PERSON","description":"Albert is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ALMAHAIRI","type":"PERSON","description":"Almahairi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BABAEI","type":"PERSON","description":"Babaei is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BASHLYKOV","type":"PERSON","description":"Bashlykov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BATRA","type":"PERSON","description":"Batra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BHARGAVA","type":"PERSON","description":"Bhargava is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BHOSALE","type":"PERSON","description":"Bhosale is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TRAAG","type":"PERSON","description":"Traag is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WALTMAN","type":"PERSON","description":"Waltman is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"VAN ECK","type":"PERSON","description":"Van Eck is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TRAJANOSKA","type":"PERSON","description":"Trajanoska is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"STOJANOV","type":"PERSON","description":"Stojanov is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TRAJANOV","type":"PERSON","description":"Trajanov is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"TRIVEDI","type":"PERSON","description":"Trivedi is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BALASUBRAMANIAN","type":"PERSON","description":"Balasubramanian is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KHOT","type":"PERSON","description":"Khot is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SABHARWAL","type":"PERSON","description":"Sabharwal is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WANG J","type":"PERSON","description":"Wang J. is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LIANG","type":"PERSON","description":"Liang is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MENG","type":"PERSON","description":"Meng is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SUN","type":"PERSON","description":"Sun is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SHI","type":"PERSON","description":"Shi is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LI","type":"PERSON","description":"Li is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023\nLi is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"QU","type":"PERSON","description":"Qu is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHOU","type":"PERSON","description":"Zhou is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WANG S","type":"PERSON","description":"Wang S. is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"KHRAMTSOVA","type":"PERSON","description":"Khramtsova is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHUANG S","type":"PERSON","description":"Zhuang S. is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZUCCON","type":"PERSON","description":"Zuccon is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WANG Y","type":"PERSON","description":"Wang Y. is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LIPKA","type":"PERSON","description":"Lipka is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ROSSI","type":"PERSON","description":"Rossi is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SIU","type":"PERSON","description":"Siu is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023\nZhang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018\nZhang is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024\nZhang is one of the authors of the paper titled \"Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"DERR","type":"PERSON","description":"Derr is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LAPATA","type":"PERSON","description":"Lapata is one of the authors of the paper titled \"Text summarization with latent queries\" published in 2021","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"QI","type":"PERSON","description":"Qi is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"BENGIO","type":"PERSON","description":"Bengio is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"COHEN","type":"PERSON","description":"Cohen is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SALAKHUTDINOV","type":"PERSON","description":"Salakhutdinov is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MANNING","type":"PERSON","description":"Manning is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"YAO","type":"PERSON","description":"Yao is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017\nYao is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023\nYao is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WAN","type":"PERSON","description":"Wan is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XIAO","type":"PERSON","description":"Xiao is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"PENG","type":"PERSON","description":"Peng is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"MAO","type":"PERSON","description":"Mao is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LUO","type":"PERSON","description":"Luo is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"GAN","type":"PERSON","description":"Gan is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHENG","type":"PERSON","description":"Zheng is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"CHIANG","type":"PERSON","description":"Chiang is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"SHENG","type":"PERSON","description":"Sheng is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ZHUANG","type":"PERSON","description":"Zhuang is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"WU","type":"PERSON","description":"Wu is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"LIN","type":"PERSON","description":"Lin is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"XING","type":"PERSON","description":"Xing is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"PERSON"},{"name":"ENHANCING RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS WITH ITERATIVE RETRIEVAL-GENERATION SYNERGY","type":"DOCUMENT","description":"The paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" was published in 2023","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"DOCUMENT"},{"name":"CAIRE-COVID","type":"TECHNOLOGY","description":"Caire-covid is a question answering and query-focused multi-document summarization system for covid-19 scholarly information management","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"MULTIHOP-RAG","type":"TECHNOLOGY","description":"MultiHop-RAG is a system for benchmarking retrieval-augmented generation for multi-hop queries","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3","entity_type":"TECHNOLOGY"},{"name":"LLAMA(\"ENTITY\"","type":"DUAN","description":"PERSON","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"},{"name":"LLAMA 2","type":"TECHNOLOGY","description":"Llama 2 is a set of open foundation and fine-tuned chat models","source_id":"d4c8ce26fd0f9a7bc6dad0efa1ce98e3"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Duan is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is one of the authors of the paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Su is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is one of the authors of the paper titled \"Text summarization with latent queries\" published in 2021Xu is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023Xu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIDDIQUE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siddique is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAREZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barezi is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fung is one of the authors of the paper titled \"Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly information management\" published in 2020<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tang is one of the authors of the paper titled \"MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is one of the authors of the paper titled \"MultiHop-RAG: Benchmarking retrieval-augmented generation for multi-hop queries\" published in 2024Yang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TOUVRON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STONE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stone is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Albert is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALMAHAIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Almahairi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BABAEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Babaei is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BASHLYKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bashlykov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Batra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BHARGAVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bhargava is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BHOSALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bhosale is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAAG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Traag is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WALTMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Waltman is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VAN ECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Van Eck is one of the authors of the paper titled \"From Louvain to Leiden: guaranteeing well-connected communities\" published in 2019<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAJANOSKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trajanoska is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STOJANOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stojanov is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRAJANOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trajanov is one of the authors of the paper titled \"Enhancing knowledge graph construction using large language models\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRIVEDI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trivedi is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BALASUBRAMANIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Balasubramanian is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khot is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SABHARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sabharwal is one of the authors of the paper titled \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions\" published in 2022<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG J\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang J. is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liang is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meng is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sun is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shi is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023Li is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qu is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou is one of the authors of the paper titled \"Is chatgpt a good nlg evaluator? a preliminary study\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG S\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang S. is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KHRAMTSOVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khramtsova is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHUANG S\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuang S. is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZUCCON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zuccon is one of the authors of the paper titled \"Feb4rag: Evaluating federated search in the context of retrieval augmented generation\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG Y\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang Y. is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIPKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lipka is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROSSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rossi is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siu is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023Zhang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018Zhang is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024Zhang is one of the authors of the paper titled \"Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DERR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Derr is one of the authors of the paper titled \"Knowledge graph prompting for multi-document question answering\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAPATA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lapata is one of the authors of the paper titled \"Text summarization with latent queries\" published in 2021<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qi is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BENGIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bengio is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cohen is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SALAKHUTDINOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Salakhutdinov is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MANNING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Manning is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in 2018<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017Yao is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023Yao is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wan is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiao is one of the authors of the paper titled \"Recent advances in document summarization\" published in 2017<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peng is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mao is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luo is one of the authors of the paper titled \"Exploring large language models for knowledge graph completion\" published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gan is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is one of the authors of the paper titled \"Causal graph discovery with retrieval-augmented generation based large language models\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chiang is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sheng is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuang is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wu is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lin is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xing is one of the authors of the paper titled \"Judging llm-as-a-judge with mt-bench and chatbot arena\" published in 2024<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ENHANCING RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS WITH ITERATIVE RETRIEVAL-GENERATION SYNERGY\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy\" was published in 2023<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"CAIRE-COVID\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Caire-covid is a question answering and query-focused multi-document summarization system for covid-19 scholarly information management<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MULTIHOP-RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MultiHop-RAG is a system for benchmarking retrieval-augmented generation for multi-hop queries<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLAMA(&quot;ENTITY&quot;\">      <data key=\"d0\">DUAN<\/data>      <data key=\"d1\">PERSON<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>    <node id=\"LLAMA 2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama 2 is a set of open foundation and fine-tuned chat models<\/data>      <data key=\"d2\">d4c8ce26fd0f9a7bc6dad0efa1ce98e3<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"93cb0d0456e0822b5fe30a3e627405f8","chunk":"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in\nLanguage Models\nAndy Zhou1 2Kai Yan1Michal Shlapentokh-Rothman1Haohan Wang1Yu-Xiong Wang1\nAbstract\nWhile language models (LMs) have shown po-\ntential across a range of decision-making tasks,\ntheir reliance on simple acting processes limits\ntheir broad deployment as autonomous agents. In\nthis paper, we introduce Language Agent Tree\nSearch (LATS) \u2013 the first general framework that\nsynergizes the capabilities of LMs in reasoning,\nacting, and planning. By leveraging the in-context\nlearning ability of LMs, we integrate Monte Carlo\nTree Search into LATS to enable LMs as agents,\nalong with LM-powered value functions and\nself-reflections for proficient exploration and en-\nhanced decision-making. A key feature of our ap-\nproach is the incorporation of an environment for\nexternal feedback, which offers a more deliberate\nand adaptive problem-solving mechanism that sur-\npasses the constraints of existing techniques. Our\nexperimental evaluation across diverse domains,\nincluding programming, interactive question-\nanswering (QA), web navigation, and math, val-\nidates the effectiveness and generality of LATS\nin decision-making while maintaining compet-\nitive or improved reasoning performance. No-\ntably, LATS achieves state-of-the-art pass@1 ac-\ncuracy (92.7%) for programming on HumanEval\nwith GPT-4 and demonstrates gradient-free per-\nformance (average score of 75.9) comparable to\ngradient-based fine-tuning for web navigation on\nWebShop with GPT-3.5. Code can be found\nathttps:\/\/github.com\/lapisrocks\/\nLanguageAgentTreeSearch .\n1. Introduction\nGeneral autonomous agents capable of reasoning and\ndecision-making in a variety of environments (Wooldridge\n1University of Illinois Urbana-Champaign.2Lapis Labs. Corre-\nspondence to: Andy Zhou <andyz3@illinois.edu >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nFigure 1. Overview of LATS. Serving as a unified framework,\nLATS leverages an external environment and an MCTS-based\nsearch algorithm to improve reasoning and decision-making.\nand Jennings, 1995) have been of longstanding interest in\nthe field of artificial intelligence. While this has tradition-\nally been studied in reinforcement learning, the recent rise\nof language models (LMs) (Brown et al., 2020; Chowdh-\nery et al., 2023; Touvron et al., 2023; OpenAI, 2023) with\nstrong reasoning and general adaptability offers an alter-\nnative paradigm. Not only have LMs excelled in standard\nnatural language processing (NLP) tasks such as summariza-\ntion (Nallapati et al., 2016) and language inference (Bow-\nman et al., 2015), but they have also been adapted to an\nincreasingly diverse set of tasks that often require advanced\ncommon-sense reasoning or quantitative skills (Cobbe et al.,\n2021; Saparov and He, 2023). In addition, LMs are capable\nof performing in complex environments that involve knowl-\nedge and reasoning, such as web navigation (Yao et al.,\n2022; Deng et al., 2023), tool-use (Schick et al., 2023), and\nopen-ended games (Fan et al., 2022).\nReasoning and acting abilities have been further improved\nby prompting techniques that augment LMs with feedback\nor observations from an external environment, as exempli-\nfied by ReAct (Yao et al., 2023b) and other work (Gao et al.,\n2023; Shinn et al., 2023). This eliminates the need to rely en-\ntirely on the base abilities of LMs, enhancing them through\nexternal tools or semantic feedback. Despite such strengths,\nthese methods are reflexive and fall short of humans\u2019 deliber-\nate and thoughtful decision-making characteristics to solve\nproblems (Sloman, 1996; Evans, 2010). In particular, they\nfail to consider multiple reasoning paths or to plan ahead.\nRecent search-guided LM work (Xie et al., 2023; Yao et al.,\n2023a; Hao et al., 2023) addresses this issue by searching\nover multiple reasoning chains. While enabling planning,\n1arXiv:2310.04406v3  [cs.AI]  6 Jun 2024Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nsuch methods operate in isolation, lacking the incorporation\nof external feedback that can improve reasoning.\nTo overcome these challenges, we propose Language Agent\nTree Search (LATS) \u2013 a unified framework for decision-\nmaking and reasoning with language models. As illustrated\nin Fig. 1, LATS synergizes LM reasoning, acting, and plan-\nning strategies by expanding ReAct (Yao et al., 2023b) into a\nsearch over a combinatorial space of possible reasoning and\nacting steps. This effort is nontrivial \u2013 adapting search algo-\nrithms to language agents and shifting from non-interactive\ntasks to interactive ones requires a substantial novel design\non nodes, prompts, and search algorithms. In particular,\nnodes and prompts must effectively store and retrieve ex-\nternal feedback, with the search algorithm incorporating\nthis information into useful heuristics for value assignment.\nIndeed, our empirical evaluation, as demonstrated on Hot-\nPotQA (Yang et al., 2018)","chunk_id":"93cb0d0456e0822b5fe30a3e627405f8","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LANGUAGE AGENT TREE SEARCH (LATS)","type":"TECHNOLOGY","description":"Language Agent Tree Search (LATS) is a general framework that integrates the capabilities of language models (LMs) in reasoning, acting, and planning by leveraging Monte Carlo Tree Search, LM-powered value functions, and self-reflections for proficient exploration and enhanced decision-making","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"ANDY ZHOU","type":"PERSON","description":"Andy Zhou is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"KAI YAN","type":"PERSON","description":"Kai Yan is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"MICHAL SHLAPENTOKH-ROTHMAN","type":"PERSON","description":"Michal Shlapentokh-Rothman is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"HAOHAN WANG","type":"PERSON","description":"Haohan Wang is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"YU-XIONG WANG","type":"PERSON","description":"Yu-Xiong Wang is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN","type":"ORGANIZATION","description":"University of Illinois Urbana-Champaign is the institution affiliated with the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"LAPIS LABS","type":"ORGANIZATION","description":"Lapis Labs is the institution affiliated with Andy Zhou, one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"TECHNOLOGY","description":"Monte Carlo Tree Search (MCTS) is a search algorithm integrated into LATS to enable language models as agents for proficient exploration and enhanced decision-making","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a language model used in the experimental evaluation of LATS, achieving state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used in the experimental evaluation of LATS, demonstrating gradient-free performance comparable to gradient-based fine-tuning for web navigation on WebShop","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used to evaluate the programming performance of LATS with GPT-4, where it achieved state-of-the-art pass@1 accuracy (92.7%)","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"WEBSHOP","type":"DATASET","description":"WebShop is a dataset used to evaluate the web navigation performance of LATS with GPT-3.5, where it demonstrated gradient-free performance comparable to gradient-based fine-tuning","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a prompting technique that augments language models with feedback or observations from an external environment, expanded upon by LATS to improve reasoning and decision-making","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used in the empirical evaluation of LATS to demonstrate its effectiveness in decision-making and reasoning","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)","type":"EVENT","description":"The International Conference on Machine Learning (ICML) is the conference where the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" was presented","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"PMLR","type":"ORGANIZATION","description":"PMLR (Proceedings of Machine Learning Research) is the publisher of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"VIENNA, AUSTRIA","type":"LOCATION","description":"Vienna, Austria is the location where the 41st International Conference on Machine Learning (ICML) was held","source_id":"93cb0d0456e0822b5fe30a3e627405f8"},{"name":"BROWN ET AL.","type":"PERSON","description":"Brown et al. are authors referenced in the paper for their work on language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"CHOWDHERY ET AL.","type":"PERSON","description":"Chowdhery et al. are authors referenced in the paper for their work on language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"TOUVRON ET AL.","type":"PERSON","description":"Touvron et al. are authors referenced in the paper for their work on language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is an organization referenced in the paper for their work on language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"ORGANIZATION"},{"name":"NALLAPATI ET AL.","type":"PERSON","description":"Nallapati et al. are authors referenced in the paper for their work on summarization using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"BOWMAN ET AL.","type":"PERSON","description":"Bowman et al. are authors referenced in the paper for their work on language inference using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"COBBE ET AL.","type":"PERSON","description":"Cobbe et al. are authors referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"SAPAROV AND HE","type":"PERSON","description":"Saparov and He are authors referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"YAO ET AL.","type":"PERSON","description":"Yao et al. are authors referenced in the paper for their work on web navigation using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"DENG ET AL.","type":"PERSON","description":"Deng et al. are authors referenced in the paper for their work on web navigation using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"SCHICK ET AL.","type":"PERSON","description":"Schick et al. are authors referenced in the paper for their work on tool-use using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"FAN ET AL.","type":"PERSON","description":"Fan et al. are authors referenced in the paper for their work on open-ended games using language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"GAO ET AL.","type":"PERSON","description":"Gao et al. are authors referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"SHINN ET AL.","type":"PERSON","description":"Shinn et al. are authors referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"SLOMAN","type":"PERSON","description":"Sloman is an author referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"EVANS","type":"PERSON","description":"Evans is an author referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"XIE ET AL.","type":"PERSON","description":"Xie et al. are authors referenced in the paper for their work on search-guided language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"HAO ET AL.","type":"PERSON","description":"Hao et al. are authors referenced in the paper for their work on search-guided language models","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"},{"name":"WOOLDRIDGE AND JENNINGS","type":"PERSON","description":"Wooldridge and Jennings are authors referenced in the paper for their work on general autonomous agents capable of reasoning and decision-making","source_id":"93cb0d0456e0822b5fe30a3e627405f8","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LANGUAGE AGENT TREE SEARCH (LATS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Agent Tree Search (LATS) is a general framework that integrates the capabilities of language models (LMs) in reasoning, acting, and planning by leveraging Monte Carlo Tree Search, LM-powered value functions, and self-reflections for proficient exploration and enhanced decision-making<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"ANDY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zhou is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"KAI YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kai Yan is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"MICHAL SHLAPENTOKH-ROTHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michal Shlapentokh-Rothman is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"HAOHAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haohan Wang is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"YU-XIONG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu-Xiong Wang is one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">University of Illinois Urbana-Champaign is the institution affiliated with the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"LAPIS LABS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Lapis Labs is the institution affiliated with Andy Zhou, one of the authors of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a search algorithm integrated into LATS to enable language models as agents for proficient exploration and enhanced decision-making<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a language model used in the experimental evaluation of LATS, achieving state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used in the experimental evaluation of LATS, demonstrating gradient-free performance comparable to gradient-based fine-tuning for web navigation on WebShop<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used to evaluate the programming performance of LATS with GPT-4, where it achieved state-of-the-art pass@1 accuracy (92.7%)<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used to evaluate the web navigation performance of LATS with GPT-3.5, where it demonstrated gradient-free performance comparable to gradient-based fine-tuning<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a prompting technique that augments language models with feedback or observations from an external environment, expanded upon by LATS to improve reasoning and decision-making<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used in the empirical evaluation of LATS to demonstrate its effectiveness in decision-making and reasoning<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The International Conference on Machine Learning (ICML) is the conference where the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" was presented<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"PMLR\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">PMLR (Proceedings of Machine Learning Research) is the publisher of the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"VIENNA, AUSTRIA\">      <data key=\"d0\">LOCATION<\/data>      <data key=\"d1\">Vienna, Austria is the location where the 41st International Conference on Machine Learning (ICML) was held<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/node>    <node id=\"BROWN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown et al. are authors referenced in the paper for their work on language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHOWDHERY ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chowdhery et al. are authors referenced in the paper for their work on language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TOUVRON ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Touvron et al. are authors referenced in the paper for their work on language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is an organization referenced in the paper for their work on language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NALLAPATI ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nallapati et al. are authors referenced in the paper for their work on summarization using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOWMAN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowman et al. are authors referenced in the paper for their work on language inference using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COBBE ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe et al. are authors referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAPAROV AND HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saparov and He are authors referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. are authors referenced in the paper for their work on web navigation using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Deng et al. are authors referenced in the paper for their work on web navigation using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SCHICK ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schick et al. are authors referenced in the paper for their work on tool-use using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FAN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan et al. are authors referenced in the paper for their work on open-ended games using language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAO ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao et al. are authors referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHINN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn et al. are authors referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SLOMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sloman is an author referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evans is an author referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIE ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xie et al. are authors referenced in the paper for their work on search-guided language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAO ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao et al. are authors referenced in the paper for their work on search-guided language models<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WOOLDRIDGE AND JENNINGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wooldridge and Jennings are authors referenced in the paper for their work on general autonomous agents capable of reasoning and decision-making<\/data>      <data key=\"d2\">93cb0d0456e0822b5fe30a3e627405f8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LATS integrates Monte Carlo Tree Search (MCTS) to enable language models as agents for proficient exploration and enhanced decision-making<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GPT-4\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses GPT-4 in its experimental evaluation, achieving state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GPT-3.5\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses GPT-3.5 in its experimental evaluation, demonstrating gradient-free performance comparable to gradient-based fine-tuning for web navigation on WebShop<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"HUMANEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">HumanEval is a dataset used to evaluate the programming performance of LATS with GPT-4<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"WEBSHOP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">WebShop is a dataset used to evaluate the web navigation performance of LATS with GPT-3.5<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"REACT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS expands upon ReAct to improve reasoning and decision-making<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"HOTPOTQA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">HotPotQA is a dataset used in the empirical evaluation of LATS<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">The paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" was presented at the International Conference on Machine Learning (ICML)<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"PMLR\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">PMLR published the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"BROWN ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Brown et al. are referenced in the paper for their work on language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"CHOWDHERY ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Chowdhery et al. are referenced in the paper for their work on language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"TOUVRON ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Touvron et al. are referenced in the paper for their work on language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"OPENAI\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">OpenAI is referenced in the paper for their work on language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"NALLAPATI ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Nallapati et al. are referenced in the paper for their work on summarization using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"BOWMAN ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Bowman et al. are referenced in the paper for their work on language inference using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"COBBE ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Cobbe et al. are referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"SAPAROV AND HE\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Saparov and He are referenced in the paper for their work on tasks requiring advanced common-sense reasoning or quantitative skills using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"YAO ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Yao et al. are referenced in the paper for their work on web navigation using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"DENG ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Deng et al. are referenced in the paper for their work on web navigation using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"SCHICK ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Schick et al. are referenced in the paper for their work on tool-use using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"FAN ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Fan et al. are referenced in the paper for their work on open-ended games using language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"GAO ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Gao et al. are referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"SHINN ET AL.\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Shinn et al. are referenced in the paper for their work on prompting techniques that augment language models with feedback or observations from an external environment<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"SLOMAN\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Sloman is referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"EVANS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Evans is referenced in the paper for their work on deliberate and thoughtful decision-making characteristics in humans<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"XIE ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Xie et al. are referenced in the paper for their work on search-guided language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"HAO ET AL.\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Hao et al. are referenced in the paper for their work on search-guided language models<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH (LATS)\" target=\"WOOLDRIDGE AND JENNINGS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Wooldridge and Jennings are referenced in the paper for their work on general autonomous agents capable of reasoning and decision-making<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"KAI YAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Andy Zhou and Kai Yan co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"MICHAL SHLAPENTOKH-ROTHMAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Andy Zhou and Michal Shlapentokh-Rothman co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"HAOHAN WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Andy Zhou and Haohan Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"YU-XIONG WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Andy Zhou and Yu-Xiong Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Andy Zhou is affiliated with the University of Illinois Urbana-Champaign<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"ANDY ZHOU\" target=\"LAPIS LABS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Andy Zhou is affiliated with Lapis Labs<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"KAI YAN\" target=\"MICHAL SHLAPENTOKH-ROTHMAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Kai Yan and Michal Shlapentokh-Rothman co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"KAI YAN\" target=\"HAOHAN WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Kai Yan and Haohan Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"KAI YAN\" target=\"YU-XIONG WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Kai Yan and Yu-Xiong Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"KAI YAN\" target=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Kai Yan is affiliated with the University of Illinois Urbana-Champaign<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"MICHAL SHLAPENTOKH-ROTHMAN\" target=\"HAOHAN WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Michal Shlapentokh-Rothman and Haohan Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"MICHAL SHLAPENTOKH-ROTHMAN\" target=\"YU-XIONG WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Michal Shlapentokh-Rothman and Yu-Xiong Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"MICHAL SHLAPENTOKH-ROTHMAN\" target=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Michal Shlapentokh-Rothman is affiliated with the University of Illinois Urbana-Champaign<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"HAOHAN WANG\" target=\"YU-XIONG WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Haohan Wang and Yu-Xiong Wang co-authored the paper titled \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\"<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"HAOHAN WANG\" target=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Haohan Wang is affiliated with the University of Illinois Urbana-Champaign<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"YU-XIONG WANG\" target=\"UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Yu-Xiong Wang is affiliated with the University of Illinois Urbana-Champaign<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>    <edge source=\"INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)\" target=\"VIENNA, AUSTRIA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">The 41st International Conference on Machine Learning (ICML) was held in Vienna, Austria<\/data>      <data key=\"d6\">93cb0d0456e0822b5fe30a3e627405f8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f8e7ed806916bf15245bcb4d52570c26","chunk":" steps. This effort is nontrivial \u2013 adapting search algo-\nrithms to language agents and shifting from non-interactive\ntasks to interactive ones requires a substantial novel design\non nodes, prompts, and search algorithms. In particular,\nnodes and prompts must effectively store and retrieve ex-\nternal feedback, with the search algorithm incorporating\nthis information into useful heuristics for value assignment.\nIndeed, our empirical evaluation, as demonstrated on Hot-\nPotQA (Yang et al., 2018) in Sec. 5.1, reveals that a simple\ncombination of existing methods is inadequate, even failing\nto surpass internal reasoning performance, despite having\naccess to the ground truth answer from the environment.\nOurkey insight underpinning LATS is adapting Monte Carlo\nTree Search (MCTS), inspired by its success in model-based\nreinforcement learning (Silver et al., 2017) and the obser-\nvation that many LM tasks allow reverting to earlier steps ,\nto language agents, repurposing pretrained LMs as agents\nwith LM-powered value functions and self-reflections for\ncleverer exploration. Leveraging the general capabilities\nand in-context learning abilities of modern LMs, we use\nlanguage as an interface between each component, allowing\nLATS to adapt planning to environmental conditions with-\nout additional training . To the best of our knowledge, LATS\nisthe first framework that incorporates reasoning, acting,\nand planning to enhance LM performance. Notably, LATS\ndoubles the performance of ReAct (Yao et al., 2023b) on\nHotPotQA (Yang et al., 2018) and raises the average score\nby22.1on WebShop (Yao et al., 2022) with GPT-3.5. When\nused with GPT-4, LATS achieves a 92.7Pass@1 rate on\nHumanEval (Chen et al., 2021), setting the state of the art.\nOurcontributions are the following: 1) We introduce LATS,\na framework based on Monte Carlo Tree Search to construct\nthe best trajectory from sampled actions, enabling more flex-\nible and adaptive problem-solving compared with reflexive\nprompting methods. 2) We propose a novel value function\nthat guides the search process and incorporates successful\nheuristics such as self-refinement and self-consistency. 3)\nBy integrating external feedback and self-reflection, LATS\nenhances model sensibility and enables agents to learn from\nexperience, surpassing reasoning-based search methods.\nThrough experiments across diverse domains, including pro-\ngramming, interactive question-answering (QA), web navi-\ngation, and math, we demonstrate the versatility of LATS\nfor enhancing autonomous reasoning and decision-making.2. Related Work\nLMs for reasoning. For LMs, reasoning involves decom-\nposing complex inputs into sequential intermediate steps\ntowards a final answer (Cobbe et al., 2021), demonstrated\nwith chain-of-thought (CoT) prompting (Wei et al., 2022)\nand its variants (Wei et al., 2022; Kojima et al., 2022; Wang\net al., 2022). However, these methods, which create chains\nautoregressively in a single step, often suffer from error\npropagation as the number of steps increases (Guo et al.,\n2018; Chen et al., 2023b), due to compound errors. Various\nadvancements aim to mitigate this issue; some approaches,\nsuch as self-consistency (Wang et al., 2022), employ ma-\njority voting over sampled chains, while others focus on\nmulti-step decomposition, such as least-to-most prompt-\ning (Zhou et al., 2022). Recently, CoT has been improved\nwith search algorithms (Yao et al., 2023a; Hao et al., 2023;\nBesta et al., 2023) that can sample trajectories more effec-\ntively. Tree-of-thought (ToT) prompting (Yao et al., 2023a)\nuses DFS or BFS-based (depth\/breadth-first) search guided\nby an LM-generated heuristic, while reasoning via planning\n(RAP) (Hao et al., 2023) uses MCTS with rollouts simu-\nlated by LMs. However, they rely solely on LM internal\nknowledge and cannot adapt to useful external feedback.\nLMs for acting. The strong reasoning and common-sense\nabilities of LMs have been further adapted for decision-\nmaking or acting tasks as a policy model in interactive\nenvironments. In robotics, LMs have been employed as\nhigh-level controllers of control policies (Ahn et al., 2022;\nHuang et al., 2022; Driess et al., 2023). Similar work (Baker\net al., 2022; Wang et al., 2023) has also adapted LM agents\nto complex multimodal games such as Minecraft (Guss et al.,\n2019; Fan et al., 2022). LMs are particularly useful in text-\nbased environments (Liu et al., 2018; Shridhar et al., 2020;\nLiu et al., 2024), where acting-based prompting techniques\nsuch as ReAct (Yao et al., 2023b) have seen success. Sim-\nilar to CoT, ReAct is limited by its simplicity and cannot\neffectively adapt to environment conditions. Many exten-\nsions have been proposed to address this issue, including\nself-refine (Madaan et al., 2023) and Reflexion (Shinn et al.,\n2023), which use self-improvement","chunk_id":"f8e7ed806916bf15245bcb4d52570c26","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNOLOGY","description":"LATS is a framework based on Monte Carlo Tree Search that constructs the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared with reflexive prompting methods","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"TECHNOLOGY","description":"Monte Carlo Tree Search (MCTS) is a search algorithm inspired by its success in model-based reinforcement learning and is adapted in LATS for language agents","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating the performance of LATS, as mentioned in the text","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a prompting technique used in text-based environments for decision-making or acting tasks, which LATS outperforms","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"WEBSHOP","type":"DATASET","description":"WebShop is a dataset used for evaluating the performance of LATS, as mentioned in the text","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a version of the GPT model used with LATS to achieve significant performance improvements","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a version of the GPT model used with LATS to achieve a 92.7 Pass@1 rate on HumanEval, setting the state of the art","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used for evaluating the performance of LATS with GPT-4, achieving a 92.7 Pass@1 rate","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SELF-REFINEMENT","type":"TECHNIQUE","description":"Self-refinement is a heuristic incorporated into LATS to guide the search process and improve model sensibility","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SELF-CONSISTENCY","type":"TECHNIQUE","description":"Self-consistency is a heuristic incorporated into LATS to guide the search process and improve model sensibility","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"COBEE","type":"PERSON","description":"Cobbe is an author who has worked on reasoning for language models, particularly in decomposing complex inputs into sequential intermediate steps\nCobbe is an author who has worked on reasoning for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"KOJIMA","type":"PERSON","description":"Kojima is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"GUO","type":"PERSON","description":"Guo is an author who has worked on addressing error propagation in chain-of-thought (CoT) prompting for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who has worked on addressing error propagation in chain-of-thought (CoT) prompting for language models\nChen is an author who has worked on the HumanEval dataset","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"ZHOU","type":"PERSON","description":"Zhou is an author who has worked on least-to-most prompting for multi-step decomposition in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"YAO","type":"PERSON","description":"Yao is an author who has worked on improving chain-of-thought (CoT) prompting with search algorithms and tree-of-thought (ToT) prompting","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on reasoning via planning (RAP) using Monte Carlo Tree Search (MCTS) with rollouts simulated by language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"BESTA","type":"PERSON","description":"Besta is an author who has worked on improving chain-of-thought (CoT) prompting with search algorithms","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"AHN","type":"PERSON","description":"Ahn is an author who has worked on adapting language models as high-level controllers in robotics","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"HUANG","type":"PERSON","description":"Huang is an author who has worked on adapting language models as high-level controllers in robotics","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"DRIESS","type":"PERSON","description":"Driess is an author who has worked on adapting language models as high-level controllers in robotics","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"BAKER","type":"PERSON","description":"Baker is an author who has worked on adapting language model agents to complex multimodal games such as Minecraft","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"GUSS","type":"PERSON","description":"Guss is an author who has worked on the Minecraft game\nGuss is an author who has worked on complex multimodal games such as Minecraft","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"FAN","type":"PERSON","description":"Fan is an author who has worked on complex multimodal games such as Minecraft\nFan is an author who has worked on the Minecraft game","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has worked on text-based environments and acting-based prompting techniques for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"SHRIDHAR","type":"PERSON","description":"Shridhar is an author who has worked on text-based environments and acting-based prompting techniques for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who has worked on self-refinement techniques for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on Reflexion, a self-improvement technique for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"PERSON"},{"name":"SILVER","type":"PERSON","description":"Silver is an author who has worked on model-based reinforcement learning and Monte Carlo Tree Search (MCTS)","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"YANG","type":"PERSON","description":"Yang is an author who has worked on the HotPotQA dataset","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"YA0","type":"PERSON","description":"Yao is an author who has worked on the WebShop dataset and ReAct prompting technique","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SEARCH ALGORITHMS","type":"TECHNOLOGY","description":"Search algorithms are methods used to navigate through data or problem spaces to find solutions, and are adapted in LATS for language agents","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"NODES","type":"TECHNOLOGY","description":"Nodes are components in search algorithms that store and retrieve external feedback, playing a crucial role in LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"PROMPTS","type":"TECHNOLOGY","description":"Prompts are inputs given to language models to guide their responses, and are used in LATS to store and retrieve external feedback","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"VALUE ASSIGNMENT","type":"TECHNOLOGY","description":"Value assignment is a process in search algorithms where values are assigned to nodes based on heuristics, and is incorporated in LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"INTERNAL REASONING PERFORMANCE","type":"TECHNOLOGY","description":"Internal reasoning performance refers to the ability of a language model to reason and solve problems without external feedback, which LATS aims to surpass","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MODEL-BASED REINFORCEMENT LEARNING","type":"TECHNOLOGY","description":"Model-based reinforcement learning is a type of machine learning where models are used to simulate environments and make decisions, inspiring the use of MCTS in LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"LM-POWERED VALUE FUNCTIONS","type":"TECHNOLOGY","description":"LM-powered value functions are value functions powered by language models, used in LATS for cleverer exploration","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"SELF-REFLECTIONS","type":"TECHNOLOGY","description":"Self-reflections are techniques where language models reflect on their own outputs to improve performance, used in LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"IN-CONTEXT LEARNING","type":"TECHNOLOGY","description":"In-context learning is a capability of modern language models to learn from the context provided in the input, leveraged by LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"PLANNING","type":"TECHNOLOGY","description":"Planning is the process of creating a sequence of actions to achieve a goal, and is a key component of LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"REASONING","type":"TECHNOLOGY","description":"Reasoning is the process of thinking about something in a logical way to form a conclusion or judgment, and is enhanced by LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"ACTING","type":"TECHNOLOGY","description":"Acting refers to the execution of actions based on decisions made by language models, and is a component of LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"DECISION-MAKING","type":"TECHNOLOGY","description":"Decision-making is the process of making choices by identifying a decision, gathering information, and assessing alternative resolutions, and is enhanced by LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"PROGRAMMING","type":"TECHNOLOGY","description":"Programming is the process of creating a set of instructions that tell a computer how to perform a task, and is one of the domains where LATS is tested","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"INTERACTIVE QUESTION-ANSWERING (QA)","type":"TECHNOLOGY","description":"Interactive question-answering (QA) is a task where language models answer questions based on interaction with the environment, and is one of the domains where LATS is tested","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"WEB NAVIGATION","type":"TECHNOLOGY","description":"Web navigation is the process of browsing the web to find information, and is one of the domains where LATS is tested","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MATH","type":"TECHNOLOGY","description":"Math is the abstract science of number, quantity, and space, and is one of the domains where LATS is tested","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"CHAIN-OF-THOUGHT (COT) PROMPTING","type":"TECHNOLOGY","description":"Chain-of-thought (CoT) prompting is a method for reasoning in language models that involves decomposing complex inputs into sequential intermediate steps","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"ERROR PROPAGATION","type":"TECHNOLOGY","description":"Error propagation refers to the accumulation of errors in a process, which can occur in chain-of-thought (CoT) prompting as the number of steps increases","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MAJORITY VOTING","type":"TECHNOLOGY","description":"Majority voting is a technique where the most common output among multiple samples is chosen, used in self-consistency to mitigate error propagation","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"MULTI-STEP DECOMPOSITION","type":"TECHNOLOGY","description":"Multi-step decomposition is a method for breaking down complex tasks into smaller, manageable steps, used in least-to-most prompting","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"LEAST-TO-MOST PROMPTING","type":"TECHNOLOGY","description":"Least-to-most prompting is a method for multi-step decomposition in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"TREE-OF-THOUGHT (TOT) PROMPTING","type":"TECHNOLOGY","description":"Tree-of-thought (ToT) prompting is a method that uses depth-first or breadth-first search guided by an LM-generated heuristic for reasoning in language models","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"REASONING VIA PLANNING (RAP)","type":"TECHNOLOGY","description":"Reasoning via planning (RAP) is a method that uses Monte Carlo Tree Search (MCTS) with rollouts simulated by language models for reasoning","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"DFS","type":"TECHNOLOGY","description":"DFS (Depth-First Search) is a search algorithm used in tree-of-thought (ToT) prompting","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"BFS","type":"TECHNOLOGY","description":"BFS (Breadth-First Search) is a search algorithm used in tree-of-thought (ToT) prompting","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"POLICY MODEL","type":"TECHNOLOGY","description":"A policy model is a model used to make decisions or take actions in an environment, used in interactive tasks for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"ROBOTICS","type":"TECHNOLOGY","description":"Robotics is the branch of technology that deals with the design, construction, operation, and application of robots, where language models have been used as high-level controllers","source_id":"f8e7ed806916bf15245bcb4d52570c26"},{"name":"CONTROL POLICIES","type":"TECHNOLOGY","description":"Control policies are strategies used to control the actions of robots or other systems, where language models have been used as high-level controllers","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"MULTIMODAL GAMES","type":"TECHNOLOGY","description":"Multimodal games are games that involve multiple modes of interaction, such as visual and textual, where language model agents have been adapted","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"MINECRAFT","type":"TECHNOLOGY","description":"Minecraft is a complex multimodal game where language model agents have been adapted","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"TEXT-BASED ENVIRONMENTS","type":"TECHNOLOGY","description":"Text-based environments are environments where interactions are primarily through text, where language models have been used for acting-based prompting techniques","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a self-improvement technique for language models","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"EXTERNAL FEEDBACK","type":"TECHNOLOGY","description":"External feedback is information from the environment that is used to improve the performance of language models, incorporated in LATS","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"SELF-IMPROVEMENT","type":"TECHNOLOGY","description":"Self-improvement refers to techniques where language models improve their own performance, such as self-refinement and Reflexion","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"ENVIRONMENTAL CONDITIONS","type":"TECHNOLOGY","description":"Environmental conditions refer to the state of the environment that language models must adapt to, which LATS can handle without additional training","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORIES","type":"TECHNOLOGY","description":"Trajectories are sequences of actions or steps taken to achieve a goal, constructed by LATS from sampled actions","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"HEURISTICS","type":"TECHNOLOGY","description":"Heuristics are strategies or principles used to guide decision-making or problem-solving, incorporated in LATS for value assignment and search processes","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"SAMPLED CHAINS","type":"TECHNOLOGY","description":"Sampled chains are sequences of steps generated by language models, used in self-consistency for majority voting","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"ROLLOUTS","type":"TECHNOLOGY","description":"Rollouts are simulations of actions or steps taken to predict outcomes, used in reasoning via planning (RAP) with Monte Carlo Tree Search (MCTS)","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"},{"name":"LM-GENERATED HEURISTIC","type":"TECHNOLOGY","description":"An LM-generated heuristic is a heuristic generated by a language model to guide search processes, used in tree-of-thought (ToT) prompting","source_id":"f8e7ed806916bf15245bcb4d52570c26","entity_type":"TECHNOLOGY"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS is a framework based on Monte Carlo Tree Search that constructs the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared with reflexive prompting methods<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a search algorithm inspired by its success in model-based reinforcement learning and is adapted in LATS for language agents<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating the performance of LATS, as mentioned in the text<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a prompting technique used in text-based environments for decision-making or acting tasks, which LATS outperforms<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used for evaluating the performance of LATS, as mentioned in the text<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a version of the GPT model used with LATS to achieve significant performance improvements<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a version of the GPT model used with LATS to achieve a 92.7 Pass@1 rate on HumanEval, setting the state of the art<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used for evaluating the performance of LATS with GPT-4, achieving a 92.7 Pass@1 rate<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SELF-REFINEMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-refinement is a heuristic incorporated into LATS to guide the search process and improve model sensibility<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-consistency is a heuristic incorporated into LATS to guide the search process and improve model sensibility<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"COBEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe is an author who has worked on reasoning for language models, particularly in decomposing complex inputs into sequential intermediate stepsCobbe is an author who has worked on reasoning for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KOJIMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kojima is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guo is an author who has worked on addressing error propagation in chain-of-thought (CoT) prompting for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who has worked on addressing error propagation in chain-of-thought (CoT) prompting for language modelsChen is an author who has worked on the HumanEval dataset<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou is an author who has worked on least-to-most prompting for multi-step decomposition in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on improving chain-of-thought (CoT) prompting with search algorithms and tree-of-thought (ToT) prompting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on reasoning via planning (RAP) using Monte Carlo Tree Search (MCTS) with rollouts simulated by language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BESTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Besta is an author who has worked on improving chain-of-thought (CoT) prompting with search algorithms<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AHN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahn is an author who has worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang is an author who has worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DRIESS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Driess is an author who has worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BAKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baker is an author who has worked on adapting language model agents to complex multimodal games such as Minecraft<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guss is an author who has worked on the Minecraft gameGuss is an author who has worked on complex multimodal games such as Minecraft<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan is an author who has worked on complex multimodal games such as MinecraftFan is an author who has worked on the Minecraft game<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has worked on text-based environments and acting-based prompting techniques for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHRIDHAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shridhar is an author who has worked on text-based environments and acting-based prompting techniques for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who has worked on self-refinement techniques for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on Reflexion, a self-improvement technique for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SILVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Silver is an author who has worked on model-based reinforcement learning and Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who has worked on the HotPotQA dataset<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"YA0\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on the WebShop dataset and ReAct prompting technique<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search algorithms are methods used to navigate through data or problem spaces to find solutions, and are adapted in LATS for language agents<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"NODES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Nodes are components in search algorithms that store and retrieve external feedback, playing a crucial role in LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompts are inputs given to language models to guide their responses, and are used in LATS to store and retrieve external feedback<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"VALUE ASSIGNMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Value assignment is a process in search algorithms where values are assigned to nodes based on heuristics, and is incorporated in LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"INTERNAL REASONING PERFORMANCE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Internal reasoning performance refers to the ability of a language model to reason and solve problems without external feedback, which LATS aims to surpass<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MODEL-BASED REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Model-based reinforcement learning is a type of machine learning where models are used to simulate environments and make decisions, inspiring the use of MCTS in LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"LM-POWERED VALUE FUNCTIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LM-powered value functions are value functions powered by language models, used in LATS for cleverer exploration<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"SELF-REFLECTIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-reflections are techniques where language models reflect on their own outputs to improve performance, used in LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">In-context learning is a capability of modern language models to learn from the context provided in the input, leveraged by LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"PLANNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Planning is the process of creating a sequence of actions to achieve a goal, and is a key component of LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reasoning is the process of thinking about something in a logical way to form a conclusion or judgment, and is enhanced by LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"ACTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Acting refers to the execution of actions based on decisions made by language models, and is a component of LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"DECISION-MAKING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Decision-making is the process of making choices by identifying a decision, gathering information, and assessing alternative resolutions, and is enhanced by LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Programming is the process of creating a set of instructions that tell a computer how to perform a task, and is one of the domains where LATS is tested<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Interactive question-answering (QA) is a task where language models answer questions based on interaction with the environment, and is one of the domains where LATS is tested<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"WEB NAVIGATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Web navigation is the process of browsing the web to find information, and is one of the domains where LATS is tested<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Math is the abstract science of number, quantity, and space, and is one of the domains where LATS is tested<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT) PROMPTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-thought (CoT) prompting is a method for reasoning in language models that involves decomposing complex inputs into sequential intermediate steps<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"ERROR PROPAGATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Error propagation refers to the accumulation of errors in a process, which can occur in chain-of-thought (CoT) prompting as the number of steps increases<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MAJORITY VOTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Majority voting is a technique where the most common output among multiple samples is chosen, used in self-consistency to mitigate error propagation<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"MULTI-STEP DECOMPOSITION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multi-step decomposition is a method for breaking down complex tasks into smaller, manageable steps, used in least-to-most prompting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"LEAST-TO-MOST PROMPTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Least-to-most prompting is a method for multi-step decomposition in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"TREE-OF-THOUGHT (TOT) PROMPTING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tree-of-thought (ToT) prompting is a method that uses depth-first or breadth-first search guided by an LM-generated heuristic for reasoning in language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"REASONING VIA PLANNING (RAP)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reasoning via planning (RAP) is a method that uses Monte Carlo Tree Search (MCTS) with rollouts simulated by language models for reasoning<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DFS (Depth-First Search) is a search algorithm used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"BFS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">BFS (Breadth-First Search) is a search algorithm used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"POLICY MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A policy model is a model used to make decisions or take actions in an environment, used in interactive tasks for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ROBOTICS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Robotics is the branch of technology that deals with the design, construction, operation, and application of robots, where language models have been used as high-level controllers<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/node>    <node id=\"CONTROL POLICIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Control policies are strategies used to control the actions of robots or other systems, where language models have been used as high-level controllers<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MULTIMODAL GAMES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multimodal games are games that involve multiple modes of interaction, such as visual and textual, where language model agents have been adapted<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MINECRAFT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Minecraft is a complex multimodal game where language model agents have been adapted<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TEXT-BASED ENVIRONMENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Text-based environments are environments where interactions are primarily through text, where language models have been used for acting-based prompting techniques<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a self-improvement technique for language models<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"EXTERNAL FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External feedback is information from the environment that is used to improve the performance of language models, incorporated in LATS<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SELF-IMPROVEMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-improvement refers to techniques where language models improve their own performance, such as self-refinement and Reflexion<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ENVIRONMENTAL CONDITIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Environmental conditions refer to the state of the environment that language models must adapt to, which LATS can handle without additional training<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectories are sequences of actions or steps taken to achieve a goal, constructed by LATS from sampled actions<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HEURISTICS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Heuristics are strategies or principles used to guide decision-making or problem-solving, incorporated in LATS for value assignment and search processes<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SAMPLED CHAINS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Sampled chains are sequences of steps generated by language models, used in self-consistency for majority voting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ROLLOUTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Rollouts are simulations of actions or steps taken to predict outcomes, used in reasoning via planning (RAP) with Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LM-GENERATED HEURISTIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An LM-generated heuristic is a heuristic generated by a language model to guide search processes, used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d2\">f8e7ed806916bf15245bcb4d52570c26<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <edge source=\"LATS\" target=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS is based on Monte Carlo Tree Search (MCTS) to construct the best trajectory from sampled actions<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS was evaluated on the HotPotQA dataset<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS outperforms ReAct on the HotPotQA dataset<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEBSHOP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS was evaluated on the WebShop dataset<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS was used with GPT-3.5 to achieve significant performance improvements<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS was used with GPT-4 to achieve a 92.7 Pass@1 rate on HumanEval<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS was evaluated on the HumanEval dataset<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-REFINEMENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS incorporates self-refinement as a heuristic to guide the search process<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-CONSISTENCY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS incorporates self-consistency as a heuristic to guide the search process<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SEARCH ALGORITHMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Search algorithms are adapted in LATS for language agents<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"NODES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nodes are used in LATS to store and retrieve external feedback<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROMPTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prompts are used in LATS to store and retrieve external feedback<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"VALUE ASSIGNMENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Value assignment is incorporated in LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INTERNAL REASONING PERFORMANCE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS aims to surpass internal reasoning performance<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LM-POWERED VALUE FUNCTIONS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LM-powered value functions are used in LATS for cleverer exploration<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-REFLECTIONS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Self-reflections are used in LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">In-context learning is leveraged by LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PLANNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Planning is a key component of LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reasoning is enhanced by LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ACTING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Acting is a component of LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DECISION-MAKING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Decision-making is enhanced by LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Programming is one of the domains where LATS is tested<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INTERACTIVE QUESTION-ANSWERING (QA)\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Interactive question-answering (QA) is one of the domains where LATS is tested<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEB NAVIGATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Web navigation is one of the domains where LATS is tested<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MATH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Math is one of the domains where LATS is tested<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXTERNAL FEEDBACK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">External feedback is incorporated in LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-IMPROVEMENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Self-improvement techniques are used in LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENTAL CONDITIONS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS can handle environmental conditions without additional training<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TRAJECTORIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Trajectories are constructed by LATS from sampled actions<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HEURISTICS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Heuristics are incorporated in LATS for value assignment and search processes<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"MODEL-BASED REINFORCEMENT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Model-based reinforcement learning inspired the use of Monte Carlo Tree Search (MCTS) in LATS<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"REASONING VIA PLANNING (RAP)\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reasoning via planning (RAP) uses Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"SELF-CONSISTENCY\" target=\"MAJORITY VOTING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Majority voting is used in self-consistency to mitigate error propagation<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"SELF-CONSISTENCY\" target=\"SAMPLED CHAINS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Sampled chains are used in self-consistency for majority voting<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"COBEE\" target=\"WEI\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Cobbe and Wei have both worked on reasoning for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"COBEE\" target=\"KOJIMA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Cobbe and Kojima have both worked on reasoning for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"COBEE\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Cobbe and Wang have both worked on reasoning for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"WEI\" target=\"KOJIMA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wei and Kojima have both worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"WEI\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wei and Wang have both worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"KOJIMA\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Kojima and Wang have both worked on chain-of-thought (CoT) prompting and its variants for reasoning in language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"GUO\" target=\"CHEN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Guo and Chen have both worked on addressing error propagation in chain-of-thought (CoT) prompting for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"YA0\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yao and Chen have both worked on the WebShop dataset and ReAct prompting technique<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"ZHOU\" target=\"YAO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Zhou and Yao have both worked on improving chain-of-thought (CoT) prompting with search algorithms<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"HAO\" target=\"BESTA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Hao and Besta have both worked on improving chain-of-thought (CoT) prompting with search algorithms<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"AHN\" target=\"HUANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Ahn and Huang have both worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"AHN\" target=\"DRIESS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Ahn and Driess have both worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"HUANG\" target=\"DRIESS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Huang and Driess have both worked on adapting language models as high-level controllers in robotics<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"BAKER\" target=\"GUSS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Baker and Guss have both worked on adapting language model agents to complex multimodal games such as Minecraft<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"BAKER\" target=\"FAN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Baker and Fan have both worked on adapting language model agents to complex multimodal games such as Minecraft<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"GUSS\" target=\"FAN\">      <data key=\"d4\">21.0<\/data>      <data key=\"d5\">Fan and Guss have both worked on the Minecraft gameGuss and Fan have both worked on adapting language model agents to complex multimodal games such as Minecraft<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"LIU\" target=\"SHRIDHAR\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Liu and Shridhar have both worked on text-based environments and acting-based prompting techniques for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"MADAAN\" target=\"SHINN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Madaan and Shinn have both worked on self-improvement techniques for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"SILVER\" target=\"YANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Silver and Yang have both worked on model-based reinforcement learning and Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"SEARCH ALGORITHMS\" target=\"TREE-OF-THOUGHT (TOT) PROMPTING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Tree-of-thought (ToT) prompting uses search algorithms<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"REASONING\" target=\"CHAIN-OF-THOUGHT (COT) PROMPTING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Chain-of-thought (CoT) prompting is a method for reasoning in language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"ACTING\" target=\"POLICY MODEL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">A policy model is used in acting tasks for language models<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"ACTING\" target=\"TEXT-BASED ENVIRONMENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Text-based environments are used for acting-based prompting techniques<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"ERROR PROPAGATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Error propagation can occur in chain-of-thought (CoT) prompting as the number of steps increases<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"MULTI-STEP DECOMPOSITION\" target=\"LEAST-TO-MOST PROMPTING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Multi-step decomposition is used in least-to-most prompting<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"DFS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">DFS is used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"BFS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">BFS is used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"LM-GENERATED HEURISTIC\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">An LM-generated heuristic is used in tree-of-thought (ToT) prompting<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"REASONING VIA PLANNING (RAP)\" target=\"ROLLOUTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Rollouts are used in reasoning via planning (RAP) with Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"ROBOTICS\" target=\"CONTROL POLICIES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Control policies are used in robotics<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"MULTIMODAL GAMES\" target=\"MINECRAFT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Minecraft is a complex multimodal game<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SELF-IMPROVEMENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reflexion is a self-improvement technique<\/data>      <data key=\"d6\">f8e7ed806916bf15245bcb4d52570c26<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c95e02c0dca4a4a36b701cbc7dd14da6","chunk":"2024), where acting-based prompting techniques\nsuch as ReAct (Yao et al., 2023b) have seen success. Sim-\nilar to CoT, ReAct is limited by its simplicity and cannot\neffectively adapt to environment conditions. Many exten-\nsions have been proposed to address this issue, including\nself-refine (Madaan et al., 2023) and Reflexion (Shinn et al.,\n2023), which use self-improvement to enhance reasoning\nand decision-making, and AdaPlanner (Sun et al., 2023),\nwhich incorporates both positive and negative feedback.\nHowever, these methods focus on refining an individual tra-\njectory and do not consider alternative choices at each step.\nIn addition, recent work (Huang et al., 2024) has suggested\nthat LMs cannot self-correct their internal reasoning, mak-\ning it critical to use external feedback. Alternatively, to pure\ndecision-making environments, the reasoning and practical\nabilities of LMs have been enhanced by providing access\nto external tools, such as APIs, search engines, calculators,\nand other models (Schick et al., 2023; Shen et al., 2023;\nSur\u00b4\u0131s et al., 2023). We summarize prior work in Tab. 1.\nTree-based search. Tree-based search, where multiple\n2Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nApproach Reasoning Acting Planning Self- External\nReflection Memory\nCoT (Wei et al., 2022) \u2713 \u00d7 \u00d7 \u00d7 \u00d7\nReAct (Yao et al., 2023b) \u2713 \u2713 \u00d7 \u00d7 \u00d7\nToT (Yao et al., 2023a) \u2713 \u00d7 \u2713 \u2713 \u2713\nRAP (Hao et al., 2023) \u2713 \u00d7 \u2713 \u00d7 \u2713\nSelf-Refine (Madaan et al., 2023) \u2713 \u00d7 \u00d7 \u2713 \u00d7\nBeam Search (Xie et al., 2023) \u2713 \u00d7 \u00d7 \u2713 \u00d7\nReflexion (Shinn et al., 2023) \u2713 \u2713 \u00d7 \u2713 \u2713\nLATS (Ours) \u2713 \u2713 \u2713 \u2713 \u2713\nTable 1. Summary of related work on reasoning, acting, and planning. LATS is the firstwork incorporating designs from allthree domains,\nallowing broad applicability in all corresponding tasks. We refer to reasoning as LM internal reasoning, acting as external decision-making,\nplanning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storing past text\ncontext for future updates of the solution.\nbranches of outcomes are explored during search, is widely\nused in many planning algorithms (Swiechowski et al., 2021;\nLaValle, 1998) and reinforcement learning (RL) (Hafner\net al., 2019; Du et al., 2023; Wu et al., 2023) algorithms for\nits good exploration-exploitation trade-off. Note that though\ntree-based search necessitates an environment model that\ncan expand from an arbitrary state (V odopivec et al., 2017),\noften requiring extra training in RL (Hafner et al., 2023),\nsuch a problem does not exist for most LM tasks. This is\nbecause we can conveniently revert to any state by setting\nthe input to be the context and the corresponding previous\noutput from the LM for many tasks. Thus, we operate on the\ntree-based framework and use MCTS (Swiechowski et al.,\n2021) to fully unlock the potential of LMs. In addition, we\navoid the cost of training a value function over language\ndescriptions by leveraging the in-context learning (Brown\net al., 2020) abilities of LMs. Concurrent work (Liu et al.,\n2023) also explores combining search algorithms with LM\nagents but uses an off-the-shelf search algorithm, which\nmay not be optimal for LMs. Finally, following Yao et al.\n(2023a) and Hao et al. (2023), we note that we use planning\nandsearch algorithms interchangeably in this paper.\n3. Preliminaries\n3.1. Problem Setting and Prompting\nWe first define our problem and outline a few established\nmethods that leverage language models for reasoning or\ndecision-making. In LM reasoning or decision making,\nwe are given an input xin natural language and a pre-\ntrained language model p\u03b8(x)parameterized by \u03b8; our goal\nis to generate a final output y\u223cp\u03b8(x)that corresponds\nto the answer (reasoning) or completes the task (decision-\nmaking). Both xandyare language sequences , which are\ncomprised of a list of tokens (the basic elements of natural\nlanguage, often words), denoted as x= (x[1], . . . , x [lx])\nandy= (y[1], . . . , y [ly])where lxandlyare the length.The LM decodes text autoregressively, i.e., without other\ninputs, the probability for an LM to generate a sequence y\nis given by p\u03b8(x) =Qlx\ni=1p\u03b8(x[i]|x[1. . . i\u22121]). Usually,\nto improve reasoning, prompts are provided along with the\ninput x, which are specific instructions or few-shot input-\noutput examples. We denote the generic process where an\ninput prompt IO(x)is transformed into an output yby LM:\ny\u223cp\u03b8(prompt IO(x)).\nChain-of-thought (CoT) prompting (Wei et al., 2022)\ncaters to scenarios where the direct mapping from xtoyis\nintricate, e.g","chunk_id":"c95e02c0dca4a4a36b701cbc7dd14da6","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is an acting-based prompting technique that has seen success in enhancing reasoning and decision-making in language models, but it is limited by its simplicity and cannot effectively adapt to environment conditions","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"COT","type":"TECHNOLOGY","description":"Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate, aiming to improve reasoning in language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-refine is a method that uses self-improvement to enhance reasoning and decision-making in language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a method that uses self-improvement to enhance reasoning and decision-making in language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"ADAPLANNER","type":"TECHNOLOGY","description":"AdaPlanner is a method that incorporates both positive and negative feedback to enhance reasoning and decision-making in language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"HUANG","type":"PERSON","description":"Huang is an author who has suggested that language models cannot self-correct their internal reasoning, making it critical to use external feedback","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"EXTERNAL TOOLS","type":"TECHNOLOGY","description":"External tools such as APIs, search engines, calculators, and other models are used to enhance the reasoning and practical abilities of language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"TREE-BASED SEARCH","type":"TECHNOLOGY","description":"Tree-based search is a method where multiple branches of outcomes are explored during search, widely used in planning and reinforcement learning algorithms for its good exploration-exploitation trade-off","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"MCTS","type":"TECHNOLOGY","description":"Monte Carlo Tree Search (MCTS) is a tree-based search algorithm used to fully unlock the potential of language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"IN-CONTEXT LEARNING","type":"TECHNOLOGY","description":"In-context learning is a capability of language models that allows them to learn from the context provided in the input without additional training","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"LATS","type":"TECHNOLOGY","description":"Language Agent Tree Search (LATS) is a method that unifies reasoning, acting, and planning in language models, incorporating designs from all three domains","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"TECHNOLOGY"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has worked on Chain-of-thought (CoT) prompting","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"YAO","type":"PERSON","description":"Yao is an author who has worked on ReAct and ToT prompting techniques","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on RAP prompting techniques","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who has worked on the Self-Refine method","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on the Reflexion method","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"XIE","type":"PERSON","description":"Xie is an author who has worked on Beam Search techniques","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"SWIECHOWSKI","type":"PERSON","description":"Swiechowski is an author who has worked on tree-based search algorithms","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"LAVALLE","type":"PERSON","description":"LaValle is an author who has worked on planning algorithms","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"HAFNER","type":"PERSON","description":"Hafner is an author who has worked on reinforcement learning algorithms","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"DU","type":"PERSON","description":"Du is an author who has worked on reinforcement learning algorithms","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"WU","type":"PERSON","description":"Wu is an author who has worked on reinforcement learning algorithms","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"VODOPIVEC","type":"PERSON","description":"Vodopivec is an author who has worked on environment models for tree-based search","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"BROWN","type":"PERSON","description":"Brown is an author who has worked on in-context learning abilities of language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has worked on combining search algorithms with language model agents","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6","entity_type":"PERSON"},{"name":"TOT","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"RAP","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"BEAM SEARCH","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"PLANNING ALGORITHMS","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"REINFORCEMENT LEARNING","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"ENVIRONMENT MODELS","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SEARCH ALGORITHMS","type":"","description":"","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SUN","type":"PERSON","description":"Sun is an author who has worked on the AdaPlanner method","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SCHICK","type":"PERSON","description":"Schick is an author who has worked on providing external tools to enhance language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SHEN","type":"PERSON","description":"Shen is an author who has worked on providing external tools to enhance language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"SUR\u00cdS","type":"PERSON","description":"Sur\u00eds is an author who has worked on providing external tools to enhance language models","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"},{"name":"YA0","type":"PERSON","description":"Yao is an author who has worked on ReAct and ToT prompting techniques","source_id":"c95e02c0dca4a4a36b701cbc7dd14da6"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is an acting-based prompting technique that has seen success in enhancing reasoning and decision-making in language models, but it is limited by its simplicity and cannot effectively adapt to environment conditions<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate, aiming to improve reasoning in language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-refine is a method that uses self-improvement to enhance reasoning and decision-making in language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a method that uses self-improvement to enhance reasoning and decision-making in language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ADAPLANNER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AdaPlanner is a method that incorporates both positive and negative feedback to enhance reasoning and decision-making in language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang is an author who has suggested that language models cannot self-correct their internal reasoning, making it critical to use external feedback<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"EXTERNAL TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External tools such as APIs, search engines, calculators, and other models are used to enhance the reasoning and practical abilities of language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TREE-BASED SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tree-based search is a method where multiple branches of outcomes are explored during search, widely used in planning and reinforcement learning algorithms for its good exploration-exploitation trade-off<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a tree-based search algorithm used to fully unlock the potential of language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">In-context learning is a capability of language models that allows them to learn from the context provided in the input without additional training<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Agent Tree Search (LATS) is a method that unifies reasoning, acting, and planning in language models, incorporating designs from all three domains<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has worked on Chain-of-thought (CoT) prompting<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on ReAct and ToT prompting techniques<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on RAP prompting techniques<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who has worked on the Self-Refine method<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on the Reflexion method<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xie is an author who has worked on Beam Search techniques<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SWIECHOWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swiechowski is an author who has worked on tree-based search algorithms<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAVALLE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">LaValle is an author who has worked on planning algorithms<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAFNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hafner is an author who has worked on reinforcement learning algorithms<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Du is an author who has worked on reinforcement learning algorithms<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wu is an author who has worked on reinforcement learning algorithms<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VODOPIVEC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vodopivec is an author who has worked on environment models for tree-based search<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brown is an author who has worked on in-context learning abilities of language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has worked on combining search algorithms with language model agents<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"BEAM SEARCH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"PLANNING ALGORITHMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"ENVIRONMENT MODELS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sun is an author who has worked on the AdaPlanner method<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schick is an author who has worked on providing external tools to enhance language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shen is an author who has worked on providing external tools to enhance language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"SUR&#205;S\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sur&#237;s is an author who has worked on providing external tools to enhance language models<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <node id=\"YA0\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on ReAct and ToT prompting techniques<\/data>      <data key=\"d2\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/node>    <edge source=\"REACT\" target=\"COT\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">ReAct is similar to CoT in that both are prompting techniques aimed at improving reasoning in language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Yao has worked on the ReAct prompting technique<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"COT\" target=\"WEI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Wei has worked on Chain-of-thought (CoT) prompting<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"REFLEXION\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Both Self-Refine and Reflexion use self-improvement to enhance reasoning and decision-making in language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"ADAPLANNER\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">AdaPlanner and Self-Refine are methods aimed at enhancing reasoning and decision-making in language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MADAAN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Madaan has worked on the Self-Refine method<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"ADAPLANNER\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">AdaPlanner and Reflexion are methods aimed at enhancing reasoning and decision-making in language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Shinn has worked on the Reflexion method<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"HUANG\" target=\"EXTERNAL TOOLS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Huang suggested that language models cannot self-correct their internal reasoning, making it critical to use external feedback, such as external tools<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"TREE-BASED SEARCH\" target=\"MCTS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">MCTS is a tree-based search algorithm used to fully unlock the potential of language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"TREE-BASED SEARCH\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">In-context learning abilities of language models are leveraged in tree-based search to avoid the cost of training a value function over language descriptions<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"TREE-BASED SEARCH\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS operates on the tree-based framework to unify reasoning, acting, and planning in language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"TREE-BASED SEARCH\" target=\"SWIECHOWSKI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Swiechowski has worked on tree-based search algorithms<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"IN-CONTEXT LEARNING\" target=\"BROWN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Brown has worked on in-context learning abilities of language models<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"YAO\" target=\"TOT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Yao has worked on the ToT prompting technique<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"HAO\" target=\"RAP\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Hao has worked on the RAP prompting technique<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"XIE\" target=\"BEAM SEARCH\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Xie has worked on Beam Search techniques<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LAVALLE\" target=\"PLANNING ALGORITHMS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LaValle has worked on planning algorithms<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"HAFNER\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Hafner has worked on reinforcement learning algorithms<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"DU\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Du has worked on reinforcement learning algorithms<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"WU\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Wu has worked on reinforcement learning algorithms<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"VODOPIVEC\" target=\"ENVIRONMENT MODELS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Vodopivec has worked on environment models for tree-based search<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>    <edge source=\"LIU\" target=\"SEARCH ALGORITHMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Liu has worked on combining search algorithms with language model agents<\/data>      <data key=\"d6\">c95e02c0dca4a4a36b701cbc7dd14da6<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"9bb90746134619cad9a3e649b8b35f24","chunk":" . i\u22121]). Usually,\nto improve reasoning, prompts are provided along with the\ninput x, which are specific instructions or few-shot input-\noutput examples. We denote the generic process where an\ninput prompt IO(x)is transformed into an output yby LM:\ny\u223cp\u03b8(prompt IO(x)).\nChain-of-thought (CoT) prompting (Wei et al., 2022)\ncaters to scenarios where the direct mapping from xtoyis\nintricate, e.g., when xis from a mathematical query or chal-\nlenging question. It hinges on creating thoughts z1, . . . , z l\nthat act as stepping stones between xandy; each thought zi\nis a language sequence. To employ CoT prompting, thoughts\nare extracted sequentially as zi\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7i\u22121), with\nthe final output being y\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7l).\nTree-of-thought (ToT) prompting (Yao et al., 2023a) ex-\ntends CoT prompting by exploring multiple reasoning paths\nover thoughts. It frames problems as a search over a tree,\nwhere each node s= [x, z1\u00b7i]represents a partial solution\nstate comprising the original input xand the thought se-\nquence z1\u00b7\u00b7\u00b7i. Thoughts ziare generated by proposal or\nsampling with CoT zi\u223cpCoT\n\u03b8(x, z1\u00b7\u00b7\u00b7i\u22121). Search algo-\nrithms like depth-first (DFS) or breadth-first (BFS) search\nare used to systematically explore the tree, guided by heuris-\ntics based on LM evaluations V(s)of each state.\nReAct (Yao et al., 2023b) extends language models to\ntasks where the mapping from xtoyis enhanced by or\nrequires interactions with an external environment, such\nas a game or API. This technique constructs an action\nspace \u02c6A=A\u222aZthat adds permissible actions a\u2208A\nto the reasoning traces z\u2208Zfrom CoT. Observations o\nfrom the environment are used to improve both reasoning\nand acting. To solve problems with ReAct, after each ob-\nservation, actions are generated from p\u03b8sequentially as\nai\u223cpReAct\n\u03b8 (x, o1\u00b7\u00b7\u00b7i\u22121, a1\u00b7\u00b7\u00b7i\u22121), with the final output be-\n3Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\ningy\u223cpReAct\n\u03b8 (x, o1\u00b7\u00b7\u00b7l, a1\u00b7\u00b7\u00b7l). In this paper, consistent\nwith other LM agent methods such as ReAct and Reflexion\n(Shinn et al., 2023), we focus on decision-making tasks\nwhere reverting between iterations is feasible .\nWhile the previously described prompting techniques im-\nprove LM performance on reasoning tasks, they falter on\ndifficult tasks that involve multifaceted decision-making\ndue to several shortcomings: 1) Flexibility : Base prompting\ndesigns (CoT or ReAct) autoregressively sample from the\nLM, neglecting potential alternative continuations from spe-\ncific states. 2) Sensibility : Reasoning-based methods (CoT,\nRAP (Hao et al., 2023), or ToT) rely solely on the inter-\nnal representations of the LM and cannot consider external\nobservations. This dependency risks fact hallucination and\nerror propagation while setting a performance ceiling. 3)\nAdaptability : Current planning strategies (RAP or ToT) use\nsimple search algorithms such as BFS or cannot leverage\nenvironmental feedback to improve planning. Additionally,\nthe agent is static and cannot reuse previous experience or\nlearn from trial and error. While RAP also adopts MCTS, it\nis constrained to tasks where the LM can become a world\nmodel and accurately predict states. These shortcomings\nlimit the ability of LMs to be deployed as general problem-\nsolving agents and form the motivation for LATS.\n3.2. Monte Carlo Tree Search (MCTS)\nMonte Carlo Tree Search (MCTS) is a heuristic search al-\ngorithm that is proved successful on many decision-making\nenvironments, such as Atari (Ye et al., 2021) and Go (Silver\net al., 2016). MCTS builds a decision tree where every node\nin the tree is a state and edge is an action. MCTS runs for k\nepisodes; for each episode, it starts from the root (i.e., initial\nstate) and iteratively conducts two steps to expand the tree:\n1)Expansion , where multiple children states sare explored\nfrom the current parent state pby sampling nactions, and 2)\nSelection , where the children with the highest UCT (Upper\nConfidence bounds applied to Trees) (Kocsis and Szepesv \u00b4ari,\n2006) value is selected for expansion by the next iteration.\nThe UCT of a child state sis calculated as follows:\nUCT (s) =V(s) +ws\nlnN(p)\nN(s), (1)\nwhere N(s)is the number of visits to a node s,V(s)is the\nvalue function (expected return) from the subtree of s,wis\nthe exploration weight, and pis the parent node of s. When\nthe end of an episode is reached, a backpropagation is car-\nried out: the return ris used for updating every V(s)along\nthe path with the formula V(s) =Vold(s)(N(s)\u22121)+r\nN(s), where\nVold(s)is the old value function. Normally, the major short-\ncoming of","chunk_id":"9bb90746134619cad9a3e649b8b35f24","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"CHAIN-OF-THOUGHT (COT) PROMPTING","type":"TECHNIQUE","description":"Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate, such as mathematical queries or challenging questions. It involves creating intermediate thoughts that act as stepping stones between the input and the output","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"TREE-OF-THOUGHT (TOT) PROMPTING","type":"TECHNIQUE","description":"Tree-of-thought (ToT) prompting extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree, where each node represents a partial solution state","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"REACT","type":"TECHNIQUE","description":"ReAct is a technique that extends language models to tasks where the mapping from input to output is enhanced by or requires interactions with an external environment, such as a game or API. It constructs an action space that adds permissible actions to the reasoning traces from CoT","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"MONTE CARLO TREE SEARCH (MCTS)","type":"ALGORITHM","description":"Monte Carlo Tree Search (MCTS) is a heuristic search algorithm that builds a decision tree where every node is a state and every edge is an action. It is used in many decision-making environments and involves steps like expansion and selection to explore and expand the tree","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"LANGUAGE MODELS (LMS)","type":"TECHNOLOGY","description":"Language Models (LMs) are models that transform input prompts into outputs. They are used in various prompting techniques like CoT, ToT, and ReAct to improve reasoning and decision-making tasks","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"RAP","type":"TECHNIQUE","description":"RAP is a reasoning-based method that relies solely on the internal representations of the language model and cannot consider external observations. It is used in planning strategies but has limitations in flexibility and adaptability","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"REFLEXION","type":"TECHNIQUE","description":"Reflexion is a method mentioned in the context of decision-making tasks where reverting between iterations is feasible. It is similar to ReAct and focuses on improving reasoning and acting","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"WEI ET AL., 2022","type":"REFERENCE","description":"Wei et al., 2022 is a reference to the authors who introduced Chain-of-thought (CoT) prompting","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"YAO ET AL., 2023A","type":"REFERENCE","description":"Yao et al., 2023a is a reference to the authors who introduced Tree-of-thought (ToT) prompting","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"YAO ET AL., 2023B","type":"REFERENCE","description":"Yao et al., 2023b is a reference to the authors who introduced ReAct","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"HAO ET AL., 2023","type":"REFERENCE","description":"Hao et al., 2023 is a reference to the authors who introduced RAP","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"SHINN ET AL., 2023","type":"REFERENCE","description":"Shinn et al., 2023 is a reference to the authors who introduced Reflexion","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"KOCSIS AND SZEPESV\u00c1RI, 2006","type":"REFERENCE","description":"Kocsis and Szepesv\u00e1ri, 2006 is a reference to the authors who introduced the UCT (Upper Confidence bounds applied to Trees) value used in MCTS","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"YE ET AL., 2021","type":"REFERENCE","description":"Ye et al., 2021 is a reference to the authors who applied MCTS to decision-making environments like Atari","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"SILVER ET AL., 2016","type":"REFERENCE","description":"Silver et al., 2016 is a reference to the authors who applied MCTS to decision-making environments like Go","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"INPUT X","type":"DATA","description":"Input x is the initial data or query provided to the language model for processing","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"OUTPUT Y","type":"DATA","description":"Output y is the final result produced by the language model after processing the input and prompt","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"THOUGHTS Z","type":"DATA","description":"Thoughts z are intermediate language sequences created during the Chain-of-thought (CoT) prompting process to act as stepping stones between the input and the output","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"DECISION-MAKING TASKS","type":"TASK","description":"Decision-making tasks are complex problems that involve multifaceted decision-making and require advanced reasoning and planning techniques","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"ENVIRONMENTAL FEEDBACK","type":"DATA","description":"Environmental feedback refers to observations and data from the external environment used to improve reasoning and acting in techniques like ReAct","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)","type":"ALGORITHM","description":"Upper Confidence bounds applied to Trees (UCT) is a value used in Monte Carlo Tree Search (MCTS) to select the best child node for expansion based on exploration and exploitation","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"EXPANSION","type":"PROCESS","description":"Expansion is a step in Monte Carlo Tree Search (MCTS) where multiple children states are explored from the current parent state by sampling actions","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"SELECTION","type":"PROCESS","description":"Selection is a step in Monte Carlo Tree Search (MCTS) where the child with the highest UCT value is selected for expansion in the next iteration","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"BACKPROPAGATION","type":"PROCESS","description":"Backpropagation is a step in Monte Carlo Tree Search (MCTS) where the return is used to update the value function of every node along the path after the end of an episode","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"RETURN R","type":"DATA","description":"Return r is the reward or outcome used in the backpropagation step of Monte Carlo Tree Search (MCTS) to update the value function of nodes","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"VALUE FUNCTION V(S)","type":"DATA","description":"Value function V(s) is the expected return from the subtree of a node s in Monte Carlo Tree Search (MCTS)","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"EXPLORATION WEIGHT W","type":"DATA","description":"Exploration weight w is a parameter used in the UCT calculation to balance exploration and exploitation in Monte Carlo Tree Search (MCTS)","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"PARENT NODE P","type":"DATA","description":"Parent node p is the current state from which child states are explored in the expansion step of Monte Carlo Tree Search (MCTS)","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"CHILD NODE S","type":"DATA","description":"Child node s is a state explored from the parent node in the expansion step of Monte Carlo Tree Search (MCTS)","source_id":"9bb90746134619cad9a3e649b8b35f24"},{"name":"PROMPT","type":"","description":"","source_id":"9bb90746134619cad9a3e649b8b35f24"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CHAIN-OF-THOUGHT (COT) PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-thought (CoT) prompting is a technique that caters to scenarios where the direct mapping from input to output is intricate, such as mathematical queries or challenging questions. It involves creating intermediate thoughts that act as stepping stones between the input and the output<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"TREE-OF-THOUGHT (TOT) PROMPTING\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tree-of-thought (ToT) prompting extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree, where each node represents a partial solution state<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">ReAct is a technique that extends language models to tasks where the mapping from input to output is enhanced by or requires interactions with an external environment, such as a game or API. It constructs an action space that adds permissible actions to the reasoning traces from CoT<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Monte Carlo Tree Search (MCTS) is a heuristic search algorithm that builds a decision tree where every node is a state and every edge is an action. It is used in many decision-making environments and involves steps like expansion and selection to explore and expand the tree<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"LANGUAGE MODELS (LMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Models (LMs) are models that transform input prompts into outputs. They are used in various prompting techniques like CoT, ToT, and ReAct to improve reasoning and decision-making tasks<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">RAP is a reasoning-based method that relies solely on the internal representations of the language model and cannot consider external observations. It is used in planning strategies but has limitations in flexibility and adaptability<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflexion is a method mentioned in the context of decision-making tasks where reverting between iterations is feasible. It is similar to ReAct and focuses on improving reasoning and acting<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"WEI ET AL., 2022\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Wei et al., 2022 is a reference to the authors who introduced Chain-of-thought (CoT) prompting<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"YAO ET AL., 2023A\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Yao et al., 2023a is a reference to the authors who introduced Tree-of-thought (ToT) prompting<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"YAO ET AL., 2023B\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Yao et al., 2023b is a reference to the authors who introduced ReAct<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"HAO ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Hao et al., 2023 is a reference to the authors who introduced RAP<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"SHINN ET AL., 2023\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Shinn et al., 2023 is a reference to the authors who introduced Reflexion<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"KOCSIS AND SZEPESV&#193;RI, 2006\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Kocsis and Szepesv&#225;ri, 2006 is a reference to the authors who introduced the UCT (Upper Confidence bounds applied to Trees) value used in MCTS<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"YE ET AL., 2021\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Ye et al., 2021 is a reference to the authors who applied MCTS to decision-making environments like Atari<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"SILVER ET AL., 2016\">      <data key=\"d0\">REFERENCE<\/data>      <data key=\"d1\">Silver et al., 2016 is a reference to the authors who applied MCTS to decision-making environments like Go<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"INPUT X\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Input x is the initial data or query provided to the language model for processing<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"OUTPUT Y\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Output y is the final result produced by the language model after processing the input and prompt<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"THOUGHTS Z\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Thoughts z are intermediate language sequences created during the Chain-of-thought (CoT) prompting process to act as stepping stones between the input and the output<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"DECISION-MAKING TASKS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Decision-making tasks are complex problems that involve multifaceted decision-making and require advanced reasoning and planning techniques<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Environmental feedback refers to observations and data from the external environment used to improve reasoning and acting in techniques like ReAct<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Upper Confidence bounds applied to Trees (UCT) is a value used in Monte Carlo Tree Search (MCTS) to select the best child node for expansion based on exploration and exploitation<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"EXPANSION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Expansion is a step in Monte Carlo Tree Search (MCTS) where multiple children states are explored from the current parent state by sampling actions<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"SELECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Selection is a step in Monte Carlo Tree Search (MCTS) where the child with the highest UCT value is selected for expansion in the next iteration<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"BACKPROPAGATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Backpropagation is a step in Monte Carlo Tree Search (MCTS) where the return is used to update the value function of every node along the path after the end of an episode<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"RETURN R\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Return r is the reward or outcome used in the backpropagation step of Monte Carlo Tree Search (MCTS) to update the value function of nodes<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"VALUE FUNCTION V(S)\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Value function V(s) is the expected return from the subtree of a node s in Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"EXPLORATION WEIGHT W\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Exploration weight w is a parameter used in the UCT calculation to balance exploration and exploitation in Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"PARENT NODE P\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Parent node p is the current state from which child states are explored in the expansion step of Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"CHILD NODE S\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Child node s is a state explored from the parent node in the expansion step of Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/node>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"TREE-OF-THOUGHT (TOT) PROMPTING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tree-of-thought (ToT) prompting extends Chain-of-thought (CoT) prompting by exploring multiple reasoning paths<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"REACT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ReAct builds on Chain-of-thought (CoT) prompting by adding interactions with an external environment<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"LANGUAGE MODELS (LMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Chain-of-thought (CoT) prompting is a technique used to improve the reasoning capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"WEI ET AL., 2022\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Wei et al., 2022 introduced Chain-of-thought (CoT) prompting<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT) PROMPTING\" target=\"THOUGHTS Z\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Chain-of-thought (CoT) prompting involves creating intermediate thoughts z to act as stepping stones between the input and the output<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"REACT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Tree-of-thought (ToT) prompting and ReAct are techniques that extend the capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"MONTE CARLO TREE SEARCH (MCTS)\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Tree-of-thought (ToT) prompting uses search algorithms like those in Monte Carlo Tree Search (MCTS) to explore reasoning paths<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"LANGUAGE MODELS (LMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Tree-of-thought (ToT) prompting is a technique used to improve the reasoning capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"TREE-OF-THOUGHT (TOT) PROMPTING\" target=\"YAO ET AL., 2023A\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Yao et al., 2023a introduced Tree-of-thought (ToT) prompting<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"REACT\" target=\"LANGUAGE MODELS (LMS)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ReAct is a technique used to improve the reasoning and acting capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO ET AL., 2023B\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Yao et al., 2023b introduced ReAct<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"REACT\" target=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ReAct uses observations from the environment to improve reasoning and acting<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"RAP\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">RAP also adopts Monte Carlo Tree Search (MCTS) for planning strategies<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"KOCSIS AND SZEPESV&#193;RI, 2006\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Kocsis and Szepesv&#225;ri, 2006 introduced the UCT value used in Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"YE ET AL., 2021\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ye et al., 2021 applied Monte Carlo Tree Search (MCTS) to decision-making environments like Atari<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"SILVER ET AL., 2016\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Silver et al., 2016 applied Monte Carlo Tree Search (MCTS) to decision-making environments like Go<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Monte Carlo Tree Search (MCTS) uses UCT to select the best child node for expansion<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"EXPANSION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Expansion is a step in Monte Carlo Tree Search (MCTS) where multiple children states are explored<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"SELECTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Selection is a step in Monte Carlo Tree Search (MCTS) where the child with the highest UCT value is selected for expansion<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"MONTE CARLO TREE SEARCH (MCTS)\" target=\"BACKPROPAGATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Backpropagation is a step in Monte Carlo Tree Search (MCTS) where the return is used to update the value function of nodes<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"RAP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">RAP is a technique used to improve the reasoning capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"LANGUAGE MODELS (LMS)\" target=\"REFLEXION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reflexion is a technique used to improve the decision-making capabilities of language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO ET AL., 2023\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Hao et al., 2023 introduced RAP<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN ET AL., 2023\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Shinn et al., 2023 introduced Reflexion<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"INPUT X\" target=\"PROMPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">A prompt is provided along with the input x to improve reasoning in language models<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"INPUT X\" target=\"OUTPUT Y\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Input x is transformed into output y by the language model<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)\" target=\"VALUE FUNCTION V(S)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">UCT uses the value function V(s) to select the best child node for expansion<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"UPPER CONFIDENCE BOUNDS APPLIED TO TREES (UCT)\" target=\"EXPLORATION WEIGHT W\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">UCT uses the exploration weight w to balance exploration and exploitation<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"EXPANSION\" target=\"PARENT NODE P\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Expansion explores multiple children states from the parent node p<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"EXPANSION\" target=\"CHILD NODE S\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Expansion explores child node s from the parent node<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"SELECTION\" target=\"CHILD NODE S\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Selection chooses the child node s with the highest UCT value for expansion<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"BACKPROPAGATION\" target=\"RETURN R\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Return r is used in the backpropagation step of Monte Carlo Tree Search (MCTS) to update the value function of nodes<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>    <edge source=\"RETURN R\" target=\"VALUE FUNCTION V(S)\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Value function V(s) is updated using the return r in the backpropagation step of Monte Carlo Tree Search (MCTS)<\/data>      <data key=\"d5\">9bb90746134619cad9a3e649b8b35f24<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c234cb83764b899335af0950677ad024","chunk":" (expected return) from the subtree of s,wis\nthe exploration weight, and pis the parent node of s. When\nthe end of an episode is reached, a backpropagation is car-\nried out: the return ris used for updating every V(s)along\nthe path with the formula V(s) =Vold(s)(N(s)\u22121)+r\nN(s), where\nVold(s)is the old value function. Normally, the major short-\ncoming of MCTS is that it requires an environment model to\nundo previous steps and form a searching tree, which could\nbe a strong assumption. However, this limitation does not\nexist for many LM tasks, as we can conveniently reset toany step by simply copy-pasting historical text input. Such\na special property is the key motivation of our work.\n4. Unifying Reasoning, Acting, and Planning\n4.1. LM Agent\nDepending on the base prompting framework design, LATS\nsupports sequential reasoning or decision-making tasks. At\ntime step t, an agent receives an observation ot\u2208Ofrom\nthe environment and takes an action at\u2208Afollowing some\npolicy \u03c0(at|x, o1\u00b7\u00b7\u00b7t\u22121, a1\u00b7\u00b7\u00b7t\u22121). We initialize the agent\nwithp\u03b8to leverage the useful language representations of\nan LM as a base decision-maker. We follow the ReAct in-\nstantiation, in which the action space \u02c6A=A\u222aZconsists\nof both the space of permissible actions Aand the language\nspace of reasoning traces Z. Actions directly affect the envi-\nronment and result in observation, while thoughts are used\nto formalize decisions by organizing information, planning\nfuture actions, or injecting internal knowledge. The exact\ninstantiation of the action space depends on the particular\nenvironment \u2013 for decision-making tasks actions might con-\nsist of commands on a website, while for reasoning tasks\nthe action space might be limited to a few external tools or\nAPIs. In environments without feedback, such as reasoning\ntasks, we use CoT as the base prompting framework.\nInstead of greedily decoding one trajectory or solution, we\nsample nactions from p\u03b8using the current state. This is\nbased on the intuition that for complex decision-making\ntasks, there is likely to be a range of potential trajectories or\nreasoning paths that are correct (Evans, 2010). Sampling a\ndiverse set of candidates at each step mitigates the stochastic\nnature of LM text generation and enables greater exploration\nin both the decision-making and reasoning space. We wrap\np\u03b8within our proposed search algorithm to deliberately\nconstruct the best trajectory from sampled actions.\n4.2. LATS\nThe main component of LATS is a search algorithm that\ncontrols the problem-solving process with planning. To find\nthe most promising trajectory and systemically balance ex-\nploration with exploitation, we adopt a variant of MCTS that\nframes decision-making as a tree search, in which each node\ns= [x, a1\u00b7\u00b7\u00b7i, o1\u00b7\u00b7\u00b7i]represents a state comprising the origi-\nnal input x, action sequence a1\u00b7i, and observation sequence\no1\u00b7i, where iis a token in the text sequence.\nOur main technical contribution is adapting MCTS to lan-\nguage agents . LATS repurposes p\u03b8as an agent, state evalua-\ntor, and feedback generator, leveraging the useful language\nrepresentations of modern LMs to facilitate planning. While\nstandard MCTS and RAP (Hao et al., 2023) rely on internal\ndynamics models to facilitate simulation, LATS uses envi-\nronment interaction and does not require a world model. As\n4Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nFigure 2. Overview of the six operations in LATS. A node is selected ,expanded ,evaluated , then simulated until a terminal node is reached,\nand then the resulting value is backpropagated . If the trajectory fails, a reflection is generated and used as additional context for future\ntrials. These operations are performed in succession until the budget is reached or the task is successful.\ndepicted in Fig. 2, LATS consists of a series of operations\n\u2013selection, expansion, evaluation, simulation, backpropa-\ngation, and reflection \u2013 performed in succession until the\ntask is successfully completed or a computational limit is\nreached after sampling ktrajectories. The full pseudocode\nof LATS can be found in Sec. A in the Appendix.\nSelection. In the first operation, the algorithm identifies\na segment of the current tree most suitable for subsequent\nexpansion. Starting from the root node, denoted as the initial\nstates0, a child node is selected at each tree level until a leaf\nnode is reached. To balance exploration and exploitation,\nwe use the UCT algorithm as shown in Eq. 1.\nExpansion. After selecting a node, the second operation\nexpands the tree by sampling nactions from p\u03b8, as described\nin the prior section. The environment receives each action\nand returns corresponding feedback as an observation. This\nresults in nnew child nodes added to the tree. This tree is\nstored in an external long-term memory structure.\nEvaluation. The third operation assigns a scalar value to\neach new child node for selection and backpropagation.\nThis value effectively quantifies the agent\u2019s progress in task\ncompletion, serving as a heuristic to steer the search algo-\nrithm towards the most promising regions of the tree. As\nLATS does not involve training, we propose a novel value\nfunction for this setting based on two components: (1) a\nself-generated LM score and (2)","chunk_id":"c234cb83764b899335af0950677ad024","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"MCTS","type":"TECHNOLOGY","description":"MCTS (Monte Carlo Tree Search) is a search algorithm that requires an environment model to undo previous steps and form a searching tree","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LM TASKS","type":"TASK","description":"LM tasks refer to tasks involving language models, which can conveniently reset to any step by simply copy-pasting historical text input","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LM AGENT","type":"TECHNOLOGY","description":"LM Agent is a system that supports sequential reasoning or decision-making tasks by leveraging language models as base decision-makers","source_id":"c234cb83764b899335af0950677ad024"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is an instantiation in which the action space consists of both permissible actions and the language space of reasoning traces","source_id":"c234cb83764b899335af0950677ad024"},{"name":"COT","type":"TECHNOLOGY","description":"CoT (Chain of Thought) is a base prompting framework used in environments without feedback, such as reasoning tasks","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a search algorithm that unifies reasoning, acting, and planning in language models by adapting MCTS to language agents","source_id":"c234cb83764b899335af0950677ad024"},{"name":"P\u0398","type":"TECHNOLOGY","description":"P\u03b8 is a language model used as an agent, state evaluator, and feedback generator in LATS","source_id":"c234cb83764b899335af0950677ad024"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a variant of MCTS that relies on internal dynamics models to facilitate simulation","source_id":"c234cb83764b899335af0950677ad024"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on RAP","source_id":"c234cb83764b899335af0950677ad024"},{"name":"UCT ALGORITHM","type":"TECHNOLOGY","description":"UCT (Upper Confidence bounds applied to Trees) is an algorithm used to balance exploration and exploitation in tree search","source_id":"c234cb83764b899335af0950677ad024"},{"name":"SELECTION","type":"PROCESS","description":"Selection is the first operation in LATS where the algorithm identifies a segment of the current tree most suitable for subsequent expansion","source_id":"c234cb83764b899335af0950677ad024"},{"name":"EXPANSION","type":"PROCESS","description":"Expansion is the second operation in LATS where the tree is expanded by sampling actions from P\u03b8 and adding new child nodes","source_id":"c234cb83764b899335af0950677ad024"},{"name":"EVALUATION","type":"PROCESS","description":"Evaluation is the third operation in LATS where a scalar value is assigned to each new child node to quantify the agent\u2019s progress in task completion","source_id":"c234cb83764b899335af0950677ad024"},{"name":"SIMULATION","type":"PROCESS","description":"Simulation is an operation in LATS where the algorithm simulates actions until a terminal node is reached","source_id":"c234cb83764b899335af0950677ad024"},{"name":"BACKPROPAGATION","type":"PROCESS","description":"Backpropagation is an operation in LATS where the resulting value from a terminal node is used to update the value function along the path","source_id":"c234cb83764b899335af0950677ad024"},{"name":"REFLECTION","type":"PROCESS","description":"Reflection is an operation in LATS where a reflection is generated and used as additional context for future trials if the trajectory fails","source_id":"c234cb83764b899335af0950677ad024"},{"name":"EVANS","type":"PERSON","description":"Evans is an author who has contributed to the understanding of complex decision-making tasks and the range of potential trajectories or reasoning paths that are correct","source_id":"c234cb83764b899335af0950677ad024"},{"name":"EXPLORATION WEIGHT","type":"TECHNOLOGY","description":"Exploration weight is a parameter used in the MCTS algorithm to balance exploration and exploitation","source_id":"c234cb83764b899335af0950677ad024"},{"name":"PARENT NODE","type":"TECHNOLOGY","description":"Parent node is a node in the MCTS tree from which child nodes are derived","source_id":"c234cb83764b899335af0950677ad024"},{"name":"RETURN","type":"TECHNOLOGY","description":"Return is a value used in the backpropagation process of MCTS to update the value function","source_id":"c234cb83764b899335af0950677ad024"},{"name":"VALUE FUNCTION","type":"TECHNOLOGY","description":"Value function is a function used in MCTS to evaluate the desirability of a state\nValue function is a function proposed for LATS based on a self-generated LM score and other components","source_id":"c234cb83764b899335af0950677ad024","entity_type":"TECHNOLOGY"},{"name":"ENVIRONMENT MODEL","type":"TECHNOLOGY","description":"Environment model is a model required by MCTS to undo previous steps and form a searching tree","source_id":"c234cb83764b899335af0950677ad024"},{"name":"HISTORICAL TEXT INPUT","type":"TECHNOLOGY","description":"Historical text input is used in LM tasks to reset to any step by copy-pasting","source_id":"c234cb83764b899335af0950677ad024"},{"name":"BASE PROMPTING FRAMEWORK","type":"TECHNOLOGY","description":"Base prompting framework is the initial design framework for LM Agent to support reasoning or decision-making tasks","source_id":"c234cb83764b899335af0950677ad024"},{"name":"OBSERVATION","type":"TECHNOLOGY","description":"Observation is the input received by an agent from the environment at each time step","source_id":"c234cb83764b899335af0950677ad024"},{"name":"ACTION","type":"TECHNOLOGY","description":"Action is the output taken by an agent following a policy in response to an observation","source_id":"c234cb83764b899335af0950677ad024"},{"name":"POLICY","type":"TECHNOLOGY","description":"Policy is a strategy used by an agent to determine actions based on observations and previous actions","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LANGUAGE REPRESENTATIONS","type":"TECHNOLOGY","description":"Language representations are useful features of a language model leveraged by LM Agent for decision-making","source_id":"c234cb83764b899335af0950677ad024"},{"name":"ACTION SPACE","type":"TECHNOLOGY","description":"Action space is the set of all possible actions an agent can take, including permissible actions and reasoning traces","source_id":"c234cb83764b899335af0950677ad024"},{"name":"REASONING TRACES","type":"TECHNOLOGY","description":"Reasoning traces are thoughts used to formalize decisions by organizing information, planning future actions, or injecting internal knowledge","source_id":"c234cb83764b899335af0950677ad024"},{"name":"EXTERNAL TOOLS","type":"TECHNOLOGY","description":"External tools are tools or APIs used in reasoning tasks as part of the action space","source_id":"c234cb83764b899335af0950677ad024"},{"name":"TRAJECTORY","type":"TECHNOLOGY","description":"Trajectory is a sequence of actions or reasoning paths sampled by an agent","source_id":"c234cb83764b899335af0950677ad024"},{"name":"SEARCH ALGORITHM","type":"TECHNOLOGY","description":"Search algorithm is the main component of LATS that controls the problem-solving process with planning","source_id":"c234cb83764b899335af0950677ad024"},{"name":"TREE SEARCH","type":"TECHNOLOGY","description":"Tree search is a method used in LATS to frame decision-making as a search through a tree of possible states","source_id":"c234cb83764b899335af0950677ad024"},{"name":"NODE","type":"TECHNOLOGY","description":"Node is a state in the tree search, representing the original input, action sequence, and observation sequence","source_id":"c234cb83764b899335af0950677ad024"},{"name":"STATE","type":"TECHNOLOGY","description":"State is a representation of the current situation in the tree search, including input, actions, and observations","source_id":"c234cb83764b899335af0950677ad024"},{"name":"LONG-TERM MEMORY STRUCTURE","type":"TECHNOLOGY","description":"Long-term memory structure is an external storage used to store the tree in LATS","source_id":"c234cb83764b899335af0950677ad024"},{"name":"SCALAR VALUE","type":"TECHNOLOGY","description":"Scalar value is a value assigned to each new child node during evaluation to quantify the agent\u2019s progress","source_id":"c234cb83764b899335af0950677ad024"},{"name":"HEURISTIC","type":"TECHNOLOGY","description":"Heuristic is a method used to steer the search algorithm towards the most promising regions of the tree","source_id":"c234cb83764b899335af0950677ad024"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MCTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MCTS (Monte Carlo Tree Search) is a search algorithm that requires an environment model to undo previous steps and form a searching tree<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LM TASKS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">LM tasks refer to tasks involving language models, which can conveniently reset to any step by simply copy-pasting historical text input<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LM AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LM Agent is a system that supports sequential reasoning or decision-making tasks by leveraging language models as base decision-makers<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is an instantiation in which the action space consists of both permissible actions and the language space of reasoning traces<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a base prompting framework used in environments without feedback, such as reasoning tasks<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a search algorithm that unifies reasoning, acting, and planning in language models by adapting MCTS to language agents<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"P&#920;\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">P&#952; is a language model used as an agent, state evaluator, and feedback generator in LATS<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a variant of MCTS that relies on internal dynamics models to facilitate simulation<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on RAP<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"UCT ALGORITHM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">UCT (Upper Confidence bounds applied to Trees) is an algorithm used to balance exploration and exploitation in tree search<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"SELECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Selection is the first operation in LATS where the algorithm identifies a segment of the current tree most suitable for subsequent expansion<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"EXPANSION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Expansion is the second operation in LATS where the tree is expanded by sampling actions from P&#952; and adding new child nodes<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Evaluation is the third operation in LATS where a scalar value is assigned to each new child node to quantify the agent&#8217;s progress in task completion<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"SIMULATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Simulation is an operation in LATS where the algorithm simulates actions until a terminal node is reached<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"BACKPROPAGATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Backpropagation is an operation in LATS where the resulting value from a terminal node is used to update the value function along the path<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Reflection is an operation in LATS where a reflection is generated and used as additional context for future trials if the trajectory fails<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evans is an author who has contributed to the understanding of complex decision-making tasks and the range of potential trajectories or reasoning paths that are correct<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"EXPLORATION WEIGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Exploration weight is a parameter used in the MCTS algorithm to balance exploration and exploitation<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"PARENT NODE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Parent node is a node in the MCTS tree from which child nodes are derived<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"RETURN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Return is a value used in the backpropagation process of MCTS to update the value function<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Value function is a function used in MCTS to evaluate the desirability of a stateValue function is a function proposed for LATS based on a self-generated LM score and other components<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ENVIRONMENT MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Environment model is a model required by MCTS to undo previous steps and form a searching tree<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"HISTORICAL TEXT INPUT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Historical text input is used in LM tasks to reset to any step by copy-pasting<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"BASE PROMPTING FRAMEWORK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Base prompting framework is the initial design framework for LM Agent to support reasoning or decision-making tasks<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Observation is the input received by an agent from the environment at each time step<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"ACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Action is the output taken by an agent following a policy in response to an observation<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"POLICY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Policy is a strategy used by an agent to determine actions based on observations and previous actions<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LANGUAGE REPRESENTATIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language representations are useful features of a language model leveraged by LM Agent for decision-making<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"ACTION SPACE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Action space is the set of all possible actions an agent can take, including permissible actions and reasoning traces<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"REASONING TRACES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reasoning traces are thoughts used to formalize decisions by organizing information, planning future actions, or injecting internal knowledge<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"EXTERNAL TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External tools are tools or APIs used in reasoning tasks as part of the action space<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"TRAJECTORY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory is a sequence of actions or reasoning paths sampled by an agent<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"SEARCH ALGORITHM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search algorithm is the main component of LATS that controls the problem-solving process with planning<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"TREE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tree search is a method used in LATS to frame decision-making as a search through a tree of possible states<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"NODE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Node is a state in the tree search, representing the original input, action sequence, and observation sequence<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"STATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">State is a representation of the current situation in the tree search, including input, actions, and observations<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"LONG-TERM MEMORY STRUCTURE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Long-term memory structure is an external storage used to store the tree in LATS<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"SCALAR VALUE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Scalar value is a value assigned to each new child node during evaluation to quantify the agent&#8217;s progress<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <node id=\"HEURISTIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Heuristic is a method used to steer the search algorithm towards the most promising regions of the tree<\/data>      <data key=\"d2\">c234cb83764b899335af0950677ad024<\/data>    <\/node>    <edge source=\"MCTS\" target=\"LATS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS adapts MCTS to language agents to unify reasoning, acting, and planning in language models<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"LM TASKS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LM tasks can conveniently reset to any step, which mitigates the major shortcoming of MCTS<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"EXPLORATION WEIGHT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Exploration weight is a parameter used in the MCTS algorithm<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"PARENT NODE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Parent node is a component of the MCTS tree<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"VALUE FUNCTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Value function is used in MCTS to evaluate the desirability of a state<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"ENVIRONMENT MODEL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Environment model is required by MCTS to undo previous steps and form a searching tree<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM TASKS\" target=\"HISTORICAL TEXT INPUT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Historical text input is used in LM tasks to reset to any step<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"LATS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LM Agent is a component within LATS that supports sequential reasoning or decision-making tasks<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"REACT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LM Agent follows the ReAct instantiation for its action space<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"COT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">CoT is used as the base prompting framework in environments without feedback for LM Agent<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"EVANS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Evans contributed to the understanding of complex decision-making tasks, which is relevant to the LM Agent<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"BASE PROMPTING FRAMEWORK\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Base prompting framework is the initial design framework for LM Agent<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"OBSERVATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Observation is the input received by an agent from the environment<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"ACTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Action is the output taken by an agent following a policy<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"POLICY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Policy is a strategy used by an agent to determine actions<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"LANGUAGE REPRESENTATIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Language representations are leveraged by LM Agent for decision-making<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"ACTION SPACE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Action space is the set of all possible actions an agent can take<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"REASONING TRACES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Reasoning traces are used to formalize decisions by organizing information<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"EXTERNAL TOOLS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">External tools are used in reasoning tasks as part of the action space<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LM AGENT\" target=\"TRAJECTORY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Trajectory is a sequence of actions or reasoning paths sampled by an agent<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"P&#920;\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">P&#952; is repurposed as an agent, state evaluator, and feedback generator in LATS<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">LATS is compared to RAP, which also relies on internal dynamics models<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SEARCH ALGORITHM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Search algorithm is the main component of LATS<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TREE SEARCH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Tree search is used in LATS to frame decision-making<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LONG-TERM MEMORY STRUCTURE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Long-term memory structure is used to store the tree in LATS<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"LATS\" target=\"VALUE FUNCTION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Value function is proposed for LATS based on a self-generated LM score<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Hao is an author who has worked on RAP<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"UCT ALGORITHM\" target=\"SELECTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The UCT algorithm is used in the Selection operation of LATS to balance exploration and exploitation<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"SELECTION\" target=\"EXPANSION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Selection is followed by Expansion in the LATS process<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"EXPANSION\" target=\"EVALUATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Expansion is followed by Evaluation in the LATS process<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"EVALUATION\" target=\"SIMULATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Evaluation is followed by Simulation in the LATS process<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"EVALUATION\" target=\"SCALAR VALUE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Scalar value is assigned to each new child node during evaluation<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"EVALUATION\" target=\"HEURISTIC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Heuristic is used to steer the search algorithm during evaluation<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"SIMULATION\" target=\"BACKPROPAGATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Simulation is followed by Backpropagation in the LATS process<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"BACKPROPAGATION\" target=\"REFLECTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Backpropagation is followed by Reflection in the LATS process<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"BACKPROPAGATION\" target=\"RETURN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Return is used in the backpropagation process to update the value function<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"TREE SEARCH\" target=\"NODE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Node is a state in the tree search<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>    <edge source=\"TREE SEARCH\" target=\"STATE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">State is a representation of the current situation in the tree search<\/data>      <data key=\"d6\">c234cb83764b899335af0950677ad024<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"02ef0185bbeaaef92c3a8ee18b7a38cf","chunk":" long-term memory structure.\nEvaluation. The third operation assigns a scalar value to\neach new child node for selection and backpropagation.\nThis value effectively quantifies the agent\u2019s progress in task\ncompletion, serving as a heuristic to steer the search algo-\nrithm towards the most promising regions of the tree. As\nLATS does not involve training, we propose a novel value\nfunction for this setting based on two components: (1) a\nself-generated LM score and (2) a self-consistency score.\nInspired by ToT, we repurpose p\u03b8into a value function by\nprompting it to reason about a given state. To obtain a scalar\nvalue, we instruct p\u03b8to end its reasoning trace with a score\nindicating the correctness of the trajectory. Our key distinc-\ntion from ToT is that we obtain this value after obtaining\nthe environmental feedback, improving value assignment.\nThis also enables scaling to more challenging environments,as it is difficult for LMs to improve their responses with-\nout external feedback (Huang et al., 2024). Additionally,\nto further improve value assignment, we introduce an ad-\nditional heuristic based on self-consistency (Wang et al.,\n2022), in which actions sampled multiple times at the same\nstate tend to be more accurate. This results in the overall\nvalue function:\nV(s) =\u03bb\u2217LM(s) + (1\u2212\u03bb)\u2217SC(s), (2)\nwhere \u03bbis a hyperparameter. Notably, our method offers\nenhanced flexibility over programmed heuristics (Campbell\net al., 2002) and greater efficiency than learned heuristics\n(Silver et al., 2017).\nSimulation. The fourth operation expands the currently se-\nlected node until a terminal state is reached. At each depth\nlevel, we sample and evaluate nodes with the same opera-\ntions but prioritize nodes of the highest value. Reaching a\nterminal state provides objective feedback on the correct-\nness of a trajectory. If the task is completed successfully,\nthen LATS terminates the search. If the solution is partially\nsuccessful or unsuccessful, then we perform two additional\noperations as described below. The success of a trajectory is\ndetermined by the design of the specific environment, such\nas finalizing a purchase in web navigation environments.\nBackpropagation. This operation updates the values of the\ntree based on the outcome of a trajectory. For each node\ns0, s1, . . . , s lin the trajectory from root (initial state s0)\nof the searching tree to leaf (terminal state sl), its value is\nupdated to reflect the outcome of the simulation by N(si) =\nN(si\u22121)+1 andV(si) =V(si\u22121)N(si\u22121)+r\nN(si), where ris the\nreward. These updated values are used in the UCT formula\n(Eq. 1) to guide the selection of the next node.\nReflection. In addition to the environmental feedback, we\nleverage self-reflection to further refine the decision-making\n5Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nBase LM 0.32\nCoT (Wei et al., 2022) 0.34\nCoT - SC (Wang et al., 2022) 0.38\nToT (Yao et al., 2023a) 0.55\nRAP (Hao et al., 2023) 0.60\nRAP ( n= 10 ) 0.60\nLATS (CoT) 0.62\nTable 2. GPT-3.5 reasoning -based prompting results on HotpotQA.\nLATS achieves the highest exact match (EM) for reasoning. We\nsample n= 5nodes during expansion and k= 50 trajectories.\nprocess (Shinn et al., 2023; Madaan et al., 2023). Upon\nencountering an unsuccessful terminal node, p\u03b8is prompted\nwith the trajectory and final reward to provide a verbal self-\nreflection that summarizes the errors in the reasoning or\nacting process and proposes superior alternatives. We store\nboth failed trajectories and corresponding reflections in the\nmemory. In subsequent iterations, these are integrated as\nadditional context to the agent and value function, refining\nboth through in-context learning. This imparts a semantic\ngradient signal more useful than a scalar value, enabling\nthe agent to learn from trial and error without the cost of\nexpensive optimization such as reinforcement learning.\nDiscussion. Conceptually, LATS has several notable advan-\ntages as a general framework for reasoning and decision-\nmaking with LM agents. (1) Generality : LATS supports\nboth reasoning and decision-making tasks by defining a\nshared space of thoughts and actions. (2) Deliberation :\nLeveraging MCTS and LM value function in LATS en-\nsures a principled search that selects options with high value\nwhile exploring promising alternatives. (3) Adaptability :\nIncorporating external feedback through observations and\nself-reflection in LATS enables greater adaptation during\nproblem-solving. (4) Flexibility : LATS can accommodate\ndifferent scenarios, environments, and resource stipulations\nby modifying state design and tree dimensions. (5) Modu-\nlarity : The base LM agent, reflection generator, and value\nfunction can be independently altered and adapted to indi-\nvidual LM properties.\n5. Experiments\nTo demonstrate the general applicability of LATS, we eval-\nuate our method on a variety of domains that require rea-\nsoning and acting: programming (Chen et al., 2021; Austin\net al., 202","chunk_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"P\u0398","type":"TECHNOLOGY","description":"P\u03b8 is a language model used within the LATS framework to reason about a given state and provide a scalar value indicating the correctness of the trajectory","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT (Tree of Thoughts) is a method that inspires the value function in LATS by prompting the language model to reason about a given state","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"HUANG","type":"PERSON","description":"Huang is an author who contributed to the research on improving value assignment in language models by incorporating environmental feedback","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author who contributed to the research on self-consistency in language models","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"CAMPBELL","type":"PERSON","description":"Campbell is an author who contributed to the research on programmed heuristics in 2002","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"SILVER","type":"PERSON","description":"Silver is an author who contributed to the research on learned heuristics in 2017","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"HOTPOTQA","type":"DATASET","description":"HotpotQA is a dataset used to evaluate reasoning-based prompting results in language models","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"DATASET"},{"name":"COT","type":"TECHNOLOGY","description":"CoT (Chain of Thought) is a prompting method used in language models to improve reasoning accuracy","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP (Reasoning and Planning) is a method used in language models to improve reasoning and planning capabilities","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used to evaluate reasoning-based prompting results on the HotpotQA dataset","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who contributed to the research on self-reflection in language models","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who contributed to the research on self-reflection in language models","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"MCTS","type":"TECHNOLOGY","description":"MCTS (Monte Carlo Tree Search) is a search algorithm used in the LATS framework to ensure a principled search that selects options with high value while exploring promising alternatives","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"TECHNOLOGY"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who contributed to the research on programming domains in 2021","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"AUSTIN","type":"PERSON","description":"Austin is an author who contributed to the research on programming domains","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf","entity_type":"PERSON"},{"name":"LATS","type":"","description":"","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SELF-CONSISTENCY","type":"TECHNOLOGY","description":"Self-consistency is a heuristic used in LATS where actions sampled multiple times at the same state tend to be more accurate","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"UCT FORMULA","type":"TECHNOLOGY","description":"The UCT (Upper Confidence bounds applied to Trees) formula is used in LATS to guide the selection of the next node based on updated values","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"ENVIRONMENTAL FEEDBACK","type":"TECHNOLOGY","description":"Environmental feedback is used in LATS to improve value assignment and scaling to more challenging environments","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"SELF-REFLECTION","type":"TECHNOLOGY","description":"Self-reflection is a process in LATS where the language model provides a verbal summary of errors and proposes superior alternatives after encountering an unsuccessful terminal node","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"IN-CONTEXT LEARNING","type":"TECHNOLOGY","description":"In-context learning is a method used in LATS to refine the agent and value function by integrating failed trajectories and corresponding reflections as additional context","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"REINFORCEMENT LEARNING","type":"TECHNOLOGY","description":"Reinforcement learning is a type of machine learning that LATS aims to avoid by using self-reflection and in-context learning for optimization","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"PROGRAMMING","type":"DOMAIN","description":"Programming is one of the domains evaluated by LATS to demonstrate its general applicability","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"},{"name":"WEB NAVIGATION","type":"DOMAIN","description":"Web navigation is an environment where LATS determines the success of a trajectory, such as finalizing a purchase","source_id":"02ef0185bbeaaef92c3a8ee18b7a38cf"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"P&#920;\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">P&#952; is a language model used within the LATS framework to reason about a given state and provide a scalar value indicating the correctness of the trajectory<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT (Tree of Thoughts) is a method that inspires the value function in LATS by prompting the language model to reason about a given state<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang is an author who contributed to the research on improving value assignment in language models by incorporating environmental feedback<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who contributed to the research on self-consistency in language models<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAMPBELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Campbell is an author who contributed to the research on programmed heuristics in 2002<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SILVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Silver is an author who contributed to the research on learned heuristics in 2017<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotpotQA is a dataset used to evaluate reasoning-based prompting results in language models<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a prompting method used in language models to improve reasoning accuracy<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP (Reasoning and Planning) is a method used in language models to improve reasoning and planning capabilities<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used to evaluate reasoning-based prompting results on the HotpotQA dataset<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who contributed to the research on self-reflection in language models<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who contributed to the research on self-reflection in language models<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MCTS (Monte Carlo Tree Search) is a search algorithm used in the LATS framework to ensure a principled search that selects options with high value while exploring promising alternatives<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who contributed to the research on programming domains in 2021<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AUSTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Austin is an author who contributed to the research on programming domains<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-consistency is a heuristic used in LATS where actions sampled multiple times at the same state tend to be more accurate<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"UCT FORMULA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The UCT (Upper Confidence bounds applied to Trees) formula is used in LATS to guide the selection of the next node based on updated values<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Environmental feedback is used in LATS to improve value assignment and scaling to more challenging environments<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-reflection is a process in LATS where the language model provides a verbal summary of errors and proposes superior alternatives after encountering an unsuccessful terminal node<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"IN-CONTEXT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">In-context learning is a method used in LATS to refine the agent and value function by integrating failed trajectories and corresponding reflections as additional context<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reinforcement learning is a type of machine learning that LATS aims to avoid by using self-reflection and in-context learning for optimization<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Programming is one of the domains evaluated by LATS to demonstrate its general applicability<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <node id=\"WEB NAVIGATION\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Web navigation is an environment where LATS determines the success of a trajectory, such as finalizing a purchase<\/data>      <data key=\"d2\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/node>    <edge source=\"TOT\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The value function in LATS is inspired by the ToT method<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"HUANG\" target=\"LATS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Huang contributed to the research on improving value assignment in language models, which is a component of LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"WANG\" target=\"LATS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wang contributed to the research on self-consistency, which is a component of the value function in LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"CAMPBELL\" target=\"LATS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Campbell's research on programmed heuristics is referenced in the context of LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SILVER\" target=\"LATS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Silver's research on learned heuristics is referenced in the context of LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS is evaluated using the HotpotQA dataset<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"COT\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses the CoT method for reasoning-based prompting<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"RAP\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses the RAP method for reasoning and planning<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS is evaluated using the GPT-3.5 language model<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"SHINN\" target=\"LATS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Shinn contributed to the research on self-reflection, which is a component of LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"MADAAN\" target=\"LATS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Madaan contributed to the research on self-reflection, which is a component of LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"LATS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LATS uses the MCTS algorithm to ensure a principled search<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"LATS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Chen contributed to the research on programming domains, which is one of the domains evaluated by LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"AUSTIN\" target=\"LATS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Austin contributed to the research on programming domains, which is one of the domains evaluated by LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-CONSISTENCY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Self-consistency is a heuristic used in LATS to improve value assignment<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"UCT FORMULA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The UCT formula is used in LATS to guide the selection of the next node<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENTAL FEEDBACK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Environmental feedback is used in LATS to improve value assignment and scaling to more challenging environments<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELF-REFLECTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Self-reflection is used in LATS to refine the decision-making process<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"IN-CONTEXT LEARNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">In-context learning is used in LATS to refine the agent and value function<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS aims to avoid the cost of expensive optimization such as reinforcement learning<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Programming is one of the domains evaluated by LATS<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEB NAVIGATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Web navigation is an environment where LATS determines the success of a trajectory<\/data>      <data key=\"d6\">02ef0185bbeaaef92c3a8ee18b7a38cf<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"fb9cb0c0984d44c3da881886ed637e55","chunk":"ulations\nby modifying state design and tree dimensions. (5) Modu-\nlarity : The base LM agent, reflection generator, and value\nfunction can be independently altered and adapted to indi-\nvidual LM properties.\n5. Experiments\nTo demonstrate the general applicability of LATS, we eval-\nuate our method on a variety of domains that require rea-\nsoning and acting: programming (Chen et al., 2021; Austin\net al., 2022), HotPotQA (Yang et al., 2018), WebShop (Yao\net al., 2022), and Game of 24 (Yao et al., 2023a).Prompt Method HotpotQA (EM) \u2191\nReAct (Yao et al., 2023b) 0.32\nReAct (best of k) 0.38\nReflexion (Shinn et al., 2023) 0.51\nToT (ReAct) 0.39\nRAP (ReAct) 0.54\nLATS (ReAct) 0.63\nLATS ( n= 3) 0.58\nLATS ( n= 10 ) 0.65\nLATS (CoT + ReAct) 0.71\nTable 3. GPT-3.5 acting -based prompting results on HotpotQA.\nLATS achieves the highest exact match (EM) for acting. We\nsample n= 5nodes and use k= 50 trajectories. We also evaluate\nsampling ReAct ktimes and using both CoT and ReAct base\nprompting designs for LATS, which achieves the best performance.\nNote that LATS outperforms ToT and RAP with ReAct prompting,\nwhich are the simple adaptations of search algorithms to decision-\nmaking.\n5.1. HotPotQA\nFor a task that can be approached with both reasoning-based\nand acting-based strategies, we consider HotPotQA (Yang\net al., 2018), a multi-hop question-answering benchmark\nthat requires retrieval over two or more Wikipedia passages.\nFor the action space, in addition to LM thoughts, we follow\nthe setup from Yao et al. (2023b), which provides the agent\nwith API calls to search and retrieve information. The output\nof these API calls and self-generated reflections form the\nobservation space. Note that consistent with previous work\n(Yao et al., 2023b; Shinn et al., 2023), we use an oracle\nsetup for HotPotQA, in which the environment provides\nfeedback about the answer\u2019s correctness upon receiving an\nanswer. This enables a fair comparison between our method\nand baselines in scenarios where the quality of feedback is\nhigh, allowing us to focus our evaluation on how well the\nagent incorporates external feedback. We use a subset of\n100 questions and three few-shot examples for each method.\nFor ToT, we use DFS as the base search algorithm. For all\nmethods that involve sampling, including LATS, we sample\nk= 50 trajectories. More details are in Appendix Sec. D.\nWe evaluate internal reasoning strategies by removing ac-\ntions and observations from the context, corresponding to\nCoT (Wei et al., 2022) and its variants, CoT-SC (Wang et al.,\n2022), ToT (Yao et al., 2023a), and RAP (Hao et al., 2023).\nThese methods rely solely on the agent\u2019s existing knowledge\nto answer the question. We further consider acting-based\nmethods ReAct, Reflexion, and LATS, which augment the\nagent with the interactive API environment and primarily\nevaluate its information retrieval abilities. We also design\na simple integration of search algorithms with LM agents,\nextending ToT and RAP with ReAct prompting to handle\n6Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method Model Pass@1 \u2191\nCoT (Wei et al., 2022) GPT-3.5 46.9\nReAct (Yao et al., 2023b) GPT-3.5 56.9\nReflexion (Shinn et al., 2023) GPT-3.5 68.1\nToT (Yao et al., 2023a) GPT-3.5 54.4\nRAP (Hao et al., 2023) GPT-3.5 63.1\nLATS (ReAct) GPT-3.5 83.8\nBase LM GPT-4 80.1\nReflexion GPT-4 91.0\nLATS (ReAct) GPT-4 92.7\nTable 4. GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval.\nPrompting with LATS achieves the best performance. We sample\n5 solutions during expansion for 8 iterations.\nexternal observations. In addition, while LATS is designed\nfor scenarios where external feedback can enhance reason-\ning, we also implement a reasoning-only version with CoT\nas the base prompting framework. Moreover, we combine\ninternal and external reasoning in LATS by first prompting\nwith a CoT-based prompt and then switching to a ReAct-\nbased prompt upon failure. This is closer to how humans\nmight approach this task by using tools to retrieve additional\ninformation only when the answer is not already known.\nResults. We observe in Tab. 2 and Tab. 3 that both in-\nternal reasoning and external retrieval strategies perform\nwell on HotPotQA. Due to their large-scale training corpus,\nmodern LMs already encode factual","chunk_id":"fb9cb0c0984d44c3da881886ed637e55","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"WEBSHOP","type":"DATASET","description":"WebShop is a dataset used to evaluate methods requiring reasoning and acting","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"GAME OF 24","type":"DATASET","description":"Game of 24 is a dataset used to evaluate methods requiring reasoning and acting","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used in various experiments, including those involving LATS, CoT, ReAct, Reflexion, ToT, and RAP","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a language model used in various experiments, including those involving LATS and Reflexion","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a prompting method used in various experiments, achieving high performance in tasks like HotPotQA and HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT (Tree of Thoughts) is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"COT","type":"TECHNOLOGY","description":"CoT (Chain of Thought) is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who has worked on programming-related tasks evaluated in the experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"AUSTIN","type":"PERSON","description":"Austin is an author who has worked on programming-related tasks evaluated in the experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YANG","type":"PERSON","description":"Yang is an author who has worked on HotPotQA, a multi-hop question-answering benchmark","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"YAO","type":"PERSON","description":"Yao is an author who has worked on various datasets and methods, including WebShop, Game of 24, ReAct, and ToT","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on Reflexion, a prompting method used in various experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has worked on CoT, a prompting method used in various experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on CoT-SC, a variant of CoT used in various experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on RAP, a prompting method used in various experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"LATS","type":"","description":"","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"COT-SC","type":"","description":"\nCoT-SC is a variant of the Chain of Thought (CoT) prompting method used in various experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55","entity_type":"TECHNOLOGY"},{"name":"EXPERIMENTS","type":"EVENT","description":"Experiments are conducted to demonstrate the general applicability of LATS across various domains requiring reasoning and acting","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"PROGRAMMING","type":"DOMAIN","description":"Programming is one of the domains evaluated in the experiments to demonstrate the general applicability of LATS","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"PROMPT METHOD","type":"TECHNOLOGY","description":"Prompt Method refers to the different prompting techniques used in the experiments, such as ReAct, Reflexion, ToT, RAP, and LATS","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"EXACT MATCH (EM)","type":"METRIC","description":"Exact Match (EM) is a metric used to evaluate the performance of different methods on HotPotQA","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"PASS@1","type":"METRIC","description":"Pass@1 is a metric used to evaluate the performance of different methods on HumanEval","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used to evaluate the performance of different methods, including LATS, on programming tasks","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"DFS","type":"TECHNOLOGY","description":"DFS (Depth-First Search) is a base search algorithm used in the ToT method for HotPotQA","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"API CALLS","type":"TECHNOLOGY","description":"API Calls are used to search and retrieve information in the HotPotQA setup, forming part of the observation space for the agent","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"ORACLE SETUP","type":"TECHNOLOGY","description":"Oracle Setup is used in HotPotQA to provide feedback about the answer\u2019s correctness, enabling fair comparison between methods","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"FEW-SHOT EXAMPLES","type":"TECHNOLOGY","description":"Few-Shot Examples are used in the experiments to evaluate different methods on a subset of 100 questions for HotPotQA","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"INTERNAL REASONING","type":"TECHNOLOGY","description":"Internal Reasoning refers to strategies that rely solely on the agent\u2019s existing knowledge to answer questions, evaluated in the experiments","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"EXTERNAL RETRIEVAL","type":"TECHNOLOGY","description":"External Retrieval refers to strategies that augment the agent with an interactive API environment to evaluate its information retrieval abilities","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"SEARCH ALGORITHMS","type":"TECHNOLOGY","description":"Search Algorithms are integrated with LM agents in the experiments, extending ToT and RAP with ReAct prompting to handle external observations","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"INTERNAL AND EXTERNAL REASONING","type":"TECHNOLOGY","description":"Internal and External Reasoning is a combined approach in LATS that uses CoT-based prompts and switches to ReAct-based prompts upon failure","source_id":"fb9cb0c0984d44c3da881886ed637e55"},{"name":"MODERN LMS","type":"TECHNOLOGY","description":"Modern LMs refer to large-scale language models that already encode factual information due to their extensive training corpus","source_id":"fb9cb0c0984d44c3da881886ed637e55"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used to evaluate methods requiring reasoning and acting<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"GAME OF 24\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Game of 24 is a dataset used to evaluate methods requiring reasoning and acting<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used in various experiments, including those involving LATS, CoT, ReAct, Reflexion, ToT, and RAP<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a language model used in various experiments, including those involving LATS and Reflexion<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a prompting method used in various experiments, achieving high performance in tasks like HotPotQA and HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT (Tree of Thoughts) is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a prompting method used in various experiments, including those involving LATS, achieving high performance in tasks like HotPotQA and HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who has worked on programming-related tasks evaluated in the experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"AUSTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Austin is an author who has worked on programming-related tasks evaluated in the experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who has worked on HotPotQA, a multi-hop question-answering benchmark<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on various datasets and methods, including WebShop, Game of 24, ReAct, and ToT<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on Reflexion, a prompting method used in various experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has worked on CoT, a prompting method used in various experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on CoT-SC, a variant of CoT used in various experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on RAP, a prompting method used in various experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\" \/>      <data key=\"d1\">CoT-SC is a variant of the Chain of Thought (CoT) prompting method used in various experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"EXPERIMENTS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Experiments are conducted to demonstrate the general applicability of LATS across various domains requiring reasoning and acting<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Programming is one of the domains evaluated in the experiments to demonstrate the general applicability of LATS<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"PROMPT METHOD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompt Method refers to the different prompting techniques used in the experiments, such as ReAct, Reflexion, ToT, RAP, and LATS<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"EXACT MATCH (EM)\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Exact Match (EM) is a metric used to evaluate the performance of different methods on HotPotQA<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"PASS@1\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Pass@1 is a metric used to evaluate the performance of different methods on HumanEval<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used to evaluate the performance of different methods, including LATS, on programming tasks<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DFS (Depth-First Search) is a base search algorithm used in the ToT method for HotPotQA<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"API CALLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">API Calls are used to search and retrieve information in the HotPotQA setup, forming part of the observation space for the agent<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"ORACLE SETUP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Oracle Setup is used in HotPotQA to provide feedback about the answer&#8217;s correctness, enabling fair comparison between methods<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"FEW-SHOT EXAMPLES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Few-Shot Examples are used in the experiments to evaluate different methods on a subset of 100 questions for HotPotQA<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"INTERNAL REASONING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Internal Reasoning refers to strategies that rely solely on the agent&#8217;s existing knowledge to answer questions, evaluated in the experiments<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"EXTERNAL RETRIEVAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External Retrieval refers to strategies that augment the agent with an interactive API environment to evaluate its information retrieval abilities<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search Algorithms are integrated with LM agents in the experiments, extending ToT and RAP with ReAct prompting to handle external observations<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"INTERNAL AND EXTERNAL REASONING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Internal and External Reasoning is a combined approach in LATS that uses CoT-based prompts and switches to ReAct-based prompts upon failure<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <node id=\"MODERN LMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Modern LMs refer to large-scale language models that already encode factual information due to their extensive training corpus<\/data>      <data key=\"d2\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/node>    <edge source=\"HOTPOTQA\" target=\"YANG\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Yang has worked on HotPotQA, a multi-hop question-answering benchmark<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"EXPERIMENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Experiments evaluate the applicability of LATS on the HotPotQA dataset<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"EXACT MATCH (EM)\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Exact Match (EM) is a metric used to evaluate the performance of different methods on HotPotQA<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"API CALLS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">API Calls are used to search and retrieve information in the HotPotQA setup<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"ORACLE SETUP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Oracle Setup is used in HotPotQA to provide feedback about the answer&#8217;s correctness<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"FEW-SHOT EXAMPLES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Few-Shot Examples are used in the experiments to evaluate different methods on a subset of 100 questions for HotPotQA<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is evaluated on WebShop, achieving high performance in reasoning and acting tasks<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"YAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yao has worked on WebShop, a dataset used to evaluate methods requiring reasoning and acting<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"EXPERIMENTS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Experiments evaluate the applicability of LATS on the WebShop dataset<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is evaluated on Game of 24, achieving high performance in reasoning and acting tasks<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"YAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yao has worked on Game of 24, a dataset used to evaluate methods requiring reasoning and acting<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"EXPERIMENTS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Experiments evaluate the applicability of LATS on the Game of 24 dataset<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"LATS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"REACT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ReAct is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"REFLEXION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reflexion is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"TOT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">ToT is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"RAP\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">RAP is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"COT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">CoT is used with GPT-3.5 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"LATS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS is used with GPT-4 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"REACT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ReAct is used with GPT-4 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"REFLEXION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reflexion is used with GPT-4 in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REACT\" target=\"LATS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS uses ReAct as a prompting method in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yao has worked on ReAct, a prompting method used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REACT\" target=\"PROMPT METHOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ReAct is one of the prompt methods used in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REACT\" target=\"EXTERNAL RETRIEVAL\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">External Retrieval strategies are evaluated using ReAct<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is compared with Reflexion in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shinn has worked on Reflexion, a prompting method used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"PROMPT METHOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reflexion is one of the prompt methods used in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"EXTERNAL RETRIEVAL\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">External Retrieval strategies are evaluated using Reflexion<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"TOT\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is compared with ToT in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"TOT\" target=\"YAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yao has worked on ToT, a prompting method used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"TOT\" target=\"PROMPT METHOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ToT is one of the prompt methods used in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"TOT\" target=\"DFS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">DFS is a base search algorithm used in the ToT method for HotPotQA<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"TOT\" target=\"SEARCH ALGORITHMS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Search Algorithms are integrated with ToT in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"RAP\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is compared with RAP in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Hao has worked on RAP, a prompting method used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"RAP\" target=\"PROMPT METHOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">RAP is one of the prompt methods used in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"RAP\" target=\"SEARCH ALGORITHMS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Search Algorithms are integrated with RAP in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"COT\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS is compared with CoT in various experiments, achieving high performance<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"COT\" target=\"WEI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wei has worked on CoT, a prompting method used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"COT\" target=\"COT-SC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">CoT-SC is a variant of the Chain of Thought (CoT) prompting method<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"COT\" target=\"INTERNAL REASONING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Internal Reasoning strategies are evaluated using CoT and its variants<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"AUSTIN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Chen and Austin have worked on programming-related tasks evaluated in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"WANG\" target=\"COT-SC\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Wang has worked on CoT-SC, a variant of CoT used in various experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXPERIMENTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Experiments are conducted to demonstrate the general applicability of LATS across various domains<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROMPT METHOD\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS is one of the prompt methods used in the experiments<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS is evaluated on HumanEval, achieving high performance in programming tasks<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXTERNAL RETRIEVAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">External Retrieval strategies are evaluated using LATS<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INTERNAL AND EXTERNAL REASONING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Internal and External Reasoning is a combined approach used in LATS<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MODERN LMS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Modern LMs are used in LATS experiments, leveraging their large-scale training corpus<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"EXPERIMENTS\" target=\"PROGRAMMING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Experiments evaluate the applicability of LATS in the domain of programming<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>    <edge source=\"PASS@1\" target=\"HUMANEVAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Pass@1 is a metric used to evaluate the performance of different methods on HumanEval<\/data>      <data key=\"d6\">fb9cb0c0984d44c3da881886ed637e55<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"99d90aededb61e04241516ed9ec656cc","chunk":" in LATS by first prompting\nwith a CoT-based prompt and then switching to a ReAct-\nbased prompt upon failure. This is closer to how humans\nmight approach this task by using tools to retrieve additional\ninformation only when the answer is not already known.\nResults. We observe in Tab. 2 and Tab. 3 that both in-\nternal reasoning and external retrieval strategies perform\nwell on HotPotQA. Due to their large-scale training corpus,\nmodern LMs already encode factual knowledge and can\noften directly answer the question correctly. While CoT can\nslightly enhance performance on questions requiring rea-\nsoning, larger gains are observed with search methods ToT\nand RAP (Tab. 2, Row 4, 5), which can sample and explore\nmore outputs. We observe similar results for acting-based\nmethods. LATS surpasses ReAct, even when sampling the\nsame number of trajectories, by expanding more nodes with\nprincipled search. This is demonstrated when modifying\nn, the number of nodes expanded during each iteration. In-\ncreasing ncan consistently improve performance, although\nat greater computational and inference costs. LATS also\noutperforms RAP on internal reasoning, but has higher per-\nformance on the decision-making setting of HotPotQA than\nthe reasoning setting. Contrary to LATS, the ReAct versions\nof ToT and RAP (Tab. 3, Row 4, 5) perform even worse than\nthe reasoning-only setting of HotPotQA, which indicates\nthat the acting-based setting is more challenging and adap-\ntation of search algorithms to decision-making scenarios\nis non-trivial . Combining internal and external reasoning\nin LATS results in the highest performance, indicating the\nimportance of external feedback in augmenting reasoning\neven in tasks where the base LM can already perform.Prompt Method Pass@1 \u2191\nCoT (Wei et al., 2022) 54.9\nReAct (Wei et al., 2022) 67.0\nReflexion (Shinn et al., 2023) 70.0\nToT (Yao et al., 2023a) 65.8\nRAP (Hao et al., 2023) 71.4\nLATS (ReAct) 81.1\nTable 5. GPT-3.5 Pass@1 accuracy on MBPP. Prompting with\nLATS achieves the highest performance. We sample 5 solutions\nduring expansion for 8 iterations.\n5.2. Programming\nTo demonstrate the importance of external observations\nfor complex reasoning tasks, we evaluate the baselines\nand LATS on programming with HumanEval (Chen et al.,\n2021)1and MBPP (Austin et al., 2022). Both datasets mea-\nsure the correctness of synthesized programs in Python from\nnatural language docstrings. We use individual solutions\nas the action space and test suite and compiler feedback as\nthe external observation. We follow Chen et al. (2023a) and\nuse an LM to generate a synthetic test suite of syntactically\nvalid \u201cassert\u201d statements for each question. For each step,\nthe solution is evaluated on this test suite, and the results,\nincluding successful and failed tests and compiler output,\nare added to the context as an observation.\nFor this task, the reasoning and acting baselines share an\naction space, but acting methods are able to incorporate\nobservations as additional context. For LATS, since each\naction corresponds to a complete solution, we skip the sim-\nulation step of LATS and directly use the percentage of\npassed tests as the backpropagated reward. We use k= 8\niterations, set the number of generated tests at 4, and sam-\nplen= 5solutions during expansion. After the search is\ncompleted, we select the solution with the highest value and\nevaluate it on the real test suite for the pass@1 accuracy\nevaluation. More details can be found in Appendix Sec. D.\nResults. Tab. 4 and Tab. 5 show that both search and seman-\ntic feedback are crucial for better performance. Despite not\nusing observations, ToT and RAP are competitive with Re-\nflexion. LATS has the highest performance on both datasets.\nRAP uses a search algorithm similar to LATS, which reveals\nthe importance of external feedback for difficult reasoning\ntasks such as programming. With GPT-4, using LATS sets\nthe state of the art for HumanEval, validating that LATS can\nbe used with more advanced LMs for higher performance.\n1Some baselines use 161 questions from HumanEval. We\nuse all 164 questions for LATS and find minimal performance\ndifferences, so we report baselines for both settings.\n7Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nMethod Score\u2191SR\u2191\nReAct (Yao et al., 2023b) 53.8 28.0\nReAct (best of k) 59.1 32.0\nReflexion (Shinn et al., 2023) 64.2 35.0\nLATS (ReAct) 75.9 38.0\nIL(Yao et al., 2022) 59.9 29.1\nIL+RL (Yao et al., 2022) 62.4 28.7\nFine-tuning (Furuta et al., 2024) 67.5 45.0\nExpert 82.1 59.6\nTable 6. Score and success rate (SR) on WebShop. Results are\norganized into prompting, RL-based training, and human","chunk_id":"99d90aededb61e04241516ed9ec656cc","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a method that combines internal reasoning and external retrieval strategies to perform well on tasks like HotPotQA and programming with HumanEval and MBPP","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"COT","type":"TECHNOLOGY","description":"CoT (Chain of Thought) is a prompting method that enhances performance on questions requiring reasoning","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a prompting method that, when combined with LATS, achieves high performance on tasks like HotPotQA and programming","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used to evaluate internal reasoning and external retrieval strategies in language models","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT (Tree of Thoughts) is a search method that can sample and explore more outputs, showing larger gains in performance on reasoning tasks","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a search method that performs well on reasoning tasks and uses a search algorithm similar to LATS","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"MBPP","type":"DATASET","description":"MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model evaluated on MBPP, where LATS achieves the highest performance","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a more advanced language model that, when used with LATS, sets the state of the art for HumanEval","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a prompting method that performs well on reasoning tasks and is competitive with ToT and RAP","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"IL","type":"TECHNOLOGY","description":"IL (Imitation Learning) is a method used in combination with RL for training language models","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"RL","type":"TECHNOLOGY","description":"RL (Reinforcement Learning) is a method used in combination with IL for training language models","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"WEBSHOP","type":"DATASET","description":"WebShop is a dataset used to evaluate the performance of various prompting and training methods in language models","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"FURUTA","type":"PERSON","description":"Furuta is an author who has worked on fine-tuning methods for language models","source_id":"99d90aededb61e04241516ed9ec656cc","entity_type":"PERSON"},{"name":"SEARCH METHODS","type":"TECHNOLOGY","description":"Search methods like ToT and RAP are used to sample and explore more outputs, showing larger gains in performance on reasoning tasks","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"PROGRAMMING","type":"ACTIVITY","description":"Programming is a task evaluated using datasets like HumanEval and MBPP to measure the correctness of synthesized programs in Python from natural language docstrings","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HUMANEVAL DATASET","type":"DATASET","description":"HumanEval is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"MBPP DATASET","type":"DATASET","description":"MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-3.5 MODEL","type":"TECHNOLOGY","description":"GPT-3.5 is a language model evaluated on MBPP, where LATS achieves the highest performance","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"GPT-4 MODEL","type":"TECHNOLOGY","description":"GPT-4 is a more advanced language model that, when used with LATS, sets the state of the art for HumanEval","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"FINE-TUNING","type":"TECHNOLOGY","description":"Fine-tuning is a method used to improve the performance of language models, as demonstrated by Furuta et al. in 2024","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"EXPERT","type":"PERSON","description":"Expert refers to a human expert whose performance is used as a benchmark in the WebShop dataset","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"WEBSHOP DATASET","type":"DATASET","description":"WebShop is a dataset used to evaluate the performance of various prompting and training methods in language models","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on the Reflexion prompting method","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on the RAP search method","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who has worked on the HumanEval dataset and synthetic test suite generation for programming tasks","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"AUSTIN","type":"PERSON","description":"Austin is an author who has worked on the MBPP dataset","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"YAO","type":"PERSON","description":"Yao is an author who has worked on multiple methods including ToT, ReAct, and IL+RL","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has worked on the CoT and ReAct prompting methods","source_id":"99d90aededb61e04241516ed9ec656cc"},{"name":"CHAIN OF THOUGHT","type":"","description":"","source_id":"99d90aededb61e04241516ed9ec656cc"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a method that combines internal reasoning and external retrieval strategies to perform well on tasks like HotPotQA and programming with HumanEval and MBPP<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a prompting method that enhances performance on questions requiring reasoning<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a prompting method that, when combined with LATS, achieves high performance on tasks like HotPotQA and programming<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used to evaluate internal reasoning and external retrieval strategies in language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT (Tree of Thoughts) is a search method that can sample and explore more outputs, showing larger gains in performance on reasoning tasks<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a search method that performs well on reasoning tasks and uses a search algorithm similar to LATS<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"MBPP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model evaluated on MBPP, where LATS achieves the highest performance<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a more advanced language model that, when used with LATS, sets the state of the art for HumanEval<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a prompting method that performs well on reasoning tasks and is competitive with ToT and RAP<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"IL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">IL (Imitation Learning) is a method used in combination with RL for training language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"RL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RL (Reinforcement Learning) is a method used in combination with IL for training language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used to evaluate the performance of various prompting and training methods in language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"FURUTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Furuta is an author who has worked on fine-tuning methods for language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SEARCH METHODS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search methods like ToT and RAP are used to sample and explore more outputs, showing larger gains in performance on reasoning tasks<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Programming is a task evaluated using datasets like HumanEval and MBPP to measure the correctness of synthesized programs in Python from natural language docstrings<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HUMANEVAL DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"MBPP DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MBPP (Mostly Basic Python Problems) is a dataset used to measure the correctness of synthesized programs in Python from natural language docstrings<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-3.5 MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model evaluated on MBPP, where LATS achieves the highest performance<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"GPT-4 MODEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a more advanced language model that, when used with LATS, sets the state of the art for HumanEval<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"FINE-TUNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Fine-tuning is a method used to improve the performance of language models, as demonstrated by Furuta et al. in 2024<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"EXPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Expert refers to a human expert whose performance is used as a benchmark in the WebShop dataset<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"WEBSHOP DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">WebShop is a dataset used to evaluate the performance of various prompting and training methods in language models<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on the Reflexion prompting method<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on the RAP search method<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who has worked on the HumanEval dataset and synthetic test suite generation for programming tasks<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"AUSTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Austin is an author who has worked on the MBPP dataset<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on multiple methods including ToT, ReAct, and IL+RL<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has worked on the CoT and ReAct prompting methods<\/data>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <node id=\"CHAIN OF THOUGHT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/node>    <edge source=\"LATS\" target=\"COT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS uses CoT-based prompts to enhance performance on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS uses ReAct-based prompts to achieve high performance on tasks like HotPotQA and programming<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS performs well on the HotPotQA dataset by combining internal reasoning and external retrieval strategies<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TOT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS and ToT both use search methods to sample and explore more outputs for better performance on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS and RAP use similar search algorithms to perform well on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MBPP\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on the MBPP dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on MBPP when evaluated with GPT-3.5<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS sets the state of the art for HumanEval when used with GPT-4<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on the HumanEval dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS outperforms Reflexion on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEBSHOP\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">LATS achieves high performance on the WebShop dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CHAIN OF THOUGHT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS uses Chain of Thought (CoT) prompts to enhance performance on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SEARCH METHODS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS uses search methods like ToT and RAP to sample and explore more outputs for better performance on reasoning tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS is evaluated on programming tasks using datasets like HumanEval and MBPP<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL DATASET\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on the HumanEval dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MBPP DATASET\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on the MBPP dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-3.5 MODEL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS achieves the highest performance on MBPP when evaluated with GPT-3.5<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"GPT-4 MODEL\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LATS sets the state of the art for HumanEval when used with GPT-4<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEBSHOP DATASET\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS achieves high performance on the WebShop dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"COT\" target=\"WEI\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Wei has worked on the CoT prompting method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"REACT\" target=\"WEBSHOP\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">ReAct is evaluated on the WebShop dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"REACT\" target=\"YAO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yao has worked on the ReAct method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"REACT\" target=\"WEI\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Wei has worked on the ReAct prompting method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"TOT\" target=\"YAO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yao has worked on the ToT method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Hao has worked on the RAP search method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"WEBSHOP\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Reflexion is evaluated on the WebShop dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Shinn has worked on the Reflexion prompting method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"IL\" target=\"RL\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">IL and RL are used in combination for training language models<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"IL\" target=\"FURUTA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Furuta has worked on fine-tuning methods that involve IL<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"IL\" target=\"YAO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yao has worked on the IL method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"RL\" target=\"FURUTA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Furuta has worked on fine-tuning methods that involve RL<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"RL\" target=\"YAO\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Yao has worked on the RL method<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"FURUTA\" target=\"FINE-TUNING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Furuta has worked on fine-tuning methods for language models<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"PROGRAMMING\" target=\"CHEN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Chen has worked on synthetic test suite generation for programming tasks<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"HUMANEVAL DATASET\" target=\"CHEN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Chen has worked on the HumanEval dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"MBPP DATASET\" target=\"AUSTIN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Austin has worked on the MBPP dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>    <edge source=\"EXPERT\" target=\"WEBSHOP DATASET\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Expert performance is used as a benchmark in the WebShop dataset<\/data>      <data key=\"d6\">99d90aededb61e04241516ed9ec656cc<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"594449768ae2dea9b2efbe677075096b","chunk":"ao et al., 2022) 59.9 29.1\nIL+RL (Yao et al., 2022) 62.4 28.7\nFine-tuning (Furuta et al., 2024) 67.5 45.0\nExpert 82.1 59.6\nTable 6. Score and success rate (SR) on WebShop. Results are\norganized into prompting, RL-based training, and human perfor-\nmance. For the same number of iterations, LATS improves both\nscore and SR and surpasses RL-based training.\n5.3. WebShop\nFor a complex decision-making environment with practi-\ncal applications, we consider WebShop (Yao et al., 2022),\nan online shopping environment composed of a website\nwith 1.18M real-world products and 12k human instructions.\nAgents must navigate a website through a variety of com-\nmands to purchase an item matching a user specification.\nWe use the preconstructed action space of search and click\ncommands and browser feedback and reflections for the\nobservation. The performance is gauged using two metrics:\nan average score, reflecting the percentage of user-specified\nattributes met by the selected product, and a success rate,\nindicating the frequency with which the chosen product ful-\nfills all given conditions. We compare against acting-based\nprompting methods and RL-based approaches. We evaluate\non 50 instructions, expand n= 5children for LATS, and set\nk= 30 for LATS, ReAct (best of k), and Reflexion. More\ndetails and prompts are in Appendix Sec. D and Sec. G.\nResults. We find in Tab. 6 that GPT-3.5 with ReAct is\ncompetitive to imitation learning (IL) and can exceed re-\ninforcement learning techniques with stronger prompting\nstrategies. Sampling k= 30 trajectories with ReAct and\nReflexion results in a similar performance, suggesting the se-\nmantic feedback is not as helpful in complex environments\nlike WebShop. Similar to Shinn et al. (2023), we find that\ngenerated reflections are often generic and do not provide\nuseful feedback, resulting in a tendency for the agent to\nbecome stuck in local minima. However, using LATS in-\ndeed results in a noticeable improvement, indicating a more\neffective exploration for the same number of iterations.\n5.4. Ablation Study and Additional Analysis\nWe further test the reasoning ability of LATS on Game of 24,\nand also conduct additional experiments on HotPotQA to\ndemonstrate the effect of each component of LATS (resultsPrompt Method Game of 24 (Success Rate) \u2191\nCoT (Wei et al., 2022) 0.08\nReflexion (Shinn et al., 2023) 0.12\nToT (Yao et al., 2023a) 0.20\nRAP (Hao et al., 2023) 0.40\nLATS (CoT) 0.44\nTable 7. Results on Game of 24 with GPT-3.5. We sample n= 5\nnodes and k= 30 trajectories.\nPrompt Method HotPotQA (EM) \u2191\nToT (ReAct) 0.39\nRAP (ReAct) 0.54\nLATS (No LM Heuristic) 0.37\nLATS (DFS) 0.42\nLATS (No Reflection) 0.58\nLATS (ReAct) 0.63\nTable 8. Ablation results on LATS and baseline variants in Hot-\nPotQA. We use ReAct as the base prompt and sample n= 5\nchildren and k= 50 trajectories. LATS requires every component\nand operation for optimal performance.\nshown in Tab. 8). More ablations for token consumption on\nHotPotQA are in Tab. 9 in Appendix Sec. C.\nReasoning on Game of 24. To show how LATS can be\napplied to purely internal reasoning tasks, we additionally\nevaluate on Game of 24 (Yao et al., 2023a), a mathematical\nreasoning task where the agent must construct 24 out of a\nset of numbers and basic operations. We use CoT as the\nbase prompting design and employ the same operations as\nin other settings. We find in Tab. 7 that LATS outperforms\nprevious methods proposed specifically for reasoning. This\nis due to our proposed value function, which incorporates\nself-consistency as an additional heuristic.\nSelf-reflection. LATS uses self-reflection to provide addi-\ntional semantic signals for the agent. In Tab. 8 (Row 5, 6),\nwe observe a 0.05performance drop when self-reflection\nis removed from LATS, validating its usefulness. This is a\nsmaller gain than the 0.19gain that Reflexion has over Re-\nAct as shown in Tab. 3, suggesting overlap between the ques-\ntions where an answer can be improved by self-reflection\nand search. This variant outperforms RAP (ReAct), reflect-\ning our improvements to MCTS.\nSearch algorithm. MCTS is a more principled search algo-\nrithm than variants like A* (Zhuang et al., 2023) or DFS and\nis the basis for observed performance gains. We observe\nthe effects of using DFS, and incorporate the LM-based\nheuristic used in ToT in which branches with low values are\npruned. This removes the selection and backpropagation\noperations, and we observe a 0","chunk_id":"594449768ae2dea9b2efbe677075096b","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"YAO","type":"PERSON","description":"Yao is an author who has contributed to multiple works mentioned in the text, including IL+RL and WebShop","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"FURUTA","type":"PERSON","description":"Furuta is an author who has contributed to the work on Fine-tuning mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"EXPERT","type":"PERSON","description":"Expert refers to the human performance benchmark mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"PERSON"},{"name":"WEBSHOP","type":"TECHNOLOGY","description":"WebShop is an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions, used for complex decision-making tasks","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS is a method that improves both score and success rate in WebShop, surpassing RL-based training","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used in the experiments, particularly with ReAct and Reflexion prompting methods","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a prompting method used in the experiments with GPT-3.5","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a prompting method used in the experiments with GPT-3.5","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has contributed to the work on Reflexion mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"PERSON"},{"name":"GAME OF 24","type":"TECHNOLOGY","description":"Game of 24 is a mathematical reasoning task where the agent must construct 24 out of a set of numbers and basic operations","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"COT","type":"TECHNOLOGY","description":"CoT is a base prompting design used in the experiments on Game of 24","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"HOTPOTQA","type":"TECHNOLOGY","description":"HotPotQA is a dataset used to test the reasoning ability of LATS and other methods","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT is a prompting method used in the experiments on HotPotQA","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a prompting method used in the experiments on HotPotQA","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"MCTS","type":"TECHNOLOGY","description":"MCTS is a search algorithm used in LATS for performance gains","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"A*","type":"TECHNOLOGY","description":"A* is a search algorithm mentioned as a variant compared to MCTS","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"DFS","type":"TECHNOLOGY","description":"DFS is a search algorithm mentioned as a variant compared to MCTS","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"ZHUANG","type":"PERSON","description":"Zhuang is an author who has contributed to the work on A* mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"PERSON"},{"name":"FINE-TUNING","type":"","description":"\nFine-tuning is a method mentioned in the text, contributed by Furuta et al., 2024","source_id":"594449768ae2dea9b2efbe677075096b","entity_type":"TECHNOLOGY"},{"name":"IL+RL","type":"TECHNOLOGY","description":"IL+RL is a method mentioned in the text, contributed by Yao et al., 2022","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has contributed to the work on RAP mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has contributed to the work on CoT mentioned in the text","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on open-endedness and AI-GAs","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author who has worked on open-endedness and AI-GAs","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author who has worked on open-endedness and AI-GAs","source_id":"594449768ae2dea9b2efbe677075096b"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs","source_id":"594449768ae2dea9b2efbe677075096b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has contributed to multiple works mentioned in the text, including IL+RL and WebShop<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"FURUTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Furuta is an author who has contributed to the work on Fine-tuning mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"EXPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Expert refers to the human performance benchmark mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebShop is an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions, used for complex decision-making tasks<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS is a method that improves both score and success rate in WebShop, surpassing RL-based training<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used in the experiments, particularly with ReAct and Reflexion prompting methods<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a prompting method used in the experiments with GPT-3.5<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a prompting method used in the experiments with GPT-3.5<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has contributed to the work on Reflexion mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAME OF 24\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Game of 24 is a mathematical reasoning task where the agent must construct 24 out of a set of numbers and basic operations<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT is a base prompting design used in the experiments on Game of 24<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">HotPotQA is a dataset used to test the reasoning ability of LATS and other methods<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT is a prompting method used in the experiments on HotPotQA<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a prompting method used in the experiments on HotPotQA<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MCTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MCTS is a search algorithm used in LATS for performance gains<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"A*\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A* is a search algorithm mentioned as a variant compared to MCTS<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DFS is a search algorithm mentioned as a variant compared to MCTS<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuang is an author who has contributed to the work on A* mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FINE-TUNING\">      <data key=\"d0\" \/>      <data key=\"d1\">Fine-tuning is a method mentioned in the text, contributed by Furuta et al., 2024<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"IL+RL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">IL+RL is a method mentioned in the text, contributed by Yao et al., 2022<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has contributed to the work on RAP mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has contributed to the work on CoT mentioned in the text<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">594449768ae2dea9b2efbe677075096b<\/data>    <\/node>    <edge source=\"YAO\" target=\"WEBSHOP\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yao is one of the authors who contributed to the development of WebShop<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"YAO\" target=\"IL+RL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yao is one of the authors who contributed to the development of IL+RL<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"FURUTA\" target=\"FINE-TUNING\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Furuta is one of the authors who contributed to the work on Fine-tuning<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"LATS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LATS is used to improve performance in the WebShop environment<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MCTS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LATS uses MCTS as a search algorithm for performance gains<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"REACT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">GPT-3.5 is used with the ReAct prompting method in the experiments<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"REFLEXION\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">GPT-3.5 is used with the Reflexion prompting method in the experiments<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"REFLEXION\" target=\"SHINN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shinn is one of the authors who contributed to the work on Reflexion<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"COT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">CoT is used as the base prompting design in the Game of 24 experiments<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"COT\" target=\"WEI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Wei is one of the authors who contributed to the work on CoT<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"TOT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">ToT is used as a prompting method in the HotPotQA experiments<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"RAP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">RAP is used as a prompting method in the HotPotQA experiments<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Hao is one of the authors who contributed to the work on RAP<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"A*\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">MCTS is compared to A* as a search algorithm<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"DFS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">MCTS is compared to DFS as a search algorithm<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"A*\" target=\"ZHUANG\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Zhuang is one of the authors who contributed to the work on A*<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"LEHMAN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Lehman have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"STANLEY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"WANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Faldor and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"STANLEY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Lehman and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"WANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Lehman and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"WANG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Stanley and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">594449768ae2dea9b2efbe677075096b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"faa2bd677c7f052136479e0175da3e5b","chunk":"ing our improvements to MCTS.\nSearch algorithm. MCTS is a more principled search algo-\nrithm than variants like A* (Zhuang et al., 2023) or DFS and\nis the basis for observed performance gains. We observe\nthe effects of using DFS, and incorporate the LM-based\nheuristic used in ToT in which branches with low values are\npruned. This removes the selection and backpropagation\noperations, and we observe a 0.21drop in performance in\n8Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nMethod Performance \u2191Sample complexity \u2193Token Consumption \u2193\nReAct (Best k= 250 ) 0.42 O(k) -\nCoT-SC ( n= 1, k= 250 ) 0.40 O(k) -\nLATS ( n= 1, k= 50 ) 0.48 O(k) -\nToT (ReAct, n= 5, k= 50 ) 0.49 O(kn) 210,215\nRAP (ReAct, n= 5, k= 50 ) 0.54 O(kn) 176,500\nLATS ( n= 5, k= 50 ) 0.63 O(kn) 173,290\nTable 9. Performance, sample complexity of different methods, average number of nodes expanded, and token consumption upon success\nby methods with tree-based search. nis the number of children nodes expanded at every step and kis the number of trajectories. LATS\nhas the same sample complexity as other methods with tree-based search and expands less nodes upon success, which indicates lower\ntoken cost.\nMethod k HotPotQA \u2191# of Nodes \u2193\nToT 10 0.34 33.97\nRAP 10 0.44 31.53\nLATS 10 0.44 28.42\nToT 30 0.39 47.54\nRAP 30 0.50 37.71\nLATS 30 0.52 34.12\nToT 50 0.49 84.05\nRAP 50 0.54 70.60\nLATS 50 0.61 66.65\nTable 10. Comparison of the cost of different methods on Hot-\nPotQA. LATS achieves the highest accuracy and the lowest av-\nerage number of nodes\/states required for success at various k\ntrajectories sampled.\nTab. 8 (Row 4) when sampling the same number of nodes\nbut outperforms ToT (ReAct). Despite also benefiting from\nground-truth feedback, LATS uses it better than ToT and\nRAP and can outperform these methods. We also find in\nTab. 8 (Row 3) that LM scoring, the main component of our\nvalue function, is crucial for leveraging external feedback\nand strong performance.\nSample complexity and token consumption. One pos-\nsible concern of LATS is that the tree-structured search\nmight consume much more tokens than existing methods.\nTo further study the computational cost of LATS compared\nto prior methods, we examine the sample complexity (i.e.,\nasymptotic token cost) of all methods considered in this\npaper and count the average number of nodes expanded\nby our method and other tree-structured methods (ToT and\nRAP) upon successful search on HotPotQA. We present the\nresults in Tab. 9 and Tab. 10, which show that our method\nhas the same sample complexity as other tree-based search\nmethods and requires fewer overall tokens and states. The\ntoken cost gap will be even larger when taking failed trajec-\ntories into account, since our method has a higher success\nrate and reaches the computational budget limit less often.\nThis is also true when sampling a smaller number of trajec-\ntories; on average, LATS requires 3.55 fewer nodes thanRAP and 12.12 fewer nodes than ToT. These findings un-\nderscore our improvements to MCTS and adaptation to LM\nagents, resulting in a more principled and efficient search\nmechanism.\n6. Conclusion\nThis work introduces Language Agent Tree Search (LATS),\nthe first framework to unify reasoning, acting, and plan-\nning for enhanced LM problem-solving. LATS addresses\nkey limitations of prior prompting techniques by deliber-\nately constructing trajectories with search algorithms, in-\ncorporating external feedback, and enabling agents to learn\nfrom experience. Our evaluation demonstrates the ability\nof LATS to harness LM capabilities for various decision-\nmaking tasks while maintaining its reasoning ability without\nadditional training . The proposed synergies between search,\ninteraction, and reflection offer a versatile approach to au-\ntonomous decision-making, highlighting the potential of\nLMs as generalist agents.\nLimitations and future directions. LATS has two main\nlimitations that should be considered before its application.\nFirst, it has a higher computational cost compared to simpler\nprompting methods like ReAct or Reflexion, which may\nlimit its practicality in certain situations. Second, LATS\nassumes the ability to revert to earlier states in decision-\nmaking environments, which may not be universally ap-\nplicable in all possible environments. Despite these limi-\ntations, it is worth noting that LATS still achieves better\nperformance and efficiency compared to similar methods,\nand the number of nodes expanded at each step provides a\ntrade-off between performance and efficiency. Additionally,\nwe expect inference-time compute costs to decrease over\ntime, thereby increasing the usefulness of LATS and other\n\u201cSystem-2\u201d LM approaches. Finally, the reversion prop-\nerty is feasible in many real-world applications, opening up\n","chunk_id":"faa2bd677c7f052136479e0175da3e5b","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"MCTS","type":"TECHNOLOGY","description":"MCTS (Monte Carlo Tree Search) is a principled search algorithm that is the basis for observed performance gains in the text","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"A*","type":"TECHNOLOGY","description":"A* is a search algorithm mentioned as a variant compared to MCTS","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"DFS","type":"TECHNOLOGY","description":"DFS (Depth-First Search) is a search algorithm mentioned as a variant compared to MCTS","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT (Tree of Thoughts) is a method that incorporates LM-based heuristics to prune branches with low values","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a framework that unifies reasoning, acting, and planning in language models, and is shown to outperform other methods in terms of performance and efficiency","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a method mentioned in the text with specific performance metrics","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COT-SC","type":"TECHNOLOGY","description":"CoT-SC is a method mentioned in the text with specific performance metrics","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a method mentioned in the text with specific performance metrics","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used to compare the cost and performance of different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"ZHUANG","type":"PERSON","description":"Zhuang is an author mentioned in the text who has worked on search algorithms","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"LANGUAGE AGENT TREE SEARCH","type":"TECHNOLOGY","description":"Language Agent Tree Search (LATS) is a framework that unifies reasoning, acting, and planning in language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"PERFORMANCE","type":"METRIC","description":"Performance is a metric used to evaluate the effectiveness of different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SAMPLE COMPLEXITY","type":"METRIC","description":"Sample complexity is a metric used to evaluate the computational cost of different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TOKEN CONSUMPTION","type":"METRIC","description":"Token consumption is a metric used to evaluate the number of tokens used by different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"NUMBER OF NODES","type":"METRIC","description":"Number of nodes is a metric used to evaluate the number of nodes expanded by different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORIES","type":"METRIC","description":"Trajectories refer to the number of paths sampled in the search process","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"GROUND-TRUTH FEEDBACK","type":"TECHNOLOGY","description":"Ground-truth feedback is used by LATS, ToT, and RAP to improve performance","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"VALUE FUNCTION","type":"TECHNOLOGY","description":"Value function is a main component of LM scoring used in LATS","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a simpler prompting method compared to LATS","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SYSTEM-2 LM APPROACHES","type":"TECHNOLOGY","description":"System-2 LM approaches refer to advanced language model techniques like LATS","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"INFERENCE-TIME COMPUTE COSTS","type":"METRIC","description":"Inference-time compute costs refer to the computational costs incurred during the inference phase of language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COMPUTATIONAL BUDGET","type":"METRIC","description":"Computational budget refers to the limit of computational resources allocated for a task","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"EXTERNAL FEEDBACK","type":"TECHNOLOGY","description":"External feedback is incorporated into LATS to improve decision-making","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REVERSION PROPERTY","type":"TECHNOLOGY","description":"Reversion property allows LATS to revert to earlier states in decision-making environments","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"DECISION-MAKING TASKS","type":"TASK","description":"Decision-making tasks are tasks that require reasoning, acting, and planning","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"PROMPTING TECHNIQUES","type":"TECHNOLOGY","description":"Prompting techniques are methods used to guide language models in generating responses","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY CONSTRUCTION","type":"TECHNOLOGY","description":"Trajectory construction is a process in LATS that involves building paths for decision-making\nTrajectory construction is a process used in LATS to deliberately construct paths with search algorithms","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"SEARCH ALGORITHMS","type":"TECHNOLOGY","description":"Search algorithms are used in LATS to construct trajectories and improve decision-making","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"INTERACTION","type":"TECHNOLOGY","description":"Interaction is a component of LATS that enables agents to learn from experience","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REFLECTION","type":"TECHNOLOGY","description":"Reflection is a component of LATS that enables agents to learn from experience","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"REASONING ABILITY","type":"METRIC","description":"Reasoning ability is a metric used to evaluate the problem-solving capabilities of language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"AUTONOMOUS DECISION-MAKING","type":"TASK","description":"Autonomous decision-making is a task that LATS aims to improve in language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"GENERALIST AGENTS","type":"TECHNOLOGY","description":"Generalist agents refer to language models that can perform a variety of tasks","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COMPUTATIONAL COST","type":"METRIC","description":"Computational cost refers to the resources required to run a method or algorithm","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"NODES EXPANDED","type":"METRIC","description":"Nodes expanded is a metric used to evaluate the efficiency of search methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SUCCESS RATE","type":"METRIC","description":"Success rate is a metric used to evaluate the effectiveness of different methods","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY SAMPLING","type":"TECHNOLOGY","description":"Trajectory sampling is a process in LATS that involves selecting paths to explore\nTrajectory sampling is a process used in LATS to explore different paths","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"COMPUTATIONAL EFFICIENCY","type":"METRIC","description":"Computational efficiency refers to the effectiveness of a method in using computational resources","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"LM SCORING","type":"TECHNOLOGY","description":"LM scoring is a component of LATS's value function that leverages external feedback","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"ASYMPTOTIC TOKEN COST","type":"METRIC","description":"Asymptotic token cost refers to the long-term token consumption of a method","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"FAILED TRAJECTORIES","type":"METRIC","description":"Failed trajectories refer to paths that do not lead to successful outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COMPUTATIONAL BUDGET LIMIT","type":"METRIC","description":"Computational budget limit refers to the maximum computational resources allocated for a task","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"PRIOR PROMPTING TECHNIQUES","type":"TECHNOLOGY","description":"Prior prompting techniques refer to earlier methods used to guide language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"EXPERIENCE LEARNING","type":"TECHNOLOGY","description":"Experience learning is a process in LATS that enables agents to learn from their actions and outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"DECISION-MAKING ENVIRONMENTS","type":"ENVIRONMENT","description":"Decision-making environments are contexts in which LATS operates to make decisions","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"INFERENCE-TIME","type":"METRIC","description":"Inference-time refers to the phase when a language model generates responses based on input","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SYSTEM-2","type":"TECHNOLOGY","description":"System-2 refers to advanced, deliberate, and logical thinking processes in language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"SYSTEM-1","type":"TECHNOLOGY","description":"System-1 refers to fast, automatic, and intuitive thinking processes in language models","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"COMPUTATIONAL RESOURCES","type":"METRIC","description":"Computational resources refer to the hardware and software resources required to run a method or algorithm","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY EXPLORATION","type":"TECHNOLOGY","description":"Trajectory exploration is a process in LATS that involves exploring different paths to find optimal solutions","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY PRUNING","type":"TECHNOLOGY","description":"Trajectory pruning is a process in LATS that involves removing less promising paths to improve efficiency","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY EVALUATION","type":"TECHNOLOGY","description":"Trajectory evaluation is a process in LATS that involves assessing the quality of paths","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY SELECTION","type":"TECHNOLOGY","description":"Trajectory selection is a process in LATS that involves choosing the best paths to follow","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY BACKPROPAGATION","type":"TECHNOLOGY","description":"Trajectory backpropagation is a process in LATS that involves updating the value of paths based on outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY UPDATING","type":"TECHNOLOGY","description":"Trajectory updating is a process in LATS that involves modifying paths based on new information","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY OPTIMIZATION","type":"TECHNOLOGY","description":"Trajectory optimization is a process in LATS that involves improving paths to achieve better outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY PLANNING","type":"TECHNOLOGY","description":"Trajectory planning is a process in LATS that involves designing paths for decision-making","source_id":"faa2bd677c7f052136479e0175da3e5b"},{"name":"TRAJECTORY EXECUTION","type":"TECHNOLOGY","description":"Trajectory execution is a process in LATS that involves carrying out paths to achieve goals","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY MONITORING","type":"TECHNOLOGY","description":"Trajectory monitoring is a process in LATS that involves tracking the progress of paths","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY ANALYSIS","type":"TECHNOLOGY","description":"Trajectory analysis is a process in LATS that involves examining paths to understand their effectiveness","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY ADJUSTMENT","type":"TECHNOLOGY","description":"Trajectory adjustment is a process in LATS that involves modifying paths based on feedback","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY CONTROL","type":"TECHNOLOGY","description":"Trajectory control is a process in LATS that involves managing paths to achieve desired outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY MANAGEMENT","type":"TECHNOLOGY","description":"Trajectory management is a process in LATS that involves overseeing paths to ensure they are effective","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY COORDINATION","type":"TECHNOLOGY","description":"Trajectory coordination is a process in LATS that involves aligning paths to achieve goals","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY SYNCHRONIZATION","type":"TECHNOLOGY","description":"Trajectory synchronization is a process in LATS that involves aligning paths to work together effectively","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY INTEGRATION","type":"TECHNOLOGY","description":"Trajectory integration is a process in LATS that involves combining paths to achieve better outcomes","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY FUSION","type":"TECHNOLOGY","description":"Trajectory fusion is a process in LATS that involves merging paths to create a unified approach","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY AGGREGATION","type":"TECHNOLOGY","description":"Trajectory aggregation is a process in LATS that involves combining multiple paths to achieve better results","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY CONSOLIDATION","type":"TECHNOLOGY","description":"Trajectory consolidation is a process in LATS that involves unifying paths to create a cohesive strategy","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY SYNTHESIS","type":"TECHNOLOGY","description":"Trajectory synthesis is a process in LATS that involves creating new paths based on existing ones","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY GENERATION","type":"TECHNOLOGY","description":"Trajectory generation is a process in LATS that involves creating new paths for decision-making","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY FORMULATION","type":"TECHNOLOGY","description":"Trajectory formulation is a process in LATS that involves designing paths for specific goals","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY DESIGN","type":"TECHNOLOGY","description":"Trajectory design is a process in LATS that involves creating paths to achieve objectives","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY DEVELOPMENT","type":"TECHNOLOGY","description":"Trajectory development is a process in LATS that involves building paths for decision-making","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"},{"name":"TRAJECTORY IMPLEMENTATION","type":"TECHNOLOGY","description":"Trajectory implementation is a process in LATS that involves putting paths into action","source_id":"faa2bd677c7f052136479e0175da3e5b","entity_type":"TECHNOLOGY"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MCTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MCTS (Monte Carlo Tree Search) is a principled search algorithm that is the basis for observed performance gains in the text<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"A*\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A* is a search algorithm mentioned as a variant compared to MCTS<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"DFS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DFS (Depth-First Search) is a search algorithm mentioned as a variant compared to MCTS<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT (Tree of Thoughts) is a method that incorporates LM-based heuristics to prune branches with low values<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a framework that unifies reasoning, acting, and planning in language models, and is shown to outperform other methods in terms of performance and efficiency<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a method mentioned in the text with specific performance metrics<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT-SC is a method mentioned in the text with specific performance metrics<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a method mentioned in the text with specific performance metrics<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used to compare the cost and performance of different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuang is an author mentioned in the text who has worked on search algorithms<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Agent Tree Search (LATS) is a framework that unifies reasoning, acting, and planning in language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"PERFORMANCE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Performance is a metric used to evaluate the effectiveness of different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SAMPLE COMPLEXITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Sample complexity is a metric used to evaluate the computational cost of different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TOKEN CONSUMPTION\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Token consumption is a metric used to evaluate the number of tokens used by different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"NUMBER OF NODES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Number of nodes is a metric used to evaluate the number of nodes expanded by different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Trajectories refer to the number of paths sampled in the search process<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"GROUND-TRUTH FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Ground-truth feedback is used by LATS, ToT, and RAP to improve performance<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Value function is a main component of LM scoring used in LATS<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a simpler prompting method compared to LATS<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SYSTEM-2 LM APPROACHES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">System-2 LM approaches refer to advanced language model techniques like LATS<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"INFERENCE-TIME COMPUTE COSTS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Inference-time compute costs refer to the computational costs incurred during the inference phase of language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COMPUTATIONAL BUDGET\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational budget refers to the limit of computational resources allocated for a task<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"EXTERNAL FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">External feedback is incorporated into LATS to improve decision-making<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REVERSION PROPERTY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reversion property allows LATS to revert to earlier states in decision-making environments<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"DECISION-MAKING TASKS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Decision-making tasks are tasks that require reasoning, acting, and planning<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"PROMPTING TECHNIQUES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompting techniques are methods used to guide language models in generating responses<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY CONSTRUCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory construction is a process in LATS that involves building paths for decision-makingTrajectory construction is a process used in LATS to deliberately construct paths with search algorithms<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search algorithms are used in LATS to construct trajectories and improve decision-making<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"INTERACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Interaction is a component of LATS that enables agents to learn from experience<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflection is a component of LATS that enables agents to learn from experience<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"REASONING ABILITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Reasoning ability is a metric used to evaluate the problem-solving capabilities of language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"AUTONOMOUS DECISION-MAKING\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Autonomous decision-making is a task that LATS aims to improve in language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"GENERALIST AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Generalist agents refer to language models that can perform a variety of tasks<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COMPUTATIONAL COST\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational cost refers to the resources required to run a method or algorithm<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"NODES EXPANDED\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Nodes expanded is a metric used to evaluate the efficiency of search methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SUCCESS RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Success rate is a metric used to evaluate the effectiveness of different methods<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY SAMPLING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory sampling is a process in LATS that involves selecting paths to exploreTrajectory sampling is a process used in LATS to explore different paths<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"COMPUTATIONAL EFFICIENCY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational efficiency refers to the effectiveness of a method in using computational resources<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"LM SCORING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LM scoring is a component of LATS's value function that leverages external feedback<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"ASYMPTOTIC TOKEN COST\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Asymptotic token cost refers to the long-term token consumption of a method<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"FAILED TRAJECTORIES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Failed trajectories refer to paths that do not lead to successful outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COMPUTATIONAL BUDGET LIMIT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational budget limit refers to the maximum computational resources allocated for a task<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"PRIOR PROMPTING TECHNIQUES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prior prompting techniques refer to earlier methods used to guide language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"EXPERIENCE LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Experience learning is a process in LATS that enables agents to learn from their actions and outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"DECISION-MAKING ENVIRONMENTS\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">Decision-making environments are contexts in which LATS operates to make decisions<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"INFERENCE-TIME\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Inference-time refers to the phase when a language model generates responses based on input<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SYSTEM-2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">System-2 refers to advanced, deliberate, and logical thinking processes in language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"SYSTEM-1\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">System-1 refers to fast, automatic, and intuitive thinking processes in language models<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"COMPUTATIONAL RESOURCES\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Computational resources refer to the hardware and software resources required to run a method or algorithm<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY EXPLORATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory exploration is a process in LATS that involves exploring different paths to find optimal solutions<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY PRUNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory pruning is a process in LATS that involves removing less promising paths to improve efficiency<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY EVALUATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory evaluation is a process in LATS that involves assessing the quality of paths<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY SELECTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory selection is a process in LATS that involves choosing the best paths to follow<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY BACKPROPAGATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory backpropagation is a process in LATS that involves updating the value of paths based on outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY UPDATING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory updating is a process in LATS that involves modifying paths based on new information<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY OPTIMIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory optimization is a process in LATS that involves improving paths to achieve better outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY PLANNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory planning is a process in LATS that involves designing paths for decision-making<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/node>    <node id=\"TRAJECTORY EXECUTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory execution is a process in LATS that involves carrying out paths to achieve goals<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY MONITORING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory monitoring is a process in LATS that involves tracking the progress of paths<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY ANALYSIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory analysis is a process in LATS that involves examining paths to understand their effectiveness<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY ADJUSTMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory adjustment is a process in LATS that involves modifying paths based on feedback<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY CONTROL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory control is a process in LATS that involves managing paths to achieve desired outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY MANAGEMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory management is a process in LATS that involves overseeing paths to ensure they are effective<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY COORDINATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory coordination is a process in LATS that involves aligning paths to achieve goals<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY SYNCHRONIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory synchronization is a process in LATS that involves aligning paths to work together effectively<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY INTEGRATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory integration is a process in LATS that involves combining paths to achieve better outcomes<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY FUSION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory fusion is a process in LATS that involves merging paths to create a unified approach<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY AGGREGATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory aggregation is a process in LATS that involves combining multiple paths to achieve better results<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY CONSOLIDATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory consolidation is a process in LATS that involves unifying paths to create a cohesive strategy<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY SYNTHESIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory synthesis is a process in LATS that involves creating new paths based on existing ones<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY GENERATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory generation is a process in LATS that involves creating new paths for decision-making<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY FORMULATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory formulation is a process in LATS that involves designing paths for specific goals<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY DESIGN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory design is a process in LATS that involves creating paths to achieve objectives<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY DEVELOPMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory development is a process in LATS that involves building paths for decision-making<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAJECTORY IMPLEMENTATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Trajectory implementation is a process in LATS that involves putting paths into action<\/data>      <data key=\"d2\">faa2bd677c7f052136479e0175da3e5b<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <edge source=\"MCTS\" target=\"A*\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">MCTS is compared to A* as a more principled search algorithm<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"MCTS\" target=\"DFS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">MCTS is compared to DFS as a more principled search algorithm<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"A*\" target=\"ZHUANG\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Zhuang is associated with the A* search algorithm<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"DFS\" target=\"ZHUANG\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Zhuang is associated with the DFS search algorithm<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"TOT\" target=\"LATS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS outperforms ToT in terms of performance and efficiency<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"TOT\" target=\"HOTPOTQA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">ToT is evaluated on the HotPotQA dataset<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS outperforms ReAct in terms of performance and efficiency<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">LATS outperforms RAP in terms of performance and efficiency<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS achieves the highest accuracy and the lowest average number of nodes\/states required for success on HotPotQA<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HOTPOTQA\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">RAP is evaluated on the HotPotQA dataset<\/data>      <data key=\"d6\">faa2bd677c7f052136479e0175da3e5b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4ae237a491bc8a84cc720e40c59a7464","chunk":" possible environments. Despite these limi-\ntations, it is worth noting that LATS still achieves better\nperformance and efficiency compared to similar methods,\nand the number of nodes expanded at each step provides a\ntrade-off between performance and efficiency. Additionally,\nwe expect inference-time compute costs to decrease over\ntime, thereby increasing the usefulness of LATS and other\n\u201cSystem-2\u201d LM approaches. Finally, the reversion prop-\nerty is feasible in many real-world applications, opening up\nnew opportunities in the LM decision-making community.\nFuture directions include scaling LATS to more complex\nenvironments or multi-agent frameworks and improving ef-\nficiency to reduce costs. A more detailed discussion about\nthe limitations of LATS can be found in Appendix Sec. B.\n9Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nImpact Statement\nLATS is a framework that enhances LM performance\nthrough interactions with an environment. This improve-\nment in autonomous decision-making may facilitate harm-\nful uses of LMs. On the other hand, LATS enhances in-\nterpretability and the potential for greater alignment, as it\ninvolves high-level linguistic reasoning and actions through\nseveral rounds of decision-making and reflection rather than\nrelying on autoregressive generation. Finally, enhancing the\ncapabilities of LM agents may raise security risks, such as\nexecuting malware. We encourage further research to fully\nunderstand and mitigate the risks of LMs.\nAcknowledgements\nWe thank Daniel Campos for useful feedback on earlier ver-\nsions of this paper. This work was supported in part by NSF\nGrant 2106825, NIFA Award 2020-67021-32799, the Jump\nARCHES endowment through the Health Care Engineering\nSystems Center at Illinois and the OSF Foundation, and the\nIBM-Illinois Discovery Accelerator Institute. This work\nused NVIDIA GPUs at NCSA Delta through allocations\nCIS220014, CIS230012, and CIS230218 from the ACCESS\nprogram.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-\notar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan\nFu, Keerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter\nPastor, Jornell Quiambao, Kanishka Rao, Jarek Retting-\nhouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei\nXia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as I can, not as I say: Grounding language\nin robotic affordances. In CoRL , 2022.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sut-\nton. Program synthesis with large language models. In\nNeurIPS , 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul\nSampedro, and Jeff Clune. Video pretraining (VPT):\nLearning to act by watching unlabeled online videos. In\nNeurIPS , 2022.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Pi-\notr Nyczyk, and Torsten Hoefler. Graph of thoughts:\nSolving elaborate problems with large language models.\narXiv:2308.09687 , 2023.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. A large annotated corpus for\nlearning natural language inference. In EMNLP , 2015.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In\nNeurIPS , 2020.\nMurray Campbell, A Joseph Hoane Jr, and Feng-hsiung\nHsu. Deep blue. Artificial intelligence , 2002.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi\nLin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\ngeneration with generated tests. In ICLR , 202","chunk_id":"4ae237a491bc8a84cc720e40c59a7464","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a framework that enhances language model (LM) performance through interactions with an environment, improving autonomous decision-making and interpretability","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"SYSTEM-2 LM APPROACHES","type":"TECHNOLOGY","description":"System-2 LM approaches refer to advanced language model methods that involve high-level reasoning and decision-making processes","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"DANIEL CAMPOS","type":"PERSON","description":"Daniel Campos provided useful feedback on earlier versions of the paper discussing LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NSF GRANT 2106825","type":"FUNDING","description":"NSF Grant 2106825 is one of the funding sources that supported the work on LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NIFA AWARD 2020-67021-32799","type":"FUNDING","description":"NIFA Award 2020-67021-32799 is one of the funding sources that supported the work on LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JUMP ARCHES ENDOWMENT","type":"FUNDING","description":"The Jump ARCHES endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation is one of the funding sources that supported the work on LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE","type":"ORGANIZATION","description":"The IBM-Illinois Discovery Accelerator Institute is one of the organizations that supported the work on LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NVIDIA GPUS","type":"TECHNOLOGY","description":"NVIDIA GPUs were used in the research on LATS through allocations from the ACCESS program","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NCSA DELTA","type":"TECHNOLOGY","description":"NCSA Delta is a computing resource used in the research on LATS through allocations from the ACCESS program","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ACCESS PROGRAM","type":"PROGRAM","description":"The ACCESS program provided allocations for the use of NVIDIA GPUs and NCSA Delta in the research on LATS","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"MICHAEL AHN","type":"PERSON","description":"Michael Ahn is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ANTHONY BROHAN","type":"PERSON","description":"Anthony Brohan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"NOAH BROWN","type":"PERSON","description":"Noah Brown is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"YEVGEN CHEBOTAR","type":"PERSON","description":"Yevgen Chebotar is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"OMAR CORTES","type":"PERSON","description":"Omar Cortes is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"BYRON DAVID","type":"PERSON","description":"Byron David is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CHUYUAN FU","type":"PERSON","description":"Chuyuan Fu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"KEERTHANA GOPALAKRISHNAN","type":"PERSON","description":"Keerthana Gopalakrishnan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"KAROL HAUSMAN","type":"PERSON","description":"Karol Hausman is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ALEX HERZOG","type":"PERSON","description":"Alex Herzog is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"DANIEL HO","type":"PERSON","description":"Daniel Ho is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JASMINE HSU","type":"PERSON","description":"Jasmine Hsu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JULIAN IBARZ","type":"PERSON","description":"Julian Ibarz is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"BRIAN ICHTER","type":"PERSON","description":"Brian Ichter is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ALEX IRPAN","type":"PERSON","description":"Alex Irpan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ROSARIO JAUREGUI RUANO","type":"PERSON","description":"Rosario Jauregui Ruano is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"KYLE JEFFREY","type":"PERSON","description":"Kyle Jeffrey is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"SALLY JESMONTH","type":"PERSON","description":"Sally Jesmonth is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"NIKHIL J JOSHI","type":"PERSON","description":"Nikhil J Joshi is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"RYAN JULIAN","type":"PERSON","description":"Ryan Julian is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"DMITRY KALASHNIKOV","type":"PERSON","description":"Dmitry Kalashnikov is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"YUHENG KUANG","type":"PERSON","description":"Yuheng Kuang is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"KUANG-HUEI LEE","type":"PERSON","description":"Kuang-Huei Lee is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"YAO LU","type":"PERSON","description":"Yao Lu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"LINDA LUU","type":"PERSON","description":"Linda Luu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CAROLINA PARADA","type":"PERSON","description":"Carolina Parada is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"PETER PASTOR","type":"PERSON","description":"Peter Pastor is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JORNELL QUIAMBAO","type":"PERSON","description":"Jornell Quiambao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"KANISHKA RAO","type":"PERSON","description":"Kanishka Rao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JAREK RETTINGHOUSE","type":"PERSON","description":"Jarek Rettinghouse is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"DIEGO REYES","type":"PERSON","description":"Diego Reyes is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"PIERRE SERMANET","type":"PERSON","description":"Pierre Sermanet is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"NICOLAS SIEVERS","type":"PERSON","description":"Nicolas Sievers is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CLAYTON TAN","type":"PERSON","description":"Clayton Tan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ALEXANDER TOSHEV","type":"PERSON","description":"Alexander Toshev is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"VINCENT VANHOUCKE","type":"PERSON","description":"Vincent Vanhoucke is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"FEI XIA","type":"PERSON","description":"Fei Xia is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"TED XIAO","type":"PERSON","description":"Ted Xiao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"PENG XU","type":"PERSON","description":"Peng Xu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"SICHUN XU","type":"PERSON","description":"Sichun Xu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"MENGYUAN YAN","type":"PERSON","description":"Mengyuan Yan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ANDY ZENG","type":"PERSON","description":"Andy Zeng is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JACOB AUSTIN","type":"PERSON","description":"Jacob Austin is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"AUGUSTUS ODENA","type":"PERSON","description":"Augustus Odena is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"MAXWELL NYE","type":"PERSON","description":"Maxwell Nye is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"HENRYK MICHALEWSKI","type":"PERSON","description":"Henryk Michalewski is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"DAVID DOHAN","type":"PERSON","description":"David Dohan is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ELLEN JIANG","type":"PERSON","description":"Ellen Jiang is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CARRIE CAI","type":"PERSON","description":"Carrie Cai is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"MICHAEL TERRY","type":"PERSON","description":"Michael Terry is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"QUOC LE","type":"PERSON","description":"Quoc Le is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"CHARLES SUTTON","type":"PERSON","description":"Charles Sutton is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"BOWEN BAKER","type":"PERSON","description":"Bowen Baker is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ILGE AKKAYA","type":"PERSON","description":"Ilge Akkaya is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"PETER ZHOKHOV","type":"PERSON","description":"Peter Zhokhov is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JOOST HUIZINGA","type":"PERSON","description":"Joost Huizinga is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JIE TANG","type":"PERSON","description":"Jie Tang is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"ADRIEN ECOFFET","type":"PERSON","description":"Adrien Ecoffet is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"BRANDON HOUGHTON","type":"PERSON","description":"Brandon Houghton is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"RAUL SAMPEDRO","type":"PERSON","description":"Raul Sampedro is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022","source_id":"4ae237a491bc8a84cc720e40c59a7464","entity_type":"PERSON"},{"name":"MACIEJ BESTA","type":"PERSON","description":"Maciej Besta is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"NILS BLACH","type":"PERSON","description":"Nils Blach is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ALES KUBICEK","type":"PERSON","description":"Ales Kubicek is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"ROBERT GERSTENBERGER","type":"PERSON","description":"Robert Gerstenberger is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"LUKAS GIANINAZZI","type":"PERSON","description":"Lukas Gianinazzi is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"JOANNA GAJDA","type":"PERSON","description":"Joanna Gajda is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"TOMASZ LEHMANN","type":"PERSON","description":"Tomasz Lehmann is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"MICHAL PODSTAWSKI","type":"PERSON","description":"Michal Podstawski is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023","source_id":"4ae237a491bc8a84cc720e40c59a7464"},{"name":"HUBERT NIEWIADOMSKI","type":"PERSON","description":"Hubert Niewiadomski is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in","source_id":"4ae237a491bc8a84cc720e40c59a7464"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a framework that enhances language model (LM) performance through interactions with an environment, improving autonomous decision-making and interpretability<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"SYSTEM-2 LM APPROACHES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">System-2 LM approaches refer to advanced language model methods that involve high-level reasoning and decision-making processes<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"DANIEL CAMPOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Campos provided useful feedback on earlier versions of the paper discussing LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NSF GRANT 2106825\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">NSF Grant 2106825 is one of the funding sources that supported the work on LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NIFA AWARD 2020-67021-32799\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">NIFA Award 2020-67021-32799 is one of the funding sources that supported the work on LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JUMP ARCHES ENDOWMENT\">      <data key=\"d0\">FUNDING<\/data>      <data key=\"d1\">The Jump ARCHES endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation is one of the funding sources that supported the work on LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"IBM-ILLINOIS DISCOVERY ACCELERATOR INSTITUTE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The IBM-Illinois Discovery Accelerator Institute is one of the organizations that supported the work on LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NVIDIA GPUS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NVIDIA GPUs were used in the research on LATS through allocations from the ACCESS program<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NCSA DELTA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NCSA Delta is a computing resource used in the research on LATS through allocations from the ACCESS program<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ACCESS PROGRAM\">      <data key=\"d0\">PROGRAM<\/data>      <data key=\"d1\">The ACCESS program provided allocations for the use of NVIDIA GPUs and NCSA Delta in the research on LATS<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"MICHAEL AHN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Ahn is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ANTHONY BROHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anthony Brohan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NOAH BROWN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Brown is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YEVGEN CHEBOTAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yevgen Chebotar is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OMAR CORTES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Cortes is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BYRON DAVID\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Byron David is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHUYUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chuyuan Fu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KEERTHANA GOPALAKRISHNAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Keerthana Gopalakrishnan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KAROL HAUSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karol Hausman is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEX HERZOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Herzog is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DANIEL HO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Ho is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASMINE HSU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jasmine Hsu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JULIAN IBARZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Ibarz is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BRIAN ICHTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Ichter is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEX IRPAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Irpan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROSARIO JAUREGUI RUANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rosario Jauregui Ruano is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KYLE JEFFREY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kyle Jeffrey is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SALLY JESMONTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sally Jesmonth is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NIKHIL J JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikhil J Joshi is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RYAN JULIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan Julian is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DMITRY KALASHNIKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dmitry Kalashnikov is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUHENG KUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuheng Kuang is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KUANG-HUEI LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuang-Huei Lee is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao Lu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LINDA LUU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linda Luu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAROLINA PARADA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carolina Parada is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETER PASTOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Pastor is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JORNELL QUIAMBAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jornell Quiambao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KANISHKA RAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kanishka Rao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAREK RETTINGHOUSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jarek Rettinghouse is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DIEGO REYES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diego Reyes is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIERRE SERMANET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Sermanet is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICOLAS SIEVERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Sievers is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLAYTON TAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clayton Tan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEXANDER TOSHEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander Toshev is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VINCENT VANHOUCKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vincent Vanhoucke is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEI XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Xia is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TED XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ted Xiao is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peng Xu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SICHUN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sichun Xu is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MENGYUAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengyuan Yan is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDY ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zeng is one of the authors of the paper \"Do as I can, not as I say: Grounding language in robotic affordances\" published in CoRL 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JACOB AUSTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Austin is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AUGUSTUS ODENA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Augustus Odena is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAXWELL NYE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maxwell Nye is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENRYK MICHALEWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henryk Michalewski is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAVID DOHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Dohan is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ELLEN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ellen Jiang is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CARRIE CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carrie Cai is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICHAEL TERRY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Terry is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc Le is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHARLES SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charles Sutton is one of the authors of the paper \"Program synthesis with large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOWEN BAKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Baker is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ILGE AKKAYA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilge Akkaya is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETER ZHOKHOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Zhokhov is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOOST HUIZINGA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joost Huizinga is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIE TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Tang is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADRIEN ECOFFET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrien Ecoffet is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BRANDON HOUGHTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brandon Houghton is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAUL SAMPEDRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Raul Sampedro is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper \"Video pretraining (VPT): Learning to act by watching unlabeled online videos\" published in NeurIPS 2022<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MACIEJ BESTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maciej Besta is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"NILS BLACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nils Blach is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ALES KUBICEK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ales Kubicek is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"ROBERT GERSTENBERGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert Gerstenberger is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"LUKAS GIANINAZZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukas Gianinazzi is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"JOANNA GAJDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joanna Gajda is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"TOMASZ LEHMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tomasz Lehmann is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"MICHAL PODSTAWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michal Podstawski is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>    <node id=\"HUBERT NIEWIADOMSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hubert Niewiadomski is one of the authors of the paper \"Graph of thoughts: Solving elaborate problems with large language models\" published on arXiv in<\/data>      <data key=\"d2\">4ae237a491bc8a84cc720e40c59a7464<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"7a48515e86161237c03c9a8373197126","chunk":"\nAmodei. Language models are few-shot learners. In\nNeurIPS , 2020.\nMurray Campbell, A Joseph Hoane Jr, and Feng-hsiung\nHsu. Deep blue. Artificial intelligence , 2002.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi\nLin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\ngeneration with generated tests. In ICLR , 2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde, Jared Kaplan, Harrison Edwards, Yura\nBurda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, David W. Cum-\nmings, Matthias Plappert, Fotios Chantzis, Elizabeth\nBarnes, Ariel Herbert-V oss, William H. Guss, Alex\nNichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nAndrew Carr, Jan Leike, Joshua Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. Evaluating large lan-\nguage models trained on code. arXiv:2107.03374 , 2021.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W.\nCohen. Program of thoughts prompting: disentangling\ncomputation from reasoning for numerical reasoning\ntasks. TMLR , 2023b. ISSN 2835-8856.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\n10Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-\nAri, Pengcheng Yin, Toju Duke, Anselm Levskaya, San-\njay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepa-\nssi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat,\nMichele Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-\nglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM:\nScaling language modeling with pathways. JMLR , 24\n(240):1\u2013113, 2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. Training verifiers to\nsolve math word problems. arXiv:2110.14168 , 2021.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel\nStevens, Boshi Wang, Huan Sun, and Yu Su. Mind2Web:\nTowards a generalist agent for the web. In NeurIPS\nDatasets and Benchmarks Track , 2023.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Haus-\nman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mor-\ndatch, and Pete Florence. PaLM-E: An embodied multi-\nmodal language model. In ICML , 2023.\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B. Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In NeurIPS , 2023.\nJonathan St BT Evans. Intuition and reasoning: A dual-\nprocess perspective. Psychological Inquiry , pages ","chunk_id":"7a48515e86161237c03c9a8373197126","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"AMODEI","type":"PERSON","description":"Amodei is an author of the paper titled \"Language models are few-shot learners\" published in NeurIPS 2020","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"NEURIPS","type":"EVENT","description":"NeurIPS is a conference where the paper \"Language models are few-shot learners\" was published in 2020","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"MURRAY CAMPBELL","type":"PERSON","description":"Murray Campbell is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"A JOSEPH HOANE JR","type":"PERSON","description":"A Joseph Hoane Jr is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FENG-HSIUNG HSU","type":"PERSON","description":"Feng-hsiung Hsu is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ARTIFICIAL INTELLIGENCE","type":"JOURNAL","description":"Artificial Intelligence is the journal where the paper \"Deep blue\" was published in 2002","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"BEI CHEN","type":"PERSON","description":"Bei Chen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"FENGJI ZHANG","type":"PERSON","description":"Fengji Zhang is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"ANH NGUYEN","type":"PERSON","description":"Anh Nguyen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"DAOGUANG ZAN","type":"PERSON","description":"Daoguang Zan is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ZEQI LIN","type":"PERSON","description":"Zeqi Lin is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JIAN-GUANG LOU","type":"PERSON","description":"Jian-Guang Lou is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ICLR","type":"EVENT","description":"ICLR is a conference where the paper \"CodeT: Code generation with generated tests\" was published in 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"EVENT"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"QIMING YUAN","type":"PERSON","description":"Qiming Yuan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"HENRIQUE PONDE","type":"PERSON","description":"Henrique Ponde is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JARED KAPLAN","type":"PERSON","description":"Jared Kaplan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"HARRISON EDWARDS","type":"PERSON","description":"Harrison Edwards is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"YURA BURDA","type":"PERSON","description":"Yura Burda is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"NICHOLAS JOSEPH","type":"PERSON","description":"Nicholas Joseph is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"GREG BROCKMAN","type":"PERSON","description":"Greg Brockman is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ALEX RAY","type":"PERSON","description":"Alex Ray is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"RAUL PURI","type":"PERSON","description":"Raul Puri is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"GRETCHEN KRUEGER","type":"PERSON","description":"Gretchen Krueger is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MICHAEL PETROV","type":"PERSON","description":"Michael Petrov is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"HEIDY KHLAAF","type":"PERSON","description":"Heidy Khlaaf is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"GIRISH SASTRY","type":"PERSON","description":"Girish Sastry is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"PAMELA MISHKIN","type":"PERSON","description":"Pamela Mishkin is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"BROOKE CHAN","type":"PERSON","description":"Brooke Chan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SCOTT GRAY","type":"PERSON","description":"Scott Gray is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"NICK RYDER","type":"PERSON","description":"Nick Ryder is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MIKHAIL PAVLOV","type":"PERSON","description":"Mikhail Pavlov is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ALETHEA POWER","type":"PERSON","description":"Alethea Power is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"CLEMENS WINTER","type":"PERSON","description":"Clemens Winter is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"PHILIPPE TILLET","type":"PERSON","description":"Philippe Tillet is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"FELIPE PETROSKI SUCH","type":"PERSON","description":"Felipe Petroski Such is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"DAVID W. CUMMINGS","type":"PERSON","description":"David W. Cummings is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"FOTIOS CHANTZIS","type":"PERSON","description":"Fotios Chantzis is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ELIZABETH BARNES","type":"PERSON","description":"Elizabeth Barnes is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ARIEL HERBERT-VOSS","type":"PERSON","description":"Ariel Herbert-Voss is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"WILLIAM H. GUSS","type":"PERSON","description":"William H. Guss is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ALEX NICHOL","type":"PERSON","description":"Alex Nichol is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"IGOR BABUSCHKIN","type":"PERSON","description":"Igor Babuschkin is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SUCHIR BALAJI","type":"PERSON","description":"Suchir Balaji is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SHANTANU JAIN","type":"PERSON","description":"Shantanu Jain is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ANDREW CARR","type":"PERSON","description":"Andrew Carr is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JAN LEIKE","type":"PERSON","description":"Jan Leike is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JOSHUA ACHIAM","type":"PERSON","description":"Joshua Achiam is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"VEDANT MISRA","type":"PERSON","description":"Vedant Misra is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"EVAN MORIKAWA","type":"PERSON","description":"Evan Morikawa is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ALEC RADFORD","type":"PERSON","description":"Alec Radford is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MATTHEW M. KNIGHT","type":"PERSON","description":"Matthew M. Knight is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MILES BRUNDAGE","type":"PERSON","description":"Miles Brundage is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MIRA MURATI","type":"PERSON","description":"Mira Murati is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"KATIE MAYER","type":"PERSON","description":"Katie Mayer is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"PETER WELINDER","type":"PERSON","description":"Peter Welinder is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"BOB MCGREW","type":"PERSON","description":"Bob McGrew is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"DARIO AMODEI","type":"PERSON","description":"Dario Amodei is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SAM MCCANDLISH","type":"PERSON","description":"Sam McCandlish is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"WOJCIECH ZAREMBA","type":"PERSON","description":"Wojciech Zaremba is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ARXIV","type":"PLATFORM","description":"arXiv is the platform where the paper \"Evaluating large language models trained on code\" was published in 2021","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PLATFORM"},{"name":"WENHU CHEN","type":"PERSON","description":"Wenhu Chen is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"XUEGUANG MA","type":"PERSON","description":"Xueguang Ma is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"XINYI WANG","type":"PERSON","description":"Xinyi Wang is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"WILLIAM W. COHEN","type":"PERSON","description":"William W. Cohen is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"TMLR","type":"JOURNAL","description":"TMLR is the journal where the paper \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" was published in 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"JOURNAL"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"JACOB DEVLIN","type":"PERSON","description":"Jacob Devlin is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"GAURAV MISHRA","type":"PERSON","description":"Gaurav Mishra is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"ADAM ROBERTS","type":"PERSON","description":"Adam Roberts is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JML\nAdam Roberts is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126","entity_type":"PERSON"},{"name":"PAUL BARHAM","type":"PERSON","description":"Paul Barham is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"CHARLES SUTTON","type":"PERSON","description":"Charles Sutton is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SEBASTIAN GEHRMANN","type":"PERSON","description":"Sebastian Gehrmann is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"PARKER SCHUH","type":"PERSON","description":"Parker Schuh is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"KENSEN SHI","type":"PERSON","description":"Kensen Shi is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"SASHA TSVYASHCHENKO","type":"PERSON","description":"Sasha Tsvyashchenko is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023","source_id":"7a48515e86161237c03c9a8373197126"},{"name":"JOSHUA MAYNEZ","type":"PERSON","description":"Joshua Maynez is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR","source_id":"7a48515e86161237c03c9a8373197126"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AMODEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amodei is an author of the paper titled \"Language models are few-shot learners\" published in NeurIPS 2020<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"NEURIPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">NeurIPS is a conference where the paper \"Language models are few-shot learners\" was published in 2020<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"MURRAY CAMPBELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Murray Campbell is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"A JOSEPH HOANE JR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A Joseph Hoane Jr is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FENG-HSIUNG HSU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Feng-hsiung Hsu is one of the authors of the paper titled \"Deep blue\" published in Artificial Intelligence in 2002<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ARTIFICIAL INTELLIGENCE\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Artificial Intelligence is the journal where the paper \"Deep blue\" was published in 2002<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"BEI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bei Chen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"FENGJI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fengji Zhang is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"ANH NGUYEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anh Nguyen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"DAOGUANG ZAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daoguang Zan is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZEQI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeqi Lin is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAN-GUANG LOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jian-Guang Lou is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is one of the authors of the paper titled \"CodeT: Code generation with generated tests\" published in ICLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICLR is a conference where the paper \"CodeT: Code generation with generated tests\" was published in 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIMING YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiming Yuan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENRIQUE PONDE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henrique Ponde is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JARED KAPLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jared Kaplan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARRISON EDWARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Edwards is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YURA BURDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yura Burda is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICHOLAS JOSEPH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Joseph is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GREG BROCKMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Greg Brockman is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEX RAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Ray is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAUL PURI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Raul Puri is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRETCHEN KRUEGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gretchen Krueger is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICHAEL PETROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Petrov is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEIDY KHLAAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heidy Khlaaf is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GIRISH SASTRY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Girish Sastry is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PAMELA MISHKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pamela Mishkin is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BROOKE CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brooke Chan is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SCOTT GRAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Scott Gray is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICK RYDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nick Ryder is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MIKHAIL PAVLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mikhail Pavlov is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALETHEA POWER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alethea Power is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLEMENS WINTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clemens Winter is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PHILIPPE TILLET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philippe Tillet is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FELIPE PETROSKI SUCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Felipe Petroski Such is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAVID W. CUMMINGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David W. Cummings is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FOTIOS CHANTZIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fotios Chantzis is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ELIZABETH BARNES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elizabeth Barnes is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARIEL HERBERT-VOSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ariel Herbert-Voss is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WILLIAM H. GUSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William H. Guss is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEX NICHOL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Nichol is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IGOR BABUSCHKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Babuschkin is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SUCHIR BALAJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suchir Balaji is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHANTANU JAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shantanu Jain is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDREW CARR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Carr is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAN LEIKE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jan Leike is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHUA ACHIAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Achiam is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VEDANT MISRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedant Misra is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EVAN MORIKAWA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evan Morikawa is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEC RADFORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alec Radford is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATTHEW M. KNIGHT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew M. Knight is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MILES BRUNDAGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Miles Brundage is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MIRA MURATI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mira Murati is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KATIE MAYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katie Mayer is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PETER WELINDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Welinder is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOB MCGREW\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bob McGrew is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DARIO AMODEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dario Amodei is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAM MCCANDLISH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam McCandlish is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WOJCIECH ZAREMBA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wojciech Zaremba is one of the authors of the paper titled \"Evaluating large language models trained on code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARXIV\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">arXiv is the platform where the paper \"Evaluating large language models trained on code\" was published in 2021<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"WENHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenhu Chen is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUEGUANG MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xueguang Ma is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XINYI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyi Wang is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WILLIAM W. COHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William W. Cohen is one of the authors of the paper titled \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" published in TMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TMLR\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">TMLR is the journal where the paper \"Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks\" was published in 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">JOURNAL<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JACOB DEVLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Devlin is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAURAV MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gaurav Mishra is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADAM ROBERTS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Roberts is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLAdam Roberts is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PAUL BARHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Paul Barham is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"CHARLES SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charles Sutton is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SEBASTIAN GEHRMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Gehrmann is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"PARKER SCHUH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Parker Schuh is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"KENSEN SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kensen Shi is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"SASHA TSVYASHCHENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sasha Tsvyashchenko is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR 2023<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>    <node id=\"JOSHUA MAYNEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Maynez is one of the authors of the paper titled \"PaLM: Scaling language modeling with pathways\" published in JMLR<\/data>      <data key=\"d2\">7a48515e86161237c03c9a8373197126<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"68e5573b596d253a03047b1e41988598","chunk":" An embodied multi-\nmodal language model. In ICML , 2023.\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B. Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In NeurIPS , 2023.\nJonathan St BT Evans. Intuition and reasoning: A dual-\nprocess perspective. Psychological Inquiry , pages 313 \u2013\n326, 2010.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. MineDojo: Building\nopen-ended embodied agents with internet-scale knowl-\nedge. In NeurIPS Datasets and Benchmarks Track , 2022.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Mat-\nsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal\nweb navigation with instruction-finetuned foundation\nmodels. In ICLR , 2024.Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei\nLiu, Yiming Yang, Jamie Callan, and Graham Neubig.\nPAL: Program-aided language models. In ICML , 2023.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and\nJun Wang. Long text generation via adversarial training\nwith leaked information. In AAAI , 2018.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Rus-\nlan Salakhutdinov. MineRL: A large-scale dataset of\nMinecraft demonstrations. In IJCAI , 2019.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Ville-\ngas, David Ha, Honglak Lee, and James Davidson. Learn-\ning latent dynamics for planning from pixels. In ICML ,\n2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timo-\nthy Lillicrap. Mastering diverse domains through world\nmodels. arXiv:2301.04104 , 2023.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning\nwith language model is planning with world model. In\nEMNLP , 2023.\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven\nZheng, Adams Wei Yu, Xinying Song, and Denny Zhou.\nLarge language models cannot self-correct reasoning yet.\nInICLR , 2024.\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Peter R. Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah\nBrown, Tomas Jackson, Linda Luu, Sergey Levine, Karol\nHausman, and Brian Ichter. Inner monologue: Embodied\nreasoning through planning with language models. In\nCoRL , 2022.\nLevente Kocsis and Csaba Szepesv \u00b4ari. Bandit based monte-\ncarlo planning. In ECML , 2006.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models\nare zero-shot reasoners. In NeurIPS , 2022.\nSteven M. LaValle. Rapidly-exploring random trees : A\nnew tool for path planning. The Annual Research Report ,\n1998.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin\nShi, and Percy Liang. Reinforcement learning on web\ninterfaces using workflow-guided exploration. In ICLR ,\n2018.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men,\n11Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nKejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng,\nZhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun\nZhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong,\nand Jie Tang. AgentBench: Evaluating LLMs as agents.\nInICLR , 2024.\nZhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi\nKe, Boyi Liu, and Zhaoran Wang. Reason for fu-\nture, act for now: A principled framework for au-\ntonomous LLM agents with provable sample efficiency.\narXiv:2309.17382 , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank\nGupta, Bodhisattwa Prasad Majumder, Katherine Her-\nmann, Sean Welle","chunk_id":"68e5573b596d253a03047b1e41988598","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"YILUN DU","type":"PERSON","description":"Yilun Du is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"MENGJIAO YANG","type":"PERSON","description":"Mengjiao Yang is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"BO DAI","type":"PERSON","description":"Bo Dai is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HANJUN DAI","type":"PERSON","description":"Hanjun Dai is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"OFIR NACHUM","type":"PERSON","description":"Ofir Nachum is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023\nOfir Nachum is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JOSHUA B. TENENBAUM","type":"PERSON","description":"Joshua B. Tenenbaum is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"NEURIPS","type":"EVENT","description":"NeurIPS is a conference where the paper \"Learning universal policies via text-guided video generation\" was published in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"JONATHAN ST BT EVANS","type":"PERSON","description":"Jonathan St BT Evans is the author of the paper titled \"Intuition and reasoning: A dual-process perspective\" published in Psychological Inquiry in 2010","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"PSYCHOLOGICAL INQUIRY","type":"JOURNAL","description":"Psychological Inquiry is a journal where the paper \"Intuition and reasoning: A dual-process perspective\" was published in 2010","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"JOURNAL"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YUNFAN JIANG","type":"PERSON","description":"Yunfan Jiang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"AJAY MANDLEKAR","type":"PERSON","description":"Ajay Mandlekar is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YUNCONG YANG","type":"PERSON","description":"Yuncong Yang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HAOYI ZHU","type":"PERSON","description":"Haoyi Zhu is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ANDREW TANG","type":"PERSON","description":"Andrew Tang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"DE-AN HUANG","type":"PERSON","description":"De-An Huang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"NEURIPS DATASETS AND BENCHMARKS TRACK","type":"EVENT","description":"NeurIPS Datasets and Benchmarks Track is a track within the NeurIPS conference where the paper \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" was published in 2022","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"HIROKI FURUTA","type":"PERSON","description":"Hiroki Furuta is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"KUANG-HUEI LEE","type":"PERSON","description":"Kuang-Huei Lee is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YUTAKA MATSUO","type":"PERSON","description":"Yutaka Matsuo is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"SHIXIANG SHANE GU","type":"PERSON","description":"Shixiang Shane Gu is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"IZZEDDIN GUR","type":"PERSON","description":"Izzeddin Gur is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ICLR","type":"EVENT","description":"ICLR is a conference where the paper \"Multimodal web navigation with instruction-finetuned foundation models\" was published in 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"AMAN MADAAN","type":"PERSON","description":"Aman Madaan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"SHUYAN ZHOU","type":"PERSON","description":"Shuyan Zhou is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JIAXIAN GUO","type":"PERSON","description":"Jiaxian Guo is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"SIDI LU","type":"PERSON","description":"Sidi Lu is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HAN CAI","type":"PERSON","description":"Han Cai is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"WEINAN ZHANG","type":"PERSON","description":"Weinan Zhang is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YONG YU","type":"PERSON","description":"Yong Yu is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JUN WANG","type":"PERSON","description":"Jun Wang is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"AAAI","type":"EVENT","description":"AAAI is a conference where the paper \"Long text generation via adversarial training with leaked information\" was published in 2018","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"WILLIAM H. GUSS","type":"PERSON","description":"William H. Guss is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"BRANDON HOUGHTON","type":"PERSON","description":"Brandon Houghton is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"NICHOLAY TOPIN","type":"PERSON","description":"Nicholay Topin is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"PHILLIP WANG","type":"PERSON","description":"Phillip Wang is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"CAYDEN CODEL","type":"PERSON","description":"Cayden Codel is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"MANUELA VELOSO","type":"PERSON","description":"Manuela Veloso is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"RUSLAN SALAKHUTDINOV","type":"PERSON","description":"Ruslan Salakhutdinov is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"IJCAI","type":"EVENT","description":"IJCAI is a conference where the paper \"MineRL: A large-scale dataset of Minecraft demonstrations\" was published in 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"DANIJAR HAFNER","type":"PERSON","description":"Danijar Hafner is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019\nDanijar Hafner is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"TIMOTHY LILLICRAP","type":"PERSON","description":"Timothy Lillicrap is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023\nTimothy Lillicrap is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"IAN FISCHER","type":"PERSON","description":"Ian Fischer is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"RUBEN VILLEGAS","type":"PERSON","description":"Ruben Villegas is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"DAVID HA","type":"PERSON","description":"David Ha is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HONGLAK LEE","type":"PERSON","description":"Honglak Lee is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JAMES DAVIDSON","type":"PERSON","description":"James Davidson is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JURGIS PASUKONIS","type":"PERSON","description":"Jurgis Pasukonis is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JIMMY BA","type":"PERSON","description":"Jimmy Ba is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ARXIV","type":"PLATFORM","description":"arXiv is a platform where the paper \"Mastering diverse domains through world models\" was published in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PLATFORM"},{"name":"SHIBO HAO","type":"PERSON","description":"Shibo Hao is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"YI GU","type":"PERSON","description":"Yi Gu is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HAODI MA","type":"PERSON","description":"Haodi Ma is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"JOSHUA JIAHUA HONG","type":"PERSON","description":"Joshua Jiahua Hong is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ZHEN WANG","type":"PERSON","description":"Zhen Wang is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"DAISY ZHE WANG","type":"PERSON","description":"Daisy Zhe Wang is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"ZHTING HU","type":"PERSON","description":"Zhiting Hu is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"EMNLP","type":"EVENT","description":"EMNLP is a conference where the paper \"Reasoning with language model is planning with world model\" was published in 2023","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"EVENT"},{"name":"JIE HUANG","type":"PERSON","description":"Jie Huang is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"XINYUN CHEN","type":"PERSON","description":"Xinyun Chen is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"},{"name":"HUAIXIU STEVEN ZHENG","type":"PERSON","description":"Huaixiu Steven Zheng is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024","source_id":"68e5573b596d253a03047b1e41988598","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"YILUN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Du is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MENGJIAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengjiao Yang is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BO DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bo Dai is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HANJUN DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanjun Dai is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OFIR NACHUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ofir Nachum is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023Ofir Nachum is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHUA B. TENENBAUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua B. Tenenbaum is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is one of the authors of the paper titled \"Learning universal policies via text-guided video generation\" published in NeurIPS 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEURIPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">NeurIPS is a conference where the paper \"Learning universal policies via text-guided video generation\" was published in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"JONATHAN ST BT EVANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan St BT Evans is the author of the paper titled \"Intuition and reasoning: A dual-process perspective\" published in Psychological Inquiry in 2010<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PSYCHOLOGICAL INQUIRY\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Psychological Inquiry is a journal where the paper \"Intuition and reasoning: A dual-process perspective\" was published in 2010<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">JOURNAL<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUNFAN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Jiang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AJAY MANDLEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ajay Mandlekar is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUNCONG YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuncong Yang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAOYI ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haoyi Zhu is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDREW TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Tang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DE-AN HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">De-An Huang is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is one of the authors of the paper titled \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" published in NeurIPS Datasets and Benchmarks Track in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEURIPS DATASETS AND BENCHMARKS TRACK\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">NeurIPS Datasets and Benchmarks Track is a track within the NeurIPS conference where the paper \"MineDojo: Building open-ended embodied agents with internet-scale knowledge\" was published in 2022<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"HIROKI FURUTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hiroki Furuta is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KUANG-HUEI LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kuang-Huei Lee is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUTAKA MATSUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yutaka Matsuo is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIXIANG SHANE GU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shixiang Shane Gu is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IZZEDDIN GUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Izzeddin Gur is one of the authors of the paper titled \"Multimodal web navigation with instruction-finetuned foundation models\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICLR is a conference where the paper \"Multimodal web navigation with instruction-finetuned foundation models\" was published in 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMAN MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHUYAN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuyan Zhou is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is one of the authors of the paper titled \"PAL: Program-aided language models\" published in ICML 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAXIAN GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiaxian Guo is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIDI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sidi Lu is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Han Cai is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEINAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weinan Zhang is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yong Yu is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JUN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jun Wang is one of the authors of the paper titled \"Long text generation via adversarial training with leaked information\" published in AAAI 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AAAI\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">AAAI is a conference where the paper \"Long text generation via adversarial training with leaked information\" was published in 2018<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"WILLIAM H. GUSS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William H. Guss is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BRANDON HOUGHTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brandon Houghton is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICHOLAY TOPIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholay Topin is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PHILLIP WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Phillip Wang is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAYDEN CODEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cayden Codel is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MANUELA VELOSO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Manuela Veloso is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUSLAN SALAKHUTDINOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruslan Salakhutdinov is one of the authors of the paper titled \"MineRL: A large-scale dataset of Minecraft demonstrations\" published in IJCAI 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IJCAI\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">IJCAI is a conference where the paper \"MineRL: A large-scale dataset of Minecraft demonstrations\" was published in 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"DANIJAR HAFNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Danijar Hafner is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019Danijar Hafner is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TIMOTHY LILLICRAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy Lillicrap is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023Timothy Lillicrap is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IAN FISCHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ian Fischer is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUBEN VILLEGAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruben Villegas is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAVID HA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Ha is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HONGLAK LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Honglak Lee is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAMES DAVIDSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James Davidson is one of the authors of the paper titled \"Learning latent dynamics for planning from pixels\" published in ICML 2019<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JURGIS PASUKONIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jurgis Pasukonis is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIMMY BA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jimmy Ba is one of the authors of the paper titled \"Mastering diverse domains through world models\" published on arXiv in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARXIV\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">arXiv is a platform where the paper \"Mastering diverse domains through world models\" was published in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"SHIBO HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shibo Hao is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YI GU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Gu is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAODI MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haodi Ma is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSHUA JIAHUA HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua Jiahua Hong is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHEN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhen Wang is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAISY ZHE WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daisy Zhe Wang is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHTING HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiting Hu is one of the authors of the paper titled \"Reasoning with language model is planning with world model\" published in EMNLP 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EMNLP\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">EMNLP is a conference where the paper \"Reasoning with language model is planning with world model\" was published in 2023<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"JIE HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Huang is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XINYUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyun Chen is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUAIXIU STEVEN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huaixiu Steven Zheng is one of the authors of the paper titled \"Large language models cannot self-correct reasoning yet\" published in ICLR 2024<\/data>      <data key=\"d2\">68e5573b596d253a03047b1e41988598<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"2d4672dfb7bd4283f0b5f23ab4f26653","chunk":" efficiency.\narXiv:2309.17382 , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank\nGupta, Bodhisattwa Prasad Majumder, Katherine Her-\nmann, Sean Welleck, Amir Yazdanbakhsh, and Peter\nClark. Self-refine: Iterative refinement with self-feedback.\nInNeurIPS , 2023.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar\nGulcehre, and Bing Xiang. Abstractive text summariza-\ntion using sequence-to-sequence RNNs and beyond. In\nSpecial Interest Group on Natural Language Learning ,\n2016.\nOpenAI. GPT-4 technical report. arXiv:2303.08774 , 2023.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill\nQian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou,\nMark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.\nToolLLM: Facilitating large language models to master\n16000+ real-world APIs. In ICLR , 2024.\nAbulhair Saparov and He He. Language models are greedy\nreasoners: A systematic formal analysis of chain-of-\nthought. In ICLR , 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language\nmodels can teach themselves to use tools. In NeurIPS ,\n2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. HuggingGPT: Solving\nAI tasks with ChatGPT and its friends in Hugging Face.\nInNeurIPS , 2023.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao. Reflex-\nion: Language agents with verbal reinforcement learning.\nInNeurIPS , 2023.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C \u02c6ot\u00b4e,\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\nALFWorld: Aligning text and embodied environments\nfor interactive learning. In ICLR , 2020.David Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering the game of Go with deep\nneural networks and tree search. Nature , 529:484\u2013489,\n2016.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering chess and Shogi by self-\nplay with a general reinforcement learning algorithm.\narXiv:1712.01815 , 2017.\nSteven A. Sloman. The empirical case for two systems of\nreasoning. Psychological Bulletin , 119:3\u201322, 1996.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and\nChao Zhang. AdaPlanner: Adaptive planning from feed-\nback with language models. In NeurIPS , 2023.\nD\u00b4\u0131dac Sur \u00b4\u0131s, Sachit Menon, and Carl V ondrick. ViperGPT:\nVisual inference via Python execution for reasoning. In\nICCV , 2023.\nMaciej Swiechowski, Konrad Godlewski, Bartosz Sawicki,\nand Jacek Ma\u2019ndziuk. Monte Carlo tree search: A re-\nview of recent modifications and applications. Artificial\nIntelligence Review , 56:2497\u20132562, 2021.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Daniel M. Bikel, Lukas Blecher, Cristian Cant \u00b4on\nFerrer, Moya Chen, Guillem Cucurull","chunk_id":"2d4672dfb7bd4283f0b5f23ab4f26653","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"AMAN MADAAN","type":"PERSON","description":"Aman Madaan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NIKET TANDON","type":"PERSON","description":"Niket Tandon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PRAKHAR GUPTA","type":"PERSON","description":"Prakhar Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SKYLER HALLINAN","type":"PERSON","description":"Skyler Hallinan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SARAH WIEGREFFE","type":"PERSON","description":"Sarah Wiegreffe is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NOUHA DZIRI","type":"PERSON","description":"Nouha Dziri is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHRIMAI PRABHUMOYE","type":"PERSON","description":"Shrimai Prabhumoye is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHASHANK GUPTA","type":"PERSON","description":"Shashank Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BODHISATTWA PRASAD MAJUMDER","type":"PERSON","description":"Bodhisattwa Prasad Majumder is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KATHERINE HERMANN","type":"PERSON","description":"Katherine Hermann is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SEAN WELLECK","type":"PERSON","description":"Sean Welleck is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AMIR YAZDANBAKHSH","type":"PERSON","description":"Amir Yazdanbakhsh is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PETER CLARK","type":"PERSON","description":"Peter Clark is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NEURIPS","type":"EVENT","description":"NeurIPS is a conference where the paper \"Self-refine: Iterative refinement with self-feedback\" was published in 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RAMESH NALLAPATI","type":"PERSON","description":"Ramesh Nallapati is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BOWEN ZHOU","type":"PERSON","description":"Bowen Zhou is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CICERO DOS SANTOS","type":"PERSON","description":"Cicero dos Santos is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CAGLAR GULCEHRE","type":"PERSON","description":"Caglar Gulcehre is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BING XIANG","type":"PERSON","description":"Bing Xiang is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SPECIAL INTEREST GROUP ON NATURAL LANGUAGE LEARNING","type":"EVENT","description":"The Special Interest Group on Natural Language Learning is an event where the paper \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" was published in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that published the GPT-4 technical report on arXiv in 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GPT-4 TECHNICAL REPORT","type":"DOCUMENT","description":"The GPT-4 technical report is a document published by OpenAI on arXiv in 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHIHAO LIANG","type":"PERSON","description":"Shihao Liang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YINING YE","type":"PERSON","description":"Yining Ye is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KUNLUN ZHU","type":"PERSON","description":"Kunlun Zhu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LAN YAN","type":"PERSON","description":"Lan Yan is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XIN CONG","type":"PERSON","description":"Xin Cong is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XIANGRU TANG","type":"PERSON","description":"Xiangru Tang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BILL QIAN","type":"PERSON","description":"Bill Qian is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SIHAN ZHAO","type":"PERSON","description":"Sihan Zhao is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RUNCHU TIAN","type":"PERSON","description":"Runchu Tian is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"RUOBING XIE","type":"PERSON","description":"Ruobing Xie is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JIE ZHOU","type":"PERSON","description":"Jie Zhou is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARK GERSTEIN","type":"PERSON","description":"Mark Gerstein is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DAHAI LI","type":"PERSON","description":"Dahai Li is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ZHIYUAN LIU","type":"PERSON","description":"Zhiyuan Liu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MAOSONG SUN","type":"PERSON","description":"Maosong Sun is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ICLR","type":"EVENT","description":"ICLR is a conference where the paper \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" was published in 2024","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ABULHAIR SAPAROV","type":"PERSON","description":"Abulhair Saparov is one of the authors of the paper titled \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\" published in ICLR 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HE HE","type":"PERSON","description":"He He is one of the authors of the paper titled \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\" published in ICLR 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"TIMO SCHICK","type":"PERSON","description":"Timo Schick is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JANE DWIVEDI-YU","type":"PERSON","description":"Jane Dwivedi-Yu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ROBERTO DESSI","type":"PERSON","description":"Roberto Dessi is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ROBERTA RAILEANU","type":"PERSON","description":"Roberta Raileanu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARIA LOMELI","type":"PERSON","description":"Maria Lomeli is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUKE ZETTLEMOYER","type":"PERSON","description":"Luke Zettlemoyer is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NICOLA CANCEDDA","type":"PERSON","description":"Nicola Cancedda is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YONGLIANG SHEN","type":"PERSON","description":"Yongliang Shen is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KAITAO SONG","type":"PERSON","description":"Kaitao Song is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XU TAN","type":"PERSON","description":"Xu Tan is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DONGSHENG LI","type":"PERSON","description":"Dongsheng Li is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"WEIMING LU","type":"PERSON","description":"Weiming Lu is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YUETING ZHUANG","type":"PERSON","description":"Yueting Zhuang is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NOAH SHINN","type":"PERSON","description":"Noah Shinn is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"FEDERICO CASSANO","type":"PERSON","description":"Federico Cassano is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BECK LABASH","type":"PERSON","description":"Beck Labash is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ASHWIN GOPINATH","type":"PERSON","description":"Ashwin Gopinath is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KARTHIK NARASIMHAN","type":"PERSON","description":"Karthik Narasimhan is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHUNYU YAO","type":"PERSON","description":"Shunyu Yao is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MOHIT SHRIDHAR","type":"PERSON","description":"Mohit Shridhar is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"XINGDI YUAN","type":"PERSON","description":"Xingdi Yuan is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARC-ALEXANDRE COTE","type":"PERSON","description":"Marc-Alexandre Cote is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YONATAN BISK","type":"PERSON","description":"Yonatan Bisk is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ADAM TRISCHLER","type":"PERSON","description":"Adam Trischler is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DAVID SILVER","type":"PERSON","description":"David Silver is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AJA HUANG","type":"PERSON","description":"Aja Huang is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CHRIS J. MADDISON","type":"PERSON","description":"Chris J. Maddison is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ARTHUR GUEZ","type":"PERSON","description":"Arthur Guez is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"L. SIFRE","type":"PERSON","description":"L. Sifre is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GEORGE VAN DEN DRIESSCHE","type":"PERSON","description":"George van den Driessche is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JULIAN SCHRITTWIESER","type":"PERSON","description":"Julian Schrittwieser is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"IOANNIS ANTONOGLOU","type":"PERSON","description":"Ioannis Antonoglou is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"VEDAVYAS PANNEERSHELVAM","type":"PERSON","description":"Vedavyas Panneershelvam is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MARC LANCTOT","type":"PERSON","description":"Marc Lanctot is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SANDER DIELEMAN","type":"PERSON","description":"Sander Dieleman is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DOMINIK GREWE","type":"PERSON","description":"Dominik Grewe is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JOHN NHAM","type":"PERSON","description":"John Nham is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NAL KALCHBRENNER","type":"PERSON","description":"Nal Kalchbrenner is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"TIMOTHY P. LILLICRAP","type":"PERSON","description":"Timothy P. Lillicrap is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MADELEINE LEACH","type":"PERSON","description":"Madeleine Leach is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KORAY KAVUKCUOGLU","type":"PERSON","description":"Koray Kavukcuoglu is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"THORE GRAEPEL","type":"PERSON","description":"Thore Graepel is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DEMIS HASSABIS","type":"PERSON","description":"Demis Hassabis is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NATURE","type":"DOCUMENT","description":"Nature is the journal where the paper \"Mastering the game of Go with deep neural networks and tree search\" was published in 2016","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"STEVEN A. SLOMAN","type":"PERSON","description":"Steven A. Sloman is the author of the paper titled \"The empirical case for two systems of reasoning\" published in Psychological Bulletin in 1996","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PSYCHOLOGICAL BULLETIN","type":"DOCUMENT","description":"Psychological Bulletin is the journal where the paper \"The empirical case for two systems of reasoning\" was published in 1996","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HAOTIAN SUN","type":"PERSON","description":"Haotian Sun is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YUCHEN ZHUANG","type":"PERSON","description":"Yuchen Zhuang is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LINGKAI KONG","type":"PERSON","description":"Lingkai Kong is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BO DAI","type":"PERSON","description":"Bo Dai is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CHAO ZHANG","type":"PERSON","description":"Chao Zhang is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DIDAC SURIS","type":"PERSON","description":"Didac Suris is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SACHIT MENON","type":"PERSON","description":"Sachit Menon is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CARL VONDRICK","type":"PERSON","description":"Carl Vondrick is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ICCV","type":"EVENT","description":"ICCV is a conference where the paper \"ViperGPT: Visual inference via Python execution for reasoning\" was published in 2023","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MACIEJ SWIECHOWSKI","type":"PERSON","description":"Maciej Swiechowski is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KONRAD GODLEWSKI","type":"PERSON","description":"Konrad Godlewski is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"BARTOSZ SAWICKI","type":"PERSON","description":"Bartosz Sawicki is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"JACEK MA'NDZIUK","type":"PERSON","description":"Jacek Ma'ndziuk is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"ARTIFICIAL INTELLIGENCE REVIEW","type":"DOCUMENT","description":"Artificial Intelligence Review is the journal where the paper \"Monte Carlo tree search: A review of recent modifications and applications\" was published in 2021","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"HUGO TOUVRON","type":"PERSON","description":"Hugo Touvron is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LOUIS MARTIN","type":"PERSON","description":"Louis Martin is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"KEVIN R. STONE","type":"PERSON","description":"Kevin R. Stone is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PETER ALBERT","type":"PERSON","description":"Peter Albert is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"AMJAD ALMAHAIRI","type":"PERSON","description":"Amjad Almahairi is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"YASMINE BABAEI","type":"PERSON","description":"Yasmine Babaei is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"NIKOLAY BASHLYKOV","type":"PERSON","description":"Nikolay Bashlykov is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SOUMYA BATRA","type":"PERSON","description":"Soumya Batra is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"PRAJJWAL BHARGAVA","type":"PERSON","description":"Prajjwal Bhargava is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"SHRUTI BHOSALE","type":"PERSON","description":"Shruti Bhosale is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"DANIEL M. BIKEL","type":"PERSON","description":"Daniel M. Bikel is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"LUKAS BLECHER","type":"PERSON","description":"Lukas Blecher is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"CRISTIAN CANTON FERRER","type":"PERSON","description":"Cristian Canton Ferrer is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"MOYA CHEN","type":"PERSON","description":"Moya Chen is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"},{"name":"GUILLEM CUCURULL","type":"PERSON","description":"Guillem Cucurull is one of the authors of the paper mentioned in the text","source_id":"2d4672dfb7bd4283f0b5f23ab4f26653"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AMAN MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NIKET TANDON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Niket Tandon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PRAKHAR GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prakhar Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SKYLER HALLINAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Skyler Hallinan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SARAH WIEGREFFE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Wiegreffe is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NOUHA DZIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nouha Dziri is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHRIMAI PRABHUMOYE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shrimai Prabhumoye is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHASHANK GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shashank Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BODHISATTWA PRASAD MAJUMDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bodhisattwa Prasad Majumder is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KATHERINE HERMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katherine Hermann is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SEAN WELLECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sean Welleck is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AMIR YAZDANBAKHSH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amir Yazdanbakhsh is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PETER CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Clark is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NEURIPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">NeurIPS is a conference where the paper \"Self-refine: Iterative refinement with self-feedback\" was published in 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RAMESH NALLAPATI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ramesh Nallapati is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BOWEN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Zhou is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CICERO DOS SANTOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cicero dos Santos is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CAGLAR GULCEHRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caglar Gulcehre is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BING XIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bing Xiang is one of the authors of the paper titled \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" published in the Special Interest Group on Natural Language Learning in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SPECIAL INTEREST GROUP ON NATURAL LANGUAGE LEARNING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Special Interest Group on Natural Language Learning is an event where the paper \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\" was published in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that published the GPT-4 technical report on arXiv in 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The GPT-4 technical report is a document published by OpenAI on arXiv in 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHIHAO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shihao Liang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YINING YE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yining Ye is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KUNLUN ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kunlun Zhu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lan Yan is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XIN CONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Cong is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XIANGRU TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiangru Tang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BILL QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bill Qian is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SIHAN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sihan Zhao is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RUNCHU TIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Runchu Tian is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"RUOBING XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruobing Xie is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JIE ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jie Zhou is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARK GERSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Gerstein is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DAHAI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahai Li is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ZHIYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Liu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MAOSONG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maosong Sun is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in ICLR 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICLR is a conference where the paper \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" was published in 2024<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ABULHAIR SAPAROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abulhair Saparov is one of the authors of the paper titled \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\" published in ICLR 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HE HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">He He is one of the authors of the paper titled \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\" published in ICLR 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"TIMO SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timo Schick is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JANE DWIVEDI-YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane Dwivedi-Yu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ROBERTO DESSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberto Dessi is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ROBERTA RAILEANU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberta Raileanu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARIA LOMELI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maria Lomeli is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUKE ZETTLEMOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luke Zettlemoyer is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NICOLA CANCEDDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicola Cancedda is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YONGLIANG SHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yongliang Shen is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KAITAO SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kaitao Song is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XU TAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Tan is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DONGSHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongsheng Li is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"WEIMING LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weiming Lu is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YUETING ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yueting Zhuang is one of the authors of the paper titled \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NOAH SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Shinn is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"FEDERICO CASSANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Federico Cassano is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BECK LABASH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Beck Labash is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ASHWIN GOPINATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashwin Gopinath is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KARTHIK NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik Narasimhan is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHUNYU YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shunyu Yao is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MOHIT SHRIDHAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohit Shridhar is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"XINGDI YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xingdi Yuan is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARC-ALEXANDRE COTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marc-Alexandre Cote is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YONATAN BISK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yonatan Bisk is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ADAM TRISCHLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Trischler is one of the authors of the paper titled \"ALFWorld: Aligning text and embodied environments for interactive learning\" published in ICLR 2020<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DAVID SILVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Silver is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AJA HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aja Huang is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CHRIS J. MADDISON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris J. Maddison is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ARTHUR GUEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Guez is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"L. SIFRE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L. Sifre is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GEORGE VAN DEN DRIESSCHE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">George van den Driessche is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JULIAN SCHRITTWIESER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Schrittwieser is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"IOANNIS ANTONOGLOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ioannis Antonoglou is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"VEDAVYAS PANNEERSHELVAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedavyas Panneershelvam is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MARC LANCTOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marc Lanctot is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SANDER DIELEMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sander Dieleman is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DOMINIK GREWE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dominik Grewe is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JOHN NHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Nham is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NAL KALCHBRENNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nal Kalchbrenner is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"TIMOTHY P. LILLICRAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy P. Lillicrap is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MADELEINE LEACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madeleine Leach is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KORAY KAVUKCUOGLU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Koray Kavukcuoglu is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"THORE GRAEPEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thore Graepel is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DEMIS HASSABIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Demis Hassabis is one of the authors of the paper titled \"Mastering the game of Go with deep neural networks and tree search\" published in Nature in 2016 and \"Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\" published on arXiv in 2017<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NATURE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Nature is the journal where the paper \"Mastering the game of Go with deep neural networks and tree search\" was published in 2016<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"STEVEN A. SLOMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven A. Sloman is the author of the paper titled \"The empirical case for two systems of reasoning\" published in Psychological Bulletin in 1996<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PSYCHOLOGICAL BULLETIN\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Psychological Bulletin is the journal where the paper \"The empirical case for two systems of reasoning\" was published in 1996<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HAOTIAN SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haotian Sun is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YUCHEN ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhuang is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LINGKAI KONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lingkai Kong is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BO DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bo Dai is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chao Zhang is one of the authors of the paper titled \"AdaPlanner: Adaptive planning from feedback with language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DIDAC SURIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Didac Suris is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SACHIT MENON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sachit Menon is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CARL VONDRICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carl Vondrick is one of the authors of the paper titled \"ViperGPT: Visual inference via Python execution for reasoning\" published in ICCV 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ICCV\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICCV is a conference where the paper \"ViperGPT: Visual inference via Python execution for reasoning\" was published in 2023<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MACIEJ SWIECHOWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maciej Swiechowski is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KONRAD GODLEWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Konrad Godlewski is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"BARTOSZ SAWICKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bartosz Sawicki is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"JACEK MA'NDZIUK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacek Ma'ndziuk is one of the authors of the paper titled \"Monte Carlo tree search: A review of recent modifications and applications\" published in Artificial Intelligence Review in 2021<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"ARTIFICIAL INTELLIGENCE REVIEW\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Artificial Intelligence Review is the journal where the paper \"Monte Carlo tree search: A review of recent modifications and applications\" was published in 2021<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"HUGO TOUVRON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hugo Touvron is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LOUIS MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Martin is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"KEVIN R. STONE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin R. Stone is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PETER ALBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Albert is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"AMJAD ALMAHAIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amjad Almahairi is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"YASMINE BABAEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yasmine Babaei is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"NIKOLAY BASHLYKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikolay Bashlykov is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SOUMYA BATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soumya Batra is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"PRAJJWAL BHARGAVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prajjwal Bhargava is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"SHRUTI BHOSALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shruti Bhosale is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"DANIEL M. BIKEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel M. Bikel is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"LUKAS BLECHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukas Blecher is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"CRISTIAN CANTON FERRER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cristian Canton Ferrer is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"MOYA CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Moya Chen is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <node id=\"GUILLEM CUCURULL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillem Cucurull is one of the authors of the paper mentioned in the text<\/data>      <data key=\"d2\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/node>    <edge source=\"AMAN MADAAN\" target=\"NIKET TANDON\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Niket Tandon co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"PRAKHAR GUPTA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Prakhar Gupta co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"SKYLER HALLINAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Skyler Hallinan co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"LUYU GAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Luyu Gao co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"SARAH WIEGREFFE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Sarah Wiegreffe co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"URI ALON\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Uri Alon co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"NOUHA DZIRI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Nouha Dziri co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"SHRIMAI PRABHUMOYE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Shrimai Prabhumoye co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"YIMING YANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Yiming Yang co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"SHASHANK GUPTA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Shashank Gupta co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"BODHISATTWA PRASAD MAJUMDER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Bodhisattwa Prasad Majumder co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"KATHERINE HERMANN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Katherine Hermann co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"SEAN WELLECK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Sean Welleck co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>    <edge source=\"AMAN MADAAN\" target=\"AMIR YAZDANBAKHSH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Aman Madaan and Amir Yazdanbakhsh co-authored the paper \"Self-refine: Iterative refinement with self-feedback\" published in NeurIPS 2023<\/data>      <data key=\"d5\">2d4672dfb7bd4283f0b5f23ab4f26653<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"8180bf20b7577f3eee40df5991e2886d","chunk":":2497\u20132562, 2021.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Daniel M. Bikel, Lukas Blecher, Cristian Cant \u00b4on\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\nabel M. Kloumann, A. V . Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom.\n12Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nLlama 2: Open foundation and fine-tuned chat models.\narXiv:2307.09288 , 2023.\nTom V odopivec, Spyridon Samothrakis, and Branko Ster.\nOn Monte Carlo tree search and reinforcement learning.\nJournal of Artificial Intelligence Research , 60:881\u2013936,\n2017.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-\nkumar. V oyager: An open-ended embodied agent with\nlarge language models. arXiv:2305.16291 , 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models. In ICLR ,\n2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language\nmodels. In NeurIPS , 2022.\nMichael Wooldridge and Nicholas R Jennings. Intelligent\nagents: Theory and practice. The Knowledge Engineering\nReview , 10:115 \u2013 152, 1995.\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models\nfor physical robot learning. In CoRL , 2023.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-\nYen Kan, Junxian He, and Qizhe Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding.\narXiv:2305.00633 , 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. HotpotQA: A dataset for diverse, ex-\nplainable multi-hop question answering. In EMNLP ,\n2018.\nShunyu Yao, Howard Chen, John Yang, and Karthik R\nNarasimhan. WebShop: Towards scalable real-world web\ninteraction with grounded language agents. In NeurIPS ,\n2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: deliberate problem solving with large\nlanguage models. In NeurIPS , 2023a.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao. ReAct: Synergizing\nreasoning and acting in language models. In ICLR , 2023b.\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel,\nand Yang Gao. Mastering Atari games with limited data.\nInNeurIPS , 2021.Denny Zhou, Nathanael Sch \u00a8arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Olivier Bous-\nquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In\nICLR , 2022.\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor\nBursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao\nZhang. ToolChain*: Efficient action space navigation in\nlarge language models with A* search. In ICLR , ","chunk_id":"8180bf20b7577f3eee40df5991e2886d","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LOUIS MARTIN","type":"PERSON","description":"Louis Martin is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KEVIN R. STONE","type":"PERSON","description":"Kevin R. Stone is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PETER ALBERT","type":"PERSON","description":"Peter Albert is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AMJAD ALMAHAIRI","type":"PERSON","description":"Amjad Almahairi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YASMINE BABAEI","type":"PERSON","description":"Yasmine Babaei is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NIKOLAY BASHLYKOV","type":"PERSON","description":"Nikolay Bashlykov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SOUMYA BATRA","type":"PERSON","description":"Soumya Batra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PRAJJWAL BHARGAVA","type":"PERSON","description":"Prajjwal Bhargava is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHRUTI BHOSALE","type":"PERSON","description":"Shruti Bhosale is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DANIEL M. BIKEL","type":"PERSON","description":"Daniel M. Bikel is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LUKAS BLECHER","type":"PERSON","description":"Lukas Blecher is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CRISTIAN CANT\u00d3N FERRER","type":"PERSON","description":"Cristian Cant\u00f3n Ferrer is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MOYA CHEN","type":"PERSON","description":"Moya Chen is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"GUILLEM CUCURULL","type":"PERSON","description":"Guillem Cucurull is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DAVID ESIOBU","type":"PERSON","description":"David Esiobu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JUDE FERNANDES","type":"PERSON","description":"Jude Fernandes is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JEREMY FU","type":"PERSON","description":"Jeremy Fu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"WENYIN FU","type":"PERSON","description":"Wenyin Fu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"BRIAN FULLER","type":"PERSON","description":"Brian Fuller is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CYNTHIA GAO","type":"PERSON","description":"Cynthia Gao is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"VEDANUJ GOSWAMI","type":"PERSON","description":"Vedanuj Goswami is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NAMAN GOYAL","type":"PERSON","description":"Naman Goyal is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANTHONY S. HARTSHORN","type":"PERSON","description":"Anthony S. Hartshorn is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SAGHAR HOSSEINI","type":"PERSON","description":"Saghar Hosseini is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RUI HOU","type":"PERSON","description":"Rui Hou is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"HAKAN INAN","type":"PERSON","description":"Hakan Inan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MARCIN KARDAS","type":"PERSON","description":"Marcin Kardas is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"VIKTOR KERKEZ","type":"PERSON","description":"Viktor Kerkez is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MADIAN KHABSA","type":"PERSON","description":"Madian Khabsa is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ISABEL M. KLOUMANN","type":"PERSON","description":"Isabel M. Kloumann is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"A. V. KORENEV","type":"PERSON","description":"A. V. Korenev is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUNIT SINGH KOURA","type":"PERSON","description":"Punit Singh Koura is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MARIE-ANNE LACHAUX","type":"PERSON","description":"Marie-Anne Lachaux is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THIBAUT LAVRIL","type":"PERSON","description":"Thibaut Lavril is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JENYA LEE","type":"PERSON","description":"Jenya Lee is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DIANA LISKOVICH","type":"PERSON","description":"Diana Liskovich is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YINGHAI LU","type":"PERSON","description":"Yinghai Lu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUNING MAO","type":"PERSON","description":"Yuning Mao is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XAVIER MARTINET","type":"PERSON","description":"Xavier Martinet is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TODOR MIHAYLOV","type":"PERSON","description":"Todor Mihaylov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUSHKAR MISHRA","type":"PERSON","description":"Pushkar Mishra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"IGOR MOLYBOG","type":"PERSON","description":"Igor Molybog is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YIXIN NIE","type":"PERSON","description":"Yixin Nie is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANDREW POULTON","type":"PERSON","description":"Andrew Poulton is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JEREMY REIZENSTEIN","type":"PERSON","description":"Jeremy Reizenstein is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RASHI RUNGTA","type":"PERSON","description":"Rashi Rungta is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KALYAN SALADI","type":"PERSON","description":"Kalyan Saladi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ALAN SCHELTEN","type":"PERSON","description":"Alan Schelten is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RUAN SILVA","type":"PERSON","description":"Ruan Silva is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ERIC MICHAEL SMITH","type":"PERSON","description":"Eric Michael Smith is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"R. SUBRAMANIAN","type":"PERSON","description":"R. Subramanian is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XIA TAN","type":"PERSON","description":"Xia Tan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"BINH TANG","type":"PERSON","description":"Binh Tang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ROSS TAYLOR","type":"PERSON","description":"Ross Taylor is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ADINA WILLIAMS","type":"PERSON","description":"Adina Williams is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JIAN XIANG KUAN","type":"PERSON","description":"Jian Xiang Kuan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PUXIN XU","type":"PERSON","description":"Puxin Xu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ZHENGXU YAN","type":"PERSON","description":"Zhengxu Yan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ILIYAN ZAROV","type":"PERSON","description":"Iliyan Zarov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUCHEN ZHANG","type":"PERSON","description":"Yuchen Zhang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANGELA FAN","type":"PERSON","description":"Angela Fan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MELANIE KAMBADUR","type":"PERSON","description":"Melanie Kambadur is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AURELIEN RODRIGUEZ","type":"PERSON","description":"Aurelien Rodriguez is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ROBERT STOJNIC","type":"PERSON","description":"Robert Stojnic is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SERGEY EDUNOV","type":"PERSON","description":"Sergey Edunov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LLAMA 2","type":"TECHNOLOGY","description":"Llama 2 is a set of open foundation and fine-tuned chat models described in a paper published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TOM VODOPIVEC","type":"PERSON","description":"Tom Vodopivec is one of the authors of the paper titled \"On Monte Carlo tree search and reinforcement learning\" published in the Journal of Artificial Intelligence Research in 2017","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"BRANKO STER","type":"PERSON","description":"Branko Ster is one of the authors of the paper titled \"On Monte Carlo tree search and reinforcement learning\" published in the Journal of Artificial Intelligence Research in 2017","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH","type":"DOCUMENT","description":"The Journal of Artificial Intelligence Research is a publication where the paper \"On Monte Carlo tree search and reinforcement learning\" was published in 2017","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUQI XIE","type":"PERSON","description":"Yuqi Xie is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUNFAN JIANG","type":"PERSON","description":"Yunfan Jiang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"AJAY MANDLEKAR","type":"PERSON","description":"Ajay Mandlekar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CHAOWEI XIAO","type":"PERSON","description":"Chaowei Xiao is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"VOYAGER","type":"TECHNOLOGY","description":"Voyager is an open-ended embodied agent with large language models described in a paper published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022\nXuezhi Wang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022\nJason Wei is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022\nDale Schuurmans is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"QUOC LE","type":"PERSON","description":"Quoc Le is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022\nQuoc Le is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"ED CHI","type":"PERSON","description":"Ed Chi is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022\nEd Chi is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022\nDenny Zhou is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"ICLR","type":"EVENT","description":"ICLR is a conference where the paper \"Self-consistency improves chain of thought reasoning in language models\" was published in 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is one of the authors of the paper titled \"Chain of thought prompting elicits reasoning in large language models\" published in NeurIPS 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NEURIPS","type":"EVENT","description":"NeurIPS is a conference where the paper \"Chain of thought prompting elicits reasoning in large language models\" was published in 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MICHAEL WOOLDRIDGE","type":"PERSON","description":"Michael Wooldridge is one of the authors of the paper titled \"Intelligent agents: Theory and practice\" published in The Knowledge Engineering Review in 1995","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NICHOLAS R JENNINGS","type":"PERSON","description":"Nicholas R Jennings is one of the authors of the paper titled \"Intelligent agents: Theory and practice\" published in The Knowledge Engineering Review in 1995","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THE KNOWLEDGE ENGINEERING REVIEW","type":"DOCUMENT","description":"The Knowledge Engineering Review is a publication where the paper \"Intelligent agents: Theory and practice\" was published in 1995","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PHILIPP WU","type":"PERSON","description":"Philipp Wu is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ALEJANDRO ESCONTRELA","type":"PERSON","description":"Alejandro Escontrela is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DANIJAR HAFNER","type":"PERSON","description":"Danijar Hafner is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023\nPieter Abbeel is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"KEN GOLDBERG","type":"PERSON","description":"Ken Goldberg is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CORL","type":"EVENT","description":"CoRL is a conference where the paper \"Daydreamer: World models for physical robot learning\" was published in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUXI XIE","type":"PERSON","description":"Yuxi Xie is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KENJI KAWAGUCHI","type":"PERSON","description":"Kenji Kawaguchi is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YIRAN ZHAO","type":"PERSON","description":"Yiran Zhao is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XU ZHAO","type":"PERSON","description":"Xu Zhao is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"MIN-YEN KAN","type":"PERSON","description":"Min-Yen Kan is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JUNXIAN HE","type":"PERSON","description":"Junxian He is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"QIZHE XIE","type":"PERSON","description":"Qizhe Xie is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"ZHILIN YANG","type":"PERSON","description":"Zhilin Yang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"PENG QI","type":"PERSON","description":"Peng Qi is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SAIZHENG ZHANG","type":"PERSON","description":"Saizheng Zhang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YOSHUA BENGIO","type":"PERSON","description":"Yoshua Bengio is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"WILLIAM W COHEN","type":"PERSON","description":"William W Cohen is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"RUSLAN SALAKHUTDINOV","type":"PERSON","description":"Ruslan Salakhutdinov is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"CHRISTOPHER D MANNING","type":"PERSON","description":"Christopher D Manning is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"EMNLP","type":"EVENT","description":"EMNLP is a conference where the paper \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" was published in 2018","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHUNYU YAO","type":"PERSON","description":"Shunyu Yao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023\nShunyu Yao is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022","source_id":"8180bf20b7577f3eee40df5991e2886d","entity_type":"PERSON"},{"name":"HOWARD CHEN","type":"PERSON","description":"Howard Chen is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JOHN YANG","type":"PERSON","description":"John Yang is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KARTHIK R NARASIMHAN","type":"PERSON","description":"Karthik R Narasimhan is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"DIAN YU","type":"PERSON","description":"Dian Yu is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"JEFFREY ZHAO","type":"PERSON","description":"Jeffrey Zhao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"IZHAK SHAFRAN","type":"PERSON","description":"Izhak Shafran is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THOMAS L. GRIFFITHS","type":"PERSON","description":"Thomas L. Griffiths is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUAN CAO","type":"PERSON","description":"Yuan Cao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"KARTHIK NARASIMHAN","type":"PERSON","description":"Karthik Narasimhan is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NAN DU","type":"PERSON","description":"Nan Du is one of the authors of the paper titled \"ReAct: Synergizing reasoning and acting in language models\" published in ICLR 2023","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"WEIRUI YE","type":"PERSON","description":"Weirui Ye is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SHAOHUAI LIU","type":"PERSON","description":"Shaohuai Liu is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"THANARD KURUTACH","type":"PERSON","description":"Thanard Kurutach is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YANG GAO","type":"PERSON","description":"Yang Gao is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NATHANAEL SCHARLI","type":"PERSON","description":"Nathanael Sch\u00e4rli is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"LE HOU","type":"PERSON","description":"Le Hou is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"NATHAN SCALES","type":"PERSON","description":"Nathan Scales is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"OLIVIER BOUSQUET","type":"PERSON","description":"Olivier Bousquet is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"YUCHEN ZHUANG","type":"PERSON","description":"Yuchen Zhuang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"XIANG CHEN","type":"PERSON","description":"Xiang Chen is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"TONG YU","type":"PERSON","description":"Tong Yu is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR","source_id":"8180bf20b7577f3eee40df5991e2886d"},{"name":"SAAYAN MITRA","type":"PERSON","description":"Saayan Mitra is one of the authors of the paper titled \"Tool","source_id":"8180bf20b7577f3eee40df5991e2886d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LOUIS MARTIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Martin is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KEVIN R. STONE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kevin R. Stone is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PETER ALBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Albert is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AMJAD ALMAHAIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amjad Almahairi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YASMINE BABAEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yasmine Babaei is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NIKOLAY BASHLYKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikolay Bashlykov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SOUMYA BATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soumya Batra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PRAJJWAL BHARGAVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prajjwal Bhargava is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHRUTI BHOSALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shruti Bhosale is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DANIEL M. BIKEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel M. Bikel is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LUKAS BLECHER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukas Blecher is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CRISTIAN CANT&#211;N FERRER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cristian Cant&#243;n Ferrer is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MOYA CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Moya Chen is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"GUILLEM CUCURULL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillem Cucurull is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DAVID ESIOBU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Esiobu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JUDE FERNANDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jude Fernandes is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JEREMY FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeremy Fu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"WENYIN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenyin Fu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"BRIAN FULLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brian Fuller is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CYNTHIA GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cynthia Gao is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"VEDANUJ GOSWAMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vedanuj Goswami is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NAMAN GOYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Naman Goyal is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANTHONY S. HARTSHORN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anthony S. Hartshorn is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SAGHAR HOSSEINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saghar Hosseini is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RUI HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Hou is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"HAKAN INAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hakan Inan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MARCIN KARDAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marcin Kardas is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"VIKTOR KERKEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Viktor Kerkez is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MADIAN KHABSA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madian Khabsa is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ISABEL M. KLOUMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Isabel M. Kloumann is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"A. V. KORENEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">A. V. Korenev is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUNIT SINGH KOURA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Punit Singh Koura is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MARIE-ANNE LACHAUX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marie-Anne Lachaux is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THIBAUT LAVRIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thibaut Lavril is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JENYA LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenya Lee is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DIANA LISKOVICH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diana Liskovich is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YINGHAI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yinghai Lu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUNING MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuning Mao is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XAVIER MARTINET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xavier Martinet is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TODOR MIHAYLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Todor Mihaylov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUSHKAR MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pushkar Mishra is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"IGOR MOLYBOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Molybog is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YIXIN NIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Nie is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANDREW POULTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Poulton is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JEREMY REIZENSTEIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeremy Reizenstein is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RASHI RUNGTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rashi Rungta is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KALYAN SALADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyan Saladi is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ALAN SCHELTEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alan Schelten is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RUAN SILVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruan Silva is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ERIC MICHAEL SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Michael Smith is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"R. SUBRAMANIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">R. Subramanian is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XIA TAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xia Tan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"BINH TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Binh Tang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ROSS TAYLOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ross Taylor is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ADINA WILLIAMS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adina Williams is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JIAN XIANG KUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jian Xiang Kuan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PUXIN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Puxin Xu is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ZHENGXU YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhengxu Yan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ILIYAN ZAROV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Iliyan Zarov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUCHEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANGELA FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Angela Fan is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MELANIE KAMBADUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Melanie Kambadur is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AURELIEN RODRIGUEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aurelien Rodriguez is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ROBERT STOJNIC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert Stojnic is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SERGEY EDUNOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Edunov is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is one of the authors of the paper titled \"Llama 2: Open foundation and fine-tuned chat models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LLAMA 2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Llama 2 is a set of open foundation and fine-tuned chat models described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TOM VODOPIVEC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tom Vodopivec is one of the authors of the paper titled \"On Monte Carlo tree search and reinforcement learning\" published in the Journal of Artificial Intelligence Research in 2017<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"BRANKO STER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Branko Ster is one of the authors of the paper titled \"On Monte Carlo tree search and reinforcement learning\" published in the Journal of Artificial Intelligence Research in 2017<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Journal of Artificial Intelligence Research is a publication where the paper \"On Monte Carlo tree search and reinforcement learning\" was published in 2017<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUQI XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuqi Xie is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUNFAN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Jiang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"AJAY MANDLEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ajay Mandlekar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CHAOWEI XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chaowei Xiao is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"VOYAGER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Voyager is an open-ended embodied agent with large language models described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022Xuezhi Wang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022Jason Wei is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022Dale Schuurmans is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc Le is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022Quoc Le is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Chi is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022Ed Chi is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022Denny Zhou is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICLR is a conference where the paper \"Self-consistency improves chain of thought reasoning in language models\" was published in 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is one of the authors of the paper titled \"Chain of thought prompting elicits reasoning in large language models\" published in NeurIPS 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NEURIPS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">NeurIPS is a conference where the paper \"Chain of thought prompting elicits reasoning in large language models\" was published in 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MICHAEL WOOLDRIDGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Wooldridge is one of the authors of the paper titled \"Intelligent agents: Theory and practice\" published in The Knowledge Engineering Review in 1995<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NICHOLAS R JENNINGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas R Jennings is one of the authors of the paper titled \"Intelligent agents: Theory and practice\" published in The Knowledge Engineering Review in 1995<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THE KNOWLEDGE ENGINEERING REVIEW\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Knowledge Engineering Review is a publication where the paper \"Intelligent agents: Theory and practice\" was published in 1995<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PHILIPP WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Wu is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ALEJANDRO ESCONTRELA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alejandro Escontrela is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DANIJAR HAFNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Danijar Hafner is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023Pieter Abbeel is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KEN GOLDBERG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ken Goldberg is one of the authors of the paper titled \"Daydreamer: World models for physical robot learning\" published in CoRL 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CORL\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">CoRL is a conference where the paper \"Daydreamer: World models for physical robot learning\" was published in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUXI XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuxi Xie is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KENJI KAWAGUCHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenji Kawaguchi is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YIRAN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Zhao is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XU ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Zhao is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"MIN-YEN KAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Min-Yen Kan is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JUNXIAN HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junxian He is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"QIZHE XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qizhe Xie is one of the authors of the paper titled \"Decomposition enhances reasoning via self-evaluation guided decoding\" published on arXiv in 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"ZHILIN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhilin Yang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"PENG QI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peng Qi is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SAIZHENG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saizheng Zhang is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YOSHUA BENGIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yoshua Bengio is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"WILLIAM W COHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William W Cohen is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"RUSLAN SALAKHUTDINOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruslan Salakhutdinov is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"CHRISTOPHER D MANNING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher D Manning is one of the authors of the paper titled \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" published in EMNLP 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"EMNLP\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">EMNLP is a conference where the paper \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\" was published in 2018<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHUNYU YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shunyu Yao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023Shunyu Yao is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HOWARD CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Howard Chen is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JOHN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Yang is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KARTHIK R NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik R Narasimhan is one of the authors of the paper titled \"WebShop: Towards scalable real-world web interaction with grounded language agents\" published in NeurIPS 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"DIAN YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dian Yu is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"JEFFREY ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeffrey Zhao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"IZHAK SHAFRAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Izhak Shafran is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THOMAS L. GRIFFITHS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas L. Griffiths is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUAN CAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuan Cao is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"KARTHIK NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik Narasimhan is one of the authors of the paper titled \"Tree of thoughts: deliberate problem solving with large language models\" published in NeurIPS 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NAN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nan Du is one of the authors of the paper titled \"ReAct: Synergizing reasoning and acting in language models\" published in ICLR 2023<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"WEIRUI YE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weirui Ye is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SHAOHUAI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaohuai Liu is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"THANARD KURUTACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thanard Kurutach is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YANG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang Gao is one of the authors of the paper titled \"Mastering Atari games with limited data\" published in NeurIPS 2021<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NATHANAEL SCHARLI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathanael Sch&#228;rli is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"LE HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Le Hou is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"NATHAN SCALES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Scales is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"OLIVIER BOUSQUET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olivier Bousquet is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"YUCHEN ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhuang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"XIANG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Chen is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"TONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tong Yu is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>    <node id=\"SAAYAN MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saayan Mitra is one of the authors of the paper titled \"Tool<\/data>      <data key=\"d2\">8180bf20b7577f3eee40df5991e2886d<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"42de130f5b6144472a86a4c8260a87c7","chunk":" Olivier Bous-\nquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In\nICLR , 2022.\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor\nBursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao\nZhang. ToolChain*: Efficient action space navigation in\nlarge language models with A* search. In ICLR , 2023.\n13Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAppendix of LATS\nThe appendix is organized as follows. First in Sec. A, we\nshow the pseudocode of our proposed algorithm, LATS. In\nSec. B, we provide further discussion of the limitations of\nour method. In Sec. C, we present additional experimental\nresults. In Sec. D, we specify the environment details in our\nexperiments. Finally, we list our prompts used for the three\nenvironments in Sec. E (HotPotQA), Sec. F (Programming),\nand Sec. G (WebShop), respectively.\nA. LATS Pseudocode\nAlg. 1 shows the pseudocode of our algorithm LATS. Nodes\nare stored explicitly in the memory. Unless otherwise speci-\nfied, in all experiments, we set the number of sampled nodes\nton= 5 and the exploration weight to w= 1. We use\na self-consistency weight of \u03bb= 0.5for HotPotQA and\nGame of 24, and \u03bb= 0.8for Programming and WebShop.\nB. More Discussion on Limitations\nAs stated in Sec. 6, LATS has two main limitations:\nComputational cost. Although LATS can improve rea-\nsoning and decision-making, this arrives at a higher com-\nputational cost relative to simpler prompting methods like\nReAct or Reflexion. However, the following facts serve as\nmitigations to this issue:\n\u2022Asymptotically, our method has the same sample com-\nplexity as ToT (Yao et al., 2023a) and RAP (Hao et al.,\n2023), but achieves better performance, expands fewer\nnodes, and uses fewer tokens on average upon success.\nThis suggests that our method is not only stronger\nin problem-solving but also has higher efficiency. A\nfull analysis of the cost can be found in Tab. 9 in Ap-\npendix C.\n\u2022The number of nodes nexpanded at every step provides\na natural trade-off between performance and efficiency.\nIn fact, setting n= 1 makes the method as efficient\nas ReAct (Yao et al., 2023b) with multiple trials or\nCoT-SC (Wang et al., 2022).\nIn general, we recommend using LATS for difficult tasks\nlike programming or for situations where performance is\nprioritized over efficiency in practice. We hope that contin-\nued advancements in LMs will reduce costs and increase\nthe applicability of LATS.\nAdditionally, there exists a minor cost from querying the en-\nvironment, which we find to be trivial for the environments\nwe study. Most LM-based environments involve API-based\ntools, which are inexpensive and fast to use. It is also worthnoting that this is cheaper than the inference cost associ-\nated with using LMs as world models, as in previous search\napproaches (Hao et al., 2023; Liu et al., 2023).\nAssumption of environment reversion in decision-\nmaking. Since our method is based on Monte Carlo\nTree Search and is model-free, one limitation of LATS on\ndecision-making tasks is that it requires the agent to be\nable to revert to earlier states in the environments. How-\never, this reversion property is feasible in many real-world\nenvironments and applications (despite being not univer-\nsally applicable in all possible environments), including\nprogramming (HumanEval (Chen et al., 2021)), web search\n(WebShop (Yao et al., 2022)), text-based manipulation tasks\n(Alfworld (Shridhar et al., 2020)), and LMs with tool use\n(ToolBench (Qin et al., 2024)). Therefore, we believe that\nleveraging the reversion property is not a shortcoming but\nrather a feature that has not been explicitly given notice\nby the LM decision-making community \u2013 it opens up new\nopportunities in the emerging LM agent community.\nAdditionally, the benchmarks we use in this paper are rel-\natively simple and focused on decision-making compared\nto the complexity of real-world interactive environments.\nMoreover, some environments might not easily support roll-\nbacks to previous states. However, the design of LATS is\nflexible and can be adjusted to various resource constraints.\nUsing planning-based prompting methods like LATS in\nenvironments like Minecraft (Fan et al., 2022) and more rea-\nsoning benchmarks would be interesting avenues for future\nwork.\nC. Additional Ablations\nIn this section, we ablate various designs of LATS. Ex-\nperiments are conducted on HotPotQA with a maximum\nofk= 50 trajectories and sampling size of n= 5 and\nHumanEval with a maximum of k= 8trajectories and sam-\npling size of n= 5. The result for HotPotQA is shown in\nTab. 8 and HumanEval in Fig. 3.\nExploration weight. We find that there is lower perfor-\nmance on HotPotQA when the exploration weight win the\nselection formula is decreased to 0.5, suggesting that this\nreduces the effectiveness of the search. Increasing w","chunk_id":"42de130f5b6144472a86a4c8260a87c7","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"OLIVIER BOUSQUET","type":"PERSON","description":"Olivier Bousquet is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"QUOC LE","type":"PERSON","description":"Quoc Le is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"ED CHI","type":"PERSON","description":"Ed Chi is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"ICLR","type":"EVENT","description":"ICLR is a conference where the paper \"Least-to-most prompting enables complex reasoning in large language models\" was published in 2022","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"YUCHEN ZHUANG","type":"PERSON","description":"Yuchen Zhuang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"XIANG CHEN","type":"PERSON","description":"Xiang Chen is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"TONG YU","type":"PERSON","description":"Tong Yu is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"SAAYAN MITRA","type":"PERSON","description":"Saayan Mitra is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"VICTOR BURSZTYN","type":"PERSON","description":"Victor Bursztyn is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"RYAN A. ROSSI","type":"PERSON","description":"Ryan A. Rossi is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"SOMDEB SARKHEL","type":"PERSON","description":"Somdeb Sarkhel is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"CHAO ZHANG","type":"PERSON","description":"Chao Zhang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is an algorithm that unifies reasoning, acting, and planning in language models","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for evaluating the performance of the LATS algorithm","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"PROGRAMMING","type":"TASK","description":"Programming is one of the tasks used for evaluating the performance of the LATS algorithm","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"WEBSHOP","type":"TASK","description":"WebShop is one of the tasks used for evaluating the performance of the LATS algorithm","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a simpler prompting method compared to LATS, used for reasoning and decision-making tasks","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"TECHNOLOGY"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a simpler prompting method compared to LATS, used for reasoning and decision-making tasks","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"TECHNOLOGY"},{"name":"TOT","type":"TECHNOLOGY","description":"ToT (Tree of Thoughts) is a method that LATS is compared to in terms of sample complexity and performance","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"TECHNOLOGY"},{"name":"RAP","type":"TECHNOLOGY","description":"RAP is a method that LATS is compared to in terms of sample complexity and performance","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"TECHNOLOGY"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset used for evaluating the performance of the LATS algorithm in programming tasks","source_id":"42de130f5b6144472a86a4c8260a87c7"},{"name":"COT-SC","type":"TECHNOLOGY","description":"CoT-SC (Chain of Thought with Self-Consistency) is a method that LATS is compared to in terms of efficiency","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"TECHNOLOGY"},{"name":"MINECRAFT","type":"ENVIRONMENT","description":"Minecraft is an environment suggested for future work using planning-based prompting methods like LATS","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"ENVIRONMENT"},{"name":"FAN","type":"PERSON","description":"Fan is an author who has worked on using planning-based prompting methods in environments like Minecraft","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"YA0","type":"PERSON","description":"Yao is an author who has worked on methods like ToT and WebShop","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"HAO","type":"PERSON","description":"Hao is an author who has worked on methods like RAP and previous search approaches","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has worked on previous search approaches using LMs as world models","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who has worked on the HumanEval dataset for programming tasks","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"SHRIDHAR","type":"PERSON","description":"Shridhar is an author who has worked on text-based manipulation tasks like Alfworld","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"QIN","type":"PERSON","description":"Qin is an author who has worked on LM-based environments like ToolBench","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"PERSON"},{"name":"ALFWORLD","type":"ENVIRONMENT","description":"Alfworld is an environment used for text-based manipulation tasks","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"ENVIRONMENT"},{"name":"TOOLBENCH","type":"ENVIRONMENT","description":"ToolBench is an LM-based environment involving API-based tools","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"ENVIRONMENT"},{"name":"LANGUAGE AGENT TREE SEARCH","type":"DOCUMENT","description":"The document \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" includes the appendix and details of the LATS algorithm\nLanguage Agent Tree Search (LATS) is a method that unifies reasoning, acting, and planning in language models","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT"},{"name":"APPENDIX","type":"DOCUMENT SECTION","description":"The appendix of the document \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" includes pseudocode, discussions, experimental results, and environment details","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. A","type":"DOCUMENT SECTION","description":"Section A of the appendix shows the pseudocode of the LATS algorithm","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. B","type":"DOCUMENT SECTION","description":"Section B of the appendix provides further discussion of the limitations of the LATS method","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. C","type":"DOCUMENT SECTION","description":"Section C of the appendix presents additional experimental results of the LATS method","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. D","type":"DOCUMENT SECTION","description":"Section D of the appendix specifies the environment details in the experiments","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. E","type":"DOCUMENT SECTION","description":"Section E of the appendix lists the prompts used for the HotPotQA environment","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. F","type":"DOCUMENT SECTION","description":"Section F of the appendix lists the prompts used for the Programming environment","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEC. G","type":"DOCUMENT SECTION","description":"Section G of the appendix lists the prompts used for the WebShop environment","source_id":"42de130f5b6144472a86a4c8260a87c7","entity_type":"DOCUMENT SECTION"},{"name":"SEARCH APPROACHES","type":"","description":"","source_id":"42de130f5b6144472a86a4c8260a87c7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"OLIVIER BOUSQUET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olivier Bousquet is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"QUOC LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc Le is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"ED CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed Chi is one of the authors of the paper titled \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"ICLR\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ICLR is a conference where the paper \"Least-to-most prompting enables complex reasoning in large language models\" was published in 2022<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"YUCHEN ZHUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuchen Zhuang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"XIANG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Chen is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"TONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tong Yu is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"SAAYAN MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saayan Mitra is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"VICTOR BURSZTYN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Bursztyn is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"RYAN A. ROSSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan A. Rossi is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"SOMDEB SARKHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Somdeb Sarkhel is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"CHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chao Zhang is one of the authors of the paper titled \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is an algorithm that unifies reasoning, acting, and planning in language models<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for evaluating the performance of the LATS algorithm<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Programming is one of the tasks used for evaluating the performance of the LATS algorithm<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">WebShop is one of the tasks used for evaluating the performance of the LATS algorithm<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a simpler prompting method compared to LATS, used for reasoning and decision-making tasks<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a simpler prompting method compared to LATS, used for reasoning and decision-making tasks<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TOT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ToT (Tree of Thoughts) is a method that LATS is compared to in terms of sample complexity and performance<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RAP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAP is a method that LATS is compared to in terms of sample complexity and performance<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset used for evaluating the performance of the LATS algorithm in programming tasks<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">CoT-SC (Chain of Thought with Self-Consistency) is a method that LATS is compared to in terms of efficiency<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MINECRAFT\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">Minecraft is an environment suggested for future work using planning-based prompting methods like LATS<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">ENVIRONMENT<\/data>    <\/node>    <node id=\"FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan is an author who has worked on using planning-based prompting methods in environments like Minecraft<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YA0\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author who has worked on methods like ToT and WebShop<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao is an author who has worked on methods like RAP and previous search approaches<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has worked on previous search approaches using LMs as world models<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who has worked on the HumanEval dataset for programming tasks<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHRIDHAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shridhar is an author who has worked on text-based manipulation tasks like Alfworld<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qin is an author who has worked on LM-based environments like ToolBench<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALFWORLD\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">Alfworld is an environment used for text-based manipulation tasks<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">ENVIRONMENT<\/data>    <\/node>    <node id=\"TOOLBENCH\">      <data key=\"d0\">ENVIRONMENT<\/data>      <data key=\"d1\">ToolBench is an LM-based environment involving API-based tools<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">ENVIRONMENT<\/data>    <\/node>    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The document \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" includes the appendix and details of the LATS algorithmLanguage Agent Tree Search (LATS) is a method that unifies reasoning, acting, and planning in language models<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"APPENDIX\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">The appendix of the document \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" includes pseudocode, discussions, experimental results, and environment details<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. A\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section A of the appendix shows the pseudocode of the LATS algorithm<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. B\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section B of the appendix provides further discussion of the limitations of the LATS method<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. C\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section C of the appendix presents additional experimental results of the LATS method<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. D\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section D of the appendix specifies the environment details in the experiments<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. E\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section E of the appendix lists the prompts used for the HotPotQA environment<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. F\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section F of the appendix lists the prompts used for the Programming environment<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEC. G\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Section G of the appendix lists the prompts used for the WebShop environment<\/data>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>      <data key=\"d3\">DOCUMENT SECTION<\/data>    <\/node>    <node id=\"SEARCH APPROACHES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/node>    <edge source=\"OLIVIER BOUSQUET\" target=\"QUOC LE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Olivier Bousquet and Quoc Le co-authored the paper \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"OLIVIER BOUSQUET\" target=\"ED CHI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Olivier Bousquet and Ed Chi co-authored the paper \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"QUOC LE\" target=\"ED CHI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Quoc Le and Ed Chi co-authored the paper \"Least-to-most prompting enables complex reasoning in large language models\" published in ICLR 2022<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"XIANG CHEN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Xiang Chen co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"TONG YU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Tong Yu co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"SAAYAN MITRA\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Saayan Mitra co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"VICTOR BURSZTYN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Victor Bursztyn co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"RYAN A. ROSSI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Ryan A. Rossi co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"YUCHEN ZHUANG\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuchen Zhuang and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"TONG YU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Tong Yu co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"SAAYAN MITRA\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Saayan Mitra co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"VICTOR BURSZTYN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Victor Bursztyn co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"RYAN A. ROSSI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Ryan A. Rossi co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"XIANG CHEN\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Xiang Chen and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"SAAYAN MITRA\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Tong Yu and Saayan Mitra co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"VICTOR BURSZTYN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Tong Yu and Victor Bursztyn co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"RYAN A. ROSSI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Tong Yu and Ryan A. Rossi co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Tong Yu and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TONG YU\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Tong Yu and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SAAYAN MITRA\" target=\"VICTOR BURSZTYN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Saayan Mitra and Victor Bursztyn co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SAAYAN MITRA\" target=\"RYAN A. ROSSI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Saayan Mitra and Ryan A. Rossi co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SAAYAN MITRA\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Saayan Mitra and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SAAYAN MITRA\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Saayan Mitra and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"VICTOR BURSZTYN\" target=\"RYAN A. ROSSI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Victor Bursztyn and Ryan A. Rossi co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"VICTOR BURSZTYN\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Victor Bursztyn and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"VICTOR BURSZTYN\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Victor Bursztyn and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"RYAN A. ROSSI\" target=\"SOMDEB SARKHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Ryan A. Rossi and Somdeb Sarkhel co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"RYAN A. ROSSI\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Ryan A. Rossi and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SOMDEB SARKHEL\" target=\"CHAO ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Somdeb Sarkhel and Chao Zhang co-authored the paper \"ToolChain*: Efficient action space navigation in large language models with A* search\" published in ICLR 2023<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS algorithm is evaluated using the HotPotQA dataset<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROGRAMMING\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS algorithm is evaluated using programming tasks<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WEBSHOP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS algorithm is evaluated using the WebShop tasks<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LATS is compared to ReAct in terms of computational cost and efficiency<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LATS is compared to Reflexion in terms of computational cost and efficiency<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TOT\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LATS is compared to ToT in terms of sample complexity and performance<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"RAP\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LATS is compared to RAP in terms of sample complexity and performance<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HUMANEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS algorithm is evaluated using the HumanEval dataset<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"COT-SC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LATS is compared to CoT-SC in terms of efficiency<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LATS\" target=\"MINECRAFT\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">LATS is suggested for future work in the Minecraft environment<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"YA0\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yao has worked on methods like WebShop<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"TOT\" target=\"YA0\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yao has worked on methods like ToT<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"RAP\" target=\"HAO\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Hao has worked on methods like RAP<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"HUMANEVAL\" target=\"CHEN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Chen has worked on the HumanEval dataset for programming tasks<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"MINECRAFT\" target=\"FAN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Fan has worked on using planning-based prompting methods in environments like Minecraft<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"HAO\" target=\"SEARCH APPROACHES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Hao has worked on previous search approaches<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"LIU\" target=\"SEARCH APPROACHES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Liu has worked on previous search approaches using LMs as world models<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"SHRIDHAR\" target=\"ALFWORLD\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Shridhar has worked on text-based manipulation tasks like Alfworld<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>    <edge source=\"QIN\" target=\"TOOLBENCH\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Qin has worked on LM-based environments like ToolBench<\/data>      <data key=\"d6\">42de130f5b6144472a86a4c8260a87c7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"48e423e2baf2ed485872756f5b4d87d8","chunk":" 5 and\nHumanEval with a maximum of k= 8trajectories and sam-\npling size of n= 5. The result for HotPotQA is shown in\nTab. 8 and HumanEval in Fig. 3.\nExploration weight. We find that there is lower perfor-\nmance on HotPotQA when the exploration weight win the\nselection formula is decreased to 0.5, suggesting that this\nreduces the effectiveness of the search. Increasing wto2.0\ndoes not lead to a performance improvement, but we tend\nto observe faster convergence. The optimal setting depends\non the particular environment and complexity of the state\nspace.\nDepth. In our main experiments we use a maximum depth\nofd= 7on HotPotQA for all methods, following previous\nwork (Yao et al., 2023b). We ablate the effect on LATS after\nreducing it to d= 4. This results in only a slight drop in\nperformance. We find that most questions can be answered\nwithin four steps, and using a greater number of steps tends\n14Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAlgorithm 1 LATS( s, p\u03b8, pV, pref, d, k, n, w, a, b )\nRequire: Initial state s, action generator p\u03b8, value function pV, reflection generator pref, number of generated actions n,\ndepth limit L, number of roll-outs K, context c, exploration weight w, and value function weight \u03bb\nInitialize action space A, observation space O\nInitialize the state-action value function pV:S\u00d7A7\u2192Rand visit counter N:S7\u2192Nto one\nfork\u21900, . . . , K \u22121do\nfort\u21900, . . . , L \u22121do\nifstnot terminal then \u25b7Expansion & Simulation\nfori\u21901, . . . , n do\nSample a(i)\nt\u223cp\u03b8(st)\nGeto(i)\ntfrom environment, s(i)\nt+1\u2190(c(i)\nt, o(i)\nt, a(i)\nt),c(i)\nt+1\u2190(o(i)\nt, a(i)\nt)\nEvaluate V(i)\nt\u223c\u03bb\u2217pV(s(i)\nt) + (1\u2212\u03bb)\u2217SC(s(i)\nt) \u25b7Evaluation\nV(st)\u2190V(i)\nt\nAdds(i)\ntto children\nend for\nend if\nifstis terminal then \u25b7Reflection\nGetrfrom environment\nifrnot success then\nreflection \u2190pref(ct)\nc\u2190reflection\nend if\nend if\nat\u2190arg max a\u2208e(st)h\nV(st) +wq\nlnN(st)\nN(st+1)i\n\u25b7Selection\nGet corresponding otfrom memory, st+1\u2190(ct, ot, at), ct+1\u2190(ot, at)\nN(st+1)\u2190N(st+1) + 1\nifatis an output action then break\nend for\nT\u2190the actual number of steps\nfort\u2190T\u22121, . . . , 0do \u25b7Backpropagation\nV(st)\u2190V(st)(N(st)\u22121)+r\nN(st)\nend for\nend for\nto force the agent into local minima and rarely improves\nsuccess.\nLM value function. The LM value function scores states\nbased on expected future reward. Without this heuristic,\nthe only signal to guide search would be from environment\nrewards for completed trajectories, which are scarce and\noften binary. When we remove the evaluation operation, we\nobserve a dramatic 0.26drop in performance.\nPerformance over time. To see the effects of increasing\nthe number of trajectories sampled, we change kto different\nvalues. We conduct this experiment on HumanEval, which\nhas a more noticeable difference due to sampling less tra-\njectories. The results are shown in Fig. 3, in which LATS\nscales better with more iterations than Reflexion.\nD. Environment Details\nD.1. HotPotQA\nHotPotQA (Yang et al., 2018) is a question-answering\ndataset that requires reasoning over multiple supporting\ndocuments to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers to\nbe diverse, multi-hop, and explainable. Questions cover a\nrange of types like entities, locations, dates, and comparison\nof shared properties between two entities. Crowdworkers\nalso provide supporting facts from the documents that justify\nthe answer. We use the HotPotQA benchmark setting with\nall the Wikipedia paragraphs to test retrieval. We use a ran-\ndomly selected subset of 100 questions for our experiments\nand a maximum depth limit of 6. Fig. 4 illustrates how\nReAct and LATS work on an example task of HotPotQA,\nand gives a qualitative example on how LATS outperforms\nReAct on the task. For value function hyperparameters, we\nuse\u03bb= 0.5for the LM score and self-consistency score.\nAction Space. We adopt the Wikipedia web API proposed\nin Yao et al. (2023b), with three types of actions to support\ninteractive information retrieval:\n(1)search [entity ], which returns the first 5 sentences\nfrom the corresponding entity wiki page if it exists,\nor else suggests top-5 similar entities from the Wikipedia\nsearch engine,\n(2)lookup [string ], which returns the next sentence in\n15Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nLATS ( w= 0.5) 0.55","chunk_id":"48e423e2baf2ed485872756f5b4d87d8","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers to be diverse, multi-hop, and explainable","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"DATASET"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a method that unifies reasoning, acting, and planning in language models. It uses various parameters like exploration weight, depth, and value function to optimize performance","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"YAO ET AL.","type":"PERSON","description":"Yao et al. are authors who have conducted previous work related to the experiments mentioned in the text","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PERSON"},{"name":"WIKIPEDIA","type":"PLATFORM","description":"Wikipedia is a platform used as a source of information for the HotPotQA dataset, providing paragraphs for retrieval and supporting facts for questions","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PLATFORM"},{"name":"REACT","type":"TECHNOLOGY","description":"ReAct is a method used in the experiments to compare performance with LATS on the HotPotQA task","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"LM VALUE FUNCTION","type":"TECHNOLOGY","description":"The LM value function scores states based on expected future reward, guiding the search process in LATS","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"REFLEXION","type":"TECHNOLOGY","description":"Reflexion is a method used in the experiments to compare performance with LATS on the HumanEval task","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"CROWDWORKERS","type":"PERSON","description":"Crowdworkers are individuals who crafted the question-answer pairs in the HotPotQA dataset and provided supporting facts from documents","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PERSON"},{"name":"ALGORITHM 1","type":"TECHNOLOGY","description":"Algorithm 1 is a specific implementation of LATS, detailing the steps and parameters required for its operation","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"FIG. 3","type":"DOCUMENT","description":"Fig. 3 is a figure in the document that shows the results of the HumanEval experiments and the performance of LATS over time","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"DOCUMENT"},{"name":"FIG. 4","type":"DOCUMENT","description":"Fig. 4 is a figure in the document that illustrates how ReAct and LATS work on an example task of HotPotQA, providing a qualitative comparison","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"DOCUMENT"},{"name":"YANG ET AL.","type":"PERSON","description":"Yang et al. are authors who created the HotPotQA dataset in 2018","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PERSON"},{"name":"\u039b","type":"PARAMETER","description":"\u03bb is a hyperparameter used in the value function for LATS, set to 0.5 for the LM score and self-consistency score","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"EXPLORATION WEIGHT","type":"PARAMETER","description":"Exploration weight (w) is a parameter in LATS that affects the effectiveness of the search, with different values tested in the experiments","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"DEPTH","type":"PARAMETER","description":"Depth (d) is a parameter in LATS that limits the number of steps in the search process, with different values tested in the experiments","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"SAMPLING SIZE","type":"PARAMETER","description":"Sampling size (n) is a parameter in LATS that determines the number of actions generated, with different values tested in the experiments","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"TRAJECTORIES","type":"PARAMETER","description":"Trajectories (k) is a parameter in LATS that determines the number of trajectories sampled, affecting performance over time","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"VALUE FUNCTION WEIGHT","type":"PARAMETER","description":"Value function weight (\u03bb) is a parameter in LATS that balances the LM score and self-consistency score","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"STATE SPACE","type":"CONCEPT","description":"State space is the environment complexity that affects the optimal settings for LATS parameters","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"SEARCH [ENTITY]","type":"ACTION","description":"Search [entity] is an action in LATS that returns the first 5 sentences from the corresponding entity's Wikipedia page or suggests top-5 similar entities","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"LOOKUP [STRING]","type":"ACTION","description":"Lookup [string] is an action in LATS that returns the next sentence in the Wikipedia page for the given string","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"REFLECTION","type":"ACTION","description":"Reflection is an action in LATS that involves generating a reflection from the context if the state is not successful","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"EVALUATION","type":"ACTION","description":"Evaluation is an action in LATS that scores states based on the value function","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"BACKPROPAGATION","type":"ACTION","description":"Backpropagation is an action in LATS that updates the value function based on the rewards received","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"EXPANSION & SIMULATION","type":"ACTION","description":"Expansion & Simulation is an action in LATS that involves generating actions and evaluating them","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"SELECTION","type":"ACTION","description":"Selection is an action in LATS that chooses the best action based on the value function and exploration weight","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"ACTION"},{"name":"TERMINAL STATE","type":"CONCEPT","description":"Terminal state is a state in LATS where no further actions can be taken","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"INITIAL STATE","type":"CONCEPT","description":"Initial state is the starting point for the LATS algorithm","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"SUCCESS","type":"CONCEPT","description":"Success is the desired outcome in LATS, often determined by environment rewards","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"ENVIRONMENT REWARDS","type":"CONCEPT","description":"Environment rewards are signals from the environment that guide the search process in LATS","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"SUPPORTING FACTS","type":"CONCEPT","description":"Supporting facts are pieces of information provided by crowdworkers in the HotPotQA dataset to justify the answers","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"QUESTION-ANSWER PAIRS","type":"CONCEPT","description":"Question-answer pairs are the main components of the HotPotQA dataset, requiring reasoning over multiple documents","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"CONCEPT"},{"name":"WIKIPEDIA WEB API","type":"TECHNOLOGY","description":"Wikipedia web API is a tool used in LATS to support interactive information retrieval from Wikipedia","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"PROMPT METHOD","type":"TECHNOLOGY","description":"Prompt Method is a technique used in the experiments to evaluate performance on HotPotQA","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"TECHNOLOGY"},{"name":"EM","type":"METRIC","description":"EM (Exact Match) is a metric used to evaluate the performance of models on the HotPotQA dataset","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"METRIC"},{"name":"TAB. 8","type":"DOCUMENT","description":"Tab. 8 is a table in the document that shows the results of the HotPotQA experiments","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"DOCUMENT"},{"name":"N","type":"PARAMETER","description":"N is a parameter in LATS that represents the visit counter for states","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"A","type":"PARAMETER","description":"A is a parameter in LATS that represents the action space","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"O","type":"PARAMETER","description":"O is a parameter in LATS that represents the observation space","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"S","type":"PARAMETER","description":"S is a parameter in LATS that represents the state space","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"P\u0398","type":"PARAMETER","description":"P\u03b8 is a parameter in LATS that represents the action generator","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"PV","type":"PARAMETER","description":"PV is a parameter in LATS that represents the value function","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"PREF","type":"PARAMETER","description":"PREF is a parameter in LATS that represents the reflection generator","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"K","type":"PARAMETER","description":"K is a parameter in LATS that represents the number of roll-outs","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"L","type":"PARAMETER","description":"L is a parameter in LATS that represents the depth limit","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"C","type":"PARAMETER","description":"C is a parameter in LATS that represents the context","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"R","type":"PARAMETER","description":"R is a parameter in LATS that represents the reward","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"T","type":"PARAMETER","description":"T is a parameter in LATS that represents the actual number of steps","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"ST","type":"PARAMETER","description":"ST is a parameter in LATS that represents the state at time t","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"AT","type":"PARAMETER","description":"AT is a parameter in LATS that represents the action at time t","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"OT","type":"PARAMETER","description":"OT is a parameter in LATS that represents the observation at time t","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"CT","type":"PARAMETER","description":"CT is a parameter in LATS that represents the context at time t","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"},{"name":"SC","type":"PARAMETER","description":"SC is a parameter in LATS that represents the self-consistency score","source_id":"48e423e2baf2ed485872756f5b4d87d8","entity_type":"PARAMETER"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers to be diverse, multi-hop, and explainable<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a method that unifies reasoning, acting, and planning in language models. It uses various parameters like exploration weight, depth, and value function to optimize performance<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YAO ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. are authors who have conducted previous work related to the experiments mentioned in the text<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WIKIPEDIA\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">Wikipedia is a platform used as a source of information for the HotPotQA dataset, providing paragraphs for retrieval and supporting facts for questions<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"REACT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ReAct is a method used in the experiments to compare performance with LATS on the HotPotQA task<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LM VALUE FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The LM value function scores states based on expected future reward, guiding the search process in LATS<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REFLEXION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflexion is a method used in the experiments to compare performance with LATS on the HumanEval task<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CROWDWORKERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Crowdworkers are individuals who crafted the question-answer pairs in the HotPotQA dataset and provided supporting facts from documents<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALGORITHM 1\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Algorithm 1 is a specific implementation of LATS, detailing the steps and parameters required for its operation<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FIG. 3\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Fig. 3 is a figure in the document that shows the results of the HumanEval experiments and the performance of LATS over time<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"FIG. 4\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Fig. 4 is a figure in the document that illustrates how ReAct and LATS work on an example task of HotPotQA, providing a qualitative comparison<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"YANG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang et al. are authors who created the HotPotQA dataset in 2018<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"&#923;\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">&#955; is a hyperparameter used in the value function for LATS, set to 0.5 for the LM score and self-consistency score<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"EXPLORATION WEIGHT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Exploration weight (w) is a parameter in LATS that affects the effectiveness of the search, with different values tested in the experiments<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"DEPTH\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Depth (d) is a parameter in LATS that limits the number of steps in the search process, with different values tested in the experiments<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"SAMPLING SIZE\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Sampling size (n) is a parameter in LATS that determines the number of actions generated, with different values tested in the experiments<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Trajectories (k) is a parameter in LATS that determines the number of trajectories sampled, affecting performance over time<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"VALUE FUNCTION WEIGHT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Value function weight (&#955;) is a parameter in LATS that balances the LM score and self-consistency score<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"STATE SPACE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">State space is the environment complexity that affects the optimal settings for LATS parameters<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SEARCH [ENTITY]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Search [entity] is an action in LATS that returns the first 5 sentences from the corresponding entity's Wikipedia page or suggests top-5 similar entities<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"LOOKUP [STRING]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Lookup [string] is an action in LATS that returns the next sentence in the Wikipedia page for the given string<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Reflection is an action in LATS that involves generating a reflection from the context if the state is not successful<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Evaluation is an action in LATS that scores states based on the value function<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"BACKPROPAGATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Backpropagation is an action in LATS that updates the value function based on the rewards received<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"EXPANSION &amp; SIMULATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Expansion &amp; Simulation is an action in LATS that involves generating actions and evaluating them<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"SELECTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Selection is an action in LATS that chooses the best action based on the value function and exploration weight<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"TERMINAL STATE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Terminal state is a state in LATS where no further actions can be taken<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INITIAL STATE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Initial state is the starting point for the LATS algorithm<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SUCCESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Success is the desired outcome in LATS, often determined by environment rewards<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ENVIRONMENT REWARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Environment rewards are signals from the environment that guide the search process in LATS<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SUPPORTING FACTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Supporting facts are pieces of information provided by crowdworkers in the HotPotQA dataset to justify the answers<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"QUESTION-ANSWER PAIRS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Question-answer pairs are the main components of the HotPotQA dataset, requiring reasoning over multiple documents<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"WIKIPEDIA WEB API\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Wikipedia web API is a tool used in LATS to support interactive information retrieval from Wikipedia<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"PROMPT METHOD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompt Method is a technique used in the experiments to evaluate performance on HotPotQA<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"EM\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">EM (Exact Match) is a metric used to evaluate the performance of models on the HotPotQA dataset<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"TAB. 8\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Tab. 8 is a table in the document that shows the results of the HotPotQA experiments<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"N\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">N is a parameter in LATS that represents the visit counter for states<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"A\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">A is a parameter in LATS that represents the action space<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"O\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">O is a parameter in LATS that represents the observation space<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"S\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">S is a parameter in LATS that represents the state space<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"P&#920;\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">P&#952; is a parameter in LATS that represents the action generator<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"PV\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">PV is a parameter in LATS that represents the value function<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"PREF\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">PREF is a parameter in LATS that represents the reflection generator<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"K\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">K is a parameter in LATS that represents the number of roll-outs<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"L\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">L is a parameter in LATS that represents the depth limit<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"C\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">C is a parameter in LATS that represents the context<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"R\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">R is a parameter in LATS that represents the reward<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"T\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">T is a parameter in LATS that represents the actual number of steps<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"ST\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">ST is a parameter in LATS that represents the state at time t<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"AT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">AT is a parameter in LATS that represents the action at time t<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"OT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">OT is a parameter in LATS that represents the observation at time t<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"CT\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">CT is a parameter in LATS that represents the context at time t<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <node id=\"SC\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">SC is a parameter in LATS that represents the self-consistency score<\/data>      <data key=\"d2\">48e423e2baf2ed485872756f5b4d87d8<\/data>      <data key=\"d3\">PARAMETER<\/data>    <\/node>    <edge source=\"HOTPOTQA\" target=\"LATS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">HotPotQA is used to evaluate the performance of LATS<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"CROWDWORKERS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Crowdworkers crafted the question-answer pairs and provided supporting facts for HotPotQA<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"WIKIPEDIA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">HotPotQA uses Wikipedia as a source of information<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"SUPPORTING FACTS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">HotPotQA includes supporting facts provided by crowdworkers to justify answers<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"QUESTION-ANSWER PAIRS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">HotPotQA consists of question-answer pairs that require reasoning over multiple documents<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"EM\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">HotPotQA performance is evaluated using the EM (Exact Match) metric<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"TAB. 8\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Tab. 8 shows the results of the HotPotQA experiments<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REACT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS is compared with ReAct in the experiments on HotPotQA<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLEXION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS is compared with Reflexion in the experiments on HumanEval<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LM VALUE FUNCTION\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses the LM value function to score states based on expected future reward<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXPLORATION WEIGHT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses exploration weight as a parameter to affect the effectiveness of the search<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"DEPTH\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses depth as a parameter to limit the number of steps in the search process<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SAMPLING SIZE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses sampling size as a parameter to determine the number of actions generated<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TRAJECTORIES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses trajectories as a parameter to determine the number of trajectories sampled<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"VALUE FUNCTION WEIGHT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">LATS uses value function weight to balance the LM score and self-consistency score<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"STATE SPACE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS performance depends on the complexity of the state space<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SEARCH [ENTITY]\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the search [entity] action to retrieve information from Wikipedia<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"LOOKUP [STRING]\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the lookup [string] action to retrieve information from Wikipedia<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"REFLECTION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the reflection action to generate reflections from the context<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EVALUATION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the evaluation action to score states based on the value function<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"BACKPROPAGATION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the backpropagation action to update the value function based on rewards<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"EXPANSION &amp; SIMULATION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the expansion &amp; simulation action to generate and evaluate actions<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SELECTION\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the selection action to choose the best action based on the value function and exploration weight<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"TERMINAL STATE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS identifies terminal states where no further actions can be taken<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"INITIAL STATE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS starts from an initial state<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SUCCESS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS aims for success, often determined by environment rewards<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ENVIRONMENT REWARDS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses environment rewards to guide the search process<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"WIKIPEDIA WEB API\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the Wikipedia web API to support interactive information retrieval<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROMPT METHOD\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses the prompt method to evaluate performance on HotPotQA<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"N\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses N as a parameter to represent the visit counter for states<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"A\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses A as a parameter to represent the action space<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"O\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses O as a parameter to represent the observation space<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"S\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses S as a parameter to represent the state space<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"P&#920;\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses P&#952; as a parameter to represent the action generator<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PV\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses PV as a parameter to represent the value function<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PREF\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses PREF as a parameter to represent the reflection generator<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"K\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses K as a parameter to represent the number of roll-outs<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"L\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses L as a parameter to represent the depth limit<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"C\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses C as a parameter to represent the context<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"R\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses R as a parameter to represent the reward<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"T\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses T as a parameter to represent the actual number of steps<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"ST\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses ST as a parameter to represent the state at time t<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"AT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses AT as a parameter to represent the action at time t<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"OT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses OT as a parameter to represent the observation at time t<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"CT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LATS uses CT as a parameter to represent the context at time t<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>    <edge source=\"LATS\" target=\"SC\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">LATS uses SC as a parameter to represent the self-consistency score<\/data>      <data key=\"d6\">48e423e2baf2ed485872756f5b4d87d8<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"fb2b4544aedd793e4d4ec3147320a51c","chunk":"interactive information retrieval:\n(1)search [entity ], which returns the first 5 sentences\nfrom the corresponding entity wiki page if it exists,\nor else suggests top-5 similar entities from the Wikipedia\nsearch engine,\n(2)lookup [string ], which returns the next sentence in\n15Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nPrompt Method HotpotQA (EM) \u2191\nLATS ( w= 0.5) 0.55\nLATS ( w= 2.0) 0.63\nLATS ( d= 4) 0.58\nLATS (CoT) 0.62\nLATS (No LM Heuristic) 0.37\nLATS ( w= 1.0,d= 7) 0.63\nTable 11. Ablation results on LATS and baseline variants in Hot-\nPotQA measured by Exact Match (EM). We test different depth d,\nexploration factor w, and versions of LATS using CoT and without\nthe LM value function. We sample n= 5andk= 50 trajectories.\nFigure 3. Performance over successive iterations on HumanEval\nwith GPT-3.5.\nthe page containing string ,\n(3)finish [answer ], which finishes the current task with\nanswer .\nThese API calls and free-form thoughts form the action\nspace for this environment.\nD.2. Programming\nThe HumanEval dataset (Chen et al., 2021) is a collection\nof 164 handwritten programming problems introduced to\nevaluate the functional correctness of models for synthe-\nsizing programs from natural language descriptions. Each\nproblem includes a function signature, docstring descrip-\ntion, reference implementation, and multiple unit tests, with\nan average of 7.7 tests per problem. The programming\ntasks assess comprehension of natural language, reasoning,\nalgorithms, and basic mathematics, at a difficulty level com-\nparable to simple software interview questions. Pass rates\nare evaluated with the pass@k metric, where k samples are\ngenerated per problem and a problem is considered solvedif any sample passes all tests. We use all 164 problems for\nour experiments and a maximum depth limit of 8. For the\nthree questions without sample test cases, we write our own.\nFor value function hyperparameters, we use \u03bb= 0.8for the\nLM score and self-consistency score. For GPT-3.5 we use\nsix internal tests, while for GPT-4 we use four internal tests.\nThe Mostly Basic Programming Problems (MBPP) (Austin\net al., 2022) benchmark contains 974 short Python functions\ndesigned to evaluate program synthesis techniques. The\ndataset was constructed by crowdsourcing from workers\nwith basic Python knowledge. Each data point consists of\na natural language description of a programming task, a\nreference solution implementation, and three test cases for\nfunctional correctness. The natural language prompts are\ntypically short, one-sentence descriptions. Solutions cover\ncommon programming constructs including mathematical\noperations, list processing, string manipulation, and usage\nof the Python standard library. On average, solutions are 6.8\nlines of code. The dataset is also supplemented with an ad-\nditional set of 426 problems that were manually verified for\nunambiguous specifications, standard function signatures,\nand accurate test cases. We use a randomly selected subset\nof 397 problems for our experiments. For value function\nhyperparameters, we use \u03bb= 0.8for the LM score and\nself-consistency score.\nD.3. WebShop\nWebShop (Yao et al., 2022) is an interactive web-based\nenvironment designed to evaluate agents on grounded\nlanguage understanding and decision-making. It simulates\nan e-commerce shopping task by providing agents with\nover 1 million real-world products scraped from Amazon,\nspanning 5 categories and 113 subcategories. These\nproducts contain rich linguistic information, with an\naverage text length of 262 words and a vocabulary size\nof 224k. In addition, there are over 800k unique product\noptions available for customization. The environment\nrenders webpages in two modes: HTML mode provides\npixel-level observations with interactive elements, while\nsimple mode converts the raw HTML into a structured text\nobservation more amenable for training agents. The action\nspace consists of query searches and button clicks, which\ntransition between 4-page types: search, results, item, and\nitem detail. Instructions are crowdsourced natural language\nspecifying product attributes and options, with a total of 12k\ncollected. Automatic rewards are computed by comparing\nthe product purchased by the agent against the attributes\nand options specified in the instruction, using both lexical\nmatching and semantic similarity metrics.\nThere are two evaluation metrics used in WebShop: (1) Task\nScore defined as (100\u00d7avg. reward ), which captures the\n16Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nFigure 4. Example trajectories on HotPotQA for ReAct ( left) and LATS ( right ). LATS can sample more actions and avoid failure from\nprevious mistakes by evaluating states with an LM to guide the search toward promising areas of the tree.\nType Argument State \u2192Next State\nsearch [Query ] Search \u2192Results\nchoose Back to search \u2217 \u2192 Search\nchoose Prev\/Next page Results \u2192Results\nchoose [Product title ] Results \u2192Item\nchoose [Option ] Item \u2192Item\nchoose Desc\/Overview Item \u2192Item-Detail\nchoose Previous Item-Detail \u2192Item\nchoose Buy Item \u2192Episode End\nTable 12. Action space of WebShop.\naverage reward obtained across episodes; and (2) Success\nRate (SR) defined as the portion","chunk_id":"fb2b4544aedd793e4d4ec3147320a51c","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"SEARCH","type":"ACTION","description":"Search is an action in Interactive Information Retrieval that returns the first 5 sentences from the corresponding entity wiki page if it exists, or suggests top-5 similar entities from the Wikipedia search engine","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"ACTION"},{"name":"LOOKUP","type":"ACTION","description":"Lookup is an action in Interactive Information Retrieval that returns the next sentence in the page containing the specified string","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"ACTION"},{"name":"FINISH","type":"ACTION","description":"Finish is an action in Interactive Information Retrieval that finishes the current task with the provided answer","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"ACTION"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a method that unifies reasoning, acting, and planning in language models, tested with various parameters on HotpotQA","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"TECHNOLOGY"},{"name":"HOTPOTQA","type":"DATASET","description":"HotpotQA is a dataset used to measure the performance of LATS and its baseline variants, evaluated by Exact Match (EM)","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"DATASET"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used to evaluate performance over successive iterations on HumanEval","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"TECHNOLOGY"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a language model used to evaluate performance over successive iterations on HumanEval","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"TECHNOLOGY"},{"name":"HUMANEVAL","type":"DATASET","description":"HumanEval is a dataset of 164 handwritten programming problems used to evaluate the functional correctness of models for synthesizing programs from natural language descriptions","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"DATASET"},{"name":"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)","type":"DATASET","description":"The Mostly Basic Programming Problems (MBPP) benchmark contains 974 short Python functions designed to evaluate program synthesis techniques","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"DATASET"},{"name":"WEBSHOP","type":"TECHNOLOGY","description":"WebShop is an interactive web-based environment designed to evaluate agents on grounded language understanding and decision-making, simulating an e-commerce shopping task","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"TECHNOLOGY"},{"name":"AMAZON","type":"ORGANIZATION","description":"Amazon is the source of over 1 million real-world products used in the WebShop environment","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"ORGANIZATION"},{"name":"TASK SCORE","type":"METRIC","description":"Task Score is an evaluation metric in WebShop defined as (100\u00d7avg. reward), capturing the average reward obtained across episodes","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"METRIC"},{"name":"SUCCESS RATE (SR)","type":"METRIC","description":"Success Rate (SR) is an evaluation metric in WebShop defined as the portion of successful episodes","source_id":"fb2b4544aedd793e4d4ec3147320a51c","entity_type":"METRIC"},{"name":"INTERACTIVE INFORMATION RETRIEVAL","type":"","description":"","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PROMPT METHOD","type":"TECHNOLOGY","description":"Prompt Method refers to the different configurations and techniques used in LATS to achieve various performance metrics on HotpotQA","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"EXACT MATCH (EM)","type":"METRIC","description":"Exact Match (EM) is a metric used to evaluate the performance of LATS and its baseline variants on HotpotQA","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"PROGRAMMING","type":"TECHNOLOGY","description":"Programming refers to the activity of writing code to solve problems, as evaluated in datasets like HumanEval and MBPP","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"CHEN ET AL.","type":"PERSON","description":"Chen et al. are the authors who introduced the HumanEval dataset in 2021","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"AUSTIN ET AL.","type":"PERSON","description":"Austin et al. are the authors who introduced the Mostly Basic Programming Problems (MBPP) benchmark in 2022","source_id":"fb2b4544aedd793e4d4ec3147320a51c"},{"name":"YA0 ET AL.","type":"PERSON","description":"Yao et al. are the authors who introduced the WebShop environment in 2022","source_id":"fb2b4544aedd793e4d4ec3147320a51c"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SEARCH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Search is an action in Interactive Information Retrieval that returns the first 5 sentences from the corresponding entity wiki page if it exists, or suggests top-5 similar entities from the Wikipedia search engine<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"LOOKUP\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Lookup is an action in Interactive Information Retrieval that returns the next sentence in the page containing the specified string<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"FINISH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Finish is an action in Interactive Information Retrieval that finishes the current task with the provided answer<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">ACTION<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a method that unifies reasoning, acting, and planning in language models, tested with various parameters on HotpotQA<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotpotQA is a dataset used to measure the performance of LATS and its baseline variants, evaluated by Exact Match (EM)<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used to evaluate performance over successive iterations on HumanEval<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a language model used to evaluate performance over successive iterations on HumanEval<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HUMANEVAL\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HumanEval is a dataset of 164 handwritten programming problems used to evaluate the functional correctness of models for synthesizing programs from natural language descriptions<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Mostly Basic Programming Problems (MBPP) benchmark contains 974 short Python functions designed to evaluate program synthesis techniques<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"WEBSHOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebShop is an interactive web-based environment designed to evaluate agents on grounded language understanding and decision-making, simulating an e-commerce shopping task<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AMAZON\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Amazon is the source of over 1 million real-world products used in the WebShop environment<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"TASK SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Task Score is an evaluation metric in WebShop defined as (100&#215;avg. reward), capturing the average reward obtained across episodes<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"SUCCESS RATE (SR)\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Success Rate (SR) is an evaluation metric in WebShop defined as the portion of successful episodes<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"INTERACTIVE INFORMATION RETRIEVAL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PROMPT METHOD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompt Method refers to the different configurations and techniques used in LATS to achieve various performance metrics on HotpotQA<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"EXACT MATCH (EM)\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Exact Match (EM) is a metric used to evaluate the performance of LATS and its baseline variants on HotpotQA<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"PROGRAMMING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Programming refers to the activity of writing code to solve problems, as evaluated in datasets like HumanEval and MBPP<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"CHEN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen et al. are the authors who introduced the HumanEval dataset in 2021<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"AUSTIN ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Austin et al. are the authors who introduced the Mostly Basic Programming Problems (MBPP) benchmark in 2022<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <node id=\"YA0 ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao et al. are the authors who introduced the WebShop environment in 2022<\/data>      <data key=\"d2\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/node>    <edge source=\"LOOKUP\" target=\"INTERACTIVE INFORMATION RETRIEVAL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Lookup is an action within the Interactive Information Retrieval system<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"FINISH\" target=\"INTERACTIVE INFORMATION RETRIEVAL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Finish is an action within the Interactive Information Retrieval system<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"LATS\" target=\"HOTPOTQA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">LATS is tested on the HotpotQA dataset<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"LATS\" target=\"PROMPT METHOD\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Prompt Method refers to the different configurations and techniques used in LATS<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"EXACT MATCH (EM)\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Exact Match (EM) is a metric used to evaluate performance on the HotpotQA dataset<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"HUMANEVAL\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">GPT-3.5 is used to evaluate performance on the HumanEval dataset<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"HUMANEVAL\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">GPT-4 is used to evaluate performance on the HumanEval dataset<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HUMANEVAL\" target=\"PROGRAMMING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Programming is evaluated using the HumanEval dataset<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"HUMANEVAL\" target=\"CHEN ET AL.\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Chen et al. introduced the HumanEval dataset in 2021<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)\" target=\"PROGRAMMING\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Programming is evaluated using the Mostly Basic Programming Problems (MBPP) benchmark<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"MOSTLY BASIC PROGRAMMING PROBLEMS (MBPP)\" target=\"AUSTIN ET AL.\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Austin et al. introduced the Mostly Basic Programming Problems (MBPP) benchmark in 2022<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"AMAZON\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">WebShop uses over 1 million real-world products scraped from Amazon<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"TASK SCORE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Task Score is an evaluation metric used in WebShop<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"SUCCESS RATE (SR)\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Success Rate (SR) is an evaluation metric used in WebShop<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"YA0 ET AL.\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Yao et al. introduced the WebShop environment in 2022<\/data>      <data key=\"d6\">fb2b4544aedd793e4d4ec3147320a51c<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"b8dd0300033963bb4a3e1bad37f8e7b9","chunk":" State\nsearch [Query ] Search \u2192Results\nchoose Back to search \u2217 \u2192 Search\nchoose Prev\/Next page Results \u2192Results\nchoose [Product title ] Results \u2192Item\nchoose [Option ] Item \u2192Item\nchoose Desc\/Overview Item \u2192Item-Detail\nchoose Previous Item-Detail \u2192Item\nchoose Buy Item \u2192Episode End\nTable 12. Action space of WebShop.\naverage reward obtained across episodes; and (2) Success\nRate (SR) defined as the portion of instructions where r= 1.\nThe reward is calculated based on the number of attributes\nsatisfied by the selected item. We use 50 environments for\nour experiments and a maximum depth limit of 15. For\nvalue function hyperparameters, we use \u03bb= 0.8for the LM\nscore and self-consistency score.\nD.4. Game of 24\nGame of 24 is a mathematical reasoning challenge where\nthe goal is to use basic arithmetic operations to construct\n24 out of 4 numbers. We follow the setup from Yao et al.\n(2023a), where we measure success if the agent produces aPrompt Method Game of 24 (Success Rate) \u2191\nLATS (CoT, \u03bb= 1) 0.40\nLATS (CoT) 0.44\nTable 13. Ablations on \u03bbin Game of 24 with GPT-3.5. \u03bb= 0.5\nused in the main paper outperforms \u03bb= 1, equivalent to removing\nself-consistency, which indicates that the self-consistency score\nimproves the performance of LATS.\ncorrect equation that equals 24 and uses each input number\nonly once. We report success rate over 50 games. We use\nk= 30 iterations and a maximum depth limit of 5. For value\nfunction hyperparameters, we use \u03bb= 0.5for the LM score\nand self-consistency score. Tab. 13 shows the performance\ncomparison between \u03bb= 0.5and\u03bb= 1, which validates\nour design of self-consistency term.\nE. HotPotQA Prompts\nE.1. Base Acting Prompt\nSolve a question answering task with interleaving Thought,\nAction, Observation steps. Thought can reason about the\ncurrent situation, and Action can be three types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n17Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nAfter each observation, provide the next Thought and next\nAction.\nHere are some examples:\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThought 2: Arthur\u2019s Magazine was started in 1844. I need\nto search First for Women next.\nAction 2: Search[First for Women]\nObservation 2: First for Women is a woman\u2019s magazine\npublished by Bauer Media Group in the USA.[1] The mag-\nazine was started in 1989. It is based in Englewood Cliffs,\nNew Jersey.[2] In 2011 the circulation of the magazine was\n1,310,696 copies.[3]\nThought 3: First for Women was started in 1989. 1844\n(Arthur\u2019s Magazine) <1989 (First for Women), so Arthur\u2019s\nMagazine was started first.\nAction 3: Finish[Arthur\u2019s Magazine]\n(examples)\nYou have attempted to answer the following question before\nand failed. The following reflection(s) give a plan to avoid\nfailing to answer the question in the same way you did\npreviously. Use them to improve your strategy of correctly\nanswering the given question.\n(trajectories)\n(input)\nE.2. Base Reasoning Prompt\nSolve a question answering task by having a Thought, then\nFinish with your answer. Thought can reason about the\ncurrent situation. Finish[answer] returns the answer and\nfinishes the task. You will be given context that you should\nuse to help you answer the question. Start your responsewith either Action or an indexed Thought\nHere are some examples:\nQuestion: What is the elevation range for the area that the\neastern sector of the Colorado orogeny extends into?\nLet\u2019s think step by step.\nThought 1: The eastern sector of Colorado orogeny extends\ninto the High Plains.\nThought 2: High Plains rise in elevation from around 1,800\nto 7,000 ft\nThought 3: The answer is 1,800 to 7,000 ft.\nAction: Finish[1,800 to 7,000 ft]\n(examples)\nPrevious trial: (trajectories)\n(input)\nE.3. Value Function Prompt\nAnalyze the trajectories of a solution to a question answering\ntask. The trajectories are labeled by environmental Obser-\nvations about the situation, Thoughts","chunk_id":"b8dd0300033963bb4a3e1bad37f8e7b9","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"WEBSHOP","type":"TECHNOLOGY","description":"WebShop is a system with an action space that includes various actions such as searching, choosing items, and buying items","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"QUERY","type":"ACTION","description":"Query is an action in WebShop that involves searching for a specific term or item","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"RESULTS","type":"STATE","description":"Results is a state in WebShop that displays the outcomes of a search query","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"PRODUCT TITLE","type":"ATTRIBUTE","description":"Product title is an attribute in WebShop that represents the name of an item in the search results","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"OPTION","type":"ATTRIBUTE","description":"Option is an attribute in WebShop that represents different choices available for a selected item","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"DESC\/OVERVIEW","type":"ATTRIBUTE","description":"Desc\/Overview is an attribute in WebShop that provides a description or overview of a selected item","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ITEM","type":"STATE","description":"Item is a state in WebShop that represents a selected product from the search results","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ITEM-DETAIL","type":"STATE","description":"Item-Detail is a state in WebShop that provides detailed information about a selected item","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"EPISODE END","type":"STATE","description":"Episode End is a state in WebShop that signifies the end of a buying episode","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"REWARD","type":"METRIC","description":"Reward is a metric in WebShop calculated based on the number of attributes satisfied by the selected item","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SUCCESS RATE","type":"METRIC","description":"Success Rate is a metric used in the Game of 24 to measure the success of the agent in producing the correct equation\nSuccess Rate (SR) is a metric in WebShop defined as the portion of instructions where the reward equals 1","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9","entity_type":"METRIC"},{"name":"VALUE FUNCTION","type":"TECHNOLOGY","description":"Value function is a component in WebShop used to evaluate the performance of actions based on certain hyperparameters\nValue function is a component in the Game of 24 used to evaluate the performance of actions based on certain hyperparameters","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9","entity_type":"TECHNOLOGY"},{"name":"GAME OF 24","type":"CHALLENGE","description":"Game of 24 is a mathematical reasoning challenge where the goal is to use basic arithmetic operations to construct the number 24 from four given numbers","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"LATS","type":"TECHNOLOGY","description":"LATS (Language Agent Tree Search) is a method used in the Game of 24 to improve success rates by using self-consistency scores","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model used in the Game of 24 to test the performance of different configurations of LATS","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"HOTPOTQA","type":"DATASET","description":"HotPotQA is a dataset used for question answering tasks that involve interleaving Thought, Action, and Observation steps","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"PROMPT","type":"TECHNIQUE","description":"Prompt is a technique used in HotPotQA to guide the language model through a series of Thought, Action, and Observation steps to solve a question answering task","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"THOUGHT","type":"ACTION","description":"Thought is an action in HotPotQA where the model reasons about the current situation","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ACTION","type":"ACTION","description":"Action is an action in HotPotQA that can involve searching for an entity, looking up a keyword, or finishing with an answer","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"OBSERVATION","type":"ACTION","description":"Observation is an action in HotPotQA where the model receives information based on the previous action","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"FINISH","type":"ACTION","description":"Finish is an action in HotPotQA where the model provides the final answer and completes the task","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ARTHUR'S MAGAZINE","type":"ENTITY","description":"Arthur's Magazine is an American literary periodical published in the 19th century, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"FIRST FOR WOMEN","type":"ENTITY","description":"First for Women is a woman's magazine published by Bauer Media Group in the USA, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"COLORADO OROGENY","type":"ENTITY","description":"Colorado orogeny is a geological event mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"HIGH PLAINS","type":"ENTITY","description":"High Plains is a region that rises in elevation from around 1,800 to 7,000 ft, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9","entity_type":"LOCATION"},{"name":"PREV\/NEXT PAGE","type":"ACTION","description":"Prev\/Next page is an action in WebShop that allows navigation through search results","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"CHOOSE","type":"ACTION","description":"Choose is an action in WebShop that involves selecting an item or option","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"BACK TO SEARCH","type":"ACTION","description":"Back to search is an action in WebShop that returns the user to the search state","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"STATE","type":"STATE","description":"State is a general term for different stages or conditions in WebShop","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"EPISODE","type":"STATE","description":"Episode is a state in WebShop that represents a complete sequence of actions from search to purchase","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"INSTRUCTIONS","type":"ATTRIBUTE","description":"Instructions are attributes in WebShop that guide the user through different actions","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ATTRIBUTES","type":"ATTRIBUTE","description":"Attributes are characteristics or features of items in WebShop","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ENVIRONMENTS","type":"ATTRIBUTE","description":"Environments are different settings or conditions under which experiments are conducted in WebShop","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"DEPTH LIMIT","type":"ATTRIBUTE","description":"Depth limit is an attribute in WebShop that sets a maximum limit for the depth of actions or searches","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"HYPERPARAMETERS","type":"ATTRIBUTE","description":"Hyperparameters are attributes in WebShop that define the settings for value functions and other components","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"LM SCORE","type":"ATTRIBUTE","description":"LM score is an attribute in WebShop that represents the score given by the language model","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SELF-CONSISTENCY SCORE","type":"ATTRIBUTE","description":"Self-consistency score is an attribute in WebShop that measures the consistency of actions or decisions","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"YA0 ET AL.","type":"ENTITY","description":"Yao et al. is a reference to a group of authors who have worked on the setup for the Game of 24","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"PROMPT METHOD","type":"TECHNIQUE","description":"Prompt Method is a technique used in the Game of 24 to guide the language model through the task","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ITERATIONS","type":"ATTRIBUTE","description":"Iterations are attributes in the Game of 24 that represent the number of times the process is repeated","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SELF-CONSISTENCY TERM","type":"ATTRIBUTE","description":"Self-consistency term is an attribute in the Game of 24 that validates the design of the self-consistency score","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"HOTPOTQA PROMPTS","type":"TECHNIQUE","description":"HotPotQA Prompts are techniques used in HotPotQA to guide the language model through question answering tasks","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"BASE ACTING PROMPT","type":"TECHNIQUE","description":"Base Acting Prompt is a technique in HotPotQA that involves interleaving Thought, Action, and Observation steps","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"BASE REASONING PROMPT","type":"TECHNIQUE","description":"Base Reasoning Prompt is a technique in HotPotQA that involves reasoning about the current situation and finishing with an answer","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"VALUE FUNCTION PROMPT","type":"TECHNIQUE","description":"Value Function Prompt is a technique in HotPotQA that involves analyzing the trajectories of a solution to a question answering task","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"WIKIPEDIA","type":"DATASET","description":"Wikipedia is a dataset used in HotPotQA for searching entities and returning relevant information","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"KEYWORD","type":"ATTRIBUTE","description":"Keyword is an attribute in HotPotQA used to look up specific terms in the current passage","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ANSWER","type":"ATTRIBUTE","description":"Answer is an attribute in HotPotQA that represents the final response to a question","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"TIMOTHY SHAY ARTHUR","type":"PERSON","description":"Timothy Shay Arthur is the editor of Arthur's Magazine, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"EDGAR A. POE","type":"PERSON","description":"Edgar A. Poe is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"J.H. INGRAHAM","type":"PERSON","description":"J.H. Ingraham is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SARAH JOSEPHA HALE","type":"PERSON","description":"Sarah Josepha Hale is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"THOMAS G. SPEAR","type":"PERSON","description":"Thomas G. Spear is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"GODEY'S LADY'S BOOK","type":"ENTITY","description":"Godey's Lady's Book is a publication that Arthur's Magazine was merged into in May 1846, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"BAUER MEDIA GROUP","type":"ORGANIZATION","description":"Bauer Media Group is the publisher of First for Women, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ENGLEWOOD CLIFFS","type":"LOCATION","description":"Englewood Cliffs is the location where First for Women is based, mentioned in a HotPotQA example","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"CIRCULATION","type":"ATTRIBUTE","description":"Circulation is an attribute in HotPotQA that represents the number of copies distributed, mentioned in the context of First for Women","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"ELEVATION","type":"ATTRIBUTE","description":"Elevation is an attribute in HotPotQA that represents the height above sea level, mentioned in the context of the High Plains","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"},{"name":"SEARCH","type":"","description":"","source_id":"b8dd0300033963bb4a3e1bad37f8e7b9"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"WEBSHOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebShop is a system with an action space that includes various actions such as searching, choosing items, and buying items<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"QUERY\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Query is an action in WebShop that involves searching for a specific term or item<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"RESULTS\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">Results is a state in WebShop that displays the outcomes of a search query<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"PRODUCT TITLE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Product title is an attribute in WebShop that represents the name of an item in the search results<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"OPTION\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Option is an attribute in WebShop that represents different choices available for a selected item<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"DESC\/OVERVIEW\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Desc\/Overview is an attribute in WebShop that provides a description or overview of a selected item<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ITEM\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">Item is a state in WebShop that represents a selected product from the search results<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ITEM-DETAIL\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">Item-Detail is a state in WebShop that provides detailed information about a selected item<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"EPISODE END\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">Episode End is a state in WebShop that signifies the end of a buying episode<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"REWARD\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Reward is a metric in WebShop calculated based on the number of attributes satisfied by the selected item<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SUCCESS RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Success Rate is a metric used in the Game of 24 to measure the success of the agent in producing the correct equationSuccess Rate (SR) is a metric in WebShop defined as the portion of instructions where the reward equals 1<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>      <data key=\"d3\">METRIC<\/data>    <\/node>    <node id=\"VALUE FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Value function is a component in WebShop used to evaluate the performance of actions based on certain hyperparametersValue function is a component in the Game of 24 used to evaluate the performance of actions based on certain hyperparameters<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GAME OF 24\">      <data key=\"d0\">CHALLENGE<\/data>      <data key=\"d1\">Game of 24 is a mathematical reasoning challenge where the goal is to use basic arithmetic operations to construct the number 24 from four given numbers<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"LATS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LATS (Language Agent Tree Search) is a method used in the Game of 24 to improve success rates by using self-consistency scores<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model used in the Game of 24 to test the performance of different configurations of LATS<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"HOTPOTQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">HotPotQA is a dataset used for question answering tasks that involve interleaving Thought, Action, and Observation steps<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompt is a technique used in HotPotQA to guide the language model through a series of Thought, Action, and Observation steps to solve a question answering task<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"THOUGHT\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Thought is an action in HotPotQA where the model reasons about the current situation<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ACTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Action is an action in HotPotQA that can involve searching for an entity, looking up a keyword, or finishing with an answer<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Observation is an action in HotPotQA where the model receives information based on the previous action<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"FINISH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Finish is an action in HotPotQA where the model provides the final answer and completes the task<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ARTHUR'S MAGAZINE\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">Arthur's Magazine is an American literary periodical published in the 19th century, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"FIRST FOR WOMEN\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">First for Women is a woman's magazine published by Bauer Media Group in the USA, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"COLORADO OROGENY\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">Colorado orogeny is a geological event mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"HIGH PLAINS\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">High Plains is a region that rises in elevation from around 1,800 to 7,000 ft, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>      <data key=\"d3\">LOCATION<\/data>    <\/node>    <node id=\"PREV\/NEXT PAGE\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Prev\/Next page is an action in WebShop that allows navigation through search results<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"CHOOSE\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Choose is an action in WebShop that involves selecting an item or option<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"BACK TO SEARCH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Back to search is an action in WebShop that returns the user to the search state<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"STATE\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">State is a general term for different stages or conditions in WebShop<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"EPISODE\">      <data key=\"d0\">STATE<\/data>      <data key=\"d1\">Episode is a state in WebShop that represents a complete sequence of actions from search to purchase<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"INSTRUCTIONS\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Instructions are attributes in WebShop that guide the user through different actions<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ATTRIBUTES\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Attributes are characteristics or features of items in WebShop<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ENVIRONMENTS\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Environments are different settings or conditions under which experiments are conducted in WebShop<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"DEPTH LIMIT\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Depth limit is an attribute in WebShop that sets a maximum limit for the depth of actions or searches<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"HYPERPARAMETERS\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Hyperparameters are attributes in WebShop that define the settings for value functions and other components<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"LM SCORE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">LM score is an attribute in WebShop that represents the score given by the language model<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY SCORE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Self-consistency score is an attribute in WebShop that measures the consistency of actions or decisions<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"YA0 ET AL.\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">Yao et al. is a reference to a group of authors who have worked on the setup for the Game of 24<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"PROMPT METHOD\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompt Method is a technique used in the Game of 24 to guide the language model through the task<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ITERATIONS\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Iterations are attributes in the Game of 24 that represent the number of times the process is repeated<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY TERM\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Self-consistency term is an attribute in the Game of 24 that validates the design of the self-consistency score<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"HOTPOTQA PROMPTS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">HotPotQA Prompts are techniques used in HotPotQA to guide the language model through question answering tasks<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"BASE ACTING PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Base Acting Prompt is a technique in HotPotQA that involves interleaving Thought, Action, and Observation steps<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"BASE REASONING PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Base Reasoning Prompt is a technique in HotPotQA that involves reasoning about the current situation and finishing with an answer<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"VALUE FUNCTION PROMPT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Value Function Prompt is a technique in HotPotQA that involves analyzing the trajectories of a solution to a question answering task<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"WIKIPEDIA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Wikipedia is a dataset used in HotPotQA for searching entities and returning relevant information<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"KEYWORD\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Keyword is an attribute in HotPotQA used to look up specific terms in the current passage<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Answer is an attribute in HotPotQA that represents the final response to a question<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy Shay Arthur is the editor of Arthur's Magazine, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"EDGAR A. POE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Edgar A. Poe is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"J.H. INGRAHAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J.H. Ingraham is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SARAH JOSEPHA HALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Josepha Hale is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"THOMAS G. SPEAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas G. Spear is an author whose work was featured in Arthur's Magazine, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"GODEY'S LADY'S BOOK\">      <data key=\"d0\">ENTITY<\/data>      <data key=\"d1\">Godey's Lady's Book is a publication that Arthur's Magazine was merged into in May 1846, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"BAUER MEDIA GROUP\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Bauer Media Group is the publisher of First for Women, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ENGLEWOOD CLIFFS\">      <data key=\"d0\">LOCATION<\/data>      <data key=\"d1\">Englewood Cliffs is the location where First for Women is based, mentioned in a HotPotQA example<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"CIRCULATION\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Circulation is an attribute in HotPotQA that represents the number of copies distributed, mentioned in the context of First for Women<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"ELEVATION\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Elevation is an attribute in HotPotQA that represents the height above sea level, mentioned in the context of the High Plains<\/data>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/node>    <edge source=\"WEBSHOP\" target=\"QUERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Query is an action in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"RESULTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Results is a state in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"PRODUCT TITLE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Product title is an attribute in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"OPTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Option is an attribute in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"DESC\/OVERVIEW\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Desc\/Overview is an attribute in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"ITEM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Item is a state in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"ITEM-DETAIL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Item-Detail is a state in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"EPISODE END\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Episode End is a state in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"REWARD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Reward is a metric used in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"SUCCESS RATE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Success Rate is a metric used in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"WEBSHOP\" target=\"VALUE FUNCTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Value function is a component used in the WebShop system<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"RESULTS\" target=\"SEARCH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Search is an action that leads to the Results state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"RESULTS\" target=\"PRODUCT TITLE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Product title is an attribute displayed in the Results state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"RESULTS\" target=\"ITEM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Item is a state that follows the Results state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"OPTION\" target=\"ITEM\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Option is an attribute associated with the Item state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"DESC\/OVERVIEW\" target=\"ITEM\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Desc\/Overview is an attribute associated with the Item state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ITEM\" target=\"ITEM-DETAIL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Item-Detail is a state that provides detailed information about the Item state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ITEM-DETAIL\" target=\"EPISODE END\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Episode End is a state that follows the Item-Detail state in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"REWARD\" target=\"SUCCESS RATE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Success Rate is a metric that depends on the Reward metric in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"REWARD\" target=\"VALUE FUNCTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Value function is a component that influences the Reward metric in WebShop<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"SUCCESS RATE\" target=\"GAME OF 24\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Success Rate is a metric used in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION\" target=\"SELF-CONSISTENCY TERM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Self-consistency term is an attribute that validates the design of the Value function in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"LATS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LATS is a method used in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"GPT-3.5\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">GPT-3.5 is a language model used in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"PROMPT METHOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prompt Method is a technique used in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"GAME OF 24\" target=\"ITERATIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Iterations are attributes used in the Game of 24<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"PROMPT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Prompt is a technique used in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"THOUGHT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Thought is an action in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"ACTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Action is an action in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"OBSERVATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Observation is an action in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"FINISH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Finish is an action in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA\" target=\"HOTPOTQA PROMPTS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">HotPotQA Prompts are techniques used in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"THOUGHT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Thought is an action used in the Prompt technique in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"ACTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Action is an action used in the Prompt technique in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"OBSERVATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Observation is an action used in the Prompt technique in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"FINISH\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Finish is an action used in the Prompt technique in HotPotQA<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"FIRST FOR WOMEN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Both Arthur's Magazine and First for Women are entities mentioned in a HotPotQA example<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Timothy Shay Arthur is the editor of Arthur's Magazine<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"EDGAR A. POE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Edgar A. Poe is an author whose work was featured in Arthur's Magazine<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"J.H. INGRAHAM\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">J.H. Ingraham is an author whose work was featured in Arthur's Magazine<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"SARAH JOSEPHA HALE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Sarah Josepha Hale is an author whose work was featured in Arthur's Magazine<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"THOMAS G. SPEAR\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Thomas G. Spear is an author whose work was featured in Arthur's Magazine<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"GODEY'S LADY'S BOOK\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Arthur's Magazine was merged into Godey's Lady's Book in May 1846<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"FIRST FOR WOMEN\" target=\"BAUER MEDIA GROUP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Bauer Media Group is the publisher of First for Women<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"FIRST FOR WOMEN\" target=\"ENGLEWOOD CLIFFS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Englewood Cliffs is the location where First for Women is based<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"FIRST FOR WOMEN\" target=\"CIRCULATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Circulation is an attribute associated with First for Women<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"COLORADO OROGENY\" target=\"HIGH PLAINS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Both Colorado orogeny and High Plains are entities mentioned in a HotPotQA exampleHigh Plains is a region affected by the Colorado orogeny<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HIGH PLAINS\" target=\"ELEVATION\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Elevation is an attribute associated with the High Plains<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA PROMPTS\" target=\"BASE ACTING PROMPT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Base Acting Prompt is a technique used in HotPotQA Prompts<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA PROMPTS\" target=\"BASE REASONING PROMPT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Base Reasoning Prompt is a technique used in HotPotQA Prompts<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>    <edge source=\"HOTPOTQA PROMPTS\" target=\"VALUE FUNCTION PROMPT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Value Function Prompt is a technique used in HotPotQA Prompts<\/data>      <data key=\"d6\">b8dd0300033963bb4a3e1bad37f8e7b9<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"357f3442ba581c9d2bdf84d90509056f","chunk":" Plains rise in elevation from around 1,800\nto 7,000 ft\nThought 3: The answer is 1,800 to 7,000 ft.\nAction: Finish[1,800 to 7,000 ft]\n(examples)\nPrevious trial: (trajectories)\n(input)\nE.3. Value Function Prompt\nAnalyze the trajectories of a solution to a question answering\ntask. The trajectories are labeled by environmental Obser-\nvations about the situation, Thoughts that can reason about\nthe current situation, and Actions that can be three types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude \u201cThus the correctness score is s\u201d, where s is an\ninteger from 1 to 10.\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\n18Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,\nmaking this the best action.\nThus the correctness score is 10\n(other examples)\n(failed trajectories)\n(context)\nE.4. Reflection Prompt\nAnalyze the trajectories of a solution to a question-\nanswering task. The trajectories are labeled by environ-\nmental Observations about the situation, Thoughts that can\nreason about the current situation, and Actions that can be\nthree types:\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n(2) Lookup[keyword], which returns the next sentence con-\ntaining keyword in the current passage.\n(3) Finish[answer], which returns the answer and finishes\nthe task.\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude \u201cThus the correctness score is s\u201d, where s is an\ninteger from 1 to 10.\nQuestion: Which magazine was started first Arthur\u2019s Maga-\nzine or First for Women?\nThought 1: I need to search Arthur\u2019s Magazine and First for\nWomen, and find which was started first.\nAction 1: Search[Arthur\u2019s Magazine]\nObservation 1: Arthur\u2019s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey\u2019s Lady\u2019s Book.[3]\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,making this the best action.\nThus the correctness score is 10\n(other examples)\n(failed trajectories)\n(context)\nF. Programming Prompts\nF.1. HumanEval function implementation example\nSample function signature:\nd e f minSubArraySum ( nums ) :\nGiven an a r r a y of i n t e g e r s nums ,\nf i n d t h e minimum sum of any\nnon \u2212empty sub \u2212 a r r a y of nums .\nExample\nminSubArraySum ( [ \u2212 1 , \u22122 , \u22123]) == \u22126\nSample function body implementation:\nmin sum = f l o a t ( \u2019 i n f \u2019 )\nf o r i i n r a n g e ( l e n ( nums ) ) :\nc u r r e n t s u m = 0\nf o r j i n r a n g e ( i , l e n ( nums ) ) :\nc u r r e n t s u m += nums [ j ]\ni f c u r r e n t s u m <min sum :\nmin sum = c u r r e n t s u m\nr e t u r n min sum\nF.2. Base Acting\/Reasoning Prompt\nYou are an AI Python assistant. You will be given your\nprevious implementation of a function, a series of unit tests\nresults","chunk_id":"357f3442ba581c9d2bdf84d90509056f","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"PLAINS","type":"GEOGRAPHICAL FEATURE","description":"Plains are large areas of flat or gently rolling land that rise in elevation from around 1,800 to 7,000 feet","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"VALUE FUNCTION PROMPT","type":"DOCUMENT","description":"The Value Function Prompt is a set of instructions for analyzing the trajectories of a solution to a question-answering task, focusing on thoughts, actions, and observations","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"SEARCH[ENTITY]","type":"ACTION","description":"Search[entity] is an action that searches for the exact entity on Wikipedia and returns the first paragraph if it exists, or similar entities if it does not","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"LOOKUP[KEYWORD]","type":"ACTION","description":"Lookup[keyword] is an action that returns the next sentence containing the specified keyword in the current passage","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"FINISH[ANSWER]","type":"ACTION","description":"Finish[answer] is an action that returns the answer and finishes the task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"ARTHUR'S MAGAZINE","type":"PUBLICATION","description":"Arthur's Magazine was an American literary periodical published in Philadelphia in the 19th century, edited by Timothy Shay Arthur, and featured work by various authors","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"FIRST FOR WOMEN","type":"PUBLICATION","description":"First for Women is a magazine mentioned in the context of a question about which magazine was started first","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"TIMOTHY SHAY ARTHUR","type":"PERSON","description":"Timothy Shay Arthur was the editor of Arthur's Magazine, an American literary periodical published in Philadelphia in the 19th century","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"GODEY'S LADY'S BOOK","type":"PUBLICATION","description":"Godey's Lady's Book was a magazine into which Arthur's Magazine was merged in May 1846","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"REFLECTION PROMPT","type":"DOCUMENT","description":"The Reflection Prompt is a set of instructions for analyzing the trajectories of a solution to a question-answering task, focusing on thoughts, actions, and observations","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE","type":"DOCUMENT","description":"The HumanEval function implementation example is a sample function signature and body implementation for finding the minimum sum of any non-empty sub-array of integers","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"BASE ACTING\/REASONING PROMPT","type":"DOCUMENT","description":"The Base Acting\/Reasoning Prompt is a set of instructions for an AI Python assistant to implement a function and evaluate unit test results","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"QUESTION","type":"DOCUMENT","description":"A question is a query that requires an answer, often used in the context of evaluating the correctness of a trajectory in a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"THOUGHT","type":"DOCUMENT","description":"A thought is a reasoning process about the current situation in a trajectory of a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"ACTION","type":"DOCUMENT","description":"An action is a step taken in a trajectory of a question-answering task, which can be of three types: Search[entity], Lookup[keyword], or Finish[answer]","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"OBSERVATION","type":"DOCUMENT","description":"An observation is an environmental input or feedback about the situation in a trajectory of a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"CORRECTNESS SCORE","type":"DOCUMENT","description":"The correctness score is an integer from 1 to 10 that evaluates the correctness of a trajectory in a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"EXAMPLES","type":"DOCUMENT","description":"Examples refer to sample trajectories provided to illustrate correct or failed solutions in a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"FAILED TRAJECTORIES","type":"DOCUMENT","description":"Failed trajectories are examples of solutions that did not correctly answer the question in a question-answering task","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"CONTEXT","type":"DOCUMENT","description":"Context refers to the background information or setting in which a question-answering task is performed","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"UNIT TEST RESULTS","type":"DOCUMENT","description":"Unit test results are the outcomes of tests run to evaluate the correctness of a function implementation","source_id":"357f3442ba581c9d2bdf84d90509056f"},{"name":"TRAJECTORIES","type":"","description":"","source_id":"357f3442ba581c9d2bdf84d90509056f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PLAINS\">      <data key=\"d0\">GEOGRAPHICAL FEATURE<\/data>      <data key=\"d1\">Plains are large areas of flat or gently rolling land that rise in elevation from around 1,800 to 7,000 feet<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"VALUE FUNCTION PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Value Function Prompt is a set of instructions for analyzing the trajectories of a solution to a question-answering task, focusing on thoughts, actions, and observations<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"SEARCH[ENTITY]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Search[entity] is an action that searches for the exact entity on Wikipedia and returns the first paragraph if it exists, or similar entities if it does not<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"LOOKUP[KEYWORD]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Lookup[keyword] is an action that returns the next sentence containing the specified keyword in the current passage<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"FINISH[ANSWER]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Finish[answer] is an action that returns the answer and finishes the task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"ARTHUR'S MAGAZINE\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">Arthur's Magazine was an American literary periodical published in Philadelphia in the 19th century, edited by Timothy Shay Arthur, and featured work by various authors<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"FIRST FOR WOMEN\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">First for Women is a magazine mentioned in the context of a question about which magazine was started first<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timothy Shay Arthur was the editor of Arthur's Magazine, an American literary periodical published in Philadelphia in the 19th century<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"GODEY'S LADY'S BOOK\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">Godey's Lady's Book was a magazine into which Arthur's Magazine was merged in May 1846<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"REFLECTION PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Reflection Prompt is a set of instructions for analyzing the trajectories of a solution to a question-answering task, focusing on thoughts, actions, and observations<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The HumanEval function implementation example is a sample function signature and body implementation for finding the minimum sum of any non-empty sub-array of integers<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"BASE ACTING\/REASONING PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Base Acting\/Reasoning Prompt is a set of instructions for an AI Python assistant to implement a function and evaluate unit test results<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A question is a query that requires an answer, often used in the context of evaluating the correctness of a trajectory in a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"THOUGHT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A thought is a reasoning process about the current situation in a trajectory of a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"ACTION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">An action is a step taken in a trajectory of a question-answering task, which can be of three types: Search[entity], Lookup[keyword], or Finish[answer]<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">An observation is an environmental input or feedback about the situation in a trajectory of a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"CORRECTNESS SCORE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The correctness score is an integer from 1 to 10 that evaluates the correctness of a trajectory in a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"EXAMPLES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Examples refer to sample trajectories provided to illustrate correct or failed solutions in a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"FAILED TRAJECTORIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Failed trajectories are examples of solutions that did not correctly answer the question in a question-answering task<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"CONTEXT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Context refers to the background information or setting in which a question-answering task is performed<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"UNIT TEST RESULTS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Unit test results are the outcomes of tests run to evaluate the correctness of a function implementation<\/data>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <node id=\"TRAJECTORIES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/node>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"SEARCH[ENTITY]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Value Function Prompt includes the action Search[entity] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"LOOKUP[KEYWORD]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Value Function Prompt includes the action Lookup[keyword] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"FINISH[ANSWER]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Value Function Prompt includes the action Finish[answer] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"TRAJECTORIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Value Function Prompt provides instructions for analyzing the trajectories of a solution to a question-answering task<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"CORRECTNESS SCORE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Value Function Prompt instructs to conclude with a correctness score from 1 to 10<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"EXAMPLES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Value Function Prompt includes examples to illustrate correct or failed solutions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"FAILED TRAJECTORIES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Value Function Prompt includes failed trajectories as examples<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"VALUE FUNCTION PROMPT\" target=\"CONTEXT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Value Function Prompt provides context for the question-answering task<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"SEARCH[ENTITY]\" target=\"REFLECTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Reflection Prompt includes the action Search[entity] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"LOOKUP[KEYWORD]\" target=\"REFLECTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Reflection Prompt includes the action Lookup[keyword] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"FINISH[ANSWER]\" target=\"REFLECTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Reflection Prompt includes the action Finish[answer] as one of the types of actions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"TIMOTHY SHAY ARTHUR\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Timothy Shay Arthur was the editor of Arthur's Magazine<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ARTHUR'S MAGAZINE\" target=\"GODEY'S LADY'S BOOK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Arthur's Magazine was merged into Godey's Lady's Book in May 1846<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"REFLECTION PROMPT\" target=\"TRAJECTORIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Reflection Prompt provides instructions for analyzing the trajectories of a solution to a question-answering task<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"REFLECTION PROMPT\" target=\"CORRECTNESS SCORE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Reflection Prompt instructs to conclude with a correctness score from 1 to 10<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"REFLECTION PROMPT\" target=\"EXAMPLES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Reflection Prompt includes examples to illustrate correct or failed solutions<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"REFLECTION PROMPT\" target=\"FAILED TRAJECTORIES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Reflection Prompt includes failed trajectories as examples<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"REFLECTION PROMPT\" target=\"CONTEXT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Reflection Prompt provides context for the question-answering task<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"HUMANEVAL FUNCTION IMPLEMENTATION EXAMPLE\" target=\"BASE ACTING\/REASONING PROMPT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The HumanEval function implementation example is part of the Base Acting\/Reasoning Prompt<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"BASE ACTING\/REASONING PROMPT\" target=\"UNIT TEST RESULTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Base Acting\/Reasoning Prompt includes unit test results to evaluate the correctness of a function implementation<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"TRAJECTORIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Trajectories are analyzed to determine the correctness of the solution to a question<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"THOUGHT\" target=\"TRAJECTORIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Trajectories include thoughts that reason about the current situation<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"TRAJECTORIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Trajectories include actions that can be of three types: Search[entity], Lookup[keyword], or Finish[answer]<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>    <edge source=\"OBSERVATION\" target=\"TRAJECTORIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Trajectories include observations about the situation<\/data>      <data key=\"d5\">357f3442ba581c9d2bdf84d90509056f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"785ad59c6a37896a4676ec5c1689735f","chunk":" , l e n ( nums ) ) :\nc u r r e n t s u m += nums [ j ]\ni f c u r r e n t s u m <min sum :\nmin sum = c u r r e n t s u m\nr e t u r n min sum\nF.2. Base Acting\/Reasoning Prompt\nYou are an AI Python assistant. You will be given your\nprevious implementation of a function, a series of unit tests\nresults, and your self-reflection on your previous implemen-\ntation. Write your full implementation (restate the function\nsignature).\nExample 1:\n[previous impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018 Given i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b . \u2019 \u2019\nr e t u r n a \u2212 b\n[unit test results from previous impl]:\nTested passed:\nTests failed:\nassert add(1, 2) == 3 # output: -1\n19Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nassert add(1, 2) == 4 # output: -1\n[reflection on previous impl]:\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from \u2018-\u2019 to \u2018+\u2019 in the return statement.\nThis will ensure that the function returns the correct output\nfor the given input.\n[improved impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018\nGiven i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b .\n\u2019 \u2019\nr e t u r n a + b\nF.3. Reflection Prompt\nYou are a Python programming assistant. You will be given\na function implementation and a series of unit test results.\nYour goal is to write a few sentences to explain why your\nimplementation is wrong, as indicated by the tests. You\nwill need this as guidance when you try again later. Only\nprovide the few sentence description in your answer, not the\nimplementation. You will be given a few examples by the\nuser.\nExample 1:\n[previous impl]:\nd e f add ( a : i n t , b : i n t ) \u2212 >i n t :\n\u2018 \u2018\nGiven i n t e g e r s a and b ,\nr e t u r n t h e t o t a l v a l u e of a and b .\n\u2019 \u2019\nr e t u r n a \u2212 b\n[unit test results from previous impl]:\nTested passed:\nTests failed:\nassert add(1, 2) == 3 # output: -1\nassert add(1, 2) == 4 # output: -1\n[reflection on previous impl]:\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from \u2018-\u2019 to \u2018+\u2019 in the return statement.This will ensure that the function returns the correct output\nfor the given input.\nF.4. Test Case Generation Prompt\nYou are an AI coding assistant that can write unique, diverse,\nand intuitive unit tests for functions given the signature and\ndocstring.\n(input)\nG. WebShop Prompts\nG.1. Acting Prompt\nWebshop\nInstruction:\ni would like a 3 ounce bottle of bright citrus deodorant for\nsensitive skin, and price lower than 50.00 dollars\n[Search]\nAction: search[3 ounce bright citrus deodorant sensitive\nskin]\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next ] [B078GWRC1J]\nBright Citrus Deodorant by Earth Mama \u2014 Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n$10.99\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama \u2014 Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant\nfor Men, Essential Oil-Based Scent, 24-Hour Odor Protec-\ntion, Cedar & Patchouli Blend, Gentle on Sensitive Skin\n(Mountain Sage, 2.7 oz, 2-Pack)\n$15.95\nAction: think[B078GWRC1J and B078GTKVXY are\nbright citrus deodorant less then 50 dollars. I can check\nB078GWRC1J first.]\n20Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[Prev]\nscent [assorted scents][bright citrus][calming laven-\nder][ginger fresh][simply non-scents] size [travel set (4-\npack)][3 ounce (pack of 1)][3-ounce (2","chunk_id":"785ad59c6a37896a4676ec5c1689735f","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"BASE ACTING\/REASONING PROMPT","type":"DOCUMENT","description":"The Base Acting\/Reasoning Prompt is a document that provides instructions for an AI Python assistant to write a full implementation of a function based on previous implementation, unit test results, and self-reflection","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"AI PYTHON ASSISTANT","type":"TECHNOLOGY","description":"An AI Python assistant is a system designed to assist with Python programming tasks, including writing and improving code based on feedback and test results","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"UNIT TEST RESULTS","type":"DATA","description":"Unit test results are the outcomes of tests run on a function to verify its correctness and identify any issues","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"SELF-REFLECTION","type":"PROCESS","description":"Self-reflection is the process where the AI Python assistant reviews its previous implementation and identifies issues and improvements","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"FUNCTION IMPLEMENTATION","type":"PROCESS","description":"Function implementation is the process of writing the code for a function to perform a specific task","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"REFLECTION PROMPT","type":"DOCUMENT","description":"The Reflection Prompt is a document that instructs the AI Python assistant to explain why a function implementation is wrong based on unit test results","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"TEST CASE GENERATION PROMPT","type":"DOCUMENT","description":"The Test Case Generation Prompt is a document that instructs the AI coding assistant to write unique, diverse, and intuitive unit tests for functions given the signature and docstring","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"WEBSHOP PROMPTS","type":"DOCUMENT","description":"WebShop Prompts are documents that provide instructions for interacting with a webshop, including searching for products and making decisions based on search results","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"ACTING PROMPT","type":"DOCUMENT","description":"The Acting Prompt is a document that provides instructions for performing actions in a webshop, such as searching for products and selecting items based on criteria","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"WEB SHOP","type":"TECHNOLOGY","description":"WebShop is an online platform where users can search for and purchase products","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"BRIGHT CITRUS DEODORANT","type":"PRODUCT","description":"Bright Citrus Deodorant is a product available in a webshop, described as natural and safe for sensitive skin, and available in a 3-ounce bottle","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"EARTH MAMA","type":"ORGANIZATION","description":"Earth Mama is a brand that produces natural and safe products for sensitive skin, including the Bright Citrus Deodorant","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"GINGER FRESH DEODORANT","type":"PRODUCT","description":"Ginger Fresh Deodorant is a product by Earth Mama, described as natural and safe for sensitive skin, and available in a 3-ounce bottle","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"BARREL AND OAK","type":"ORGANIZATION","description":"Barrel and Oak is a brand that produces aluminum-free deodorants for men, including essential oil-based scents","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"CEDAR & PATCHOULI BLEND DEODORANT","type":"PRODUCT","description":"Cedar & Patchouli Blend Deodorant is a product by Barrel and Oak, described as gentle on sensitive skin and providing 24-hour odor protection","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"MIN SUM","type":"VARIABLE","description":"Min sum is a variable used in the code snippet to store the minimum sum found during the iteration","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"CURRENT SUM","type":"VARIABLE","description":"Current sum is a variable used in the code snippet to store the sum of the current subarray being evaluated","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"NUMS","type":"VARIABLE","description":"Nums is a variable representing the list of numbers being processed in the code snippet","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"ADD FUNCTION","type":"FUNCTION","description":"The add function is a Python function that takes two integers as input and returns their sum","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"UNIT TEST","type":"DATA","description":"Unit test is a type of software testing where individual units or components of a software are tested","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"LANGUAGE AGENT TREE SEARCH","type":"TECHNOLOGY","description":"Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"ACTING","type":"PROCESS","description":"Acting is the process of performing actions based on instructions or decisions","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"REASONING","type":"PROCESS","description":"Reasoning is the process of thinking about something in a logical way to form a conclusion or judgment","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"PLANNING","type":"PROCESS","description":"Planning is the process of making plans for something, especially in the context of AI and language models","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"BRIGHT CITRUS","type":"ATTRIBUTE","description":"Bright citrus is a scent attribute of the deodorant products mentioned in the WebShop prompts","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"SENSITIVE SKIN","type":"ATTRIBUTE","description":"Sensitive skin is an attribute describing the type of skin for which the deodorant products are suitable","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"PRICE","type":"ATTRIBUTE","description":"Price is an attribute representing the cost of the deodorant products mentioned in the WebShop prompts","source_id":"785ad59c6a37896a4676ec5c1689735f"},{"name":"3 OUNCE","type":"ATTRIBUTE","description":"3 ounce is an attribute representing the size of the deodorant products mentioned in the WebShop prompts","source_id":"785ad59c6a37896a4676ec5c1689735f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"BASE ACTING\/REASONING PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Base Acting\/Reasoning Prompt is a document that provides instructions for an AI Python assistant to write a full implementation of a function based on previous implementation, unit test results, and self-reflection<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"AI PYTHON ASSISTANT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An AI Python assistant is a system designed to assist with Python programming tasks, including writing and improving code based on feedback and test results<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"UNIT TEST RESULTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Unit test results are the outcomes of tests run on a function to verify its correctness and identify any issues<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Self-reflection is the process where the AI Python assistant reviews its previous implementation and identifies issues and improvements<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"FUNCTION IMPLEMENTATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Function implementation is the process of writing the code for a function to perform a specific task<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"REFLECTION PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Reflection Prompt is a document that instructs the AI Python assistant to explain why a function implementation is wrong based on unit test results<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"TEST CASE GENERATION PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Test Case Generation Prompt is a document that instructs the AI coding assistant to write unique, diverse, and intuitive unit tests for functions given the signature and docstring<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"WEBSHOP PROMPTS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">WebShop Prompts are documents that provide instructions for interacting with a webshop, including searching for products and making decisions based on search results<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"ACTING PROMPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Acting Prompt is a document that provides instructions for performing actions in a webshop, such as searching for products and selecting items based on criteria<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"WEB SHOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebShop is an online platform where users can search for and purchase products<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Bright Citrus Deodorant is a product available in a webshop, described as natural and safe for sensitive skin, and available in a 3-ounce bottle<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"EARTH MAMA\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Earth Mama is a brand that produces natural and safe products for sensitive skin, including the Bright Citrus Deodorant<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"GINGER FRESH DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Ginger Fresh Deodorant is a product by Earth Mama, described as natural and safe for sensitive skin, and available in a 3-ounce bottle<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"BARREL AND OAK\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Barrel and Oak is a brand that produces aluminum-free deodorants for men, including essential oil-based scents<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"CEDAR &amp; PATCHOULI BLEND DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Cedar &amp; Patchouli Blend Deodorant is a product by Barrel and Oak, described as gentle on sensitive skin and providing 24-hour odor protection<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"MIN SUM\">      <data key=\"d0\">VARIABLE<\/data>      <data key=\"d1\">Min sum is a variable used in the code snippet to store the minimum sum found during the iteration<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"CURRENT SUM\">      <data key=\"d0\">VARIABLE<\/data>      <data key=\"d1\">Current sum is a variable used in the code snippet to store the sum of the current subarray being evaluated<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"NUMS\">      <data key=\"d0\">VARIABLE<\/data>      <data key=\"d1\">Nums is a variable representing the list of numbers being processed in the code snippet<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"ADD FUNCTION\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">The add function is a Python function that takes two integers as input and returns their sum<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"UNIT TEST\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Unit test is a type of software testing where individual units or components of a software are tested<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Agent Tree Search is a method that unifies reasoning, acting, and planning in language models<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"ACTING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Acting is the process of performing actions based on instructions or decisions<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Reasoning is the process of thinking about something in a logical way to form a conclusion or judgment<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"PLANNING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Planning is the process of making plans for something, especially in the context of AI and language models<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"BRIGHT CITRUS\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Bright citrus is a scent attribute of the deodorant products mentioned in the WebShop prompts<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"SENSITIVE SKIN\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Sensitive skin is an attribute describing the type of skin for which the deodorant products are suitable<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"PRICE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Price is an attribute representing the cost of the deodorant products mentioned in the WebShop prompts<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <node id=\"3 OUNCE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">3 ounce is an attribute representing the size of the deodorant products mentioned in the WebShop prompts<\/data>      <data key=\"d2\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/node>    <edge source=\"BASE ACTING\/REASONING PROMPT\" target=\"AI PYTHON ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Base Acting\/Reasoning Prompt provides instructions for the AI Python assistant to write a full implementation of a function<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"AI PYTHON ASSISTANT\" target=\"UNIT TEST RESULTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The AI Python assistant uses unit test results to verify the correctness of its function implementations<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"AI PYTHON ASSISTANT\" target=\"SELF-REFLECTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The AI Python assistant engages in self-reflection to identify issues and improvements in its previous implementations<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"AI PYTHON ASSISTANT\" target=\"FUNCTION IMPLEMENTATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The AI Python assistant is responsible for writing function implementations<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"AI PYTHON ASSISTANT\" target=\"REFLECTION PROMPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Reflection Prompt instructs the AI Python assistant to explain why a function implementation is wrong based on unit test results<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"AI PYTHON ASSISTANT\" target=\"TEST CASE GENERATION PROMPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Test Case Generation Prompt instructs the AI Python assistant to write unique, diverse, and intuitive unit tests for functions<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"UNIT TEST RESULTS\" target=\"UNIT TEST\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Unit test results are the outcomes of running unit tests on the add function<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"WEBSHOP PROMPTS\" target=\"WEB SHOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">WebShop Prompts provide instructions for interacting with the WebShop platform<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"ACTING PROMPT\" target=\"WEB SHOP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Acting Prompt provides instructions for performing actions in the WebShop<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"EARTH MAMA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Bright Citrus Deodorant is a product by Earth Mama<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"BRIGHT CITRUS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Bright citrus is the scent attribute of the Bright Citrus Deodorant<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"SENSITIVE SKIN\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Bright Citrus Deodorant is suitable for sensitive skin<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"PRICE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Bright Citrus Deodorant has a price attribute<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"3 OUNCE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Bright Citrus Deodorant is available in a 3-ounce size<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"EARTH MAMA\" target=\"GINGER FRESH DEODORANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Ginger Fresh Deodorant is a product by Earth Mama<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"GINGER FRESH DEODORANT\" target=\"SENSITIVE SKIN\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Ginger Fresh Deodorant is suitable for sensitive skin<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"GINGER FRESH DEODORANT\" target=\"PRICE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ginger Fresh Deodorant has a price attribute<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"GINGER FRESH DEODORANT\" target=\"3 OUNCE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Ginger Fresh Deodorant is available in a 3-ounce size<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"BARREL AND OAK\" target=\"CEDAR &amp; PATCHOULI BLEND DEODORANT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Cedar &amp; Patchouli Blend Deodorant is a product by Barrel and Oak<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"CEDAR &amp; PATCHOULI BLEND DEODORANT\" target=\"SENSITIVE SKIN\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Cedar &amp; Patchouli Blend Deodorant is gentle on sensitive skin<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"CEDAR &amp; PATCHOULI BLEND DEODORANT\" target=\"PRICE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cedar &amp; Patchouli Blend Deodorant has a price attribute<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"CEDAR &amp; PATCHOULI BLEND DEODORANT\" target=\"3 OUNCE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Cedar &amp; Patchouli Blend Deodorant is available in a 3-ounce size<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"MIN SUM\" target=\"CURRENT SUM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Min sum is updated based on the value of current sum during the iteration in the code snippet<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"CURRENT SUM\" target=\"NUMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Current sum is calculated by adding elements from the nums list<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"ADD FUNCTION\" target=\"UNIT TEST\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The add function is tested using unit tests to verify its correctness<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"REASONING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Language Agent Tree Search unifies reasoning in language models<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"ACTING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Language Agent Tree Search unifies acting in language models<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"PLANNING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Language Agent Tree Search unifies planning in language models<\/data>      <data key=\"d5\">785ad59c6a37896a4676ec5c1689735f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6f486e20e3102c7a285e357d356417ad","chunk":"RC1J first.]\n20Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[Prev]\nscent [assorted scents][bright citrus][calming laven-\nder][ginger fresh][simply non-scents] size [travel set (4-\npack)][3 ounce (pack of 1)][3-ounce (2-pack)] Bright Citrus\nDeodorant by Earth Mama \u2014 Natural and Safe for Sensi-\ntive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[For 3 ounce bottle of bright citrus deodorant\nfor sensitive skin, the item has options \u2018bright citrus\u2019 and \u20183\nounce (pack of 1)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).\nAction: click[Buy Now]\nG.2. Value Function Prompt\nGiven an item to purchase and a trajectory that aims to buy\nan item that exactly matches the specification, which cor-\nresponds to the ideal score of 1.0, analyze the following\ntrajectory, then at the last line conclude, \u201cThus the correct-\nness score is s\u201d, where s is an integer from 1 to 10.\nHere are some examples\nWebshop\nInstruction:i am looking for dairy free and apple variety pack of chips,\nand price lower than 30.00 dollars\n[Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next]\n[B07HRFJWP8]\nEnjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut\nFree Bars, Soy Free, Dairy Free, Non GMO, Gluten Free,\nVegan, Variety Pack, 4 Boxes (20 Bars Total)\n$100.0\n[B01KMHY5PG]\nEnjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free\nBars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total\nBars)\n$21.49\n[B008D2X8C4]\nEnjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy\nFree, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags\n(0.8 oz)\n$100.0\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.]\nObservation: OK.\nAction: click[B008D2X8C4]\nObservation:\n[Back to Search]\n[Prev]\nflavor name [dill and sour cream][garlic & parmesan][light\nsea salt][margherita pizza][thai chili lime][variety pack] size\n[0.8 ounce (pack of 24)][4 ounce (pack of 12)]\nPrice: $100.0\nRating: N.A.\n[Description]\n[Features]\n21Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\n[Reviews]\n[Buy Now]\nAction: think[The item has options \u2018variety pack\u2019 and \u20180.8\nounce (pack of 24)\u2019 and seems good to buy.]\nObservation: OK.\nAction: click[variety pack]\nObservation: You have clicked variety pack.\nAction: click[0.8 ounce (pack of 24)]\nObservation: You have clicked 0.8 ounce (pack of 24).\nAction: click[Buy Now]\nReflection: In this attempt, I was unsuccessful. I acciden-\ntally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[\u201cvariety pack of chip\u201d;]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\nThus the correctness score is 5\n(input)\nG.3. Reflection Prompt\nYou are an advanced reasoning agent that can improve based\non self-reflection. You will be given a previous reasoning\ntrial in which you were given access to a shopping website\nand a specific type of item to buy. You were given access\nto relevant context and an item to purchase. You were un-\nsuccessful in buying the correct item either because you did\nnot find an item meeting all of the required specifications\nor because you did not select the correct item. The ideal\nscore is 1.0, and anything less is incorrect. In a few sen-\ntences, Diagnose a possible reason for failure and devise a\nnew, concise, high-level plan that aims to mitigate the same\nfailure. Use complete sentences. Here are some examples:\nPrevious Trial Instruction: i am looking for dairy free and\napple variety pack of chips, and price lower than 30.00\ndollars [Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation: [Back to Search] Page 1 (Total results: 50)\n[Next >] [B07HRFJWP8] Enjoy Life Foods Soft Baked\nOvals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free,\nNon GMO, Gluten Free","chunk_id":"6f486e20e3102c7a285e357d356417ad","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"LANGUAGE AGENT TREE SEARCH","type":"TECHNOLOGY","description":"Language Agent Tree Search is a system that unifies reasoning, acting, and planning in language models","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"EARTH MAMA","type":"BRAND","description":"Earth Mama is a brand that offers natural and safe products for sensitive skin, pregnancy, and breastfeeding, including the Bright Citrus Deodorant","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"BRIGHT CITRUS DEODORANT","type":"PRODUCT","description":"Bright Citrus Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, available in a 3-ounce size","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ENJOY LIFE FOODS","type":"BRAND","description":"Enjoy Life Foods is a brand that offers various allergen-free food products, including soft baked ovals, chewy bars, and lentil chips","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS","type":"PRODUCT","description":"Dairy Free and Apple Variety Pack of Chips is a product that meets specific dietary requirements, including being dairy-free and available in a variety pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"VALUE FUNCTION PROMPT","type":"TECHNOLOGY","description":"Value Function Prompt is a system that evaluates the correctness of a trajectory in purchasing an item based on a given specification, scoring from 1 to 10","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"REFLECTION PROMPT","type":"TECHNOLOGY","description":"Reflection Prompt is a system that allows an advanced reasoning agent to improve based on self-reflection by diagnosing failures and devising new plans","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"TRAVEL SET (4-PACK)","type":"PRODUCT","description":"Travel Set (4-Pack) is a product option for the Bright Citrus Deodorant by Earth Mama, available in a set of four 3-ounce bottles","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"3 OUNCE (PACK OF 1)","type":"PRODUCT","description":"3 Ounce (Pack of 1) is a product option for the Bright Citrus Deodorant by Earth Mama, available as a single 3-ounce bottle","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"GINGER FRESH","type":"PRODUCT","description":"Ginger Fresh is a scent option for the Bright Citrus Deodorant by Earth Mama","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"CALMING LAVENDER","type":"PRODUCT","description":"Calming Lavender is a scent option for the Bright Citrus Deodorant by Earth Mama","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"SIMPLY NON-SCENTS","type":"PRODUCT","description":"Simply Non-Scents is a scent option for the Bright Citrus Deodorant by Earth Mama","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ENJOY LIFE FOODS SOFT BAKED OVALS","type":"PRODUCT","description":"Enjoy Life Foods Soft Baked Ovals are breakfast bars that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan, available in a variety pack of 4 boxes","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ENJOY LIFE SOFT BAKED CHEWY BARS","type":"PRODUCT","description":"Enjoy Life Soft Baked Chewy Bars are nut-free, soy-free, dairy-free, gluten-free bars available in a variety pack of 6 boxes","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ENJOY LIFE LENTIL CHIPS VARIETY PACK","type":"PRODUCT","description":"Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, gluten-free chips available in a pack of 24 bags","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"DILL AND SOUR CREAM","type":"PRODUCT","description":"Dill and Sour Cream is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"GARLIC & PARMESAN","type":"PRODUCT","description":"Garlic & Parmesan is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"LIGHT SEA SALT","type":"PRODUCT","description":"Light Sea Salt is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"MARGHERITA PIZZA","type":"PRODUCT","description":"Margherita Pizza is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"THAI CHILI LIME","type":"PRODUCT","description":"Thai Chili Lime is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"VARIETY PACK","type":"PRODUCT","description":"Variety Pack is a flavor option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"0.8 OUNCE (PACK OF 24)","type":"PRODUCT","description":"0.8 Ounce (Pack of 24) is a size option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"4 OUNCE (PACK OF 12)","type":"PRODUCT","description":"4 Ounce (Pack of 12) is a size option for the Enjoy Life Lentil Chips Variety Pack","source_id":"6f486e20e3102c7a285e357d356417ad"},{"name":"ASSORTED SCENTS","type":"","description":"","source_id":"6f486e20e3102c7a285e357d356417ad"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"LANGUAGE AGENT TREE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language Agent Tree Search is a system that unifies reasoning, acting, and planning in language models<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"EARTH MAMA\">      <data key=\"d0\">BRAND<\/data>      <data key=\"d1\">Earth Mama is a brand that offers natural and safe products for sensitive skin, pregnancy, and breastfeeding, including the Bright Citrus Deodorant<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Bright Citrus Deodorant by Earth Mama is a natural and safe deodorant for sensitive skin, pregnancy, and breastfeeding, available in a 3-ounce size<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS\">      <data key=\"d0\">BRAND<\/data>      <data key=\"d1\">Enjoy Life Foods is a brand that offers various allergen-free food products, including soft baked ovals, chewy bars, and lentil chips<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Dairy Free and Apple Variety Pack of Chips is a product that meets specific dietary requirements, including being dairy-free and available in a variety pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"VALUE FUNCTION PROMPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Value Function Prompt is a system that evaluates the correctness of a trajectory in purchasing an item based on a given specification, scoring from 1 to 10<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"REFLECTION PROMPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reflection Prompt is a system that allows an advanced reasoning agent to improve based on self-reflection by diagnosing failures and devising new plans<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"TRAVEL SET (4-PACK)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Travel Set (4-Pack) is a product option for the Bright Citrus Deodorant by Earth Mama, available in a set of four 3-ounce bottles<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"3 OUNCE (PACK OF 1)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">3 Ounce (Pack of 1) is a product option for the Bright Citrus Deodorant by Earth Mama, available as a single 3-ounce bottle<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"GINGER FRESH\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Ginger Fresh is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"CALMING LAVENDER\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Calming Lavender is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"SIMPLY NON-SCENTS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Simply Non-Scents is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS SOFT BAKED OVALS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Foods Soft Baked Ovals are breakfast bars that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan, available in a variety pack of 4 boxes<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ENJOY LIFE SOFT BAKED CHEWY BARS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Soft Baked Chewy Bars are nut-free, soy-free, dairy-free, gluten-free bars available in a variety pack of 6 boxes<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, gluten-free chips available in a pack of 24 bags<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"DILL AND SOUR CREAM\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Dill and Sour Cream is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"GARLIC &amp; PARMESAN\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Garlic &amp; Parmesan is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"LIGHT SEA SALT\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Light Sea Salt is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"MARGHERITA PIZZA\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Margherita Pizza is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"THAI CHILI LIME\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Thai Chili Lime is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Variety Pack is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"0.8 OUNCE (PACK OF 24)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">0.8 Ounce (Pack of 24) is a size option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"4 OUNCE (PACK OF 12)\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">4 Ounce (Pack of 12) is a size option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <node id=\"ASSORTED SCENTS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/node>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"VALUE FUNCTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Language Agent Tree Search uses the Value Function Prompt to evaluate the correctness of purchasing trajectories<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"LANGUAGE AGENT TREE SEARCH\" target=\"REFLECTION PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Language Agent Tree Search uses the Reflection Prompt to improve its reasoning and planning capabilities<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"EARTH MAMA\" target=\"BRIGHT CITRUS DEODORANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Earth Mama is the brand that produces the Bright Citrus Deodorant<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"ASSORTED SCENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Assorted Scents is a product option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"TRAVEL SET (4-PACK)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Travel Set (4-Pack) is a product option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"3 OUNCE (PACK OF 1)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">3 Ounce (Pack of 1) is a product option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"GINGER FRESH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ginger Fresh is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"CALMING LAVENDER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Calming Lavender is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"BRIGHT CITRUS DEODORANT\" target=\"SIMPLY NON-SCENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Simply Non-Scents is a scent option for the Bright Citrus Deodorant by Earth Mama<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"DAIRY FREE AND APPLE VARIETY PACK OF CHIPS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Enjoy Life Foods produces the Dairy Free and Apple Variety Pack of Chips<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"ENJOY LIFE FOODS SOFT BAKED OVALS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Foods Soft Baked Ovals is a product offered by Enjoy Life Foods<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"ENJOY LIFE SOFT BAKED CHEWY BARS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Soft Baked Chewy Bars is a product offered by Enjoy Life Foods<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS\" target=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Lentil Chips Variety Pack is a product offered by Enjoy Life Foods<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"DILL AND SOUR CREAM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dill and Sour Cream is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"GARLIC &amp; PARMESAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Garlic &amp; Parmesan is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"LIGHT SEA SALT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Light Sea Salt is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"MARGHERITA PIZZA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Margherita Pizza is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"THAI CHILI LIME\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Thai Chili Lime is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"VARIETY PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Variety Pack is a flavor option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"0.8 OUNCE (PACK OF 24)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">0.8 Ounce (Pack of 24) is a size option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"4 OUNCE (PACK OF 12)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">4 Ounce (Pack of 12) is a size option for the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">6f486e20e3102c7a285e357d356417ad<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4ed5aa10872b585d02aa2daf4ff8f7fd","chunk":" i am looking for dairy free and\napple variety pack of chips, and price lower than 30.00\ndollars [Search]\nAction: search[dairy free and apple variety pack of chips]\nObservation: [Back to Search] Page 1 (Total results: 50)\n[Next >] [B07HRFJWP8] Enjoy Life Foods Soft Baked\nOvals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free,\nNon GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes(20 Bars Total) $100.0 [B01KMHY5PG] Enjoy Life Soft\nBaked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free,\nDairy Free, Gluten Free, 6 Boxes (30 Total Bars) $21.49\n[B008D2X8C4] Enjoy Life Lentil Chips Variety Pack, Dairy\nFree Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten\nFree, 24 Bags (0.8 oz) $100.0\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.] Observation: OK.\nAction: click[B008D2X8C4] Observation: [Back to Search]\n[<Prev] flavor name [dill and sour cream][garlic & parme-\nsan][light sea salt][margherita pizza][thai chili lime][variety\npack] size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]\nPrice: $100.0 Rating: N.A. [Description] [Features] [Re-\nviews] [Buy Now]\nAction: think[The item has options \u2018variety pack\u2019 and \u20180.8\nounce (pack of 24)\u2019 and seems good to buy.] Observation:\nOK.\nAction: click[variety pack] Observation: You have clicked\nvariety pack.\nAction: click[0.8 ounce (pack of 24)] Observation: You\nhave clicked 0.8 ounce (pack of 24).\nAction: click[Buy Now]\nSTATUS: FAIL\nReflection: In this attempt, I was unsuccessful. I acciden-\ntally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[\u201cvariety pack of chips\u201d]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\nPrevious Trial Instruction: i need gluten free vegetarian\nsmoked peppered bacon - 4 ounce (pack of 2), and price\nlower than 40.00 dollars [Search]\nAction: search[gluten free vegetarian smoked peppered ba-\ncon 4 ounce pack of 2] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten, No\nMSG, Non-GMO (12 total oz.) $29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers & Sea\nSalt - Gluten-Free, Kosher, No MSG, Non-GMO (12 total\noz.) $29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) $42.99\n22Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\nAction: think[The first two items are not what I want. The\nthird item is not what I want, but it is the closest one.]\nObservation: OK.\nAction: click[B07GJTKYJQ] Observation: [Back to\nSearch] [ <Prev] Louisville Vegan Jerky - 5 Flavor Variety\nPack, Non-GMO Soy Protein, Gluten-Free (Black Pepper,\nBuffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ,\n3 Ounces) Price: $42.99 Rating: N.A. [Description] [Fea-\ntures] [Reviews] [Buy Now]\nAction: think[The price is too high and it is not what I want.]\nObservation: OK.\nAction: click[ <Prev] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten,\nNo MSG, Non-GMO (12 total oz.) 29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers &\nSea Salt - Gluten-Free, Kosher, No MSG, Non-GMO (12\ntotal oz.) 29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack,","chunk_id":"4ed5aa10872b585d02aa2daf4ff8f7fd","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":1200,"entities":[{"name":"DAIRY FREE","type":"DIETARY PREFERENCE","description":"Dairy free refers to products that do not contain any dairy ingredients","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"APPLE VARIETY PACK OF CHIPS","type":"PRODUCT","description":"A variety pack of chips that includes apple-flavored options","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"PRICE LOWER THAN 30.00 DOLLARS","type":"PRICE CONSTRAINT","description":"A budget constraint where the price of the desired product should be less than $30.00","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"ENJOY LIFE FOODS SOFT BAKED OVALS","type":"PRODUCT","description":"Enjoy Life Foods Soft Baked Ovals are breakfast bars that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan. They come in a variety pack of 4 boxes with a total of 20 bars, priced at $100.00","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"ENJOY LIFE SOFT BAKED CHEWY BARS","type":"PRODUCT","description":"Enjoy Life Soft Baked Chewy Bars are nut-free, soy-free, dairy-free, gluten-free bars that come in a variety pack of 6 boxes with a total of 30 bars, priced at $21.49","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"ENJOY LIFE LENTIL CHIPS VARIETY PACK","type":"PRODUCT","description":"Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free chips that come in 24 bags of 0.8 oz each, priced at $100.00","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"VARIETY PACK","type":"PRODUCT OPTION","description":"An option for the Enjoy Life Lentil Chips that includes multiple flavors in one pack","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"0.8 OUNCE (PACK OF 24)","type":"PRODUCT SIZE","description":"A size option for the Enjoy Life Lentil Chips that includes 24 bags of 0.8 ounces each","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"BUY NOW","type":"ACTION","description":"An action to purchase the selected product immediately","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON","type":"PRODUCT","description":"A gluten-free, vegetarian product that mimics smoked peppered bacon, available in a 4-ounce pack of 2, with a price constraint of less than $40.00","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SMOKED BACON SEA SALT 3-PACK","type":"PRODUCT","description":"A 3-pack of smoked bacon sea salt flavors including Smoked Bacon Chipotle, Smoked Bacon and Onion, and Smoked Peppered Bacon, all-natural, gluten-free, non-GMO, priced at $29.99","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SPICY HOT PEPPER SEA SALT 3-PACK","type":"PRODUCT","description":"A 3-pack of spicy hot pepper sea salt flavors including Ghost Pepper, Jalapeno, and Habanero, all-natural, gluten-free, kosher, non-GMO, priced at $29.99","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK","type":"PRODUCT","description":"Louisville Vegan Jerky 5 Flavor Variety Pack includes flavors like Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ, made from non-GMO soy protein, gluten-free, priced at $42.99","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SEARCH","type":"ACTION","description":"An action to look for products that meet specific criteria","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THINK","type":"ACTION","description":"An action to reflect on the suitability of the search results or product options","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK","type":"ACTION","description":"An action to select a specific product or option","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"REFLECTION","type":"ACTION","description":"An action to evaluate the outcome of a previous attempt and plan for future actions","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"PREVIOUS TRIAL INSTRUCTION","type":"ACTION","description":"Instructions given for a previous search attempt","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION","type":"ACTION","description":"An action to note the results or feedback from a previous action","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"STATUS: FAIL","type":"OUTCOME","description":"An outcome indicating that the attempt was unsuccessful","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"NEXT TIME","type":"FUTURE ACTION","description":"A plan for future actions to improve the search results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SEARCH RESULTS","type":"RESULT","description":"The list of products returned from a search action","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"PAGE 1","type":"RESULT","description":"The first page of search results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"TOTAL RESULTS: 50","type":"RESULT","description":"The total number of search results returned","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"NEXT >","type":"NAVIGATION","description":"An option to navigate to the next page of search results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"BACK TO SEARCH","type":"NAVIGATION","description":"An option to return to the search page","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"PREV","type":"NAVIGATION","description":"An option to navigate to the previous page of search results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"FLAVOR NAME","type":"PRODUCT ATTRIBUTE","description":"An attribute of the product indicating the available flavors","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"DILL AND SOUR CREAM","type":"FLAVOR","description":"A flavor option for the Enjoy Life Lentil Chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"GARLIC & PARMESAN","type":"FLAVOR","description":"A flavor option for the Enjoy Life Lentil Chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"LIGHT SEA SALT","type":"FLAVOR","description":"A flavor option for the Enjoy Life Lentil Chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"MARGHERITA PIZZA","type":"FLAVOR","description":"A flavor option for the Enjoy Life Lentil Chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THAI CHILI LIME","type":"FLAVOR","description":"A flavor option for the Enjoy Life Lentil Chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"DESCRIPTION","type":"PRODUCT ATTRIBUTE","description":"A section providing details about the product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"FEATURES","type":"PRODUCT ATTRIBUTE","description":"A section highlighting the key features of the product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"REVIEWS","type":"PRODUCT ATTRIBUTE","description":"A section containing customer reviews of the product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"RATING: N.A.","type":"PRODUCT ATTRIBUTE","description":"An attribute indicating that the product does not have a rating","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"ENJOY LIFE FOODS","type":"ORGANIZATION","description":"Enjoy Life Foods is the company that produces various dairy-free, gluten-free, and vegan products","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SEARCH[DAIRY FREE AND APPLE VARIETY PACK OF CHIPS]","type":"ACTION","description":"An action to search for dairy-free and apple variety pack of chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SEARCH[GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON 4 OUNCE PACK OF 2]","type":"ACTION","description":"An action to search for gluten-free vegetarian smoked peppered bacon in a 4-ounce pack of 2","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"SEARCH[\"VARIETY PACK OF CHIPS\"]","type":"ACTION","description":"An action to search for a variety pack of chips","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS THE DAIRY FREE AND APPLE VARIETY PACK OF CHIPS I WANT.]","type":"ACTION","description":"An action to reflect on the suitability of the search results, identifying the third item as the desired product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THINK[THE ITEM HAS OPTIONS \u2018VARIETY PACK\u2019 AND \u20180.8 OUNCE (PACK OF 24)\u2019 AND SEEMS GOOD TO BUY.]","type":"ACTION","description":"An action to reflect on the suitability of the product options, deciding that the item seems good to buy","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS NOT WHAT I WANT, BUT IT IS THE CLOSEST ONE.]","type":"ACTION","description":"An action to reflect on the suitability of the search results, identifying the third item as the closest match","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"THINK[THE PRICE IS TOO HIGH AND IT IS NOT WHAT I WANT.]","type":"ACTION","description":"An action to reflect on the suitability of the product, deciding that the price is too high and it is not what is wanted","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[B008D2X8C4]","type":"ACTION","description":"An action to select the product with the identifier B008D2X8C4","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[B07GJTKYJQ]","type":"ACTION","description":"An action to select the product with the identifier B07GJTKYJQ","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[<PREV]","type":"ACTION","description":"An action to navigate to the previous page of search results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[VARIETY PACK]","type":"ACTION","description":"An action to select the variety pack option for a product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[0.8 OUNCE (PACK OF 24)]","type":"ACTION","description":"An action to select the 0.8 ounce (pack of 24) option for a product","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"CLICK[BUY NOW]","type":"ACTION","description":"An action to purchase the selected product immediately","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[OK.]","type":"ACTION","description":"An action to note the results or feedback from a previous action, indicating that the action was acknowledged","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[BACK TO SEARCH]","type":"ACTION","description":"An action to note the navigation back to the search page","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[PAGE 1 (TOTAL RESULTS: 50)]","type":"RESULT","description":"An observation indicating that the search results are displayed on page 1 with a total of 50 results","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[<PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]","type":"RESULT","description":"An observation noting the details of the Louisville Vegan Jerky 5 Flavor Variety Pack","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[<PREV] SMOKED BACON SEA SALT 3-PACK: SMOKED BACON CHIPOTLE, SMOKED BACON AND ONION, SMOKED PEPPERED BACON - ALL-NATURAL SEA SALTS SLOWLY SMOKED OVER ALDERWOOD - NO GLUTEN, NO MSG, NON-GMO (12 TOTAL OZ.) 29.99 [B06Y96N1KG] SPICY HOT PEPPER SEA SALT 3-PACK: GHOST PEPPER, JALAPENO, HABANERO - ALL NATURAL, DELICIOUS BLENDS OF PEPPERS & SEA SALT - GLUTEN-FREE, KOSHER, NO MSG, NON-GMO (12 TOTAL OZ.) 29.99 [B07GJTKYJQ] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK,","type":"RESULT","description":"An observation noting the details of the Smoked Bacon Sea Salt 3-Pack and Spicy Hot Pepper Sea Salt 3-Pack","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT >] [B07HRFJWP8] ENJOY LIFE FOODS SOFT BAKED OVALS, BREAKFAST BARS, NUT FREE BARS, SOY FREE, DAIRY FREE, NON GMO, GLUTEN FREE, VEGAN, VARIETY PACK, 4 BOXES(20 BARS TOTAL) $100.0 [B01KMHY5PG] ENJOY LIFE SOFT BAKED CHEWY BARS, VARIETY PACK, NUT FREE BARS, SOY FREE, DAIRY FREE, GLUTEN FREE, 6 BOXES (30 TOTAL BARS) $21.49 [B008D2X8C4] ENJOY LIFE LENTIL CHIPS VARIETY PACK, DAIRY FREE CHIPS, SOY FREE, NUT FREE, NON GMO, VEGAN, GLUTEN FREE, 24 BAGS (0.8 OZ) $100.0","type":"RESULT","description":"An observation noting the details of the Enjoy Life Foods Soft Baked Ovals, Enjoy Life Soft Baked Chewy Bars, and Enjoy Life Lentil Chips Variety Pack","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[BACK TO SEARCH] [<PREV] FLAVOR NAME [DILL AND SOUR CREAM][GARLIC & PARMESAN][LIGHT SEA SALT][MARGHERITA PIZZA][THAI CHILI LIME][VARIETY PACK] SIZE [0.8 OUNCE (PACK OF 24)][4 OUNCE (PACK OF 12)] PRICE: $100.0 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]","type":"RESULT","description":"An observation noting the details of the Enjoy Life Lentil Chips Variety Pack, including flavor options, size options, price, and rating","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT >] [B06Y96MXJV] SMOKED BACON SEA SALT 3-PACK: SMOKED BACON CHIPOTLE, SMOKED BACON AND ONION, SMOKED PEPPERED BACON - ALL-NATURAL SEA SALTS SLOWLY SMOKED OVER ALDERWOOD - NO GLUTEN, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B06Y96N1KG] SPICY HOT PEPPER SEA SALT 3-PACK: GHOST PEPPER, JALAPENO, HABANERO - ALL NATURAL, DELICIOUS BLENDS OF PEPPERS & SEA SALT - GLUTEN-FREE, KOSHER, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B07GJTKYJQ] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) $42.99","type":"RESULT","description":"An observation noting the details of the Smoked Bacon Sea Salt 3-Pack, Spicy Hot Pepper Sea Salt 3-Pack, and Louisville Vegan Jerky 5 Flavor Variety Pack","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"},{"name":"OBSERVATION[BACK TO SEARCH] [<PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]","type":"","description":"","source_id":"4ed5aa10872b585d02aa2daf4ff8f7fd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DAIRY FREE\">      <data key=\"d0\">DIETARY PREFERENCE<\/data>      <data key=\"d1\">Dairy free refers to products that do not contain any dairy ingredients<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"APPLE VARIETY PACK OF CHIPS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A variety pack of chips that includes apple-flavored options<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"PRICE LOWER THAN 30.00 DOLLARS\">      <data key=\"d0\">PRICE CONSTRAINT<\/data>      <data key=\"d1\">A budget constraint where the price of the desired product should be less than $30.00<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS SOFT BAKED OVALS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Foods Soft Baked Ovals are breakfast bars that are nut-free, soy-free, dairy-free, non-GMO, gluten-free, and vegan. They come in a variety pack of 4 boxes with a total of 20 bars, priced at $100.00<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"ENJOY LIFE SOFT BAKED CHEWY BARS\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Soft Baked Chewy Bars are nut-free, soy-free, dairy-free, gluten-free bars that come in a variety pack of 6 boxes with a total of 30 bars, priced at $21.49<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Enjoy Life Lentil Chips Variety Pack are dairy-free, soy-free, nut-free, non-GMO, vegan, and gluten-free chips that come in 24 bags of 0.8 oz each, priced at $100.00<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"VARIETY PACK\">      <data key=\"d0\">PRODUCT OPTION<\/data>      <data key=\"d1\">An option for the Enjoy Life Lentil Chips that includes multiple flavors in one pack<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"0.8 OUNCE (PACK OF 24)\">      <data key=\"d0\">PRODUCT SIZE<\/data>      <data key=\"d1\">A size option for the Enjoy Life Lentil Chips that includes 24 bags of 0.8 ounces each<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"BUY NOW\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to purchase the selected product immediately<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A gluten-free, vegetarian product that mimics smoked peppered bacon, available in a 4-ounce pack of 2, with a price constraint of less than $40.00<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SMOKED BACON SEA SALT 3-PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A 3-pack of smoked bacon sea salt flavors including Smoked Bacon Chipotle, Smoked Bacon and Onion, and Smoked Peppered Bacon, all-natural, gluten-free, non-GMO, priced at $29.99<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A 3-pack of spicy hot pepper sea salt flavors including Ghost Pepper, Jalapeno, and Habanero, all-natural, gluten-free, kosher, non-GMO, priced at $29.99<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Louisville Vegan Jerky 5 Flavor Variety Pack includes flavors like Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ, made from non-GMO soy protein, gluten-free, priced at $42.99<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to look for products that meet specific criteria<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THINK\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to reflect on the suitability of the search results or product options<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to select a specific product or option<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to evaluate the outcome of a previous attempt and plan for future actions<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"PREVIOUS TRIAL INSTRUCTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Instructions given for a previous search attempt<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to note the results or feedback from a previous action<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"STATUS: FAIL\">      <data key=\"d0\">OUTCOME<\/data>      <data key=\"d1\">An outcome indicating that the attempt was unsuccessful<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"NEXT TIME\">      <data key=\"d0\">FUTURE ACTION<\/data>      <data key=\"d1\">A plan for future actions to improve the search results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SEARCH RESULTS\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">The list of products returned from a search action<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"PAGE 1\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">The first page of search results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"TOTAL RESULTS: 50\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">The total number of search results returned<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"NEXT &gt;\">      <data key=\"d0\">NAVIGATION<\/data>      <data key=\"d1\">An option to navigate to the next page of search results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"BACK TO SEARCH\">      <data key=\"d0\">NAVIGATION<\/data>      <data key=\"d1\">An option to return to the search page<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"PREV\">      <data key=\"d0\">NAVIGATION<\/data>      <data key=\"d1\">An option to navigate to the previous page of search results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"FLAVOR NAME\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">An attribute of the product indicating the available flavors<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"DILL AND SOUR CREAM\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">A flavor option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"GARLIC &amp; PARMESAN\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">A flavor option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"LIGHT SEA SALT\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">A flavor option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"MARGHERITA PIZZA\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">A flavor option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THAI CHILI LIME\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">A flavor option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"DESCRIPTION\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">A section providing details about the product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"FEATURES\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">A section highlighting the key features of the product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"REVIEWS\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">A section containing customer reviews of the product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"RATING: N.A.\">      <data key=\"d0\">PRODUCT ATTRIBUTE<\/data>      <data key=\"d1\">An attribute indicating that the product does not have a rating<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"ENJOY LIFE FOODS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Enjoy Life Foods is the company that produces various dairy-free, gluten-free, and vegan products<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SEARCH[DAIRY FREE AND APPLE VARIETY PACK OF CHIPS]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to search for dairy-free and apple variety pack of chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SEARCH[GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON 4 OUNCE PACK OF 2]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to search for gluten-free vegetarian smoked peppered bacon in a 4-ounce pack of 2<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"SEARCH[&quot;VARIETY PACK OF CHIPS&quot;]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to search for a variety pack of chips<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS THE DAIRY FREE AND APPLE VARIETY PACK OF CHIPS I WANT.]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to reflect on the suitability of the search results, identifying the third item as the desired product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THINK[THE ITEM HAS OPTIONS &#8216;VARIETY PACK&#8217; AND &#8216;0.8 OUNCE (PACK OF 24)&#8217; AND SEEMS GOOD TO BUY.]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to reflect on the suitability of the product options, deciding that the item seems good to buy<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS NOT WHAT I WANT, BUT IT IS THE CLOSEST ONE.]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to reflect on the suitability of the search results, identifying the third item as the closest match<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"THINK[THE PRICE IS TOO HIGH AND IT IS NOT WHAT I WANT.]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to reflect on the suitability of the product, deciding that the price is too high and it is not what is wanted<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[B008D2X8C4]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to select the product with the identifier B008D2X8C4<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[B07GJTKYJQ]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to select the product with the identifier B07GJTKYJQ<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[&lt;PREV]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to navigate to the previous page of search results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[VARIETY PACK]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to select the variety pack option for a product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[0.8 OUNCE (PACK OF 24)]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to select the 0.8 ounce (pack of 24) option for a product<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"CLICK[BUY NOW]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to purchase the selected product immediately<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[OK.]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to note the results or feedback from a previous action, indicating that the action was acknowledged<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[BACK TO SEARCH]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">An action to note the navigation back to the search page<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[PAGE 1 (TOTAL RESULTS: 50)]\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation indicating that the search results are displayed on page 1 with a total of 50 results<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[&lt;PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation noting the details of the Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[&lt;PREV] SMOKED BACON SEA SALT 3-PACK: SMOKED BACON CHIPOTLE, SMOKED BACON AND ONION, SMOKED PEPPERED BACON - ALL-NATURAL SEA SALTS SLOWLY SMOKED OVER ALDERWOOD - NO GLUTEN, NO MSG, NON-GMO (12 TOTAL OZ.) 29.99 [B06Y96N1KG] SPICY HOT PEPPER SEA SALT 3-PACK: GHOST PEPPER, JALAPENO, HABANERO - ALL NATURAL, DELICIOUS BLENDS OF PEPPERS &amp; SEA SALT - GLUTEN-FREE, KOSHER, NO MSG, NON-GMO (12 TOTAL OZ.) 29.99 [B07GJTKYJQ] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK,\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation noting the details of the Smoked Bacon Sea Salt 3-Pack and Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT &gt;] [B07HRFJWP8] ENJOY LIFE FOODS SOFT BAKED OVALS, BREAKFAST BARS, NUT FREE BARS, SOY FREE, DAIRY FREE, NON GMO, GLUTEN FREE, VEGAN, VARIETY PACK, 4 BOXES(20 BARS TOTAL) $100.0 [B01KMHY5PG] ENJOY LIFE SOFT BAKED CHEWY BARS, VARIETY PACK, NUT FREE BARS, SOY FREE, DAIRY FREE, GLUTEN FREE, 6 BOXES (30 TOTAL BARS) $21.49 [B008D2X8C4] ENJOY LIFE LENTIL CHIPS VARIETY PACK, DAIRY FREE CHIPS, SOY FREE, NUT FREE, NON GMO, VEGAN, GLUTEN FREE, 24 BAGS (0.8 OZ) $100.0\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation noting the details of the Enjoy Life Foods Soft Baked Ovals, Enjoy Life Soft Baked Chewy Bars, and Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[BACK TO SEARCH] [&lt;PREV] FLAVOR NAME [DILL AND SOUR CREAM][GARLIC &amp; PARMESAN][LIGHT SEA SALT][MARGHERITA PIZZA][THAI CHILI LIME][VARIETY PACK] SIZE [0.8 OUNCE (PACK OF 24)][4 OUNCE (PACK OF 12)] PRICE: $100.0 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation noting the details of the Enjoy Life Lentil Chips Variety Pack, including flavor options, size options, price, and rating<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT &gt;] [B06Y96MXJV] SMOKED BACON SEA SALT 3-PACK: SMOKED BACON CHIPOTLE, SMOKED BACON AND ONION, SMOKED PEPPERED BACON - ALL-NATURAL SEA SALTS SLOWLY SMOKED OVER ALDERWOOD - NO GLUTEN, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B06Y96N1KG] SPICY HOT PEPPER SEA SALT 3-PACK: GHOST PEPPER, JALAPENO, HABANERO - ALL NATURAL, DELICIOUS BLENDS OF PEPPERS &amp; SEA SALT - GLUTEN-FREE, KOSHER, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B07GJTKYJQ] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) $42.99\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">An observation noting the details of the Smoked Bacon Sea Salt 3-Pack, Spicy Hot Pepper Sea Salt 3-Pack, and Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <node id=\"OBSERVATION[BACK TO SEARCH] [&lt;PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/node>    <edge source=\"DAIRY FREE\" target=\"ENJOY LIFE FOODS SOFT BAKED OVALS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Enjoy Life Foods Soft Baked Ovals are dairy-free<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"DAIRY FREE\" target=\"ENJOY LIFE SOFT BAKED CHEWY BARS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Enjoy Life Soft Baked Chewy Bars are dairy-free<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"DAIRY FREE\" target=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack is dairy-free<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"DAIRY FREE\" target=\"SEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The search action includes a criterion for dairy-free products<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"APPLE VARIETY PACK OF CHIPS\" target=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack includes apple-flavored chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"APPLE VARIETY PACK OF CHIPS\" target=\"SEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The search action includes a criterion for an apple variety pack of chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"PRICE LOWER THAN 30.00 DOLLARS\" target=\"ENJOY LIFE SOFT BAKED CHEWY BARS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Enjoy Life Soft Baked Chewy Bars are priced at $21.49, which is within the budget constraint<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"PRICE LOWER THAN 30.00 DOLLARS\" target=\"SEARCH\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The search action includes a budget constraint of less than $30.00<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS SOFT BAKED OVALS\" target=\"THINK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user reflects that the Enjoy Life Foods Soft Baked Ovals are not what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE FOODS SOFT BAKED OVALS\" target=\"ENJOY LIFE FOODS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Foods is the company that produces the Soft Baked Ovals<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE SOFT BAKED CHEWY BARS\" target=\"THINK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user reflects that the Enjoy Life Soft Baked Chewy Bars are not what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE SOFT BAKED CHEWY BARS\" target=\"ENJOY LIFE FOODS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Foods is the company that produces the Soft Baked Chewy Bars<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"THINK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user reflects that the Enjoy Life Lentil Chips Variety Pack is what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"CLICK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user clicks on the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"FLAVOR NAME\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack has multiple flavor options<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"DESCRIPTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack has a description section<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"FEATURES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack has a features section<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"REVIEWS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack has a reviews section<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"RATING: N.A.\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Enjoy Life Lentil Chips Variety Pack does not have a rating<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"ENJOY LIFE FOODS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Enjoy Life Foods is the company that produces the Lentil Chips Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS THE DAIRY FREE AND APPLE VARIETY PACK OF CHIPS I WANT.]\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user reflects that the Enjoy Life Lentil Chips Variety Pack is the desired product<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"THINK[THE ITEM HAS OPTIONS &#8216;VARIETY PACK&#8217; AND &#8216;0.8 OUNCE (PACK OF 24)&#8217; AND SEEMS GOOD TO BUY.]\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user reflects that the Enjoy Life Lentil Chips Variety Pack seems good to buy<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"ENJOY LIFE LENTIL CHIPS VARIETY PACK\" target=\"CLICK[B008D2X8C4]\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user clicks on the Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"VARIETY PACK\" target=\"CLICK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user clicks on the variety pack option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"VARIETY PACK\" target=\"CLICK[VARIETY PACK]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user clicks on the variety pack option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"0.8 OUNCE (PACK OF 24)\" target=\"CLICK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user clicks on the 0.8 ounce (pack of 24) option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"0.8 OUNCE (PACK OF 24)\" target=\"CLICK[0.8 OUNCE (PACK OF 24)]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user clicks on the 0.8 ounce (pack of 24) option for the Enjoy Life Lentil Chips<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"BUY NOW\" target=\"CLICK\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The user clicks on the Buy Now option to purchase the product<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"BUY NOW\" target=\"CLICK[BUY NOW]\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The user clicks on the Buy Now option to purchase the product<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"SMOKED BACON SEA SALT 3-PACK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Smoked Bacon Sea Salt 3-Pack is a potential alternative to the gluten-free vegetarian smoked peppered bacon<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is a potential alternative to the gluten-free vegetarian smoked peppered bacon<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON\" target=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The Louisville Vegan Jerky 5 Flavor Variety Pack is a potential alternative to the gluten-free vegetarian smoked peppered bacon<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SMOKED BACON SEA SALT 3-PACK\" target=\"THINK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user reflects that the Smoked Bacon Sea Salt 3-Pack is not what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SMOKED BACON SEA SALT 3-PACK\" target=\"OBSERVATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the details of the Smoked Bacon Sea Salt 3-Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"THINK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user reflects that the Spicy Hot Pepper Sea Salt 3-Pack is not what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"OBSERVATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The observation action notes the details of the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"THINK\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user reflects that the Louisville Vegan Jerky 5 Flavor Variety Pack is not what they want<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"CLICK\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The user clicks on the Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"OBSERVATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the details of the Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS NOT WHAT I WANT, BUT IT IS THE CLOSEST ONE.]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user reflects that the Louisville Vegan Jerky 5 Flavor Variety Pack is the closest match<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"THINK[THE PRICE IS TOO HIGH AND IT IS NOT WHAT I WANT.]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user reflects that the price of the Louisville Vegan Jerky 5 Flavor Variety Pack is too high and it is not what is wanted<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"CLICK[B07GJTKYJQ]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user clicks on the Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"OBSERVATION[&lt;PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the details of the Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH\" target=\"NEXT TIME\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user plans to refine their search criteria next time<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH\" target=\"PREVIOUS TRIAL INSTRUCTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The previous trial instruction includes a search action<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"CLICK\" target=\"PREV\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The user clicks on the Prev option to navigate back to the previous page of search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"REFLECTION\" target=\"STATUS: FAIL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The user reflects on the unsuccessful attempt<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"OBSERVATION\" target=\"SEARCH RESULTS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"NEXT TIME\" target=\"SEARCH[&quot;VARIETY PACK OF CHIPS&quot;]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user plans to refine their search criteria next time by searching for \"variety pack of chips\"<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"PAGE 1\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The search results are displayed on page 1<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"TOTAL RESULTS: 50\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The search results include a total of 50 results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"NEXT &gt;\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Next option allows navigation to the next page of search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"BACK TO SEARCH\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Back to Search option allows navigation back to the search page<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"PREV\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The Prev option allows navigation to the previous page of search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"OBSERVATION[BACK TO SEARCH]\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The observation action notes the navigation back to the search page<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH RESULTS\" target=\"OBSERVATION[PAGE 1 (TOTAL RESULTS: 50)]\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">The observation action notes that the search results are displayed on page 1 with a total of 50 results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH[DAIRY FREE AND APPLE VARIETY PACK OF CHIPS]\" target=\"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT &gt;] [B07HRFJWP8] ENJOY LIFE FOODS SOFT BAKED OVALS, BREAKFAST BARS, NUT FREE BARS, SOY FREE, DAIRY FREE, NON GMO, GLUTEN FREE, VEGAN, VARIETY PACK, 4 BOXES(20 BARS TOTAL) $100.0 [B01KMHY5PG] ENJOY LIFE SOFT BAKED CHEWY BARS, VARIETY PACK, NUT FREE BARS, SOY FREE, DAIRY FREE, GLUTEN FREE, 6 BOXES (30 TOTAL BARS) $21.49 [B008D2X8C4] ENJOY LIFE LENTIL CHIPS VARIETY PACK, DAIRY FREE CHIPS, SOY FREE, NUT FREE, NON GMO, VEGAN, GLUTEN FREE, 24 BAGS (0.8 OZ) $100.0\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The search action for dairy-free and apple variety pack of chips resulted in a list of products including Enjoy Life Foods Soft Baked Ovals, Enjoy Life Soft Baked Chewy Bars, and Enjoy Life Lentil Chips Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"SEARCH[GLUTEN FREE VEGETARIAN SMOKED PEPPERED BACON 4 OUNCE PACK OF 2]\" target=\"OBSERVATION[BACK TO SEARCH] PAGE 1 (TOTAL RESULTS: 50) [NEXT &gt;] [B06Y96MXJV] SMOKED BACON SEA SALT 3-PACK: SMOKED BACON CHIPOTLE, SMOKED BACON AND ONION, SMOKED PEPPERED BACON - ALL-NATURAL SEA SALTS SLOWLY SMOKED OVER ALDERWOOD - NO GLUTEN, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B06Y96N1KG] SPICY HOT PEPPER SEA SALT 3-PACK: GHOST PEPPER, JALAPENO, HABANERO - ALL NATURAL, DELICIOUS BLENDS OF PEPPERS &amp; SEA SALT - GLUTEN-FREE, KOSHER, NO MSG, NON-GMO (12 TOTAL OZ.) $29.99 [B07GJTKYJQ] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) $42.99\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The search action for gluten-free vegetarian smoked peppered bacon resulted in a list of products including Smoked Bacon Sea Salt 3-Pack, Spicy Hot Pepper Sea Salt 3-Pack, and Louisville Vegan Jerky 5 Flavor Variety Pack<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS THE DAIRY FREE AND APPLE VARIETY PACK OF CHIPS I WANT.]\" target=\"OBSERVATION[OK.]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the user's reflection on the suitability of the search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"THINK[THE ITEM HAS OPTIONS &#8216;VARIETY PACK&#8217; AND &#8216;0.8 OUNCE (PACK OF 24)&#8217; AND SEEMS GOOD TO BUY.]\" target=\"OBSERVATION[OK.]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the user's reflection on the suitability of the product options<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"THINK[THE FIRST TWO ITEMS ARE NOT WHAT I WANT. THE THIRD ITEM IS NOT WHAT I WANT, BUT IT IS THE CLOSEST ONE.]\" target=\"OBSERVATION[OK.]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the user's reflection on the suitability of the search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"THINK[THE PRICE IS TOO HIGH AND IT IS NOT WHAT I WANT.]\" target=\"OBSERVATION[OK.]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The observation action notes the user's reflection on the suitability of the product<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>    <edge source=\"CLICK[&lt;PREV]\" target=\"OBSERVATION[BACK TO SEARCH] [&lt;PREV] LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK, NON-GMO SOY PROTEIN, GLUTEN-FREE (BLACK PEPPER, BUFFALO DILL, PEPPERONI, MAPLE BACON, AND CAROLINA BBQ, 3 OUNCES) PRICE: $42.99 RATING: N.A. [DESCRIPTION] [FEATURES] [REVIEWS] [BUY NOW]\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The user clicks on the Prev option to navigate back to the previous page of search results<\/data>      <data key=\"d5\">4ed5aa10872b585d02aa2daf4ff8f7fd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"5d356b8ff719763a38cecff22c4e17b7","chunk":" Non-GMO (12 total oz.) 29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers &\nSea Salt - Gluten-Free, Kosher, No MSG, Non-GMO (12\ntotal oz.) 29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) 42.99\nAction: think[The first two items are not what I want.]\nObservation: OK.\nAction: click[Next >] Observation: Invalid action!\nAction: think[I need to refine my search.] Observation: OK.\nAction: click[ <Back to Search] Observation: Invalid ac-\ntion!\nAction: click[Back to Search]\nSTATUS: FAIL\nReflection: In this attempt, I was unsuccessful. The initial\nsearch results were not the products that I was looking for.\nNext time, I will do search[\u201cvegetarian bacon\u201d] for a broader\nsearch. I will check that the new results will fulfill the gluten\nfree and 4 ounce pack of 2 constraints. I will continue to\nrefine my searches so that I can find more products.\nPrevious trial: trajectory Reflection:\u201d\u2019\n23","chunk_id":"5d356b8ff719763a38cecff22c4e17b7","document_ids":["5597680041a0eb518b5a1bd4fee55c60"],"n_tokens":309,"entities":[{"name":"NON-GMO","type":"ATTRIBUTE","description":"Non-GMO refers to products that are not genetically modified","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"SPICY HOT PEPPER SEA SALT 3-PACK","type":"PRODUCT","description":"A 3-pack of Spicy Hot Pepper Sea Salt that includes Ghost Pepper, Jalapeno, and Habanero flavors. It is all-natural, gluten-free, kosher, and non-GMO","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"GHOST PEPPER","type":"INGREDIENT","description":"Ghost Pepper is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"JALAPENO","type":"INGREDIENT","description":"Jalapeno is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"HABANERO","type":"INGREDIENT","description":"Habanero is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"LOUISVILLE VEGAN JERKY","type":"PRODUCT","description":"Louisville Vegan Jerky is a 5-flavor variety pack of vegan jerky that is non-GMO, soy protein-based, and gluten-free. The flavors include Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"BLACK PEPPER","type":"FLAVOR","description":"Black Pepper is one of the flavors included in the Louisville Vegan Jerky variety pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"BUFFALO DILL","type":"FLAVOR","description":"Buffalo Dill is one of the flavors included in the Louisville Vegan Jerky variety pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"PEPPERONI","type":"FLAVOR","description":"Pepperoni is one of the flavors included in the Louisville Vegan Jerky variety pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"MAPLE BACON","type":"FLAVOR","description":"Maple Bacon is one of the flavors included in the Louisville Vegan Jerky variety pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"CAROLINA BBQ","type":"FLAVOR","description":"Carolina BBQ is one of the flavors included in the Louisville Vegan Jerky variety pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"SEARCH","type":"ACTION","description":"Search is an action taken to find specific products or information","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"CLICK","type":"ACTION","description":"Click is an action taken to select or navigate to a different page or item","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"REFINE SEARCH","type":"ACTION","description":"Refine Search is an action taken to narrow down search results to find more relevant products","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"VEGETARIAN BACON","type":"PRODUCT","description":"Vegetarian Bacon is a product that the user intends to search for in the next attempt","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"GLUTEN-FREE","type":"ATTRIBUTE","description":"Gluten-Free refers to products that do not contain gluten","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"4 OUNCE PACK OF 2","type":"ATTRIBUTE","description":"4 Ounce Pack of 2 refers to the packaging size and quantity constraint for the product the user is searching for","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"B07GJTKYJQ","type":"PRODUCT CODE","description":"B07GJTKYJQ is the product code for the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"12 TOTAL OZ.","type":"ATTRIBUTE","description":"12 total oz. refers to the total weight of the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"29.99","type":"PRICE","description":"29.99 is the price of the Spicy Hot Pepper Sea Salt 3-Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK","type":"PRODUCT","description":"Louisville Vegan Jerky - 5 Flavor Variety Pack is a product that includes five different flavors of vegan jerky","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"NON-GMO SOY PROTEIN","type":"ATTRIBUTE","description":"Non-GMO Soy Protein is an attribute of the Louisville Vegan Jerky - 5 Flavor Variety Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"42.99","type":"PRICE","description":"42.99 is the price of the Louisville Vegan Jerky - 5 Flavor Variety Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"3 OUNCES","type":"ATTRIBUTE","description":"3 ounces refers to the weight of each pack in the Louisville Vegan Jerky - 5 Flavor Variety Pack","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"ACTION","type":"ACTION","description":"Action refers to the steps taken by the user during the search process","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"THINK","type":"ACTION","description":"Think is an action where the user reflects on the search results and decides on the next steps","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"OBSERVATION","type":"ACTION","description":"Observation is an action where the user notes the outcome of their previous action","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"CLICK[NEXT >]","type":"ACTION","description":"Click[Next >] is an action where the user attempts to navigate to the next page of search results","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"CLICK[ <BACK TO SEARCH]","type":"ACTION","description":"Click[ <Back to Search] is an action where the user attempts to return to the search page","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"STATUS: FAIL","type":"RESULT","description":"Status: Fail indicates that the user's attempt was unsuccessful","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"REFLECTION","type":"ACTION","description":"Reflection is an action where the user considers what went wrong and plans for future attempts","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"PREVIOUS TRIAL","type":"EVENT","description":"Previous trial refers to the user's earlier attempt at searching for products","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"TRAJECTORY","type":"EVENT","description":"Trajectory refers to the path or sequence of actions taken by the user during the search process","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"KOSHER","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"NO MSG","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"B06Y96N1KG","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"RESULT","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"},{"name":"EVENT","type":"","description":"","source_id":"5d356b8ff719763a38cecff22c4e17b7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NON-GMO\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Non-GMO refers to products that are not genetically modified<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">A 3-pack of Spicy Hot Pepper Sea Salt that includes Ghost Pepper, Jalapeno, and Habanero flavors. It is all-natural, gluten-free, kosher, and non-GMO<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"GHOST PEPPER\">      <data key=\"d0\">INGREDIENT<\/data>      <data key=\"d1\">Ghost Pepper is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"JALAPENO\">      <data key=\"d0\">INGREDIENT<\/data>      <data key=\"d1\">Jalapeno is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"HABANERO\">      <data key=\"d0\">INGREDIENT<\/data>      <data key=\"d1\">Habanero is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"LOUISVILLE VEGAN JERKY\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Louisville Vegan Jerky is a 5-flavor variety pack of vegan jerky that is non-GMO, soy protein-based, and gluten-free. The flavors include Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"BLACK PEPPER\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">Black Pepper is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"BUFFALO DILL\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">Buffalo Dill is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"PEPPERONI\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">Pepperoni is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"MAPLE BACON\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">Maple Bacon is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"CAROLINA BBQ\">      <data key=\"d0\">FLAVOR<\/data>      <data key=\"d1\">Carolina BBQ is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Search is an action taken to find specific products or information<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"CLICK\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Click is an action taken to select or navigate to a different page or item<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"REFINE SEARCH\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Refine Search is an action taken to narrow down search results to find more relevant products<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"VEGETARIAN BACON\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Vegetarian Bacon is a product that the user intends to search for in the next attempt<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"GLUTEN-FREE\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Gluten-Free refers to products that do not contain gluten<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"4 OUNCE PACK OF 2\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">4 Ounce Pack of 2 refers to the packaging size and quantity constraint for the product the user is searching for<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"B07GJTKYJQ\">      <data key=\"d0\">PRODUCT CODE<\/data>      <data key=\"d1\">B07GJTKYJQ is the product code for the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"12 TOTAL OZ.\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">12 total oz. refers to the total weight of the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"29.99\">      <data key=\"d0\">PRICE<\/data>      <data key=\"d1\">29.99 is the price of the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\">      <data key=\"d0\">PRODUCT<\/data>      <data key=\"d1\">Louisville Vegan Jerky - 5 Flavor Variety Pack is a product that includes five different flavors of vegan jerky<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"NON-GMO SOY PROTEIN\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">Non-GMO Soy Protein is an attribute of the Louisville Vegan Jerky - 5 Flavor Variety Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"42.99\">      <data key=\"d0\">PRICE<\/data>      <data key=\"d1\">42.99 is the price of the Louisville Vegan Jerky - 5 Flavor Variety Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"3 OUNCES\">      <data key=\"d0\">ATTRIBUTE<\/data>      <data key=\"d1\">3 ounces refers to the weight of each pack in the Louisville Vegan Jerky - 5 Flavor Variety Pack<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"ACTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Action refers to the steps taken by the user during the search process<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"THINK\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Think is an action where the user reflects on the search results and decides on the next steps<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"OBSERVATION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Observation is an action where the user notes the outcome of their previous action<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"CLICK[NEXT &gt;]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Click[Next &gt;] is an action where the user attempts to navigate to the next page of search results<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"CLICK[ &lt;BACK TO SEARCH]\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Click[ &lt;Back to Search] is an action where the user attempts to return to the search page<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"STATUS: FAIL\">      <data key=\"d0\">RESULT<\/data>      <data key=\"d1\">Status: Fail indicates that the user's attempt was unsuccessful<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">ACTION<\/data>      <data key=\"d1\">Reflection is an action where the user considers what went wrong and plans for future attempts<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"PREVIOUS TRIAL\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Previous trial refers to the user's earlier attempt at searching for products<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"TRAJECTORY\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Trajectory refers to the path or sequence of actions taken by the user during the search process<\/data>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"KOSHER\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"NO MSG\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"B06Y96N1KG\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"RESULT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <node id=\"EVENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/node>    <edge source=\"NON-GMO\" target=\"SPICY HOT PEPPER SEA SALT 3-PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is labeled as Non-GMO<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"NON-GMO\" target=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Louisville Vegan Jerky - 5 Flavor Variety Pack is labeled as Non-GMO<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"NON-GMO\" target=\"B06Y96N1KG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The product with code B06Y96N1KG is labeled as Non-GMO<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"GHOST PEPPER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Ghost Pepper is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"JALAPENO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jalapeno is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"HABANERO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Habanero is one of the flavors included in the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"GLUTEN-FREE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is labeled as Gluten-Free<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"KOSHER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is labeled as Kosher<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"NO MSG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is labeled as No MSG<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"B07GJTKYJQ\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The product with code B07GJTKYJQ is the Spicy Hot Pepper Sea Salt 3-Pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"12 TOTAL OZ.\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack weighs 12 total ounces<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SPICY HOT PEPPER SEA SALT 3-PACK\" target=\"29.99\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Spicy Hot Pepper Sea Salt 3-Pack is priced at 29.99<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"BLACK PEPPER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Black Pepper is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"BUFFALO DILL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Buffalo Dill is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"PEPPERONI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Pepperoni is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"MAPLE BACON\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Maple Bacon is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY\" target=\"CAROLINA BBQ\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Carolina BBQ is one of the flavors included in the Louisville Vegan Jerky variety pack<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SEARCH\" target=\"CLICK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Click is an action that can be taken after a search to select or navigate to a different page or item<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"SEARCH\" target=\"REFINE SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Refine Search is an action taken to narrow down search results to find more relevant products<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"VEGETARIAN BACON\" target=\"GLUTEN-FREE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Vegetarian Bacon is a product that the user intends to search for, and it must fulfill the gluten-free constraint<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"VEGETARIAN BACON\" target=\"4 OUNCE PACK OF 2\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Vegetarian Bacon is a product that the user intends to search for, and it must fulfill the 4 ounce pack of 2 constraint<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"GLUTEN-FREE\" target=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Louisville Vegan Jerky - 5 Flavor Variety Pack is labeled as Gluten-Free<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"42.99\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Louisville Vegan Jerky - 5 Flavor Variety Pack is priced at 42.99<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"3 OUNCES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Each pack in the Louisville Vegan Jerky - 5 Flavor Variety Pack weighs 3 ounces<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"LOUISVILLE VEGAN JERKY - 5 FLAVOR VARIETY PACK\" target=\"NON-GMO SOY PROTEIN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Louisville Vegan Jerky - 5 Flavor Variety Pack is made with Non-GMO Soy Protein<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"THINK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Think is an action taken by the user during the search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"OBSERVATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Observation is an action taken by the user during the search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"CLICK[NEXT &gt;]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Click[Next &gt;] is an action taken by the user during the search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"CLICK[ &lt;BACK TO SEARCH]\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Click[ &lt;Back to Search] is an action taken by the user during the search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"ACTION\" target=\"REFLECTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Reflection is an action taken by the user during the search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"STATUS: FAIL\" target=\"RESULT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Status: Fail is the result of the user's search attempt<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"PREVIOUS TRIAL\" target=\"EVENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Previous trial is an event in the user's search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>    <edge source=\"TRAJECTORY\" target=\"EVENT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Trajectory is an event in the user's search process<\/data>      <data key=\"d5\">5d356b8ff719763a38cecff22c4e17b7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"c3d0436082aada237ee4bee645f16059","chunk":"2024-8-19\nAutomated Design of Agentic Systems\nShengran Hu1,2, Cong Lu1,2and Jeff Clune1,2,3\n1University of British Columbia,2Vector Institute,3Canada CIFAR AI Chair\nResearchers are investing substantial effort in developing powerful general-purpose agents, wherein\nFoundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection,\nToolformer). However, the history of machine learning teaches us that hand-designed solutions are\neventually replaced by learned solutions. We formulate a new research area, Automated Design of\nAgentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including\ninventing novel building blocks and\/or combining them in new ways. We further demonstrate that there\nis an unexplored yet promising approach within ADAS where agents can be defined in code and new\nagents can be automatically discovered by a meta agent programming ever better ones in code. Given\nthat programming languages are Turing Complete, this approach theoretically enables the learning\nofany possible agentic system: including novel prompts, tool use, control flows, and combinations\nthereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea,\nwhere a meta agent iteratively programs interesting new agents based on an ever-growing archive of\nprevious discoveries. Through extensive experiments across multiple domains including coding, science,\nand math, we show that our algorithm can progressively invent agents with novel designs that greatly\noutperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising\nresult that agents invented by Meta Agent Search maintain superior performance even when transferred\nacross domains and models, demonstrating their robustness and generality. Provided we develop it\nsafely, our work illustrates the potential of an exciting new research direction toward automatically\ndesigning ever-more powerful agentic systems to benefit humanity.\n\/githubhttps:\/\/github.com\/ShengranHu\/ADAS\n1. Introduction\nFoundation Models (FMs) such as GPT (OpenAI, 2022, 2024) and Claude (Anthropic, 2024b) are\nquicklybeingadoptedaspowerfulgeneral-purposeagentsforagentictasksthatneedflexiblereasoning\nand planning (Wang et al., 2024). Despite recent advancements in FMs, solving problems reliably\noften requires an agent to be a compound agentic system with multiple components instead of a\nmonolithic model query (Rockt\u00e4schel, 2024; Zaharia et al., 2024). Additionally, to enable agents to\nsolve complex real-world tasks, they often need access to external tools such as search engines, code\nexecution, and database queries. As a result, many effective building blocks of agentic systems have\nbeen proposed, such as chain-of-thought planning and reasoning (Hu & Clune, 2024; Wei et al., 2022;\nYao et al., 2023), memory structures (Lewis et al., 2020; Zhang et al., 2024c), tool use (Qu et al.,\n2024; Schick et al., 2023), and self-reflection (Madaan et al., 2024; Shinn et al., 2023). Although\nthese agents have already seen significant success across various applications (Wang et al., 2024),\ndeveloping these building blocks and combining them into complex agentic systems often requires\ndomain-specific manual tuning and substantial effort from both researchers and engineers.\nHowever, the history of machine learning reveals a recurring theme: manually created arti-\nfacts become replaced by learned, more efficient solutions over time as we get more compute and\ndata (Clune, 2019). An early example is from computer vision, where hand-designed features like\nHOG (Dalal & Triggs, 2005) were eventually replaced by learned features from Convolutional Neural\nCorresponding author(s): Shengran Hu (srhu@cs.ubc.ca)arXiv:2408.08435v1  [cs.AI]  15 Aug 2024Automated Design of Agentic Systems\nSummary and motivation : \u201cBased on \nthe insights from previous agents \u2026\u201d,\nName: \u201cDivide and Conquer Agent\u201d,\nCode: \u201cdef forward(Task):\n \u2026\u2026\n return Answer\u201d\nMeta AgentNext interesting agent\nAgent ArchiveTest performance on tasks InputRefine until novel \nand error -free\nExamples of Discovered Agents\nMulti -step Peer Review AgentExperts\nAnswers\nReviewersT ask\nVerified Multimodal AgentT ask\nVisual \nParadigm\nVerifier\nVerified \nParadigmVisual \nAnalyzer\nCOTAnswerT askSub -problem \nDivision subsubsub\nsubsub\nExpertsAnswersEnsembleAnswer\nDivide and Conquer AgentReviewsand add to archiveNew Agent\n\u2026\nFigure 1|Overview of the proposed algorithm Meta Agent Search and examples of discovered\nagents. In our algorithm, we instruct the \u201cmeta\u201d agent to iteratively program new agents, test their\nperformance on tasks, add them to an archive of discovered agents, and use this archive to inform the\nmeta agent in subsequent iterations. We show three example agents across our runs, with all names\ngenerated by the meta agent. The detailed code of example agents can be found in Appendix F.\nNetworks (CNNs, Krizhevsky et al. (2012)). More recently, AutoML methods (Hutter et al., 2019)\nand AI-Generating Algorithms (AI-GAs, Clune (2019)) have also demonstrated the superiority of\nlearned AI systems compared to hand-designed AI systems. For example, the current best-performing\nCNN models come from Neural Architecture Search (Elsken","chunk_id":"c3d0436082aada237ee4bee645f16059","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is the corresponding author of the paper \"Automated Design of Agentic Systems\"\nShengran Hu is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia and the Vector Institute","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia and the Vector Institute","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia, the Vector Institute, and the Canada CIFAR AI Chair","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"UNIVERSITY OF BRITISH COLUMBIA","type":"ORGANIZATION","description":"The University of British Columbia is an academic institution where some of the authors of the paper \"Automated Design of Agentic Systems\" are affiliated","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"VECTOR INSTITUTE","type":"ORGANIZATION","description":"The Vector Institute is an organization where some of the authors of the paper \"Automated Design of Agentic Systems\" are affiliated","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CANADA CIFAR AI CHAIR","type":"ORGANIZATION","description":"The Canada CIFAR AI Chair is an organization where one of the authors of the paper \"Automated Design of Agentic Systems\" is affiliated","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)","type":"RESEARCH AREA","description":"Automated Design of Agentic Systems (ADAS) is a new research area that aims to automatically create powerful agentic system designs, including inventing novel building blocks and\/or combining them in new ways","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is an algorithm that iteratively programs new agents, tests their performance on tasks, and adds them to an archive of discovered agents to inform subsequent iterations","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models (FMs) such as GPT and Claude are powerful general-purpose agents used for agentic tasks that need flexible reasoning and planning","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"GPT","type":"TECHNOLOGY","description":"GPT is a Foundation Model developed by OpenAI, mentioned as a powerful general-purpose agent in the text","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CLAUDE","type":"TECHNOLOGY","description":"Claude is a Foundation Model developed by Anthropic, mentioned as a powerful general-purpose agent in the text","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought is a planning and reasoning technique used as a building block in agentic systems","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"SELF-REFLECTION","type":"TECHNOLOGY","description":"Self-Reflection is a technique used in agentic systems to improve performance through iterative self-assessment\nSelf-Reflection is a technique in agentic systems that involves iterative self-assessment to improve performance","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"TOOLFORMER","type":"TECHNOLOGY","description":"Toolformer is a module used within agentic systems to enhance their capabilities","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"HOG","type":"TECHNOLOGY","description":"HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision, eventually replaced by learned features from Convolutional Neural Networks","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"CONVOLUTIONAL NEURAL NETWORKS (CNNS)","type":"TECHNOLOGY","description":"Convolutional Neural Networks (CNNs) are a type of neural network used in computer vision, which replaced hand-designed features like HOG","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNOLOGY","description":"Neural Architecture Search is a method used to automatically design neural network architectures, leading to the best-performing CNN models","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AUTOML","type":"TECHNOLOGY","description":"AutoML (Automated Machine Learning) methods are used to automate the process of applying machine learning to real-world problems","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AI-GENERATING ALGORITHMS (AI-GAS)","type":"TECHNOLOGY","description":"AI-Generating Algorithms (AI-GAs) are methods that demonstrate the superiority of learned AI systems compared to hand-designed AI systems","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"MULTI-STEP PEER REVIEW AGENT","type":"TECHNOLOGY","description":"Multi-Step Peer Review Agent is an agent discovered by Meta Agent Search, designed to review and verify answers\nMulti-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to review and verify answers","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"VERIFIED MULTIMODAL AGENT","type":"TECHNOLOGY","description":"Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to handle tasks involving multiple modalities\nVerified Multimodal Agent is an agent discovered by Meta Agent Search, designed to handle tasks involving multiple modalities","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"DIVIDE AND CONQUER AGENT","type":"TECHNOLOGY","description":"Divide and Conquer Agent is an agent discovered by Meta Agent Search, designed to divide tasks into sub-problems and solve them\nDivide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to divide tasks into sub-problems and solve them","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that developed the GPT Foundation Model","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"ORGANIZATION"},{"name":"ANTHROPIC","type":"ORGANIZATION","description":"Anthropic is the organization that developed the Claude Foundation Model","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"ORGANIZATION"},{"name":"ROCKT\u00c4SCHEL","type":"PERSON","description":"Rockt\u00e4schel is an author mentioned in the text, contributing to the development of agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"ZAHARIA","type":"PERSON","description":"Zaharia is an author mentioned in the text, contributing to the development of agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"HU","type":"PERSON","description":"Hu is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning\nClune is an author mentioned in the text, contributing to the development of AI-Generating Algorithms","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"WEI","type":"PERSON","description":"Wei is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"YAO","type":"PERSON","description":"Yao is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"LEWIS","type":"PERSON","description":"Lewis is an author mentioned in the text, contributing to the development of memory structures","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author mentioned in the text, contributing to the development of memory structures","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"QU","type":"PERSON","description":"Qu is an author mentioned in the text, contributing to the development of tool use in agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"SCHICK","type":"PERSON","description":"Schick is an author mentioned in the text, contributing to the development of tool use in agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author mentioned in the text, contributing to the development of self-reflection in agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author mentioned in the text, contributing to the development of self-reflection in agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author mentioned in the text, contributing to the development of agentic systems","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"DALAL","type":"PERSON","description":"Dalal is an author mentioned in the text, contributing to the development of HOG features in computer vision","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"TRIGGS","type":"PERSON","description":"Triggs is an author mentioned in the text, contributing to the development of HOG features in computer vision","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"KRIZHEVSKY","type":"PERSON","description":"Krizhevsky is an author mentioned in the text, contributing to the development of Convolutional Neural Networks","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"HUTTER","type":"PERSON","description":"Hutter is an author mentioned in the text, contributing to the development of AutoML methods","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"PERSON"},{"name":"GITHUB","type":"WEBSITE","description":"GitHub is a platform where the code for the Automated Design of Agentic Systems (ADAS) can be found","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"WEBSITE"},{"name":"ARXIV","type":"WEBSITE","description":"arXiv is a platform where the paper \"Automated Design of Agentic Systems\" is published","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"WEBSITE"},{"name":"MEMORY STRUCTURES","type":"","description":"\nMemory Structures are components used in agentic systems to store and retrieve information","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"TOOL USE","type":"","description":"\nTool Use is a technique in agentic systems that involves using external tools such as search engines, code execution, and database queries","source_id":"c3d0436082aada237ee4bee645f16059","entity_type":"TECHNOLOGY"},{"name":"META AGENT","type":"TECHNOLOGY","description":"Meta Agent is an agent that iteratively programs new agents, tests their performance, and adds them to an archive of discovered agents","source_id":"c3d0436082aada237ee4bee645f16059"},{"name":"AGENT ARCHIVE","type":"TECHNOLOGY","description":"Agent Archive is a repository where discovered agents are stored and used to inform the meta agent in subsequent iterations","source_id":"c3d0436082aada237ee4bee645f16059"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is the corresponding author of the paper \"Automated Design of Agentic Systems\"Shengran Hu is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia and the Vector Institute<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia and the Vector Institute<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"Automated Design of Agentic Systems\" and is affiliated with the University of British Columbia, the Vector Institute, and the Canada CIFAR AI Chair<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"UNIVERSITY OF BRITISH COLUMBIA\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The University of British Columbia is an academic institution where some of the authors of the paper \"Automated Design of Agentic Systems\" are affiliated<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"VECTOR INSTITUTE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The Vector Institute is an organization where some of the authors of the paper \"Automated Design of Agentic Systems\" are affiliated<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CANADA CIFAR AI CHAIR\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The Canada CIFAR AI Chair is an organization where one of the authors of the paper \"Automated Design of Agentic Systems\" is affiliated<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) is a new research area that aims to automatically create powerful agentic system designs, including inventing novel building blocks and\/or combining them in new ways<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm that iteratively programs new agents, tests their performance on tasks, and adds them to an archive of discovered agents to inform subsequent iterations<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) such as GPT and Claude are powerful general-purpose agents used for agentic tasks that need flexible reasoning and planning<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"GPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT is a Foundation Model developed by OpenAI, mentioned as a powerful general-purpose agent in the text<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CLAUDE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude is a Foundation Model developed by Anthropic, mentioned as a powerful general-purpose agent in the text<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought is a planning and reasoning technique used as a building block in agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Reflection is a technique used in agentic systems to improve performance through iterative self-assessmentSelf-Reflection is a technique in agentic systems that involves iterative self-assessment to improve performance<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TOOLFORMER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Toolformer is a module used within agentic systems to enhance their capabilities<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"HOG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">HOG (Histogram of Oriented Gradients) is a hand-designed feature used in computer vision, eventually replaced by learned features from Convolutional Neural Networks<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"CONVOLUTIONAL NEURAL NETWORKS (CNNS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Convolutional Neural Networks (CNNs) are a type of neural network used in computer vision, which replaced hand-designed features like HOG<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural Architecture Search is a method used to automatically design neural network architectures, leading to the best-performing CNN models<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AUTOML\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoML (Automated Machine Learning) methods are used to automate the process of applying machine learning to real-world problems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS (AI-GAS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) are methods that demonstrate the superiority of learned AI systems compared to hand-designed AI systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multi-Step Peer Review Agent is an agent discovered by Meta Agent Search, designed to review and verify answersMulti-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to review and verify answers<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to handle tasks involving multiple modalitiesVerified Multimodal Agent is an agent discovered by Meta Agent Search, designed to handle tasks involving multiple modalities<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Divide and Conquer Agent is an agent discovered by Meta Agent Search, designed to divide tasks into sub-problems and solve themDivide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm, designed to divide tasks into sub-problems and solve them<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that developed the GPT Foundation Model<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"ANTHROPIC\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Anthropic is the organization that developed the Claude Foundation Model<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rockt&#228;schel is an author mentioned in the text, contributing to the development of agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZAHARIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zaharia is an author mentioned in the text, contributing to the development of agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hu is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoningClune is an author mentioned in the text, contributing to the development of AI-Generating Algorithms<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author mentioned in the text, contributing to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis is an author mentioned in the text, contributing to the development of memory structures<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author mentioned in the text, contributing to the development of memory structures<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qu is an author mentioned in the text, contributing to the development of tool use in agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schick is an author mentioned in the text, contributing to the development of tool use in agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author mentioned in the text, contributing to the development of self-reflection in agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author mentioned in the text, contributing to the development of self-reflection in agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author mentioned in the text, contributing to the development of agentic systems<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DALAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dalal is an author mentioned in the text, contributing to the development of HOG features in computer vision<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRIGGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Triggs is an author mentioned in the text, contributing to the development of HOG features in computer vision<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KRIZHEVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Krizhevsky is an author mentioned in the text, contributing to the development of Convolutional Neural Networks<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hutter is an author mentioned in the text, contributing to the development of AutoML methods<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GITHUB\">      <data key=\"d0\">WEBSITE<\/data>      <data key=\"d1\">GitHub is a platform where the code for the Automated Design of Agentic Systems (ADAS) can be found<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">WEBSITE<\/data>    <\/node>    <node id=\"ARXIV\">      <data key=\"d0\">WEBSITE<\/data>      <data key=\"d1\">arXiv is a platform where the paper \"Automated Design of Agentic Systems\" is published<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">WEBSITE<\/data>    <\/node>    <node id=\"MEMORY STRUCTURES\">      <data key=\"d0\" \/>      <data key=\"d1\">Memory Structures are components used in agentic systems to store and retrieve information<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\" \/>      <data key=\"d1\">Tool Use is a technique in agentic systems that involves using external tools such as search engines, code execution, and database queries<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent is an agent that iteratively programs new agents, tests their performance, and adds them to an archive of discovered agents<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <node id=\"AGENT ARCHIVE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agent Archive is a repository where discovered agents are stored and used to inform the meta agent in subsequent iterations<\/data>      <data key=\"d2\">c3d0436082aada237ee4bee645f16059<\/data>    <\/node>    <edge source=\"SHENGRAN HU\" target=\"CONG LU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shengran Hu and Cong Lu co-authored the paper \"Automated Design of Agentic Systems\"<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"JEFF CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shengran Hu and Jeff Clune co-authored the paper \"Automated Design of Agentic Systems\"<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"UNIVERSITY OF BRITISH COLUMBIA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Shengran Hu is affiliated with the University of British Columbia<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"VECTOR INSTITUTE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Shengran Hu is affiliated with the Vector Institute<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"GITHUB\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shengran Hu has a GitHub repository related to the Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONG LU\" target=\"JEFF CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Cong Lu and Jeff Clune co-authored the paper \"Automated Design of Agentic Systems\"<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONG LU\" target=\"UNIVERSITY OF BRITISH COLUMBIA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Cong Lu is affiliated with the University of British Columbia<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONG LU\" target=\"VECTOR INSTITUTE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Cong Lu is affiliated with the Vector Institute<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"UNIVERSITY OF BRITISH COLUMBIA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Jeff Clune is affiliated with the University of British Columbia<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"VECTOR INSTITUTE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Jeff Clune is affiliated with the Vector Institute<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"JEFF CLUNE\" target=\"CANADA CIFAR AI CHAIR\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Jeff Clune is affiliated with the Canada CIFAR AI Chair<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"META AGENT SEARCH\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Meta Agent Search is an algorithm used within the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"FOUNDATION MODELS (FMS)\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Foundation Models (FMs) are used as modules within agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Chain-of-Thought is a building block used within agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"SELF-REFLECTION\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Self-Reflection is a building block used within agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"TOOLFORMER\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Toolformer is a module used within agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"AUTOML\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AutoML methods are related to the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"AI-GENERATING ALGORITHMS (AI-GAS)\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AI-Generating Algorithms (AI-GAs) are related to the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"GITHUB\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">The code for the Automated Design of Agentic Systems (ADAS) can be found on GitHub<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"ARXIV\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">The paper \"Automated Design of Agentic Systems\" is published on arXiv<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"ROCKT&#196;SCHEL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Rockt&#228;schel contributed to the development of agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"ZAHARIA\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zaharia contributed to the development of agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOMATED DESIGN OF AGENTIC SYSTEMS (ADAS)\" target=\"WANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Wang contributed to the development of agentic systems in the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Multi-Step Peer Review Agent is an example of an agent discovered by the Meta Agent Search algorithm<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Verified Multimodal Agent is an example of an agent discovered by the Meta Agent Search algorithm<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Divide and Conquer Agent is an example of an agent discovered by the Meta Agent Search algorithm<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"GPT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">GPT is an example of a Foundation Model<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"FOUNDATION MODELS (FMS)\" target=\"CLAUDE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Claude is an example of a Foundation Model<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"GPT\" target=\"OPENAI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">OpenAI developed the GPT Foundation Model<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CLAUDE\" target=\"ANTHROPIC\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Anthropic developed the Claude Foundation Model<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"HU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Hu contributed to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Clune contributed to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"WEI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Wei contributed to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"YAO\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yao contributed to the development of chain-of-thought planning and reasoning<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"MADAAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Madaan contributed to the development of self-reflection in agentic systems<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"SHINN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shinn contributed to the development of self-reflection in agentic systems<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HOG\" target=\"CONVOLUTIONAL NEURAL NETWORKS (CNNS)\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">HOG features were eventually replaced by learned features from Convolutional Neural Networks<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HOG\" target=\"DALAL\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Dalal contributed to the development of HOG features in computer vision<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"HOG\" target=\"TRIGGS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Triggs contributed to the development of HOG features in computer vision<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONVOLUTIONAL NEURAL NETWORKS (CNNS)\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Neural Architecture Search is a method that led to the best-performing Convolutional Neural Networks<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"CONVOLUTIONAL NEURAL NETWORKS (CNNS)\" target=\"KRIZHEVSKY\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Krizhevsky contributed to the development of Convolutional Neural Networks<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"HUTTER\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Hutter contributed to the development of AutoML methods<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS (AI-GAS)\" target=\"CLUNE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Clune contributed to the development of AI-Generating Algorithms<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"LEWIS\" target=\"MEMORY STRUCTURES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Lewis contributed to the development of memory structures<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"ZHANG\" target=\"MEMORY STRUCTURES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zhang contributed to the development of memory structures<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"QU\" target=\"TOOL USE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Qu contributed to the development of tool use in agentic systems<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>    <edge source=\"SCHICK\" target=\"TOOL USE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Schick contributed to the development of tool use in agentic systems<\/data>      <data key=\"d6\">c3d0436082aada237ee4bee645f16059<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"81c504ffbcc5ed882e234802135295ba","chunk":" The detailed code of example agents can be found in Appendix F.\nNetworks (CNNs, Krizhevsky et al. (2012)). More recently, AutoML methods (Hutter et al., 2019)\nand AI-Generating Algorithms (AI-GAs, Clune (2019)) have also demonstrated the superiority of\nlearned AI systems compared to hand-designed AI systems. For example, the current best-performing\nCNN models come from Neural Architecture Search (Elsken et al., 2019; Shen et al., 2023) instead\nof manual design; in LLM alignment, learned loss functions (Lu et al., 2024a) outperform most\nhand-designed ones such as DPO (Rafailov et al., 2024); The AI Scientist (Lu et al., 2024b) demon-\nstrates an automated research pipeline, including the development of novel ML algorithms; and\nan endless number of robotics learning environments can be automatically generated in works like\nOMNI-EPIC (Faldor et al., 2024), which demonstrate surprising creativity in generated environments\nand allow more efficient environment creation than the manual approach (see more examples in\nSection 5). Therefore, in this paper, we propose a new research question: Can we automate the design\nof agentic systems rather than relying on manual efforts?\nTo explore the above research question, we formulate a new research area we call Automated\nDesign of AgenticSystems (ADAS), which aims to automatically invent novel building blocks and\ndesign powerful agentic systems (Section 2). We argue that ADAS may prove to be the fastest path to\ndeveloping powerful agents, and show initial evidence that learned agents can greatly outperform\nhand-designed agents. Considering the tremendous number of building blocks yet to be discovered in\nagentic systems (Section 5), it would take a long time for our research community to discover them\nall. Even if we successfully discover most of the useful building blocks, combining them into effective\nagentic systems for massive real-world applications would still be challenging and time-consuming,\ngiven the many different ways the building blocks can combine and interact with each other. In\ncontrast, with ADAS, the building blocks and agents can be learned in an automated fashion. ADAS\n2Automated Design of Agentic Systems\nmay not only potentially save human effort in developing powerful agents but also could be a faster\npath to more effective solutions than manual design.\nAlthough a few existing works can be considered as ADAS methods, most of them focus only on\ndesigning prompts (Fernando et al., 2024; Yang et al., 2024), greatly limiting their ability to invent\nflexible design patterns in agents (Section 5). In this paper, we show that there is an unexplored\nyet promising approach to ADAS where we can define the entire agentic system in code and new\nagents can be automatically discovered by a \u201cmeta\u201d agent programming even better ones in code.\nGiven that most programming languages, such as Python, which we use in this paper, are Turing\nComplete (Boyer & Moore, 1983; Ladha, 2024), searching within a code space theoretically enables a\nADAS algorithm to discover anypossible agentic systems, including all components such as prompts,\ntool use, control flows, and more. Furthermore, with recent FMs being increasingly proficient in\ncoding, we can use FMs as a meta agent to create new agents in code for ADAS, enabling novel agents\nto be programmed in an automated manner.\nFollowing the aforementioned ideas, we present Meta Agent Search in this paper as one of the first\nalgorithms in ADAS that enables complete design in code space (Figure 1). The core concept of Meta\nAgent Search is to instruct a meta agent to iteratively create interestingly new agents, evaluate them,\nadd them to an archive that stores discovered agents, and use this archive to help the meta agent in\nsubsequent iterations create yet more interestingly new agents. Similar to existing open-endedness\nalgorithms that leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a),\nwe encourage the meta agent to explore interesting (e.g., novel or worthwhile) agents. To validate\nthe proposed approach, we evaluate the proposed Meta Agent Search on: (1) the challenging ARC\nlogic puzzle task (Chollet, 2019) that aims to test the general intelligence of an AI system, (2) four\npopular benchmarks on reading comprehension, math, science questions, and multi-task problem\nsolving, and (3) the transferability of discovered agents to held-out domains and models (Section 4).\nOur experiments show that the discovered agents substantially outperform state-of-the-art hand-\ndesigned baselines. For instance, our agents improve F1 scores on reading comprehension tasks in\nDROP (Dua et al., 2019) by 13.6\/100 and accuracy rates on math tasks in MGSM (Shi et al., 2023) by\n14.4%. Additionally, they improve accuracy over baselines by 25.9%and13.2%on GSM8K (Cobbe\net al., 2021) and GSM-Hard (Gao et al., 2023) math tasks, respectively, after transferring across\ndomains. The promising performance of our algorithm over hand-designed solutions illustrates\nthe potential of ADAS in automating the design of agentic systems. Furthermore, the experiments\ndemonstrate that the discovered agents not only perform well when transferring across similar\ndomains but also exhibit strong performance when transferring across dissimilar domains, such as\nfrom mathematics to reading comprehension. This highlights the robustness and transferability of the\nag","chunk_id":"81c504ffbcc5ed882e234802135295ba","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"CNN","type":"TECHNOLOGY","description":"Convolutional Neural Networks (CNNs) are a class of deep neural networks, most commonly applied to analyzing visual imagery. They were popularized by Krizhevsky et al. in 2012","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"AUTOML","type":"TECHNOLOGY","description":"AutoML refers to the process of automating the end-to-end process of applying machine learning to real-world problems. It was discussed by Hutter et al. in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"AI-GAS","type":"TECHNOLOGY","description":"AI-Generating Algorithms (AI-GAs) are algorithms that generate AI systems, as discussed by Clune in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNOLOGY","description":"Neural Architecture Search (NAS) is a process of automating the design of artificial neural networks, as discussed by Elsken et al. in 2019 and Shen et al. in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"LLM ALIGNMENT","type":"TECHNOLOGY","description":"LLM alignment refers to the process of aligning large language models with desired outcomes, including the use of learned loss functions as discussed by Lu et al. in 2024a","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"DPO","type":"TECHNOLOGY","description":"DPO refers to a hand-designed loss function used in LLM alignment, as discussed by Rafailov et al. in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"AI SCIENTIST","type":"TECHNOLOGY","description":"The AI Scientist is an automated research pipeline that includes the development of novel machine learning algorithms, as discussed by Lu et al. in 2024b","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"OMNI-EPIC is a system that automatically generates robotics learning environments, demonstrating creativity and efficiency, as discussed by Faldor et al. in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"ADAS","type":"TECHNOLOGY","description":"Automated Design of Agentic Systems (ADAS) is a research area aimed at automatically inventing novel building blocks and designing powerful agentic systems","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is an algorithm in ADAS that enables the complete design of agentic systems in code space, iteratively creating and evaluating new agents","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"ARC","type":"TASK","description":"The ARC (Abstraction and Reasoning Corpus) is a logic puzzle task aimed at testing the general intelligence of an AI system, as discussed by Chollet in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TASK"},{"name":"DROP","type":"TASK","description":"DROP is a reading comprehension task that measures F1 scores, as discussed by Dua et al. in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TASK"},{"name":"MGSM","type":"TASK","description":"MGSM is a math task that measures accuracy rates, as discussed by Shi et al. in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TASK"},{"name":"GSM8K","type":"TASK","description":"GSM8K is a math task that measures accuracy rates, as discussed by Cobbe et al. in 2021","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TASK"},{"name":"GSM-HARD","type":"TASK","description":"GSM-Hard is a math task that measures accuracy rates, as discussed by Gao et al. in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TASK"},{"name":"PYTHON","type":"TECHNOLOGY","description":"Python is a programming language that is Turing Complete, enabling the search within a code space for ADAS algorithms, as discussed by Boyer & Moore in 1983 and Ladha in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"FM","type":"TECHNOLOGY","description":"Foundation Models (FMs) are large-scale models proficient in coding, used as meta agents to create new agents in code for ADAS","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"TECHNOLOGY"},{"name":"KRIZHEVSKY","type":"PERSON","description":"Krizhevsky is an author who popularized Convolutional Neural Networks (CNNs) in 2012","source_id":"81c504ffbcc5ed882e234802135295ba"},{"name":"HUTTER","type":"PERSON","description":"Hutter is an author who discussed AutoML methods in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author who discussed AI-Generating Algorithms (AI-GAs) in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"ELSKEN","type":"PERSON","description":"Elsken is an author who discussed Neural Architecture Search (NAS) in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"SHEN","type":"PERSON","description":"Shen is an author who discussed Neural Architecture Search (NAS) in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"LU","type":"PERSON","description":"Lu is an author who discussed learned loss functions in LLM alignment in 2024a, the AI Scientist in 2024b, and open-endedness algorithms in 2024c","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"RAFAILOV","type":"PERSON","description":"Rafailov is an author who discussed DPO, a hand-designed loss function, in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who discussed OMNI-EPIC and open-endedness algorithms in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"CHOLLET","type":"PERSON","description":"Chollet is an author who discussed the ARC logic puzzle task in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"DUA","type":"PERSON","description":"Dua is an author who discussed the DROP reading comprehension task in 2019","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"SHI","type":"PERSON","description":"Shi is an author who discussed the MGSM math task in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"COBBE","type":"PERSON","description":"Cobbe is an author who discussed the GSM8K math task in 2021","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"GAO","type":"PERSON","description":"Gao is an author who discussed the GSM-Hard math task in 2023","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"BOYER","type":"PERSON","description":"Boyer is an author who discussed the Turing Completeness of Python in 1983","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"MOORE","type":"PERSON","description":"Moore is an author who discussed the Turing Completeness of Python in 1983","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"LADHA","type":"PERSON","description":"Ladha is an author who discussed the Turing Completeness of Python in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"FERNANDO","type":"PERSON","description":"Fernando is an author who discussed ADAS methods focusing on designing prompts in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"YANG","type":"PERSON","description":"Yang is an author who discussed ADAS methods focusing on designing prompts in 2024","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author who discussed open-endedness algorithms in 2024a","source_id":"81c504ffbcc5ed882e234802135295ba","entity_type":"PERSON"},{"name":"OPEN-ENDEDNESS ALGORITHMS","type":"","description":"","source_id":"81c504ffbcc5ed882e234802135295ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CNN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Convolutional Neural Networks (CNNs) are a class of deep neural networks, most commonly applied to analyzing visual imagery. They were popularized by Krizhevsky et al. in 2012<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AUTOML\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoML refers to the process of automating the end-to-end process of applying machine learning to real-world problems. It was discussed by Hutter et al. in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AI-GAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) are algorithms that generate AI systems, as discussed by Clune in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural Architecture Search (NAS) is a process of automating the design of artificial neural networks, as discussed by Elsken et al. in 2019 and Shen et al. in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLM ALIGNMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM alignment refers to the process of aligning large language models with desired outcomes, including the use of learned loss functions as discussed by Lu et al. in 2024a<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DPO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DPO refers to a hand-designed loss function used in LLM alignment, as discussed by Rafailov et al. in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AI SCIENTIST\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The AI Scientist is an automated research pipeline that includes the development of novel machine learning algorithms, as discussed by Lu et al. in 2024b<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI-EPIC is a system that automatically generates robotics learning environments, demonstrating creativity and efficiency, as discussed by Faldor et al. in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) is a research area aimed at automatically inventing novel building blocks and designing powerful agentic systems<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm in ADAS that enables the complete design of agentic systems in code space, iteratively creating and evaluating new agents<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">The ARC (Abstraction and Reasoning Corpus) is a logic puzzle task aimed at testing the general intelligence of an AI system, as discussed by Chollet in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">DROP is a reading comprehension task that measures F1 scores, as discussed by Dua et al. in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">MGSM is a math task that measures accuracy rates, as discussed by Shi et al. in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">GSM8K is a math task that measures accuracy rates, as discussed by Cobbe et al. in 2021<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">GSM-Hard is a math task that measures accuracy rates, as discussed by Gao et al. in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TASK<\/data>    <\/node>    <node id=\"PYTHON\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Python is a programming language that is Turing Complete, enabling the search within a code space for ADAS algorithms, as discussed by Boyer &amp; Moore in 1983 and Ladha in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are large-scale models proficient in coding, used as meta agents to create new agents in code for ADAS<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"KRIZHEVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Krizhevsky is an author who popularized Convolutional Neural Networks (CNNs) in 2012<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <node id=\"HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hutter is an author who discussed AutoML methods in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author who discussed AI-Generating Algorithms (AI-GAs) in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ELSKEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elsken is an author who discussed Neural Architecture Search (NAS) in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shen is an author who discussed Neural Architecture Search (NAS) in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author who discussed learned loss functions in LLM alignment in 2024a, the AI Scientist in 2024b, and open-endedness algorithms in 2024c<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAFAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafailov is an author who discussed DPO, a hand-designed loss function, in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who discussed OMNI-EPIC and open-endedness algorithms in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHOLLET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chollet is an author who discussed the ARC logic puzzle task in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dua is an author who discussed the DROP reading comprehension task in 2019<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shi is an author who discussed the MGSM math task in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe is an author who discussed the GSM8K math task in 2021<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao is an author who discussed the GSM-Hard math task in 2023<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Boyer is an author who discussed the Turing Completeness of Python in 1983<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MOORE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Moore is an author who discussed the Turing Completeness of Python in 1983<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LADHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ladha is an author who discussed the Turing Completeness of Python in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FERNANDO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando is an author who discussed ADAS methods focusing on designing prompts in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who discussed ADAS methods focusing on designing prompts in 2024<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author who discussed open-endedness algorithms in 2024a<\/data>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OPEN-ENDEDNESS ALGORITHMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/node>    <edge source=\"CNN\" target=\"KRIZHEVSKY\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Krizhevsky is one of the authors who popularized Convolutional Neural Networks (CNNs) in 2012<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"HUTTER\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Hutter is one of the authors who discussed AutoML methods in 2019<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AI-GAS\" target=\"CLUNE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Clune is one of the authors who discussed AI-Generating Algorithms (AI-GAs) in 2019<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"NEURAL ARCHITECTURE SEARCH\" target=\"ELSKEN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Elsken is one of the authors who discussed Neural Architecture Search (NAS) in 2019<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"NEURAL ARCHITECTURE SEARCH\" target=\"SHEN\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Shen is one of the authors who discussed Neural Architecture Search (NAS) in 2023<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"LLM ALIGNMENT\" target=\"LU\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Lu is one of the authors who discussed learned loss functions in LLM alignment in 2024a<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"AI SCIENTIST\" target=\"LU\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Lu is one of the authors who discussed the AI Scientist in 2024b<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"OMNI-EPIC\" target=\"FALDOR\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Faldor is one of the authors who discussed OMNI-EPIC in 2024<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FERNANDO\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Fernando is one of the authors who discussed ADAS methods focusing on designing prompts in 2024<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"YANG\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Yang is one of the authors who discussed ADAS methods focusing on designing prompts in 2024<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"ARC\" target=\"CHOLLET\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Chollet is one of the authors who discussed the ARC logic puzzle task in 2019<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"DROP\" target=\"DUA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Dua is one of the authors who discussed the DROP reading comprehension task in 2019<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"SHI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Shi is one of the authors who discussed the MGSM math task in 2023<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"GSM8K\" target=\"COBBE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Cobbe is one of the authors who discussed the GSM8K math task in 2021<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"GSM-HARD\" target=\"GAO\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Gao is one of the authors who discussed the GSM-Hard math task in 2023<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"PYTHON\" target=\"BOYER\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Boyer is one of the authors who discussed the Turing Completeness of Python in 1983<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"PYTHON\" target=\"MOORE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Moore is one of the authors who discussed the Turing Completeness of Python in 1983<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"PYTHON\" target=\"LADHA\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Ladha is one of the authors who discussed the Turing Completeness of Python in 2024<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"LU\" target=\"OPEN-ENDEDNESS ALGORITHMS\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Lu is one of the authors who discussed open-endedness algorithms in 2024c<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>    <edge source=\"ZHANG\" target=\"OPEN-ENDEDNESS ALGORITHMS\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Zhang is one of the authors who discussed open-endedness algorithms in 2024a<\/data>      <data key=\"d6\">81c504ffbcc5ed882e234802135295ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4884e8429ca1e567dadf5e22b4b68274","chunk":" et al., 2023) math tasks, respectively, after transferring across\ndomains. The promising performance of our algorithm over hand-designed solutions illustrates\nthe potential of ADAS in automating the design of agentic systems. Furthermore, the experiments\ndemonstrate that the discovered agents not only perform well when transferring across similar\ndomains but also exhibit strong performance when transferring across dissimilar domains, such as\nfrom mathematics to reading comprehension. This highlights the robustness and transferability of the\nagentic systems discovered by Meta Agent Search. In conclusion, our work opens up many exciting\nresearch directions and encourages further studies (Section 6).\n2. New Research Area: Automated Design of Agentic Systems (ADAS)\nAt the time of writing, the community has not reached a consensus on the definitions or terminologies\nof agents. Here, by agents we refer to agentic systems that involve Foundation Models (FMs) as\nmodules in the control flow to solve tasks by planning, using tools, and carrying out multiple, iterative\nsteps of processing (Chase, 2024; Ng, 2024).\nIn this paper, we propose a new research area Automated Design of Agentic Systems (ADAS).\nSimilar to research areas in AI-GAs (Clune, 2019) and AutoML (Hutter et al., 2019), such as Neural\nArchitecture Search (Elsken et al., 2019), we formulate ADAS as an optimization process and identify\nthree key components of ADAS algorithms (Figure 2).\n3Automated Design of Agentic Systems\nSearch Space\nE.g. Agents defined by code\nSearch Algorithm\nE.g. LLM defines agents using code\nEvaluation Function\nE.g. Accuracy on the taskWhere is the capital of CanadaOttawa\n\u2705SampleNew AgentEvaluate the ObjectivesAgent\u2026\u20261 + 1 = ?\nFigure 2|The three key components of Automated Design of Agentic Systems (ADAS). The search\nspace determines which agentic systems can be represented in ADAS. The search algorithm specifies\nhow the ADAS method explores the search space. The evaluation function defines how to evaluate a\ncandidate agent on target objectives such as performance.\nFormulation\nAutomated Design of Agentic Systems (ADAS) involves using a search algorithm to discover\nagentic systems across a search space thatoptimize anevaluation function .\n\u2022Search Space : The search space defines which agentic systems can be represented and thus\ndiscovered in ADAS. For example, works like PromptBreeder (Fernando et al., 2024) mutate only\nthe text prompts of an agent, but their other components, such as control flow, remain the same.\nThus, in these search spaces, agents that have a different control flow than the predefined one can\nnot be represented. Existing works also explore search spaces such as graph structures (Zhuge\net al., 2024) and feed-forward networks (Liu et al., 2023).\n\u2022Search Algorithm : The search algorithm defines how ADAS algorithms explore the search space.\nSince the search space is often very large or even unbounded, the exploration-exploitation trade-\noff (Sutton & Barto, 2018) should be considered. Ideally, the algorithm can both quickly discover\nhigh-performance agentic systems and avoid remaining stuck in a local optimum. Existing ap-\nproachesincludeusingReinforcementLearning(Zhugeetal.,2024)oranFMiterativelygenerating\nnew solutions (Fernando et al., 2024) as search algorithms.\n\u2022Evaluation Function : Depending on the application of the ADAS algorithm, we may consider\ndifferentobjectivestooptimize,suchasperformance,cost,latency,orsafetyofagents. Anevaluation\nfunction defines how to evaluate a candidate agent on those objectives. For example, to assess\nthe agent\u2019s performance on unseen future data, a simple method is to calculate the accuracy rate\non the validation data for a task, which is commonly adopted in existing works (Fernando et al.,\n2024; Zhuge et al., 2024).\nAlthoughmanysearchspacedesignsarepossibleandsomehavealreadybeenexplored(Section5),\nthere is an unexplored yet promising approach where we can define the entire agentic system in\ncode and new agents can be automatically discovered by a meta agent programming even better\nones in code. Searching within a code space theoretically enables the ADAS algorithm to discover\nanypossible building blocks (e.g., prompts, tool use, control flow) and agentic systems that combine\nany of these building blocks in any way. This approach also offers better interpretability for agent\ndesign patterns since the program code is often readable, making debugging easier and enhancing AI\nsafety. Additionally, compared to search spaces using networks (Liu et al., 2023) or graphs (Zhuge\net al., 2024), searching in a code space allows us to more easily build on existing human efforts. For\nexample, it is possible to search within open-source agent frameworks like LangChain (LangChainAI,\n2022) and build upon all existing building blocks (e.g., RAG, search engine tools). Finally, since FMs\n4Automated Design of Agentic Systems\nare proficient in coding, utilizing a code search space allows us to leverage existing expertise from\nFMs during the search process. In contrast, search algorithms in custom search spaces, such as graphs,\nmay be much less efficient due to the absence of these priors. Therefore, we argue that the approach\nof using programming languages as the search space should be studied more in ADAS.\n3. Our Algorithm: Meta Agent Search\nIn this section, we present Meta Agent Search, a simple yet effective algorithm to demonstrate the\napproach of defining and searching for agents","chunk_id":"4884e8429ca1e567dadf5e22b4b68274","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"ADAS","type":"RESEARCH AREA","description":"Automated Design of Agentic Systems (ADAS) is a new research area that involves using a search algorithm to discover agentic systems across a search space that optimize an evaluation function","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"META AGENT SEARCH","type":"ALGORITHM","description":"Meta Agent Search is an algorithm that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FOUNDATION MODELS","type":"TECHNOLOGY","description":"Foundation Models (FMs) are used as modules in the control flow of agentic systems to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"PROMPTBREEDER","type":"TECHNOLOGY","description":"PromptBreeder is a system that mutates only the text prompts of an agent, while other components such as control flow remain the same","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is an open-source agent framework that allows building upon existing building blocks like RAG and search engine tools","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"CHRISTOPHER CHASE","type":"PERSON","description":"Christopher Chase is an author mentioned in the text who has contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"NG","type":"PERSON","description":"Ng is an author mentioned in the text who has contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author mentioned in the text who has contributed to the research area of AI-GAs","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"HUTTER","type":"PERSON","description":"Hutter is an author mentioned in the text who has contributed to the research area of AutoML","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ELSKEN","type":"PERSON","description":"Elsken is an author mentioned in the text who has contributed to the research area of Neural Architecture Search","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FERNANDO","type":"PERSON","description":"Fernando is an author mentioned in the text who has contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ZHU","type":"PERSON","description":"Zhu is an author mentioned in the text who has contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LIU","type":"PERSON","description":"Liu is an author mentioned in the text who has contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SUTTON","type":"PERSON","description":"Sutton is an author mentioned in the text who has contributed to the research area of reinforcement learning","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"BARTO","type":"PERSON","description":"Barto is an author mentioned in the text who has contributed to the research area of reinforcement learning","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"AGENTIC SYSTEMS","type":"TECHNOLOGY","description":"Agentic systems involve Foundation Models (FMs) as modules in the control flow to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"AI-GAS","type":"RESEARCH AREA","description":"AI-GAs (Artificial Intelligence-Generative Algorithms) is a research area related to the automated design and optimization of algorithms","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"AUTOML","type":"RESEARCH AREA","description":"AutoML (Automated Machine Learning) is a research area focused on automating the process of applying machine learning to real-world problems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"RESEARCH AREA","description":"Neural Architecture Search is a research area within AutoML that focuses on automating the design of neural network architectures","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"REINFORCEMENT LEARNING","type":"RESEARCH AREA","description":"Reinforcement Learning is a research area focused on how agents should take actions in an environment to maximize cumulative reward","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"GRAPH STRUCTURES","type":"TECHNOLOGY","description":"Graph structures are used as a search space in ADAS to represent agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FEED-FORWARD NETWORKS","type":"TECHNOLOGY","description":"Feed-forward networks are used as a search space in ADAS to represent agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"RAG","type":"TECHNOLOGY","description":"RAG (Retrieval-Augmented Generation) is a building block used in agentic systems within the LangChain framework","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SEARCH ENGINE TOOLS","type":"TECHNOLOGY","description":"Search engine tools are building blocks used in agentic systems within the LangChain framework","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FERNANDO ET AL.","type":"PERSON","description":"Fernando et al. are authors who have contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"ZHUGE ET AL.","type":"PERSON","description":"Zhuge et al. are authors who have contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LIU ET AL.","type":"PERSON","description":"Liu et al. are authors who have contributed to the research area of agentic systems","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SUTTON & BARTO","type":"PERSON","description":"Sutton & Barto are authors who have contributed to the research area of reinforcement learning","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"LANGCHAINAI","type":"ORGANIZATION","description":"LangChainAI is the organization behind the open-source agent framework LangChain","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"SECTION 6","type":"DOCUMENT","description":"Section 6 is a part of the document that encourages further studies and opens up new research directions","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FIGURE 2","type":"DOCUMENT","description":"Figure 2 is a part of the document that illustrates the three key components of Automated Design of Agentic Systems (ADAS)","source_id":"4884e8429ca1e567dadf5e22b4b68274"},{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models (FMs) are used as modules in the control flow of agentic systems to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing","source_id":"4884e8429ca1e567dadf5e22b4b68274"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADAS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) is a new research area that involves using a search algorithm to discover agentic systems across a search space that optimize an evaluation function<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">ALGORITHM<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are used as modules in the control flow of agentic systems to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"PROMPTBREEDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">PromptBreeder is a system that mutates only the text prompts of an agent, while other components such as control flow remain the same<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is an open-source agent framework that allows building upon existing building blocks like RAG and search engine tools<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"CHRISTOPHER CHASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher Chase is an author mentioned in the text who has contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"NG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ng is an author mentioned in the text who has contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author mentioned in the text who has contributed to the research area of AI-GAs<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hutter is an author mentioned in the text who has contributed to the research area of AutoML<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ELSKEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elsken is an author mentioned in the text who has contributed to the research area of Neural Architecture Search<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FERNANDO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando is an author mentioned in the text who has contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhu is an author mentioned in the text who has contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author mentioned in the text who has contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sutton is an author mentioned in the text who has contributed to the research area of reinforcement learning<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"BARTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barto is an author mentioned in the text who has contributed to the research area of reinforcement learning<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic systems involve Foundation Models (FMs) as modules in the control flow to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"AI-GAS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">AI-GAs (Artificial Intelligence-Generative Algorithms) is a research area related to the automated design and optimization of algorithms<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"AUTOML\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">AutoML (Automated Machine Learning) is a research area focused on automating the process of applying machine learning to real-world problems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">Neural Architecture Search is a research area within AutoML that focuses on automating the design of neural network architectures<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">Reinforcement Learning is a research area focused on how agents should take actions in an environment to maximize cumulative reward<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"GRAPH STRUCTURES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Graph structures are used as a search space in ADAS to represent agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FEED-FORWARD NETWORKS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Feed-forward networks are used as a search space in ADAS to represent agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is a building block used in agentic systems within the LangChain framework<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SEARCH ENGINE TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search engine tools are building blocks used in agentic systems within the LangChain framework<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FERNANDO ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando et al. are authors who have contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"ZHUGE ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuge et al. are authors who have contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LIU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. are authors who have contributed to the research area of agentic systems<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SUTTON &amp; BARTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sutton &amp; Barto are authors who have contributed to the research area of reinforcement learning<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"LANGCHAINAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">LangChainAI is the organization behind the open-source agent framework LangChain<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"SECTION 6\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Section 6 is a part of the document that encourages further studies and opens up new research directions<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FIGURE 2\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Figure 2 is a part of the document that illustrates the three key components of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are used as modules in the control flow of agentic systems to solve tasks by planning, using tools, and carrying out multiple, iterative steps of processing<\/data>      <data key=\"d2\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/node>    <edge source=\"ADAS\" target=\"META AGENT SEARCH\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search is an algorithm used within the research area of Automated Design of Agentic Systems (ADAS)<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FOUNDATION MODELS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Foundation Models are used as modules in the control flow of agentic systems within the research area of ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"PROMPTBREEDER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">PromptBreeder is an example of a system that mutates text prompts within the search space of ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"LANGCHAIN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LangChain is an open-source agent framework that can be used within the search space of ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"CLUNE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Clune has contributed to the research area of AI-GAs, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"HUTTER\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Hutter has contributed to the research area of AutoML, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ELSKEN\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Elsken has contributed to the research area of Neural Architecture Search, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FERNANDO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Fernando has contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ZHU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Zhu has contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"LIU\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Liu has contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ADAS involves the automated design of agentic systems<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AI-GAS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS is related to the research area of AI-GAs<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AUTOML\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS is related to the research area of AutoML<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS is related to the research area of Neural Architecture Search<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"REINFORCEMENT LEARNING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ADAS involves search algorithms that can include reinforcement learning<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"GRAPH STRUCTURES\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Graph structures are used as a search space in ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FEED-FORWARD NETWORKS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Feed-forward networks are used as a search space in ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"RAG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">RAG is a building block used in agentic systems within the ADAS framework<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SEARCH ENGINE TOOLS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Search engine tools are building blocks used in agentic systems within the ADAS framework<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FERNANDO ET AL.\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Fernando et al. have contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ZHUGE ET AL.\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Zhuge et al. have contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"LIU ET AL.\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Liu et al. have contributed to the research area of agentic systems, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SUTTON &amp; BARTO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Sutton &amp; Barto have contributed to the research area of reinforcement learning, which is related to ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SECTION 6\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Section 6 of the document encourages further studies and opens up new research directions in ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FIGURE 2\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Figure 2 of the document illustrates the three key components of ADAS<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"LANGCHAINAI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LangChain is an open-source agent framework developed by LangChainAI<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"CHRISTOPHER CHASE\" target=\"NG\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Christopher Chase and Ng have both contributed to the research area of agentic systems<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"SUTTON\" target=\"BARTO\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Sutton and Barto have both contributed to the research area of reinforcement learning<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>    <edge source=\"AGENTIC SYSTEMS\" target=\"FOUNDATION MODELS (FMS)\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Foundation Models (FMs) are used as modules in the control flow of agentic systems<\/data>      <data key=\"d5\">4884e8429ca1e567dadf5e22b4b68274<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"24d7b89ae9522ae60d2317984951355b","chunk":" leverage existing expertise from\nFMs during the search process. In contrast, search algorithms in custom search spaces, such as graphs,\nmay be much less efficient due to the absence of these priors. Therefore, we argue that the approach\nof using programming languages as the search space should be studied more in ADAS.\n3. Our Algorithm: Meta Agent Search\nIn this section, we present Meta Agent Search, a simple yet effective algorithm to demonstrate the\napproach of defining and searching for agents in code. The core idea of Meta Agent Search is to adopt\nFMs as meta agents to iteratively program interestingly new agents based on an ever-growing archive\nof previous discoveries. Although any possible building blocks and agentic systems can theoretically\nbe programmed by the meta agent from scratch, it is inefficient in practice to avoid providing the\nmeta agent any basic functions such as FM query APIs or existing tools. Therefore, in this paper, we\ndefine a simple framework (within 100 lines of code) for the meta agent, providing it with a basic\nset of essential functions like querying FMs or formatting prompts. As a result, the meta agent only\nneeds to program a \u201cforward\u201d function to define a new agentic system, similar to the practice in\nFunSearch (Romera-Paredes et al., 2024). This function takes in the information of the task and\noutputs the agent\u2019s response to the task. Details of the framework codes and examples of the agents\ndefined with this framework can be found in Appendix B.\nAs shown in Figure 1, the core idea of Meta Agent Search is to have a meta agent iteratively\nprogram new agents in code. We show the main prompt for the meta agent to program new agents\nbelow, where variables in the prompts are highlighted. Similar to existing open-endedness algorithms\nthat leverage human notions of interestingness (Lu et al., 2024c; Zhang et al., 2024a), we encourage\nthe meta agent to explore interestingly new (e.g., novel or worthwhile) agents based on an ever-\ngrowing archive of previous discoveries. We also adopt self-reflection (Madaan et al., 2024; Shinn\net al., 2023) iterations in our meta agent, where it performs two iterations of refinement on the\nnovelty and correctness of the proposal and performs up to three refinements when errors occur\nwhile running the code. Full details of the prompt are presented in Appendix A.\nAfter a new agent is generated, we evaluate it using the validation data from the target domain.\nHere, we calculate the performance (e.g., success rate or F1 score) and 95% bootstrap confidence\ninterval as the metrics for the meta agent to maximize. The generated agent is then added to the\narchive with the evaluation metrics, and the iteration continues with the updated archive until the\nmaximum number of iterations is reached.\nMain prompt for the meta agent.\nYou are an expert machine learning researcher testing different agentic systems.\n[BriefDescriptionoftheDomain]\n[FrameworkCode]\n[OutputInstructionsandExamples]\n[DiscoveredAgentArchive] (initializedwithbaselines,updatedateveryiteration)\n# Your task\nYou are deeply familiar with prompting techniques and the agent works from the literature. Your goal is\nto maximize the performance by proposing interestingly new agents ......\nUse the knowledge from the archive and inspiration from academic literature to propose the next\ninteresting agentic system design.\n5Automated Design of Agentic Systems\n4. Experiments\nWe conduct extensive experiments on: (1) the challenging ARC logic puzzle task (Chollet, 2019)\n(Section 4.1), (2) four popular benchmarks assessing the agent\u2019s abilities on reading comprehension,\nmath, science questions, and multi-task problem solving (Section 4.2), (3) the transferability of\nthe discovered agents on ARC to three held-out models, and (4) the transferability of discovered\nagents on Math to four held-out math tasks and three tasks that are beyond math (Section 4.3).\nAcross all experiments, we find that the discovered agents substantially outperform baseline state-\nof-the-art hand-designed agents. Notably, our discovered agents improve over baselines on reading\ncomprehension tasks in DROP (Dua et al., 2019) by 13.6\/100 (F1 score) and on math tasks in\nMGSM (Shi et al., 2023) by 14.4%(accuracy rate). Additionally, our discovered agents improve over\nthe baseline on ARC tasks by 14%(accuracy rate) after transferring from GPT-3.5 to GPT-4, and by\n25.9%and13.2%(accuracy rate) after transferring from MGSM math tasks to held-out math tasks in\nGSM8K (Cobbe et al., 2021) and GSM-Hard (Gao et al., 2023) respectively. All code, prompts, and\nexperiment results are available at https:\/\/github.com\/ShengranHu\/ADAS .\n0 5 10 15 20 25\nIteration468101214Held-out T est Accuracy (%)\nInitially tested generating high-level strategies\nbefore implementing low-level details.An important strategy emerged: using multiple COT s\nto generate possible answers, refining them, and\nfinally ensembling the best answers.Introduced dynamic memory for doing more refinements.Scaled up the previous idea.Best agent: introduced multiple\ncritics for enhanced refinement.Meta-Agent Search on ARC\nChain-of-Thought\nSelf-Refine\nLLM DebateCOT-SC\nQuality-Diversity\nMeta-Agent Search\n(a)\nTask5 COTs\n5 Answers\nHuman -like \nCritic\nFeedbackEfficiency Expert\nReadability Expert\nSimplicity","chunk_id":"24d7b89ae9522ae60d2317984951355b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is an algorithm that defines and searches for agents in code by adopting FMs as meta agents to iteratively program new agents based on an ever-growing archive of previous discoveries","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"FMS","type":"TECHNOLOGY","description":"FMs (Foundation Models) are used as meta agents in the Meta Agent Search algorithm to iteratively program new agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"FUNSEARCH","type":"TECHNOLOGY","description":"FunSearch is a practice referenced in the Meta Agent Search algorithm where a \"forward\" function is programmed to define a new agentic system","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ROMERA-PAREDES","type":"PERSON","description":"Romera-Paredes is an author who has worked on the FunSearch practice mentioned in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"LU","type":"PERSON","description":"Lu is an author who has worked on open-endedness algorithms that leverage human notions of interestingness","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author who has worked on open-endedness algorithms that leverage human notions of interestingness","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who has worked on self-reflection iterations in meta agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on self-reflection iterations in meta agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ARC","type":"TASK","description":"ARC (Abstraction and Reasoning Corpus) is a challenging logic puzzle task used in experiments to evaluate the performance of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"DROP","type":"TASK","description":"DROP is a reading comprehension task used in experiments to evaluate the performance of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"MGSM","type":"TASK","description":"MGSM is a math task used in experiments to evaluate the performance of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GSM8K","type":"TASK","description":"GSM8K is a held-out math task used in experiments to evaluate the transferability of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GSM-HARD","type":"TASK","description":"GSM-Hard is a held-out math task used in experiments to evaluate the transferability of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a model from which discovered agents were transferred to GPT-4, showing improved performance on ARC tasks","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a model to which discovered agents were transferred from GPT-3.5, showing improved performance on ARC tasks","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is associated with the ADAS project and has made all code, prompts, and experiment results available on GitHub","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ADAS","type":"PROJECT","description":"ADAS (Automated Design of Agentic Systems) is the project under which the Meta Agent Search algorithm and related experiments were conducted","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"GITHUB","type":"PLATFORM","description":"GitHub is the platform where all code, prompts, and experiment results related to the ADAS project are available","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"PROGRAMMING LANGUAGES","type":"TECHNOLOGY","description":"Programming languages are suggested as a search space in ADAS for defining and searching for agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SEARCH ALGORITHMS","type":"TECHNOLOGY","description":"Search algorithms in custom search spaces like graphs may be less efficient due to the absence of priors","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"CODE","type":"TECHNOLOGY","description":"Code is used in the Meta Agent Search algorithm to define new agentic systems","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"QUERY APIS","type":"TECHNOLOGY","description":"Query APIs are basic functions provided to the meta agent in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"PROMPTS","type":"TECHNOLOGY","description":"Prompts are used by the meta agent in the Meta Agent Search algorithm to program new agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"VALIDATION DATA","type":"TECHNOLOGY","description":"Validation data from the target domain is used to evaluate the performance of generated agents in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"PERFORMANCE METRICS","type":"TECHNOLOGY","description":"Performance metrics like success rate or F1 score are calculated to evaluate the performance of generated agents in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"BOOTSTRAP CONFIDENCE INTERVAL","type":"TECHNOLOGY","description":"Bootstrap confidence interval is used as a metric for the meta agent to maximize in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ITERATIONS","type":"TECHNOLOGY","description":"Iterations are used in the Meta Agent Search algorithm to refine the novelty and correctness of the proposed agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ARC LOGIC PUZZLE TASK","type":"TASK","description":"ARC logic puzzle task is a challenging task used in experiments to evaluate the performance of discovered agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"READING COMPREHENSION","type":"TASK","description":"Reading comprehension is one of the benchmarks used to assess the agent's abilities in experiments","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"MATH","type":"TASK","description":"Math is one of the benchmarks used to assess the agent's abilities in experiments","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SCIENCE QUESTIONS","type":"TASK","description":"Science questions are one of the benchmarks used to assess the agent's abilities in experiments","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"MULTI-TASK PROBLEM SOLVING","type":"TASK","description":"Multi-task problem solving is one of the benchmarks used to assess the agent's abilities in experiments","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"TRANSFERABILITY","type":"TECHNOLOGY","description":"Transferability is assessed in experiments to evaluate how well discovered agents perform on different tasks and models","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SUCCESS RATE","type":"TECHNOLOGY","description":"Success rate is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"F1 SCORE","type":"TECHNOLOGY","description":"F1 score is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"ACCURACY RATE","type":"TECHNOLOGY","description":"Accuracy rate is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought is a strategy used in Meta Agent Search to generate possible answers, refine them, and ensemble the best answers","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine is a strategy used in Meta Agent Search for enhanced refinement of generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"COT-SC","type":"TECHNOLOGY","description":"COT-SC is a strategy used in Meta Agent Search for generating and refining answers","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"HUMAN-LIKE CRITIC","type":"TECHNOLOGY","description":"Human-like critic is a component used in Meta Agent Search for providing feedback on generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"FEEDBACK","type":"TECHNOLOGY","description":"Feedback is provided by human-like critics in Meta Agent Search to refine generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"EFFICIENCY EXPERT","type":"TECHNOLOGY","description":"Efficiency expert is a component used in Meta Agent Search for providing feedback on the efficiency of generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"READABILITY EXPERT","type":"TECHNOLOGY","description":"Readability expert is a component used in Meta Agent Search for providing feedback on the readability of generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"},{"name":"SIMPLICITY","type":"TECHNOLOGY","description":"Simplicity is a component used in Meta Agent Search for providing feedback on the simplicity of generated agents","source_id":"24d7b89ae9522ae60d2317984951355b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is an algorithm that defines and searches for agents in code by adopting FMs as meta agents to iteratively program new agents based on an ever-growing archive of previous discoveries<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"FMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FMs (Foundation Models) are used as meta agents in the Meta Agent Search algorithm to iteratively program new agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"FUNSEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FunSearch is a practice referenced in the Meta Agent Search algorithm where a \"forward\" function is programmed to define a new agentic system<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ROMERA-PAREDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Romera-Paredes is an author who has worked on the FunSearch practice mentioned in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author who has worked on open-endedness algorithms that leverage human notions of interestingness<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author who has worked on open-endedness algorithms that leverage human notions of interestingness<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who has worked on self-reflection iterations in meta agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on self-reflection iterations in meta agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">ARC (Abstraction and Reasoning Corpus) is a challenging logic puzzle task used in experiments to evaluate the performance of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">DROP is a reading comprehension task used in experiments to evaluate the performance of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">MGSM is a math task used in experiments to evaluate the performance of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">GSM8K is a held-out math task used in experiments to evaluate the transferability of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">GSM-Hard is a held-out math task used in experiments to evaluate the transferability of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a model from which discovered agents were transferred to GPT-4, showing improved performance on ARC tasks<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a model to which discovered agents were transferred from GPT-3.5, showing improved performance on ARC tasks<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is associated with the ADAS project and has made all code, prompts, and experiment results available on GitHub<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) is the project under which the Meta Agent Search algorithm and related experiments were conducted<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"GITHUB\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">GitHub is the platform where all code, prompts, and experiment results related to the ADAS project are available<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"PROGRAMMING LANGUAGES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Programming languages are suggested as a search space in ADAS for defining and searching for agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search algorithms in custom search spaces like graphs may be less efficient due to the absence of priors<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"CODE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Code is used in the Meta Agent Search algorithm to define new agentic systems<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"QUERY APIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Query APIs are basic functions provided to the meta agent in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Prompts are used by the meta agent in the Meta Agent Search algorithm to program new agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"VALIDATION DATA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Validation data from the target domain is used to evaluate the performance of generated agents in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"PERFORMANCE METRICS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Performance metrics like success rate or F1 score are calculated to evaluate the performance of generated agents in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"BOOTSTRAP CONFIDENCE INTERVAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Bootstrap confidence interval is used as a metric for the meta agent to maximize in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ITERATIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Iterations are used in the Meta Agent Search algorithm to refine the novelty and correctness of the proposed agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ARC LOGIC PUZZLE TASK\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">ARC logic puzzle task is a challenging task used in experiments to evaluate the performance of discovered agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Reading comprehension is one of the benchmarks used to assess the agent's abilities in experiments<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Math is one of the benchmarks used to assess the agent's abilities in experiments<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SCIENCE QUESTIONS\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Science questions are one of the benchmarks used to assess the agent's abilities in experiments<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"MULTI-TASK PROBLEM SOLVING\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Multi-task problem solving is one of the benchmarks used to assess the agent's abilities in experiments<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"TRANSFERABILITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Transferability is assessed in experiments to evaluate how well discovered agents perform on different tasks and models<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SUCCESS RATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Success rate is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"F1 SCORE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">F1 score is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"ACCURACY RATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Accuracy rate is a performance metric used to evaluate the performance of generated agents in the Meta Agent Search algorithm<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought is a strategy used in Meta Agent Search to generate possible answers, refine them, and ensemble the best answers<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine is a strategy used in Meta Agent Search for enhanced refinement of generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT-SC is a strategy used in Meta Agent Search for generating and refining answers<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"HUMAN-LIKE CRITIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Human-like critic is a component used in Meta Agent Search for providing feedback on generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Feedback is provided by human-like critics in Meta Agent Search to refine generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"EFFICIENCY EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Efficiency expert is a component used in Meta Agent Search for providing feedback on the efficiency of generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"READABILITY EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Readability expert is a component used in Meta Agent Search for providing feedback on the readability of generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <node id=\"SIMPLICITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Simplicity is a component used in Meta Agent Search for providing feedback on the simplicity of generated agents<\/data>      <data key=\"d2\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"FMS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search uses FMs as meta agents to iteratively program new agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FUNSEARCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search references the practice of FunSearch for programming a \"forward\" function to define new agentic systems<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the ARC logic puzzle task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DROP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the DROP reading comprehension task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the MGSM math task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM8K\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the GSM8K held-out math task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM-HARD\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the GSM-Hard held-out math task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search showed improved performance on ARC tasks after transferring from GPT-3.5 to GPT-4<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search showed improved performance on ARC tasks after transferring from GPT-3.5 to GPT-4<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ADAS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search is part of the ADAS project<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Lu has worked on open-endedness algorithms that leverage human notions of interestingness, which are referenced in Meta Agent Search<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ZHANG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Zhang has worked on open-endedness algorithms that leverage human notions of interestingness, which are referenced in Meta Agent Search<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MADAAN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Madaan has worked on self-reflection iterations in meta agents, which are adopted in Meta Agent Search<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SHINN\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Shinn has worked on self-reflection iterations in meta agents, which are adopted in Meta Agent Search<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"PROGRAMMING LANGUAGES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses programming languages as the search space for defining and searching for agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SEARCH ALGORITHMS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Meta Agent Search contrasts with search algorithms in custom search spaces like graphs<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CODE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search uses code to define new agentic systems<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"QUERY APIS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search provides the meta agent with basic functions like query APIs<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"PROMPTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search uses prompts for the meta agent to program new agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"VALIDATION DATA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses validation data from the target domain to evaluate the performance of generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"PERFORMANCE METRICS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search calculates performance metrics like success rate or F1 score to evaluate generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"BOOTSTRAP CONFIDENCE INTERVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses bootstrap confidence interval as a metric for the meta agent to maximize<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ITERATIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses iterations to refine the novelty and correctness of proposed agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARC LOGIC PUZZLE TASK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using the ARC logic puzzle task<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"READING COMPREHENSION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using reading comprehension benchmarks<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MATH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using math benchmarks<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SCIENCE QUESTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using science question benchmarks<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-TASK PROBLEM SOLVING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search was evaluated using multi-task problem solving benchmarks<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"TRANSFERABILITY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search assesses the transferability of discovered agents to different tasks and models<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SUCCESS RATE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses success rate as a performance metric to evaluate generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"F1 SCORE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses F1 score as a performance metric to evaluate generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ACCURACY RATE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses accuracy rate as a performance metric to evaluate generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses Chain-of-Thought strategy to generate, refine, and ensemble answers<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFINE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses Self-Refine strategy for enhanced refinement of generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"COT-SC\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses COT-SC strategy for generating and refining answers<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"HUMAN-LIKE CRITIC\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses human-like critics to provide feedback on generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FEEDBACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses feedback from human-like critics to refine generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"EFFICIENCY EXPERT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses efficiency experts to provide feedback on the efficiency of generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"READABILITY EXPERT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Meta Agent Search uses readability experts to provide feedback on the readability of generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SIMPLICITY\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Meta Agent Search uses simplicity experts to provide feedback on the simplicity of generated agents<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"FUNSEARCH\" target=\"ROMERA-PAREDES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Romera-Paredes has worked on the FunSearch practice mentioned in the Meta Agent Search algorithm<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"ADAS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Shengran Hu is associated with the ADAS project and has made all related resources available on GitHub<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"GITHUB\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">All code, prompts, and experiment results related to the ADAS project are available on GitHub<\/data>      <data key=\"d5\">24d7b89ae9522ae60d2317984951355b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"1a6353c9d196dc2debad7c27c902bcd7","chunk":" refining them, and\nfinally ensembling the best answers.Introduced dynamic memory for doing more refinements.Scaled up the previous idea.Best agent: introduced multiple\ncritics for enhanced refinement.Meta-Agent Search on ARC\nChain-of-Thought\nSelf-Refine\nLLM DebateCOT-SC\nQuality-Diversity\nMeta-Agent Search\n(a)\nTask5 COTs\n5 Answers\nHuman -like \nCritic\nFeedbackEfficiency Expert\nReadability Expert\nSimplicity ExpertExperts\nFeedback\nRefinement\n3 timesAll \nAnswers\nEvaluateTop-3 \nAnswersEnsembleFinal \nAnswer\nStructured Feedback and Ensemble AgentThe Best Discovered Agent on ARC (b)\nFigure 3|The results of Meta Agent Search on the ARC challenge. (a) Meta Agent Search progres-\nsively discovers high-performance agents based on an ever-growing archive of previous discoveries.\nWe report the median accuracy and the 95% bootstrap confidence interval on a held-out test set by\nevaluating agents five times. (b) The visualization of the best agent discovered by Meta Agent Search\non the ARC challenge. Detailed implementation of this agent is available in Appendix C.\n4.1. Case Study: ARC Challenge\nWefirstdemonstratehowMetaAgentSearchdiscoversnovelagenticsystemsandoutperformsexisting\nstate-of-the-art hand-designed agents in the Abstraction and Reasoning Corpus (ARC) challenge (Chol-\nlet, 2019). This challenge aims to evaluate the general intelligence of AI systems through their ability\nto efficiently acquire new skills. Questions in ARC include (1) showing multiple examples of visual\ninput-output grid patterns, (2) the AI system learning the transformation rule of grid patterns from\nexamples, and (3) predicting the output grid pattern given a test input grid pattern. Since each\nquestion in ARC has a unique transformation rule, it requires the AI system to learn efficiently with\n6Automated Design of Agentic Systems\nfew-shot examples, leveraging capabilities in number counting, geometry, and topology.\nSetup. Following common practice (Greenblatt, 2024), we require the agent to write code for the\ntransformation rule instead of answering directly. We provide tool functions in the framework that\nevaluate the generated transformation code. Given the significant challenge that ARC poses to current\nAI systems, we sample our data from questions with grid dimensions \u22645\u00d75in the \u201cPublic Training\nSet (Easy)\u201d. We sample a validation set and a test set with 20 and 60 questions, respectively, for\nsearching and testing. We calculate the validation and test accuracy of an agent by assessing it over\nthe validation and test sets five times to reduce the variance from the stochastic sampling of FMs. We\nevaluate all discovered agents on the held-out test set and report the test accuracy in Figure 3. Meta\nAgent Search runs for 25 iterations and the meta agent uses GPT-4 (OpenAI, 2024), while discovered\nagents and baselines are evaluated using GPT-3.5 (OpenAI, 2022) to reduce compute cost. More\nalgorithmic details and examples of ARC questions can be found in Appendix C.\nBaselines. We compared against five state-of-the-art hand-designed agents: (1) Chain-of-Thought\n(COT) (Wei et al., 2022), which instructs the agent to output the reasoning before answering to\nimprove complex problem-solving through intermediate steps; (2) Self-Consistency with Chain-of-\nThought (COT-SC) (Wang et al., 2023b), which ensembles multiple parallel answers from COT to\nproduce a more accurate answer; (3) Self-Refine (Madaan et al., 2024; Shinn et al., 2023), which\nallows iterative self-reflection to correct mistakes made in previous attempts; (4) LLM-Debate (Du\net al., 2023), which enables different LLMs to debate with each other, leveraging diverse perspectives\nto find better answers; (5) Quality-Diversity, a simplified version of Intelligent Go-Explore (Lu et al.,\n2024c), which produces and ensembles diverse answers to better explore potential solutions. We also\nuse all baselines as initial seeds in the archive for Meta Agent Search. More details about baselines\ncan be found in Appendix E.\nResults and Analysis. As shown in Figure 3a, Meta Agent Search effectively and progressively\ndiscovers agents that perform better than state-of-the-art hand-designed baselines. Important break-\nthroughs are highlighted in the text boxes. As is critical in prior works on open-endedness and AI-GAs\n(Faldor et al., 2024; Lehman & Stanley, 2011; Wang et al., 2019, 2020; Zhang et al., 2024a), Meta\nAgent Search innovates based on a growing archive of previous stepping stones. For example, an\nimportant design pattern emerged in iteration 3 where it uses multiple COTs to generate possible\nanswers, refines them, and finally ensembles the best answers. This became a crucial stepping\nstone that subsequent designs tended to utilize. Additionally, the best-discovered agent is shown\nin Figure 3b, where a complex feedback mechanism is adopted to refine answers more effectively.\nCareful observation of the search progress reveals that this sophisticated feedback mechanism did\nnot appear suddenly. Instead, the ideas of incorporating diverse feedback, evaluating for various\nspecific traits (via experts) such as efficiency and simplicity, and simulating human-like feedback\nemerged in iterations 5, 11, and 12, respectively. The final mechanism is an innovation based on\nthese three stepping stones. This illustrates that even though these stepping stones did not achieve","chunk_id":"1a6353c9d196dc2debad7c27c902bcd7","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that progressively discovers high-performance agents based on an ever-growing archive of previous discoveries. It is used to evaluate agents on the ARC challenge and runs for 25 iterations using GPT-4 for the meta agent and GPT-3.5 for discovered agents and baselines","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"ARC CHALLENGE","type":"EVENT","description":"The ARC (Abstraction and Reasoning Corpus) challenge aims to evaluate the general intelligence of AI systems through their ability to efficiently acquire new skills by learning transformation rules of grid patterns from examples and predicting output grid patterns given a test input grid pattern","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"EVENT"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought (COT) is a method that instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Self-Consistency with Chain-of-Thought (COT-SC) is a method that ensembles multiple parallel answers from COT to produce a more accurate answer","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine is a method that allows iterative self-reflection to correct mistakes made in previous attempts","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"LLM-DEBATE","type":"TECHNOLOGY","description":"LLM-Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a language model developed by OpenAI, used by the meta agent in Meta Agent Search","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a language model developed by OpenAI, used to evaluate discovered agents and baselines in Meta Agent Search","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"TECHNOLOGY"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on open-endedness and AI-GAs","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author who has worked on open-endedness and AI-GAs","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author who has worked on open-endedness and AI-GAs","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author who has worked on open-endedness and AI-GAs","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"GREENBLATT","type":"PERSON","description":"Greenblatt is an author who has worked on the setup for the ARC challenge","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that developed GPT-4 and GPT-3.5, which are used in Meta Agent Search","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"ORGANIZATION"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has worked on Chain-of-Thought (COT)","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who has worked on Self-Refine","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who has worked on Self-Refine","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"DU","type":"PERSON","description":"Du is an author who has worked on LLM-Debate","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"LU","type":"PERSON","description":"Lu is an author who has worked on Quality-Diversity","source_id":"1a6353c9d196dc2debad7c27c902bcd7","entity_type":"PERSON"},{"name":"DYNAMIC MEMORY","type":"TECHNOLOGY","description":"Dynamic Memory is a system introduced for doing more refinements in the context of Meta Agent Search","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"MULTIPLE CRITICS","type":"TECHNOLOGY","description":"Multiple Critics is a feature introduced in the best agent for enhanced refinement in Meta Agent Search","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"CRITIC","type":"TECHNOLOGY","description":"Critic is a component of the best agent in Meta Agent Search that provides feedback for refinement","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"EFFICIENCY EXPERT","type":"TECHNOLOGY","description":"Efficiency Expert is a type of expert used in the best agent in Meta Agent Search to evaluate efficiency","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"READABILITY EXPERT","type":"TECHNOLOGY","description":"Readability Expert is a type of expert used in the best agent in Meta Agent Search to evaluate readability","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SIMPLICITY EXPERT","type":"TECHNOLOGY","description":"Simplicity Expert is a type of expert used in the best agent in Meta Agent Search to evaluate simplicity","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"STRUCTURED FEEDBACK AND ENSEMBLE AGENT","type":"TECHNOLOGY","description":"Structured Feedback and Ensemble Agent is the best discovered agent on ARC by Meta Agent Search, which uses a complex feedback mechanism to refine answers more effectively","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"ABSTRACTION AND REASONING CORPUS","type":"EVENT","description":"The Abstraction and Reasoning Corpus (ARC) is a challenge that evaluates the general intelligence of AI systems through their ability to efficiently acquire new skills by learning transformation rules of grid patterns from examples and predicting output grid patterns given a test input grid pattern","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"TRANSFORMATION RULE","type":"TECHNOLOGY","description":"Transformation Rule is a rule that AI systems need to learn from examples in the ARC challenge to predict output grid patterns given a test input grid pattern","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"TOOL FUNCTIONS","type":"TECHNOLOGY","description":"Tool Functions are provided in the framework to evaluate the generated transformation code in the ARC challenge","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"PUBLIC TRAINING SET (EASY)","type":"TECHNOLOGY","description":"Public Training Set (Easy) is a dataset used in the ARC challenge with grid dimensions \u22645\u00d75","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"VALIDATION SET","type":"TECHNOLOGY","description":"Validation Set is a dataset used to validate the accuracy of agents in the ARC challenge","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"TEST SET","type":"TECHNOLOGY","description":"Test Set is a dataset used to test the accuracy of agents in the ARC challenge","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"STOCHASTIC SAMPLING OF FMS","type":"TECHNOLOGY","description":"Stochastic Sampling of FMs is a method used to reduce variance in the evaluation of agents in the ARC challenge","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNOLOGY","description":"Chain-of-Thought (COT) is a method that instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)","type":"TECHNOLOGY","description":"Self-Consistency with Chain-of-Thought (COT-SC) is a method that ensembles multiple parallel answers from COT to produce a more accurate answer","source_id":"1a6353c9d196dc2debad7c27c902bcd7"},{"name":"INTELLIGENT GO-EXPLORE","type":"TECHNOLOGY","description":"Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers","source_id":"1a6353c9d196dc2debad7c27c902bcd7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that progressively discovers high-performance agents based on an ever-growing archive of previous discoveries. It is used to evaluate agents on the ARC challenge and runs for 25 iterations using GPT-4 for the meta agent and GPT-3.5 for discovered agents and baselines<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARC CHALLENGE\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The ARC (Abstraction and Reasoning Corpus) challenge aims to evaluate the general intelligence of AI systems through their ability to efficiently acquire new skills by learning transformation rules of grid patterns from examples and predicting output grid patterns given a test input grid pattern<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought (COT) is a method that instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Consistency with Chain-of-Thought (COT-SC) is a method that ensembles multiple parallel answers from COT to produce a more accurate answer<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine is a method that allows iterative self-reflection to correct mistakes made in previous attempts<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM-Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a language model developed by OpenAI, used by the meta agent in Meta Agent Search<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a language model developed by OpenAI, used to evaluate discovered agents and baselines in Meta Agent Search<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GREENBLATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Greenblatt is an author who has worked on the setup for the ARC challenge<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that developed GPT-4 and GPT-3.5, which are used in Meta Agent Search<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has worked on Chain-of-Thought (COT)<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who has worked on Self-Refine<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who has worked on Self-Refine<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Du is an author who has worked on LLM-Debate<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author who has worked on Quality-Diversity<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DYNAMIC MEMORY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Dynamic Memory is a system introduced for doing more refinements in the context of Meta Agent Search<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"MULTIPLE CRITICS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multiple Critics is a feature introduced in the best agent for enhanced refinement in Meta Agent Search<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"CRITIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Critic is a component of the best agent in Meta Agent Search that provides feedback for refinement<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"EFFICIENCY EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Efficiency Expert is a type of expert used in the best agent in Meta Agent Search to evaluate efficiency<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"READABILITY EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Readability Expert is a type of expert used in the best agent in Meta Agent Search to evaluate readability<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SIMPLICITY EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Simplicity Expert is a type of expert used in the best agent in Meta Agent Search to evaluate simplicity<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Structured Feedback and Ensemble Agent is the best discovered agent on ARC by Meta Agent Search, which uses a complex feedback mechanism to refine answers more effectively<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"ABSTRACTION AND REASONING CORPUS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Abstraction and Reasoning Corpus (ARC) is a challenge that evaluates the general intelligence of AI systems through their ability to efficiently acquire new skills by learning transformation rules of grid patterns from examples and predicting output grid patterns given a test input grid pattern<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"TRANSFORMATION RULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Transformation Rule is a rule that AI systems need to learn from examples in the ARC challenge to predict output grid patterns given a test input grid pattern<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"TOOL FUNCTIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Tool Functions are provided in the framework to evaluate the generated transformation code in the ARC challenge<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"PUBLIC TRAINING SET (EASY)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Public Training Set (Easy) is a dataset used in the ARC challenge with grid dimensions &#8804;5&#215;5<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"VALIDATION SET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Validation Set is a dataset used to validate the accuracy of agents in the ARC challenge<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"TEST SET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Test Set is a dataset used to test the accuracy of agents in the ARC challenge<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"STOCHASTIC SAMPLING OF FMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Stochastic Sampling of FMs is a method used to reduce variance in the evaluation of agents in the ARC challenge<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought (COT) is a method that instructs the agent to output the reasoning before answering to improve complex problem-solving through intermediate steps<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Consistency with Chain-of-Thought (COT-SC) is a method that ensembles multiple parallel answers from COT to produce a more accurate answer<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <node id=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers<\/data>      <data key=\"d2\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"ARC CHALLENGE\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Meta Agent Search is used to evaluate agents on the ARC challenge<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Chain-of-Thought as one of the baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Self-Consistency with Chain-of-Thought as one of the baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFINE\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Self-Refine as one of the baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LLM-DEBATE\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses LLM-Debate as one of the baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta Agent Search uses Quality-Diversity as one of the baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search uses GPT-4 for the meta agent<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search uses GPT-3.5 to evaluate discovered agents and baselines<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"WEI\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Wei has worked on Chain-of-Thought<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MADAAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Madaan has worked on Self-Refine<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"SHINN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Shinn has worked on Self-Refine<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"DU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Du has worked on LLM-Debate<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"LU\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Lu has worked on Quality-Diversity<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"OPENAI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">OpenAI developed GPT-4<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"GPT-3.5\" target=\"OPENAI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">OpenAI developed GPT-3.5<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"LEHMAN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Lehman have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"STANLEY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"ZHANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Faldor and Zhang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"STANLEY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lehman and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lehman and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"ZHANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lehman and Zhang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Stanley and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"ZHANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Stanley and Zhang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>    <edge source=\"WANG\" target=\"ZHANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wang and Zhang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d6\">1a6353c9d196dc2debad7c27c902bcd7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bc26e68b0b2783ba912b9e5606d9eb0b","chunk":" is adopted to refine answers more effectively.\nCareful observation of the search progress reveals that this sophisticated feedback mechanism did\nnot appear suddenly. Instead, the ideas of incorporating diverse feedback, evaluating for various\nspecific traits (via experts) such as efficiency and simplicity, and simulating human-like feedback\nemerged in iterations 5, 11, and 12, respectively. The final mechanism is an innovation based on\nthese three stepping stones. This illustrates that even though these stepping stones did not achieve\nhigh performance immediately upon emergence, later discoveries benefited from these innovations\nby combining different stepping stones, resembling crossover in evolution via LLMs (Meyerson et al.,\n2023). Overall, the results showcase the potential of ADAS and the effectiveness of Meta Agent Search\nto progressively discover agents that outperform state-of-the-art hand-designed baselines and invent\nnovel design patterns through the innovation and combination of various stepping stones.\n4.2. Reasoning and Problem-Solving Domains\nSetup. Next,weinvestigatethepotentialofouralgorithmtoimprovethecapabilitiesofagentsacross\nmath, reading, and reasoning domains. We test Meta Agent Search on four popular benchmarks: (1)\nDROP (Dua et al., 2019) for evaluating Reading Comprehension ; (2) MGSM (Shi et al., 2023) for\n7Automated Design of Agentic Systems\nAgent NameF1 Score Accuracy (%)\nReading Comprehension Math Multi-task Science\nState-of-the-art Hand-designed Agents\nChain-of-Thought (Wei et al., 2022) 64.2\u00b10.9 28 .0\u00b13.1 65 .4\u00b13.3 29 .2\u00b13.1\nCOT-SC (Wang et al., 2023b) 64.4\u00b10.8 28 .2\u00b13.165.9\u00b13.230.5\u00b13.2\nSelf-Refine (Madaan et al., 2024) 59.2\u00b10.9 27 .5\u00b13.1 63 .5\u00b13.431.6\u00b13.2\nLLM Debate (Du et al., 2023) 60.6\u00b10.9 39.0\u00b13.465.6\u00b13.3 31 .4\u00b13.2\nStep-back Abstraction (Zheng et al., 2023) 60.4\u00b11.0 31 .1\u00b13.2 65 .1\u00b13.3 26 .9\u00b13.0\nQuality-Diversity (Lu et al., 2024c) 61.8\u00b10.9 23 .8\u00b13.0 65 .1\u00b13.3 30 .2\u00b13.1\nRole Assignment (Xu et al., 2023) 65.8\u00b10.9 30.1\u00b13.2 64 .5\u00b13.3 31 .1\u00b13.1\nAutomated Design of Agentic Systems on Different Domains\nBest Agents from Meta Agent Search 79.4\u00b10.8 53 .4\u00b13.5 69 .6\u00b13.2 34 .6\u00b13.2\nTable 1|Performance comparison between Meta Agent Search and state-of-the-art hand-\ndesigned agents across multiple domains. Meta Agent Search discovers superior agents compared\nto the baselines in every domain. We report the test accuracy and the 95% bootstrap confidence\ninterval on held-out test sets. The search is conducted independently for each domain.\nevaluating Mathcapability under a multi-lingual setting; (3) MMLU (Hendrycks et al., 2021) for\nevaluating Multi-task Problem Solving; and (4) GPQA (Rein et al., 2023) for evaluating the capability\nof solving hard (graduate-level) questions in Science. The search is conducted independently within\neach domain. Meta Agent Search runs for 30 iterations. The meta agent uses GPT-4 (OpenAI, 2024),\nwhile the discovered agents and baselines are evaluated using GPT-3.5 (OpenAI, 2022). More details\nabout datasets and experiment settings can be found in Appendix D.\nBaselines. We adopt all baselines introduced in Section 4.1. Additionally, since the above domains\nrequirestrongreasoningskills,weincludetwoadditionalbaselinesthatspecificallyfocusonenhancing\nthereasoningcapabilitiesofagentsforamorethoroughcomparison: (1)Step-backAbstraction(Zheng\net al., 2023), which instructs agents to first consider the principles involved in solving the task for\nbetter reasoning; (2) Role Assignment, which assigns different roles to FMs similar to Xu et al. (2023)\nto obtain better answers. More details about the baselines can be found in Appendix E.\nResults and Analysis. The results across multiple domains demonstrate that Meta Agent Search\ncan discover agents that outperform state-of-the-art hand-designed agents (Table 1). We want to\nhighlight the substantial gap between the learned agents and hand-designed agents in the Reading\nComprehension and Math domains, with improvements in F1 scores by 13.6\/100 and accuracy rates\nby14.4%, respectively. While Meta Agent Search also outperforms baselines in the Multi-task and\nScience domains, the gap is smaller. We hypothesize that for challenging questions in the Science\nand Multi-task domains, the knowledge in FMs is not sufficient to solve the questions, limiting the\nimprovement through optimizing agentic systems, which is a problem that will diminish as FMs\nimprove. In","chunk_id":"bc26e68b0b2783ba912b9e5606d9eb0b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"ADAS","type":"TECHNOLOGY","description":"ADAS (Automated Design of Agentic Systems) is a system that showcases the potential to progressively discover agents that outperform state-of-the-art hand-designed baselines and invent novel design patterns through the innovation and combination of various stepping stones","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"DROP","type":"BENCHMARK","description":"DROP (Dua et al., 2019) is a benchmark used for evaluating Reading Comprehension","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MGSM","type":"BENCHMARK","description":"MGSM (Shi et al., 2023) is a benchmark used for evaluating Math capability under a multi-lingual setting","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU (Hendrycks et al., 2021) is a benchmark used for evaluating Multi-task Problem Solving","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPQA","type":"BENCHMARK","description":"GPQA (Rein et al., 2023) is a benchmark used for evaluating the capability of solving hard (graduate-level) questions in Science","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 (OpenAI, 2024) is a language model used by the meta agent in Meta Agent Search","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 (OpenAI, 2022) is a language model used to evaluate the discovered agents and baselines in Meta Agent Search","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought (Wei et al., 2022) is a state-of-the-art hand-designed agent used for comparison in the study","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"COT-SC","type":"TECHNOLOGY","description":"COT-SC (Wang et al., 2023b) is a state-of-the-art hand-designed agent used for comparison in the study","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine (Madaan et al., 2024) is a state-of-the-art hand-designed agent used for comparison in the study","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"LLM DEBATE","type":"TECHNOLOGY","description":"LLM Debate (Du et al., 2023) is a state-of-the-art hand-designed agent used for comparison in the study","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNOLOGY","description":"Step-back Abstraction (Zheng et al., 2023) is a state-of-the-art hand-designed agent that instructs agents to first consider the principles involved in solving the task for better reasoning","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity (Lu et al., 2024c) is a state-of-the-art hand-designed agent used for comparison in the study","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ROLE ASSIGNMENT","type":"TECHNOLOGY","description":"Role Assignment (Xu et al., 2023) is a state-of-the-art hand-designed agent that assigns different roles to FMs to obtain better answers","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"DUA","type":"PERSON","description":"Dua is an author associated with the DROP benchmark for evaluating Reading Comprehension","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"SHI","type":"PERSON","description":"Shi is an author associated with the MGSM benchmark for evaluating Math capability under a multi-lingual setting","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"HENDRYCKS","type":"PERSON","description":"Hendrycks is an author associated with the MMLU benchmark for evaluating Multi-task Problem Solving","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"REIN","type":"PERSON","description":"Rein is an author associated with the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in Science","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"WEI","type":"PERSON","description":"Wei is an author associated with the Chain-of-Thought state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"WANG","type":"PERSON","description":"Wang is an author associated with the COT-SC state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author associated with the Self-Refine state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"DU","type":"PERSON","description":"Du is an author associated with the LLM Debate state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"ZHENG","type":"PERSON","description":"Zheng is an author associated with the Step-back Abstraction state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"LU","type":"PERSON","description":"Lu is an author associated with the Quality-Diversity state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"XU","type":"PERSON","description":"Xu is an author associated with the Role Assignment state-of-the-art hand-designed agent","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b","entity_type":"PERSON"},{"name":"FEEDBACK MECHANISM","type":"TECHNOLOGY","description":"The feedback mechanism is a sophisticated system used to refine answers more effectively by incorporating diverse feedback, evaluating various specific traits, and simulating human-like feedback","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ITERATION 5","type":"EVENT","description":"Iteration 5 is a specific stage in the search progress where the idea of incorporating diverse feedback emerged","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ITERATION 11","type":"EVENT","description":"Iteration 11 is a specific stage in the search progress where the idea of evaluating for various specific traits such as efficiency and simplicity emerged","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ITERATION 12","type":"EVENT","description":"Iteration 12 is a specific stage in the search progress where the idea of simulating human-like feedback emerged","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MECHANISM","type":"TECHNOLOGY","description":"The final mechanism is an innovation based on the ideas from iterations 5, 11, and 12, used to refine answers more effectively","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"CROSSOVER","type":"PROCESS","description":"Crossover in evolution via LLMs refers to the combination of different stepping stones to achieve better performance","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"REASONING AND PROBLEM-SOLVING DOMAINS","type":"DOMAIN","description":"Reasoning and Problem-Solving Domains refer to the areas of math, reading, and reasoning where the algorithm's capabilities are tested","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"SETUP","type":"PROCESS","description":"Setup refers to the process of investigating the potential of the algorithm to improve the capabilities of agents across various domains","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"BENCHMARKS","type":"TECHNOLOGY","description":"Benchmarks are the standards or points of reference against which the performance of Meta Agent Search is compared","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"EXPERIMENT SETTINGS","type":"PROCESS","description":"Experiment settings refer to the specific conditions and parameters under which the Meta Agent Search is tested","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"BASELINES","type":"TECHNOLOGY","description":"Baselines are the initial standards or reference points used for comparison in the Meta Agent Search","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"RESULTS AND ANALYSIS","type":"PROCESS","description":"Results and Analysis refer to the evaluation and interpretation of the performance of Meta Agent Search across multiple domains","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"F1 SCORE","type":"METRIC","description":"F1 Score is a metric used to evaluate the performance of agents in the Reading Comprehension and Math domains","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"ACCURACY RATE","type":"METRIC","description":"Accuracy Rate is a metric used to evaluate the performance of agents in the Reading Comprehension and Math domains","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MULTI-TASK DOMAIN","type":"DOMAIN","description":"Multi-task Domain refers to the area where the algorithm's capability to solve multiple tasks is tested","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"SCIENCE DOMAIN","type":"DOMAIN","description":"Science Domain refers to the area where the algorithm's capability to solve hard (graduate-level) questions in Science is tested","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"FMS","type":"TECHNOLOGY","description":"FMs (Foundation Models) are the models whose knowledge is used to solve challenging questions in the Science and Multi-task domains","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"MEYERSON","type":"PERSON","description":"Meyerson is an author associated with the concept of crossover in evolution via LLMs","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"},{"name":"SEARCH PROGRESS","type":"","description":"","source_id":"bc26e68b0b2783ba912b9e5606d9eb0b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) is a system that showcases the potential to progressively discover agents that outperform state-of-the-art hand-designed baselines and invent novel design patterns through the innovation and combination of various stepping stones<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP (Dua et al., 2019) is a benchmark used for evaluating Reading Comprehension<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MGSM (Shi et al., 2023) is a benchmark used for evaluating Math capability under a multi-lingual setting<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU (Hendrycks et al., 2021) is a benchmark used for evaluating Multi-task Problem Solving<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GPQA (Rein et al., 2023) is a benchmark used for evaluating the capability of solving hard (graduate-level) questions in Science<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 (OpenAI, 2024) is a language model used by the meta agent in Meta Agent Search<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 (OpenAI, 2022) is a language model used to evaluate the discovered agents and baselines in Meta Agent Search<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought (Wei et al., 2022) is a state-of-the-art hand-designed agent used for comparison in the study<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT-SC (Wang et al., 2023b) is a state-of-the-art hand-designed agent used for comparison in the study<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine (Madaan et al., 2024) is a state-of-the-art hand-designed agent used for comparison in the study<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM Debate (Du et al., 2023) is a state-of-the-art hand-designed agent used for comparison in the study<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Step-back Abstraction (Zheng et al., 2023) is a state-of-the-art hand-designed agent that instructs agents to first consider the principles involved in solving the task for better reasoning<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity (Lu et al., 2024c) is a state-of-the-art hand-designed agent used for comparison in the study<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Role Assignment (Xu et al., 2023) is a state-of-the-art hand-designed agent that assigns different roles to FMs to obtain better answers<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dua is an author associated with the DROP benchmark for evaluating Reading Comprehension<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shi is an author associated with the MGSM benchmark for evaluating Math capability under a multi-lingual setting<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hendrycks is an author associated with the MMLU benchmark for evaluating Multi-task Problem Solving<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"REIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rein is an author associated with the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in Science<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author associated with the Chain-of-Thought state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author associated with the COT-SC state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author associated with the Self-Refine state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Du is an author associated with the LLM Debate state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is an author associated with the Step-back Abstraction state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author associated with the Quality-Diversity state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is an author associated with the Role Assignment state-of-the-art hand-designed agent<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEEDBACK MECHANISM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The feedback mechanism is a sophisticated system used to refine answers more effectively by incorporating diverse feedback, evaluating various specific traits, and simulating human-like feedback<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ITERATION 5\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Iteration 5 is a specific stage in the search progress where the idea of incorporating diverse feedback emerged<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ITERATION 11\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Iteration 11 is a specific stage in the search progress where the idea of evaluating for various specific traits such as efficiency and simplicity emerged<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ITERATION 12\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Iteration 12 is a specific stage in the search progress where the idea of simulating human-like feedback emerged<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MECHANISM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The final mechanism is an innovation based on the ideas from iterations 5, 11, and 12, used to refine answers more effectively<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"CROSSOVER\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Crossover in evolution via LLMs refers to the combination of different stepping stones to achieve better performance<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Reasoning and Problem-Solving Domains refer to the areas of math, reading, and reasoning where the algorithm's capabilities are tested<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"SETUP\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Setup refers to the process of investigating the potential of the algorithm to improve the capabilities of agents across various domains<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"BENCHMARKS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Benchmarks are the standards or points of reference against which the performance of Meta Agent Search is compared<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"EXPERIMENT SETTINGS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Experiment settings refer to the specific conditions and parameters under which the Meta Agent Search is tested<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"BASELINES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Baselines are the initial standards or reference points used for comparison in the Meta Agent Search<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"RESULTS AND ANALYSIS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Results and Analysis refer to the evaluation and interpretation of the performance of Meta Agent Search across multiple domains<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"F1 SCORE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">F1 Score is a metric used to evaluate the performance of agents in the Reading Comprehension and Math domains<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"ACCURACY RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Accuracy Rate is a metric used to evaluate the performance of agents in the Reading Comprehension and Math domains<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MULTI-TASK DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Multi-task Domain refers to the area where the algorithm's capability to solve multiple tasks is tested<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"SCIENCE DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Science Domain refers to the area where the algorithm's capability to solve hard (graduate-level) questions in Science is tested<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"FMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FMs (Foundation Models) are the models whose knowledge is used to solve challenging questions in the Science and Multi-task domains<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"MEYERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meyerson is an author associated with the concept of crossover in evolution via LLMs<\/data>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <node id=\"SEARCH PROGRESS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/node>    <edge source=\"ADAS\" target=\"META AGENT SEARCH\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">ADAS showcases the potential of Meta Agent Search to progressively discover agents that outperform state-of-the-art hand-designed baselines<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DROP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested on the DROP benchmark for evaluating Reading Comprehension<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested on the MGSM benchmark for evaluating Math capability under a multi-lingual setting<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MMLU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested on the MMLU benchmark for evaluating Multi-task Problem Solving<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPQA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested on the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in Science<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Meta Agent Search uses GPT-4 as the language model for the meta agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Meta Agent Search uses GPT-3.5 to evaluate the discovered agents and baselines<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Meta Agent Search is tested in the Reasoning and Problem-Solving Domains<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SETUP\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Setup involves investigating the potential of Meta Agent Search to improve the capabilities of agents<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"BENCHMARKS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Meta Agent Search is tested on various benchmarks<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"EXPERIMENT SETTINGS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Experiment settings provide the conditions under which Meta Agent Search is tested<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"BASELINES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Meta Agent Search uses baselines as initial seeds in the archive<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"RESULTS AND ANALYSIS\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Results and Analysis demonstrate the performance of Meta Agent Search across multiple domains<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"F1 SCORE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">F1 Score is used to evaluate the performance of Meta Agent Search in the Reading Comprehension and Math domains<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ACCURACY RATE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Accuracy Rate is used to evaluate the performance of Meta Agent Search in the Reading Comprehension and Math domains<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-TASK DOMAIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested in the Multi-task Domain<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SCIENCE DOMAIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Meta Agent Search is tested in the Science Domain<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FMS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The knowledge in FMs is used to solve challenging questions in the Science and Multi-task domains<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"DROP\" target=\"DUA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Dua is an author associated with the DROP benchmark for evaluating Reading Comprehension<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"SHI\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Shi is an author associated with the MGSM benchmark for evaluating Math capability under a multi-lingual setting<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"HENDRYCKS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Hendrycks is an author associated with the MMLU benchmark for evaluating Multi-task Problem Solving<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"REIN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Rein is an author associated with the GPQA benchmark for evaluating the capability of solving hard (graduate-level) questions in Science<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"WEI\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wei is an author associated with the Chain-of-Thought state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"WANG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Wang is an author associated with the COT-SC state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MADAAN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Madaan is an author associated with the Self-Refine state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"DU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Du is an author associated with the LLM Debate state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"ZHENG\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Zheng is an author associated with the Step-back Abstraction state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"LU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Lu is an author associated with the Quality-Diversity state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"XU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Xu is an author associated with the Role Assignment state-of-the-art hand-designed agent<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"FEEDBACK MECHANISM\" target=\"SEARCH PROGRESS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The search progress reveals the development of the feedback mechanism through various iterations<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"FEEDBACK MECHANISM\" target=\"ITERATION 5\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The idea of incorporating diverse feedback emerged in iteration 5<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"FEEDBACK MECHANISM\" target=\"ITERATION 11\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The idea of evaluating for various specific traits such as efficiency and simplicity emerged in iteration 11<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"FEEDBACK MECHANISM\" target=\"ITERATION 12\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The idea of simulating human-like feedback emerged in iteration 12<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"ITERATION 5\" target=\"MECHANISM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The final mechanism is based on the idea from iteration 5<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"ITERATION 11\" target=\"MECHANISM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The final mechanism is based on the idea from iteration 11<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"ITERATION 12\" target=\"MECHANISM\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The final mechanism is based on the idea from iteration 12<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"MECHANISM\" target=\"CROSSOVER\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">The final mechanism resembles crossover in evolution via LLMs<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>    <edge source=\"CROSSOVER\" target=\"MEYERSON\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Meyerson is an author associated with the concept of crossover in evolution via LLMs<\/data>      <data key=\"d6\">bc26e68b0b2783ba912b9e5606d9eb0b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"2901d5e2711fa4f32d39cd8eea36cd71","chunk":"1 scores by 13.6\/100 and accuracy rates\nby14.4%, respectively. While Meta Agent Search also outperforms baselines in the Multi-task and\nScience domains, the gap is smaller. We hypothesize that for challenging questions in the Science\nand Multi-task domains, the knowledge in FMs is not sufficient to solve the questions, limiting the\nimprovement through optimizing agentic systems, which is a problem that will diminish as FMs\nimprove. In contrast, in the Reading Comprehension and Math domains, FMs possess adequate\nknowledge to solve the questions, and errors could mainly be hallucinations or calculation mistakes,\nwhich can be mitigated through well-designed agentic systems, like the ones discovered by Meta\nAgent Search. Overall, the results across various domains showcase the effectiveness of Meta Agent\nSearch in searching for agents tailored to specific domains. This could be increasingly useful for\nsaving human efforts and developing better task-specific agents as we continue to create agents for a\ndiverse set of applications (Wang et al., 2024).\n8Automated Design of Agentic Systems\n4.3. Generalization and transferability\nAgent NameAccuracy on ARC (%)\nGPT-3.5 Claude-Haiku GPT-4 Claude-Sonnet\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 6.0\u00b12.7 4.3\u00b12.2 17 .7\u00b14.4 25 .3\u00b15.0\nCOT-SC (Wang et al., 2023b) 8.0\u00b13.2 5.3\u00b12.5 19 .7\u00b14.5 26 .3\u00b14.9\nLLM Debate (Du et al., 2023) 4.0\u00b12.2 1.7\u00b11.5 19 .0\u00b14.5 24 .7\u00b14.8\nSelf-Refine (Madaan et al., 2024) 6.7\u00b12.7 6.3\u00b12.8 23 .0\u00b15.2 39 .3\u00b15.5\nQuality-Diversity (Lu et al., 2024c) 7.0\u00b12.9 3.3\u00b12.2 23.0\u00b14.7 31.7\u00b15.3\nTop Agents Searched with GPT-3.5 Transferred to Other FMs\nStructured Feedback and Ensemble Agent 13.7\u00b13.9 5.0\u00b12.5 30 .0\u00b15.2 38 .7\u00b15.5\nHierarchical Committee Reinforcement Agent 13.3\u00b13.8 8.3\u00b13.2 32 .3\u00b18.9 39 .7\u00b15.5\nDynamic Memory and Refinement Agent\u202012.7\u00b13.9 9.7\u00b13.3 37 .0\u00b15.3 48 .3\u00b15.7\nTable 2|Performance on ARC when transferring top agents from GPT-3.5 to other FMs. Agents\ndiscovered by Meta Agent Search consistently outperform the baselines across different models. We\nreport the test accuracy and the 95% bootstrap confidence interval. The names of top agents are\ngenerated by Meta Agent Search.\u2020We manually changed this name because the original generated\nname was confusing.\nIn the previous sections, we illustrated that Meta Agent Search can find effective agents for\nindividual tasks. In this section, we further demonstrate the transferability and generalizability of the\ndiscovered agents. To show that the invented building blocks and design patterns are generalizable,\nwe conduct experiments on the transferability of the discovered agents.\nTransferability Across Foundation Models. We first transfer discovered agents from GPT-3.5 (Ope-\nnAI, 2022) to other FMs on ARC to test whether agents found when performing Meta Agent Search\nwith one FM generalize to others. We test the top 3 agents with the best test accuracy evaluated with\nGPT-3.5 on ARC and then transfer them to three popular models: Claude-Haiku (Anthropic, 2024a),\nGPT-4 (OpenAI, 2024), and Claude-Sonnet (Anthropic, 2024b). We adopt the same baselines as\nthose used in ARC (Section 4.1) and MGSM (Section 4.2). As shown in Table 2, we observe that the\nsearched agents consistently outperform the hand-designed agents with a substantial gap. Notably,\nwe found that Claude-Sonnet, the most powerful model from Anthropic, performs the best among all\ntested models, enabling our best agent to achieve nearly 50% accuracy on ARC.\nTransferability Across Domains. Next, we transfer the discovered agent from the MGSM (Math)\ndomain to other math domains to test whether the invented agents can generalize across different\ndomains. Similarly, we test the top 3 agents from MGSM and transfer them to (1) four popular math\ndomains: GSM8K (Cobbe et al., 2021), GSM-Hard (Gao et al., 2023), SVAMP (Patel et al., 2021),\nand ASDiv (Miao et al., 2020) and (2) three domains beyond math adopted in Section 4.2. As shown\nin Table 3, we observe a similar superiority in the performance of Meta Agent Search compared to\nbaselines. Notably, our agents improve accuracy by 25.9%and13.2%on GSM8K (Cobbe et al., 202","chunk_id":"2901d5e2711fa4f32d39cd8eea36cd71","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"MULTI-TASK DOMAIN","type":"DOMAIN","description":"The Multi-task domain is one of the areas where Meta Agent Search outperforms baselines, although the gap is smaller compared to other domains","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"SCIENCE DOMAIN","type":"DOMAIN","description":"The Science domain is one of the areas where Meta Agent Search outperforms baselines, although the gap is smaller compared to other domains","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"READING COMPREHENSION DOMAIN","type":"DOMAIN","description":"The Reading Comprehension domain is one of the areas where Meta Agent Search shows significant improvement over baselines","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"MATH DOMAIN","type":"DOMAIN","description":"The Math domain is one of the areas where Meta Agent Search shows significant improvement over baselines","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"FMS","type":"TECHNOLOGY","description":"Foundation Models (FMs) are models whose knowledge is leveraged in various domains, and their improvement is expected to enhance the performance of agentic systems","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs and is cited in the text","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a foundation model used as a baseline for testing the performance of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71"},{"name":"CLAUDE-HAIKU","type":"TECHNOLOGY","description":"Claude-Haiku is a foundation model from Anthropic used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a foundation model from OpenAI used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"CLAUDE-SONNET","type":"TECHNOLOGY","description":"Claude-Sonnet is a foundation model from Anthropic used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought is a manually designed agent used as a baseline for comparison in the text","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"COT-SC","type":"TECHNOLOGY","description":"COT-SC is a manually designed agent used as a baseline for comparison in the text","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"LLM DEBATE","type":"TECHNOLOGY","description":"LLM Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine is a system that iteratively refines answers to improve performance","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"STRUCTURED FEEDBACK AND ENSEMBLE AGENT","type":"TECHNOLOGY","description":"Structured Feedback and Ensemble Agent is one of the top agents discovered by Meta Agent Search and tested for transferability","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"HIERARCHICAL COMMITTEE REINFORCEMENT AGENT","type":"TECHNOLOGY","description":"Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search and tested for transferability","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"DYNAMIC MEMORY AND REFINEMENT AGENT","type":"TECHNOLOGY","description":"Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search and tested for transferability","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"TECHNOLOGY"},{"name":"ARC","type":"DATASET","description":"ARC (AI2 Reasoning Challenge) is a dataset used to test the performance of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"},{"name":"MGSM","type":"DATASET","description":"MGSM (Math Generalization and Synthesis Model) is a dataset used to test the performance of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-Hard is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"},{"name":"SVAMP","type":"DATASET","description":"SVAMP is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"},{"name":"ASDIV","type":"DATASET","description":"ASDiv is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search","source_id":"2901d5e2711fa4f32d39cd8eea36cd71","entity_type":"DATASET"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"MULTI-TASK DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">The Multi-task domain is one of the areas where Meta Agent Search outperforms baselines, although the gap is smaller compared to other domains<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"SCIENCE DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">The Science domain is one of the areas where Meta Agent Search outperforms baselines, although the gap is smaller compared to other domains<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"READING COMPREHENSION DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">The Reading Comprehension domain is one of the areas where Meta Agent Search shows significant improvement over baselines<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"MATH DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">The Math domain is one of the areas where Meta Agent Search shows significant improvement over baselines<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"FMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are models whose knowledge is leveraged in various domains, and their improvement is expected to enhance the performance of agentic systems<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs and is cited in the text<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a foundation model used as a baseline for testing the performance of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/node>    <node id=\"CLAUDE-HAIKU\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude-Haiku is a foundation model from Anthropic used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a foundation model from OpenAI used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CLAUDE-SONNET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude-Sonnet is a foundation model from Anthropic used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought is a manually designed agent used as a baseline for comparison in the text<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT-SC is a manually designed agent used as a baseline for comparison in the text<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM Debate is a system that enables different Large Language Models (LLMs) to debate with each other to leverage diverse perspectives and find better answers<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine is a system that iteratively refines answers to improve performance<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore that produces and ensembles diverse answers to better explore potential solutions<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Structured Feedback and Ensemble Agent is one of the top agents discovered by Meta Agent Search and tested for transferability<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"HIERARCHICAL COMMITTEE REINFORCEMENT AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Hierarchical Committee Reinforcement Agent is one of the top agents discovered by Meta Agent Search and tested for transferability<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DYNAMIC MEMORY AND REFINEMENT AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Dynamic Memory and Refinement Agent is one of the top agents discovered by Meta Agent Search and tested for transferability<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ARC (AI2 Reasoning Challenge) is a dataset used to test the performance of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MGSM (Math Generalization and Synthesis Model) is a dataset used to test the performance of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-Hard is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"SVAMP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">SVAMP is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"ASDIV\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ASDiv is a math domain dataset used to test the transferability of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-TASK DOMAIN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search outperforms baselines in the Multi-task domain<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SCIENCE DOMAIN\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search outperforms baselines in the Science domain<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"READING COMPREHENSION DOMAIN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search shows significant improvement over baselines in the Reading Comprehension domain<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MATH DOMAIN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search shows significant improvement over baselines in the Math domain<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"FMS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search leverages the knowledge in Foundation Models (FMs) to solve questions in various domains<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-3.5\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search uses GPT-3.5 as a baseline for testing the performance of discovered agents<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CLAUDE-HAIKU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on Claude-Haiku<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GPT-4\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on GPT-4<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CLAUDE-SONNET\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on Claude-Sonnet<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search compares its discovered agents against the Chain-of-Thought baseline<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"COT-SC\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search compares its discovered agents against the COT-SC baseline<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"LLM DEBATE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search compares its discovered agents against the LLM Debate baseline<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SELF-REFINE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search compares its discovered agents against the Self-Refine baseline<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Meta Agent Search compares its discovered agents against the Quality-Diversity baseline<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"STRUCTURED FEEDBACK AND ENSEMBLE AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search discovered the Structured Feedback and Ensemble Agent<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"HIERARCHICAL COMMITTEE REINFORCEMENT AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search discovered the Hierarchical Committee Reinforcement Agent<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DYNAMIC MEMORY AND REFINEMENT AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search discovered the Dynamic Memory and Refinement Agent<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ARC\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the performance of discovered agents on the ARC dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MGSM\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the performance of discovered agents on the MGSM dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM8K\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on the GSM8K dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM-HARD\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on the GSM-Hard dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SVAMP\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on the SVAMP dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ASDIV\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Meta Agent Search tests the transferability of discovered agents on the ASDiv dataset<\/data>      <data key=\"d6\">2901d5e2711fa4f32d39cd8eea36cd71<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0b6b4880e77d40e284702da16be4ef64","chunk":"3), SVAMP (Patel et al., 2021),\nand ASDiv (Miao et al., 2020) and (2) three domains beyond math adopted in Section 4.2. As shown\nin Table 3, we observe a similar superiority in the performance of Meta Agent Search compared to\nbaselines. Notably, our agents improve accuracy by 25.9%and13.2%on GSM8K (Cobbe et al., 2021)\nand GSM-Hard (Gao et al., 2023), respectively, compared to the baselines. More surprisingly, we\nobserve that agents discovered in the math domain can be transferred to non-math domains (Table 4).\nWhile the performance of agents originally searched in the math domain does not fully match that of\nagents specifically designed for the target domains, they still outperform (in Reading Comprehension\nand Multi-task) or match (in Science) the state-of-the-art hand-designed agent baselines. These\nresults illustrate that Meta Agent Search can discover generalizable design patterns and agentic\nsystems.\n9Automated Design of Agentic Systems\nAgent NameAccuracy (%)\nMGSM GSM8K GSM-Hard SVAMP ASDiv\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 28.0\u00b13.134.9\u00b13.2 15 .0\u00b12.5 77 .8\u00b12.8 88 .9\u00b12.2\nCOT-SC (Wang et al., 2023b) 28.2\u00b13.137.8\u00b13.4 15 .5\u00b12.5 78 .2\u00b12.8 89 .0\u00b12.1\nSelf-Refine (Madaan et al., 2024) 27.5\u00b13.138.9\u00b13.4 15 .1\u00b12.478.5\u00b12.8 89 .2\u00b12.2\nLLM Debate (Du et al., 2023) 39.0\u00b13.443.6\u00b13.417.4\u00b12.6 76 .0\u00b13.0 88 .9\u00b12.2\nStep-back Abstraction (Zheng et al., 2023) 31.1\u00b13.231.5\u00b13.3 12 .2\u00b12.3 76 .1\u00b13.0 87 .8\u00b12.3\nQuality-Diversity (Lu et al., 2024c) 23.8\u00b13.028.0\u00b13.1 14 .1\u00b12.4 69 .8\u00b13.2 80 .1\u00b12.8\nRole Assignment (Xu et al., 2023) 30.1\u00b13.237.0\u00b13.418.0\u00b12.773.0\u00b13.0 83 .1\u00b12.6\nTop Agents Searched on MGSM (Math) Transferred to Other Math Domains\nDynamic Role-Playing Architecture 53.4\u00b13.569.5\u00b13.2 31 .2\u00b13.281.5\u00b12.691.8\u00b11.8\nStructured Multimodal Feedback Loop 50.2\u00b13.564.5\u00b13.4 30 .1\u00b13.282.6\u00b12.689.9\u00b12.1\nInteractive Multimodal Feedback Loop 47.4\u00b13.564.9\u00b13.3 27 .6\u00b13.2 80 .6\u00b12.8 89 .8\u00b12.1\nTable 3|Performance on different math domains when transferring top agents from MGSM to\nother math domains. Agents discovered by Meta Agent Search consistently outperform the baselines\nacross different math domains. We report the test accuracy and the 95% bootstrap confidence interval.\nThe names of top agents are generated by Meta Agent Search.\n5. Related Work\nAgentic Systems. Researchers develop various building blocks and design patterns for different\napplications. Important building blocks for agentic systems includes: prompting techniques (Chen\net al., 2023a; Schulhoff et al., 2024), chain-of-thought-based planning and reasoning methods (Hu\n& Clune, 2024; Wei et al., 2022; Yao et al., 2023), reflection (Madaan et al., 2024; Shinn et al.,\n2023), developing new skills for embodied agents in code (Vemprala et al., 2023; Wang et al., 2023a),\nexternal memory and RAG (Lewis et al., 2020; Zhang et al., 2024c), tool use (Nakano et al., 2021;\nQu et al., 2024; Schick et al., 2023), assigning FM modules in the agentic system with different\nroles and enabling them to collaborate (Hong et al., 2023; Qian et al., 2023, 2024; Wu et al., 2023;\nXu et al., 2023), and enabling the agent to instruct itself for the next action (Richards, 2023), etc.\nWhile the community has invested substantial effort in developing all the above important techniques,\nthis is only a partial list of the discovered building blocks, and many more remain to be uncovered.\nTherefore, in this paper, we propose a new research area, ADAS, which aims to invent novel building\nblocks and design powerful agentic systems in an automated manner.\nAI-Generating Algorithms and AutoML. Following the lessons learned from the history of machine\nlearning, research in AI-Generating Algorithms (AI-GAs","chunk_id":"0b6b4880e77d40e284702da16be4ef64","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"SVAMP","type":"DATASET","description":"SVAMP is a dataset mentioned in the text, referenced by Patel et al., 2021","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ASDIV","type":"DATASET","description":"ASDiv is a dataset mentioned in the text, referenced by Miao et al., 2020","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that discovers agents that perform better than state-of-the-art hand-designed baselines and can transfer to non-math domains","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a dataset mentioned in the text, referenced by Cobbe et al., 2021","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"GSM-HARD","type":"DATASET","description":"GSM-Hard is a dataset mentioned in the text, referenced by Gao et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNIQUE","description":"Chain-of-Thought is a manually designed agent technique mentioned in the text, referenced by Wei et al., 2022","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"COT-SC","type":"TECHNIQUE","description":"COT-SC is a manually designed agent technique mentioned in the text, referenced by Wang et al., 2023b","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a manually designed agent technique mentioned in the text, referenced by Madaan et al., 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"LLM DEBATE","type":"TECHNIQUE","description":"LLM Debate is a manually designed agent technique mentioned in the text, referenced by Du et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a manually designed agent technique mentioned in the text, referenced by Zheng et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a manually designed agent technique mentioned in the text, referenced by Lu et al., 2024c","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a manually designed agent technique mentioned in the text, referenced by Xu et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"DYNAMIC ROLE-PLAYING ARCHITECTURE","type":"TECHNIQUE","description":"Dynamic Role-Playing Architecture is a top agent technique discovered by Meta Agent Search","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"STRUCTURED MULTIMODAL FEEDBACK LOOP","type":"TECHNIQUE","description":"Structured Multimodal Feedback Loop is a top agent technique discovered by Meta Agent Search","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"INTERACTIVE MULTIMODAL FEEDBACK LOOP","type":"TECHNIQUE","description":"Interactive Multimodal Feedback Loop is a top agent technique discovered by Meta Agent Search","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"PROMPTING TECHNIQUES","type":"TECHNIQUE","description":"Prompting techniques are important building blocks for agentic systems, referenced by Chen et al., 2023a and Schulhoff et al., 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS","type":"TECHNIQUE","description":"Chain-of-thought-based planning and reasoning methods are important building blocks for agentic systems, referenced by Hu & Clune, 2024; Wei et al., 2022; Yao et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"REFLECTION","type":"TECHNIQUE","description":"Reflection is an important building block for agentic systems, referenced by Madaan et al., 2024; Shinn et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE","type":"TECHNIQUE","description":"Developing new skills for embodied agents in code is an important building block for agentic systems, referenced by Vemprala et al., 2023; Wang et al., 2023a","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"EXTERNAL MEMORY AND RAG","type":"TECHNIQUE","description":"External memory and RAG are important building blocks for agentic systems, referenced by Lewis et al., 2020; Zhang et al., 2024c","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"TOOL USE","type":"TECHNIQUE","description":"Tool use is an important building block for agentic systems, referenced by Nakano et al., 2021; Qu et al., 2024; Schick et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES","type":"TECHNIQUE","description":"Assigning FM modules in the agentic system with different roles and enabling them to collaborate is an important building block for agentic systems, referenced by Hong et al., 2023; Qian et al., 2023, 2024; Wu et al., 2023; Xu et al., 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION","type":"TECHNIQUE","description":"Enabling the agent to instruct itself for the next action is an important building block for agentic systems, referenced by Richards, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ADAS","type":"RESEARCH AREA","description":"ADAS is a proposed research area that aims to invent novel building blocks and design powerful agentic systems in an automated manner","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"AI-GENERATING ALGORITHMS","type":"RESEARCH AREA","description":"AI-Generating Algorithms (AI-GAs) is a research area mentioned in the text","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"MIAO","type":"PERSON","description":"Miao is an author referenced in the text, associated with the ASDiv dataset, 2020","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"COBBE","type":"PERSON","description":"Cobbe is an author referenced in the text, associated with the GSM8K dataset, 2021","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"GAO","type":"PERSON","description":"Gao is an author referenced in the text, associated with the GSM-Hard dataset, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"WEI","type":"PERSON","description":"Wei is an author referenced in the text, associated with the Chain-of-Thought technique, 2022","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"WANG","type":"PERSON","description":"Wang is an author referenced in the text, associated with the COT-SC technique, 2023b","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author referenced in the text, associated with the Self-Refine technique, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"DU","type":"PERSON","description":"Du is an author referenced in the text, associated with the LLM Debate technique, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ZHENG","type":"PERSON","description":"Zheng is an author referenced in the text, associated with the Step-back Abstraction technique, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"LU","type":"PERSON","description":"Lu is an author referenced in the text, associated with the Quality-Diversity technique, 2024c","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"XU","type":"PERSON","description":"Xu is an author referenced in the text, associated with the Role Assignment technique, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"CHEN","type":"PERSON","description":"Chen is an author referenced in the text, associated with prompting techniques, 2023a","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SCHULHOFF","type":"PERSON","description":"Schulhoff is an author referenced in the text, associated with prompting techniques, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"HU","type":"PERSON","description":"Hu is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"YAO","type":"PERSON","description":"Yao is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author referenced in the text, associated with reflection, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"VEMPRALA","type":"PERSON","description":"Vemprala is an author referenced in the text, associated with developing new skills for embodied agents in code, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"LEWIS","type":"PERSON","description":"Lewis is an author referenced in the text, associated with external memory and RAG, 2020","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author referenced in the text, associated with external memory and RAG, 2024c","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"NAKANO","type":"PERSON","description":"Nakano is an author referenced in the text, associated with tool use, 2021","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"QU","type":"PERSON","description":"Qu is an author referenced in the text, associated with tool use, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"SCHICK","type":"PERSON","description":"Schick is an author referenced in the text, associated with tool use, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"HONG","type":"PERSON","description":"Hong is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"QIAN","type":"PERSON","description":"Qian is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023, 2024","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"WU","type":"PERSON","description":"Wu is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"RICHARDS","type":"PERSON","description":"Richards is an author referenced in the text, associated with enabling the agent to instruct itself for the next action, 2023","source_id":"0b6b4880e77d40e284702da16be4ef64"},{"name":"PATEL","type":"","description":"","source_id":"0b6b4880e77d40e284702da16be4ef64"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SVAMP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">SVAMP is a dataset mentioned in the text, referenced by Patel et al., 2021<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ASDIV\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ASDiv is a dataset mentioned in the text, referenced by Miao et al., 2020<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that discovers agents that perform better than state-of-the-art hand-designed baselines and can transfer to non-math domains<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a dataset mentioned in the text, referenced by Cobbe et al., 2021<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"GSM-HARD\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM-Hard is a dataset mentioned in the text, referenced by Gao et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought is a manually designed agent technique mentioned in the text, referenced by Wei et al., 2022<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">COT-SC is a manually designed agent technique mentioned in the text, referenced by Wang et al., 2023b<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a manually designed agent technique mentioned in the text, referenced by Madaan et al., 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM Debate is a manually designed agent technique mentioned in the text, referenced by Du et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a manually designed agent technique mentioned in the text, referenced by Zheng et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a manually designed agent technique mentioned in the text, referenced by Lu et al., 2024c<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a manually designed agent technique mentioned in the text, referenced by Xu et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Dynamic Role-Playing Architecture is a top agent technique discovered by Meta Agent Search<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Structured Multimodal Feedback Loop is a top agent technique discovered by Meta Agent Search<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Interactive Multimodal Feedback Loop is a top agent technique discovered by Meta Agent Search<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"PROMPTING TECHNIQUES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Prompting techniques are important building blocks for agentic systems, referenced by Chen et al., 2023a and Schulhoff et al., 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-thought-based planning and reasoning methods are important building blocks for agentic systems, referenced by Hu &amp; Clune, 2024; Wei et al., 2022; Yao et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Reflection is an important building block for agentic systems, referenced by Madaan et al., 2024; Shinn et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Developing new skills for embodied agents in code is an important building block for agentic systems, referenced by Vemprala et al., 2023; Wang et al., 2023a<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"EXTERNAL MEMORY AND RAG\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">External memory and RAG are important building blocks for agentic systems, referenced by Lewis et al., 2020; Zhang et al., 2024c<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Tool use is an important building block for agentic systems, referenced by Nakano et al., 2021; Qu et al., 2024; Schick et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Assigning FM modules in the agentic system with different roles and enabling them to collaborate is an important building block for agentic systems, referenced by Hong et al., 2023; Qian et al., 2023, 2024; Wu et al., 2023; Xu et al., 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Enabling the agent to instruct itself for the next action is an important building block for agentic systems, referenced by Richards, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS is a proposed research area that aims to invent novel building blocks and design powerful agentic systems in an automated manner<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) is a research area mentioned in the text<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"MIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Miao is an author referenced in the text, associated with the ASDiv dataset, 2020<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cobbe is an author referenced in the text, associated with the GSM8K dataset, 2021<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gao is an author referenced in the text, associated with the GSM-Hard dataset, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author referenced in the text, associated with the Chain-of-Thought technique, 2022<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author referenced in the text, associated with the COT-SC technique, 2023b<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author referenced in the text, associated with the Self-Refine technique, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Du is an author referenced in the text, associated with the LLM Debate technique, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is an author referenced in the text, associated with the Step-back Abstraction technique, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author referenced in the text, associated with the Quality-Diversity technique, 2024c<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is an author referenced in the text, associated with the Role Assignment technique, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author referenced in the text, associated with prompting techniques, 2023a<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SCHULHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schulhoff is an author referenced in the text, associated with prompting techniques, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hu is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yao is an author referenced in the text, associated with chain-of-thought-based planning and reasoning methods, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author referenced in the text, associated with reflection, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"VEMPRALA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vemprala is an author referenced in the text, associated with developing new skills for embodied agents in code, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis is an author referenced in the text, associated with external memory and RAG, 2020<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author referenced in the text, associated with external memory and RAG, 2024c<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nakano is an author referenced in the text, associated with tool use, 2021<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"QU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qu is an author referenced in the text, associated with tool use, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Schick is an author referenced in the text, associated with tool use, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hong is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qian is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023, 2024<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wu is an author referenced in the text, associated with assigning FM modules in the agentic system with different roles, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"RICHARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richards is an author referenced in the text, associated with enabling the agent to instruct itself for the next action, 2023<\/data>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <node id=\"PATEL\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/node>    <edge source=\"SVAMP\" target=\"PATEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Patel is an author associated with the SVAMP dataset, 2021<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ASDIV\" target=\"MIAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Miao is an author associated with the ASDiv dataset, 2020<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM8K\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search improves accuracy on the GSM8K dataset<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"GSM-HARD\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search improves accuracy on the GSM-Hard dataset<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Dynamic Role-Playing Architecture is a top agent discovered by Meta Agent Search<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Structured Multimodal Feedback Loop is a top agent discovered by Meta Agent Search<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Interactive Multimodal Feedback Loop is a top agent discovered by Meta Agent Search<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"GSM8K\" target=\"COBBE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cobbe is an author associated with the GSM8K dataset, 2021<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"GSM-HARD\" target=\"GAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Gao is an author associated with the GSM-Hard dataset, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Chain-of-Thought is a technique related to chain-of-thought-based planning and reasoning methods<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"WEI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wei is an author associated with the Chain-of-Thought technique, 2022<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">COT-SC is a technique related to chain-of-thought-based planning and reasoning methods<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"WANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang is an author associated with the COT-SC technique, 2023b<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"REFLECTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Refine is a technique related to reflection<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"MADAAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Madaan is an author associated with the Self-Refine technique, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"TOOL USE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">LLM Debate is a technique related to tool use<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"DU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Du is an author associated with the LLM Debate technique, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"PROMPTING TECHNIQUES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Step-back Abstraction is a technique related to prompting techniques<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"ZHENG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zheng is an author associated with the Step-back Abstraction technique, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Quality-Diversity is a technique related to assigning FM modules in the agentic system with different roles<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"LU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Lu is an author associated with the Quality-Diversity technique, 2024c<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Role Assignment is a technique related to assigning FM modules in the agentic system with different roles<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"XU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xu is an author associated with the Role Assignment technique, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"PROMPTING TECHNIQUES\" target=\"CHEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen is an author associated with prompting techniques, 2023a<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"PROMPTING TECHNIQUES\" target=\"SCHULHOFF\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Schulhoff is an author associated with prompting techniques, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\" target=\"HU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hu is an author associated with chain-of-thought-based planning and reasoning methods, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\" target=\"CLUNE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Clune is an author associated with chain-of-thought-based planning and reasoning methods, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT-BASED PLANNING AND REASONING METHODS\" target=\"YAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yao is an author associated with chain-of-thought-based planning and reasoning methods, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"REFLECTION\" target=\"SHINN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shinn is an author associated with reflection, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"DEVELOPING NEW SKILLS FOR EMBODIED AGENTS IN CODE\" target=\"VEMPRALA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Vemprala is an author associated with developing new skills for embodied agents in code, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"EXTERNAL MEMORY AND RAG\" target=\"LEWIS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Lewis is an author associated with external memory and RAG, 2020<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"EXTERNAL MEMORY AND RAG\" target=\"ZHANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zhang is an author associated with external memory and RAG, 2024c<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"NAKANO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Nakano is an author associated with tool use, 2021<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"QU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qu is an author associated with tool use, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"SCHICK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Schick is an author associated with tool use, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\" target=\"HONG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hong is an author associated with assigning FM modules in the agentic system with different roles, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\" target=\"QIAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qian is an author associated with assigning FM modules in the agentic system with different roles, 2023, 2024<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ASSIGNING FM MODULES IN THE AGENTIC SYSTEM WITH DIFFERENT ROLES\" target=\"WU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wu is an author associated with assigning FM modules in the agentic system with different roles, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ENABLING THE AGENT TO INSTRUCT ITSELF FOR THE NEXT ACTION\" target=\"RICHARDS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Richards is an author associated with enabling the agent to instruct itself for the next action, 2023<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AI-GENERATING ALGORITHMS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">ADAS is a proposed research area that builds on the lessons learned from AI-Generating Algorithms<\/data>      <data key=\"d5\">0b6b4880e77d40e284702da16be4ef64<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"7c08d98f503d722d7de13be55375c8cb","chunk":"While the community has invested substantial effort in developing all the above important techniques,\nthis is only a partial list of the discovered building blocks, and many more remain to be uncovered.\nTherefore, in this paper, we propose a new research area, ADAS, which aims to invent novel building\nblocks and design powerful agentic systems in an automated manner.\nAI-Generating Algorithms and AutoML. Following the lessons learned from the history of machine\nlearning, research in AI-Generating Algorithms (AI-GAs) (Clune, 2019) and AutoML (Hutter et al.,\n2019) continually strives to learn more components in AI systems to replace handcrafted ones. There\nare mainly three pillars in this field: (1) meta-learning architectures, (2) meta-learning the learning\nalgorithms, and (3) generating effective learning environments and training data (Clune, 2019). For\nexample, Neural Architecture Search (Elsken et al., 2019; Hu et al., 2021; Lu et al., 2019) aims to\nautomate the design of neural network architectures like convolution, which falls under the first pillar.\nThe second pillar includes works like MAML (Finn et al., 2017) and Meta-RL (Duan et al., 2017;\nNorman & Clune, 2023; Wang et al., 2016; Zintgraf et al., 2021a,b), which allow \u201clearning to learn\u201d\nfor better sample efficiency, generalizability, and continuous learning of multiple tasks. Additionally,\nworks like POET (Dharna et al., 2020; Wang et al., 2019, 2020) and OMNI-EPIC (Faldor et al., 2024)\nunder the third pillar aim to generate learning environments in an open-ended manner. We believe\n10Automated Design of Agentic Systems\nAgent NameAccuracy (%) F1 Score Accuracy (%)\nMath Reading Comprehension Multi-task Science\nManually Designed Agents\nChain-of-Thought (Wei et al., 2022) 28.0\u00b13.1 64.2\u00b10.9 65 .4\u00b13.3 29 .2\u00b13.1\nCOT-SC (Wang et al., 2023b) 28.2\u00b13.1 64.4\u00b10.8 65.9\u00b13.230.5\u00b13.2\nSelf-Refine (Madaan et al., 2024) 27.5\u00b13.1 59.2\u00b10.9 63 .5\u00b13.431.6\u00b13.2\nLLM Debate (Du et al., 2023) 39.0\u00b13.4 60.6\u00b10.9 65 .6\u00b13.3 31 .4\u00b13.2\nStep-back Abstraction (Zheng et al., 2023) 31.1\u00b13.2 60.4\u00b11.0 65 .1\u00b13.3 26 .9\u00b13.0\nQuality-Diversity (Lu et al., 2024c) 23.8\u00b13.0 61.8\u00b10.9 65 .1\u00b13.1 30 .2\u00b13.1\nRole Assignment (Xu et al., 2023) 30.1\u00b13.2 65.8\u00b10.9 64.5\u00b13.3 31 .1\u00b13.1\nTop Agents Searched on Math (MGSM) Transferred beyond Math Domains\nDynamic Role-Playing Architecture 53.4\u00b13.5 70.4\u00b10.9 62 .4\u00b13.4 28 .6\u00b13.1\nStructured Multimodal Feedback Loop 50.2\u00b13.5 70.4\u00b10.9 67.0\u00b13.228.7\u00b13.1\nInteractive Multimodal Feedback Loop 47.4\u00b13.5 71.9\u00b10.8 64.8\u00b13.329.9\u00b13.2\nTable 4|Performance across multiple domains when transferring top agents from the Math\n(MGSM) domain to non-math domains. Agents discovered by Meta Agent Search in the math\ndomain can outperform or match the performance of baselines after being transferred to domains\nbeyond math. We report the test accuracy and the 95% bootstrap confidence interval.\nthat the proposed Automated Design of Agentic Systems belongs to both the first and second pillars:\nPillar one because ADAS meta-learns the architecture of agentic systems, but also Pillar two because\nagents are proficient in in-context learning, thus ADAS can also be considered as learning to learn, as\ndemonstrated in the ARC challenge (Section 4.1).\nAdditionally, recent AI-GA and AutoML works have incorporated Foundation Models (FMs) to\nwrite code. For example, in FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024),\nFMs write code to discover better optimization algorithms. In DiscoPOP (Lu et al., 2024a), FMs\nprogram the loss function for preference learning in FM alignment training (Rafailov et al., 2024).\nAdditionally, Eureka (Ma et al., 2023) and language-to-reward (Yu et al., 2023) enable FMs to write\nreward functions for reinforcement learning in robotics. Finally, OMNI-EPIC (Faldor et al., 2024)\nenables FMs to create robotics learning environments by programming in code","chunk_id":"7c08d98f503d722d7de13be55375c8cb","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"ADAS","type":"RESEARCH AREA","description":"ADAS (Automated Design of Agentic Systems) is a proposed research area aimed at inventing novel building blocks and designing powerful agentic systems in an automated manner","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"AI-GENERATING ALGORITHMS","type":"TECHNOLOGY","description":"AI-Generating Algorithms (AI-GAs) are a field of research that aims to learn more components in AI systems to replace handcrafted ones","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"AUTOML","type":"TECHNOLOGY","description":"AutoML is a field of research that aims to automate the process of machine learning, including the design of neural network architectures and learning algorithms","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNOLOGY","description":"Neural Architecture Search is a method that aims to automate the design of neural network architectures like convolution","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MAML","type":"TECHNOLOGY","description":"MAML (Model-Agnostic Meta-Learning) is a method that allows \"learning to learn\" for better sample efficiency, generalizability, and continuous learning of multiple tasks","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"META-RL","type":"TECHNOLOGY","description":"Meta-RL (Meta-Reinforcement Learning) is a method that allows \"learning to learn\" for better sample efficiency, generalizability, and continuous learning of multiple tasks","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"POET","type":"TECHNOLOGY","description":"POET (Paired Open-Ended Trailblazer) is a method that aims to generate learning environments in an open-ended manner","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"OMNI-EPIC is a method that enables Foundation Models to create robotics learning environments by programming in code","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"COT-SC","type":"TECHNOLOGY","description":"COT-SC is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LLM DEBATE","type":"TECHNOLOGY","description":"LLM Debate is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNOLOGY","description":"Step-back Abstraction is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ROLE ASSIGNMENT","type":"TECHNOLOGY","description":"Role Assignment is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DYNAMIC ROLE-PLAYING ARCHITECTURE","type":"TECHNOLOGY","description":"Dynamic Role-Playing Architecture is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"STRUCTURED MULTIMODAL FEEDBACK LOOP","type":"TECHNOLOGY","description":"Structured Multimodal Feedback Loop is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"INTERACTIVE MULTIMODAL FEEDBACK LOOP","type":"TECHNOLOGY","description":"Interactive Multimodal Feedback Loop is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FUNSEARCH","type":"TECHNOLOGY","description":"FunSearch is a method where Foundation Models write code to discover better optimization algorithms","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"EOH","type":"TECHNOLOGY","description":"EoH is a method where Foundation Models write code to discover better optimization algorithms","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DISCOPOP","type":"TECHNOLOGY","description":"DiscoPOP is a method where Foundation Models program the loss function for preference learning in FM alignment training","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"EUREKA","type":"TECHNOLOGY","description":"Eureka is a method that enables Foundation Models to write reward functions for reinforcement learning in robotics","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LANGUAGE-TO-REWARD","type":"TECHNOLOGY","description":"Language-to-Reward is a method that enables Foundation Models to write reward functions for reinforcement learning in robotics","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author who has contributed to research in AI-Generating Algorithms and AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"HUTTER","type":"PERSON","description":"Hutter is an author who has contributed to research in AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ELSKEN","type":"PERSON","description":"Elsken is an author who has contributed to research in Neural Architecture Search","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"HU","type":"PERSON","description":"Hu is an author who has contributed to research in Neural Architecture Search","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LU","type":"PERSON","description":"Lu is an author who has contributed to research in Neural Architecture Search and Quality-Diversity","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FINN","type":"PERSON","description":"Finn is an author who has contributed to research in MAML","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DUAN","type":"PERSON","description":"Duan is an author who has contributed to research in Meta-RL","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"NORMAN","type":"PERSON","description":"Norman is an author who has contributed to research in Meta-RL","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has contributed to research in Meta-RL and POET","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ZINTGRAF","type":"PERSON","description":"Zintgraf is an author who has contributed to research in Meta-RL","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DHARNA","type":"PERSON","description":"Dharna is an author who has contributed to research in POET","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has contributed to research in OMNI-EPIC","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ROMERA-PAREDES","type":"PERSON","description":"Romera-Paredes is an author who has contributed to research in FunSearch","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has contributed to research in EoH","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"RAFALIOV","type":"PERSON","description":"Rafailov is an author who has contributed to research in FM alignment training","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MA","type":"PERSON","description":"Ma is an author who has contributed to research in Eureka","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"YU","type":"PERSON","description":"Yu is an author who has contributed to research in Language-to-Reward","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"WEI","type":"PERSON","description":"Wei is an author who has contributed to research in Chain-of-Thought","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"XU","type":"PERSON","description":"Xu is an author who has contributed to research in Role Assignment","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ZHENG","type":"PERSON","description":"Zheng is an author who has contributed to research in Step-back Abstraction","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who has contributed to research in Self-Refine","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DU","type":"PERSON","description":"Du is an author who has contributed to research in LLM Debate","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"NORMAN & CLUNE","type":"PERSON","description":"Norman & Clune are authors who have contributed to research in Meta-RL","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"WANG ET AL.","type":"PERSON","description":"Wang et al. are authors who have contributed to research in POET\nWang et al. are authors who have contributed to research in Meta-RL and POET","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"PERSON"},{"name":"ZINTGRAF ET AL.","type":"PERSON","description":"Zintgraf et al. are authors who have contributed to research in Meta-RL","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"DHARNA ET AL.","type":"PERSON","description":"Dharna et al. are authors who have contributed to research in POET","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FALDOR ET AL.","type":"PERSON","description":"Faldor et al. are authors who have contributed to research in OMNI-EPIC","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ROMERA-PAREDES ET AL.","type":"PERSON","description":"Romera-Paredes et al. are authors who have contributed to research in FunSearch","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"LIU ET AL.","type":"PERSON","description":"Liu et al. are authors who have contributed to research in EoH","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"RAFALIOV ET AL.","type":"PERSON","description":"Rafailov et al. are authors who have contributed to research in FM alignment training","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MA ET AL.","type":"PERSON","description":"Ma et al. are authors who have contributed to research in Eureka","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"YU ET AL.","type":"PERSON","description":"Yu et al. are authors who have contributed to research in Language-to-Reward","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ARC CHALLENGE","type":"EVENT","description":"The ARC challenge is an event that demonstrates the proficiency of agents in in-context learning, which is part of the ADAS research area","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FOUNDATION MODELS","type":"TECHNOLOGY","description":"Foundation Models (FMs) are large-scale models used in various AI-GA and AutoML works to write code, discover optimization algorithms, and create learning environments","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"MATH DOMAIN","type":"DOMAIN","description":"The Math domain is a specific area where agents discovered by Meta Agent Search are tested and evaluated","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"NON-MATH DOMAINS","type":"DOMAIN","description":"Non-math domains are areas beyond the Math domain where agents discovered by Meta Agent Search are transferred and evaluated","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"MGSM","type":"TECHNOLOGY","description":"MGSM (Math Generalization and Specialization Model) is a model used to evaluate the performance of agents in the Math domain","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"PREFERENCE LEARNING","type":"TECHNOLOGY","description":"Preference Learning is a method used in FM alignment training to program the loss function","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"FM ALIGNMENT TRAINING","type":"TECHNOLOGY","description":"FM Alignment Training is a process where Foundation Models are trained to align with specific preferences using methods like DiscoPOP","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"REINFORCEMENT LEARNING","type":"TECHNOLOGY","description":"Reinforcement Learning is a type of machine learning where agents learn to make decisions by receiving rewards, as demonstrated in methods like Eureka and Language-to-Reward","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"ROBOTICS","type":"DOMAIN","description":"Robotics is a domain where Foundation Models are used to write reward functions for reinforcement learning and create learning environments","source_id":"7c08d98f503d722d7de13be55375c8cb"},{"name":"AGENTIC SYSTEMS","type":"TECHNOLOGY","description":"Agentic Systems are powerful systems designed in an automated manner, as proposed in the ADAS research area","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"LEARNING ENVIRONMENTS","type":"TECHNOLOGY","description":"Learning Environments are generated in an open-ended manner to facilitate the training of AI systems, as seen in methods like POET and OMNI-EPIC","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"TRAINING DATA","type":"TECHNOLOGY","description":"Training Data is generated to create effective learning environments and improve AI systems, as part of the third pillar of AI-GAs and AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"META-LEARNING","type":"TECHNOLOGY","description":"Meta-Learning is a process of learning to learn, which includes meta-learning architectures and learning algorithms, as part of the first and second pillars of AI-GAs and AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"LEARNING ALGORITHMS","type":"TECHNOLOGY","description":"Learning Algorithms are meta-learned to improve sample efficiency, generalizability, and continuous learning of multiple tasks, as part of the second pillar of AI-GAs and AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"ARCHITECTURES","type":"TECHNOLOGY","description":"Architectures are meta-learned to design neural network structures, as part of the first pillar of AI-GAs and AutoML","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"CONVOLUTION","type":"TECHNOLOGY","description":"Convolution is a type of neural network architecture that is designed using methods like Neural Architecture Search","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"SAMPLE EFFICIENCY","type":"TECHNOLOGY","description":"Sample Efficiency is improved through methods like MAML and Meta-RL, which allow \"learning to learn\"","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"GENERALIZABILITY","type":"TECHNOLOGY","description":"Generalizability is enhanced through meta-learning methods like MAML and Meta-RL, allowing AI systems to perform well across multiple tasks","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"CONTINUOUS LEARNING","type":"TECHNOLOGY","description":"Continuous Learning is facilitated by meta-learning methods like MAML and Meta-RL, enabling AI systems to learn multiple tasks over time","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"OPTIMIZATION ALGORITHMS","type":"TECHNOLOGY","description":"Optimization Algorithms are discovered by Foundation Models in methods like FunSearch and EoH","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"LOSS FUNCTION","type":"TECHNOLOGY","description":"Loss Function is programmed by Foundation Models in methods like DiscoPOP for preference learning in FM alignment training","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"REWARD FUNCTIONS","type":"TECHNOLOGY","description":"Reward Functions are written by Foundation Models in methods like Eureka and Language-to-Reward for reinforcement learning in robotics","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"AGENTS","type":"TECHNOLOGY","description":"Agents are systems designed to perform tasks like Math, Reading Comprehension, Multi-task, and Science, as seen in various manually designed and top agents discovered by Meta Agent Search","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"MATH","type":"DOMAIN","description":"Math is a domain where agents are tested and evaluated for their performance","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"DOMAIN"},{"name":"READING COMPREHENSION","type":"DOMAIN","description":"Reading Comprehension is a domain where agents are tested and evaluated for their performance","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"DOMAIN"},{"name":"MULTI-TASK","type":"DOMAIN","description":"Multi-task is a domain where agents are tested and evaluated for their performance","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"DOMAIN"},{"name":"SCIENCE","type":"DOMAIN","description":"Science is a domain where agents are tested and evaluated for their performance","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"DOMAIN"},{"name":"ARC","type":"EVENT","description":"ARC (AI Research Challenge) is an event that demonstrates the proficiency of agents in in-context learning, which is part of the ADAS research area","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"EVENT"},{"name":"BOOTSTRAP CONFIDENCE INTERVAL","type":"TECHNOLOGY","description":"Bootstrap Confidence Interval is a statistical method used to report the test accuracy of agents across multiple domains","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"TEST ACCURACY","type":"TECHNOLOGY","description":"Test Accuracy is a metric used to evaluate the performance of agents across multiple domains","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"PERFORMANCE","type":"TECHNOLOGY","description":"Performance is evaluated across multiple domains when transferring top agents from the Math domain to non-math domains","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"DOMAIN","type":"TECHNOLOGY","description":"Domain refers to specific areas like Math, Reading Comprehension, Multi-task, and Science where agents are tested and evaluated","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"BASELINES","type":"TECHNOLOGY","description":"Baselines are initial seeds used in the archive for Meta Agent Search to progressively discover better-performing agents","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"},{"name":"ARCHIVE","type":"TECHNOLOGY","description":"Archive is used in Meta Agent Search to store initial seeds and progressively discover better-performing agents","source_id":"7c08d98f503d722d7de13be55375c8cb","entity_type":"TECHNOLOGY"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ADAS\">      <data key=\"d0\">RESEARCH AREA<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) is a proposed research area aimed at inventing novel building blocks and designing powerful agentic systems in an automated manner<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"AI-GENERATING ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-Generating Algorithms (AI-GAs) are a field of research that aims to learn more components in AI systems to replace handcrafted ones<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"AUTOML\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoML is a field of research that aims to automate the process of machine learning, including the design of neural network architectures and learning algorithms<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural Architecture Search is a method that aims to automate the design of neural network architectures like convolution<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MAML\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MAML (Model-Agnostic Meta-Learning) is a method that allows \"learning to learn\" for better sample efficiency, generalizability, and continuous learning of multiple tasks<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"META-RL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta-RL (Meta-Reinforcement Learning) is a method that allows \"learning to learn\" for better sample efficiency, generalizability, and continuous learning of multiple tasks<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"POET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">POET (Paired Open-Ended Trailblazer) is a method that aims to generate learning environments in an open-ended manner<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI-EPIC is a method that enables Foundation Models to create robotics learning environments by programming in code<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT-SC is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LLM DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM Debate is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Step-back Abstraction is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Role Assignment is a manually designed agent that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Dynamic Role-Playing Architecture is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Structured Multimodal Feedback Loop is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Interactive Multimodal Feedback Loop is a top agent discovered by Meta Agent Search that performs tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FUNSEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FunSearch is a method where Foundation Models write code to discover better optimization algorithms<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"EOH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">EoH is a method where Foundation Models write code to discover better optimization algorithms<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DISCOPOP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DiscoPOP is a method where Foundation Models program the loss function for preference learning in FM alignment training<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Eureka is a method that enables Foundation Models to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LANGUAGE-TO-REWARD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language-to-Reward is a method that enables Foundation Models to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author who has contributed to research in AI-Generating Algorithms and AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hutter is an author who has contributed to research in AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ELSKEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elsken is an author who has contributed to research in Neural Architecture Search<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hu is an author who has contributed to research in Neural Architecture Search<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author who has contributed to research in Neural Architecture Search and Quality-Diversity<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Finn is an author who has contributed to research in MAML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Duan is an author who has contributed to research in Meta-RL<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"NORMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Norman is an author who has contributed to research in Meta-RL<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has contributed to research in Meta-RL and POET<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ZINTGRAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zintgraf is an author who has contributed to research in Meta-RL<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DHARNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dharna is an author who has contributed to research in POET<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has contributed to research in OMNI-EPIC<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ROMERA-PAREDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Romera-Paredes is an author who has contributed to research in FunSearch<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has contributed to research in EoH<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"RAFALIOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafailov is an author who has contributed to research in FM alignment training<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ma is an author who has contributed to research in Eureka<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu is an author who has contributed to research in Language-to-Reward<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei is an author who has contributed to research in Chain-of-Thought<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is an author who has contributed to research in Role Assignment<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is an author who has contributed to research in Step-back Abstraction<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who has contributed to research in Self-Refine<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Du is an author who has contributed to research in LLM Debate<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"NORMAN &amp; CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Norman &amp; Clune are authors who have contributed to research in Meta-RL<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"WANG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang et al. are authors who have contributed to research in POETWang et al. are authors who have contributed to research in Meta-RL and POET<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZINTGRAF ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zintgraf et al. are authors who have contributed to research in Meta-RL<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"DHARNA ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dharna et al. are authors who have contributed to research in POET<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FALDOR ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor et al. are authors who have contributed to research in OMNI-EPIC<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ROMERA-PAREDES ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Romera-Paredes et al. are authors who have contributed to research in FunSearch<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"LIU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu et al. are authors who have contributed to research in EoH<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"RAFALIOV ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafailov et al. are authors who have contributed to research in FM alignment training<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MA ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ma et al. are authors who have contributed to research in Eureka<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"YU ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu et al. are authors who have contributed to research in Language-to-Reward<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ARC CHALLENGE\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The ARC challenge is an event that demonstrates the proficiency of agents in in-context learning, which is part of the ADAS research area<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FOUNDATION MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are large-scale models used in various AI-GA and AutoML works to write code, discover optimization algorithms, and create learning environments<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MATH DOMAIN\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">The Math domain is a specific area where agents discovered by Meta Agent Search are tested and evaluated<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"NON-MATH DOMAINS\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Non-math domains are areas beyond the Math domain where agents discovered by Meta Agent Search are transferred and evaluated<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MGSM (Math Generalization and Specialization Model) is a model used to evaluate the performance of agents in the Math domain<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"PREFERENCE LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Preference Learning is a method used in FM alignment training to program the loss function<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"FM ALIGNMENT TRAINING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM Alignment Training is a process where Foundation Models are trained to align with specific preferences using methods like DiscoPOP<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"REINFORCEMENT LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reinforcement Learning is a type of machine learning where agents learn to make decisions by receiving rewards, as demonstrated in methods like Eureka and Language-to-Reward<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"ROBOTICS\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Robotics is a domain where Foundation Models are used to write reward functions for reinforcement learning and create learning environments<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic Systems are powerful systems designed in an automated manner, as proposed in the ADAS research area<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LEARNING ENVIRONMENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Learning Environments are generated in an open-ended manner to facilitate the training of AI systems, as seen in methods like POET and OMNI-EPIC<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TRAINING DATA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Training Data is generated to create effective learning environments and improve AI systems, as part of the third pillar of AI-GAs and AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META-LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta-Learning is a process of learning to learn, which includes meta-learning architectures and learning algorithms, as part of the first and second pillars of AI-GAs and AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LEARNING ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Learning Algorithms are meta-learned to improve sample efficiency, generalizability, and continuous learning of multiple tasks, as part of the second pillar of AI-GAs and AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARCHITECTURES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Architectures are meta-learned to design neural network structures, as part of the first pillar of AI-GAs and AutoML<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CONVOLUTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Convolution is a type of neural network architecture that is designed using methods like Neural Architecture Search<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SAMPLE EFFICIENCY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Sample Efficiency is improved through methods like MAML and Meta-RL, which allow \"learning to learn\"<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GENERALIZABILITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Generalizability is enhanced through meta-learning methods like MAML and Meta-RL, allowing AI systems to perform well across multiple tasks<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CONTINUOUS LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Continuous Learning is facilitated by meta-learning methods like MAML and Meta-RL, enabling AI systems to learn multiple tasks over time<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPTIMIZATION ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Optimization Algorithms are discovered by Foundation Models in methods like FunSearch and EoH<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LOSS FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Loss Function is programmed by Foundation Models in methods like DiscoPOP for preference learning in FM alignment training<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"REWARD FUNCTIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Reward Functions are written by Foundation Models in methods like Eureka and Language-to-Reward for reinforcement learning in robotics<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agents are systems designed to perform tasks like Math, Reading Comprehension, Multi-task, and Science, as seen in various manually designed and top agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Math is a domain where agents are tested and evaluated for their performance<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">DOMAIN<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Reading Comprehension is a domain where agents are tested and evaluated for their performance<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">DOMAIN<\/data>    <\/node>    <node id=\"MULTI-TASK\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Multi-task is a domain where agents are tested and evaluated for their performance<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">DOMAIN<\/data>    <\/node>    <node id=\"SCIENCE\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Science is a domain where agents are tested and evaluated for their performance<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">DOMAIN<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ARC (AI Research Challenge) is an event that demonstrates the proficiency of agents in in-context learning, which is part of the ADAS research area<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"BOOTSTRAP CONFIDENCE INTERVAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Bootstrap Confidence Interval is a statistical method used to report the test accuracy of agents across multiple domains<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"TEST ACCURACY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Test Accuracy is a metric used to evaluate the performance of agents across multiple domains<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"PERFORMANCE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Performance is evaluated across multiple domains when transferring top agents from the Math domain to non-math domains<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DOMAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Domain refers to specific areas like Math, Reading Comprehension, Multi-task, and Science where agents are tested and evaluated<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"BASELINES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Baselines are initial seeds used in the archive for Meta Agent Search to progressively discover better-performing agents<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARCHIVE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Archive is used in Meta Agent Search to store initial seeds and progressively discover better-performing agents<\/data>      <data key=\"d2\">7c08d98f503d722d7de13be55375c8cb<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <edge source=\"ADAS\" target=\"AI-GENERATING ALGORITHMS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">ADAS aims to invent novel building blocks and design powerful agentic systems in an automated manner, which aligns with the goals of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AUTOML\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">ADAS aims to invent novel building blocks and design powerful agentic systems in an automated manner, which aligns with the goals of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"AUTOML\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Both AI-Generating Algorithms and AutoML aim to automate the design and learning processes in AI systems<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Neural Architecture Search is a method under the first pillar of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"MAML\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">MAML is a method under the second pillar of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"META-RL\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta-RL is a method under the second pillar of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"POET\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">POET is a method under the third pillar of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AI-GENERATING ALGORITHMS\" target=\"OMNI-EPIC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">OMNI-EPIC is a method under the third pillar of AI-Generating Algorithms<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Neural Architecture Search is a method under the first pillar of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"MAML\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">MAML is a method under the second pillar of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"META-RL\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta-RL is a method under the second pillar of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"POET\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">POET is a method under the third pillar of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"AUTOML\" target=\"OMNI-EPIC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">OMNI-EPIC is a method under the third pillar of AutoML<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"COT-SC\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and COT-SC are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"SELF-REFINE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and Self-Refine are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"LLM DEBATE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and LLM Debate are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and Step-back Abstraction are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and Quality-Diversity are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Chain-of-Thought and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"SELF-REFINE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both COT-SC and Self-Refine are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"LLM DEBATE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both COT-SC and LLM Debate are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both COT-SC and Step-back Abstraction are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both COT-SC and Quality-Diversity are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"COT-SC\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both COT-SC and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"LLM DEBATE\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Self-Refine and LLM Debate are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Self-Refine and Step-back Abstraction are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Self-Refine and Quality-Diversity are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both Self-Refine and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"STEP-BACK ABSTRACTION\">      <data key=\"d4\">10.0<\/data>      <data key=\"d5\">Both LLM Debate and Step-back Abstraction are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both LLM Debate and Quality-Diversity are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"LLM DEBATE\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both LLM Debate and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"QUALITY-DIVERSITY\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both Step-back Abstraction and Quality-Diversity are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both Step-back Abstraction and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"ROLE ASSIGNMENT\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both Quality-Diversity and Role Assignment are manually designed agents that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\" target=\"STRUCTURED MULTIMODAL FEEDBACK LOOP\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both Dynamic Role-Playing Architecture and Structured Multimodal Feedback Loop are top agents discovered by Meta Agent Search that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>    <edge source=\"DYNAMIC ROLE-PLAYING ARCHITECTURE\" target=\"INTERACTIVE MULTIMODAL FEEDBACK LOOP\">      <data key=\"d4\">5.0<\/data>      <data key=\"d5\">Both Dynamic Role-Playing Architecture and Interactive Multimodal Feedback Loop are top agents discovered by Meta Agent Search that perform tasks like Math, Reading Comprehension, Multi-task, and Science<\/data>      <data key=\"d6\">7c08d98f503d722d7de13be55375c8cb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"dc55f071b95dec721a9820d39cdb3ccd","chunk":"a), FMs\nprogram the loss function for preference learning in FM alignment training (Rafailov et al., 2024).\nAdditionally, Eureka (Ma et al., 2023) and language-to-reward (Yu et al., 2023) enable FMs to write\nreward functions for reinforcement learning in robotics. Finally, OMNI-EPIC (Faldor et al., 2024)\nenables FMs to create robotics learning environments by programming in code. Here, we adopt a\nsimilar idea that enables FMs to program new agents in code.\nExisting Attempts to ADAS. There are two categories of works that can be considered attempts at\nADAS in the literature: those that learn better prompts only, and those that learn more components\nin agents than just prompts. Most works fall into the first category: learning prompts only. Works like\nOPRO (Yang et al., 2024), PromptBreeder (Fernando et al., 2024), and Self-Discover (Zhou et al.,\n2024a) adopt FMs to automate prompt engineering for agents, primarily focusing on the phrasing of\ninstructions in the prompt to enhance the reasoning capability of agents. Thus, the learned prompts\nare domain-specific and difficult to generalize. Beyond instructions, works like EvoAgent (Yuan\net al., 2024) and AgentVerse (Chen et al., 2023b) optimize role definition in the prompt, as assigning\npersonas or roles to agents has been shown to be beneficial (Xu et al., 2023). Although tuning prompts\neffectively improves performance, other important components in agentic systems remain fixed and\nhand-designed, vastly limiting the space of agents that can be discovered.\nThere are far fewer attempts in the second category, which involves learning more components\nthan just prompts in agentic systems. Most represent agents as networks or graphs in the search\nspace. In these formulations, the FM with a certain prompt is considered a transformation function\nfor text on nodes, and the information flow of the text is considered as edges. DyLAN (Liu et al.,\n11Automated Design of Agentic Systems\n2023) starts with a fully connected feed-forward network and uses FMs to score the response quality\nof nodes in each layer to prune the connections. DSPy (Khattab et al., 2024) first generates a set\nof possible nodes and then optimizes across the Cartesian product of these nodes while optimizing\nthe few-shot examples for nodes. GPT-Swarm (Zhuge et al., 2024) represents an agentic system in\na graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize\nthe possible connections between nodes while optimizing the prompt for each node in a separate\nstage. Although these works allow the learning of control flow (optimizing edges in networks or\ngraphs), many other components, such as whether and which tools to learn or even how many nodes\nto have, are still not learned, greatly limiting the space of agents that can be discovered. Besides\nlearning prompts and control flow, AgentOptimizer (Zhang et al., 2024b) learns the tools used in\nagents, and Agent Symbolic Learning (Zhou et al., 2024b) learns prompts, tools, and control flow\ntogether. While Agent Symbolic Learning shares similar motivations to learn more components in\nagents, it manually designs the search space for each component separately, which may make it a\nharder search space for search algorithms. In addition, it mainly improves agents based on an existing\ncomplex agent, without showing the emergence of new design patterns or building blocks. In contrast,\nour work represents all possible components in code, allowing the search to be easier by leveraging\nhuman efforts in the existing codebase of agents and FMs\u2019 expertise in coding. We also demonstrate\nhow novel and diverse building blocks and design patterns emerge from a set of basic agent designs,\nillustrating the potential creativity that can emerge from ADAS.\n6. Discussion and Conclusion\nSafety Considerations. We strongly advise researchers to be aware of the safety concerns\nwhen executing untrusted model-generated code in Meta Agent Search and other research\ninvolvingcodegeneration. While it is highly unlikely that model-generated code will perform overtly\nmaliciousactionsinourcurrentsettingsandwiththeFoundationModels(FMs)weuse, suchcodemay\nstill act destructively due to limitations in model capability or alignment (Chen et al., 2021; Rokon\net al., 2020). Ideally, sandbox environments can be used to safely run untrusted model-generated\ncode (Chen et al., 2021; Yee et al., 2010).\nMore broadly, research on more powerful AI systems raises the question of whether we should\nbe conducting research to advance AI capabilities at all. That topic clearly includes the proposed\nAutomatedDesignofAgenticSystems(ADAS)asanewareainAI-GAresearch, whichcouldpotentially\ncontribute to an even faster way to create Artificial General Intelligence (AGI) than the current manual\napproach (Clune, 2019). The question of whether and why we should pursue AGI and AI-GA has\nbeen discussed in many papers (Bengio et al., 2024; Bostrom, 2002; Clune, 2019; Ecoffet et al., 2020;\nYudkowsky et al., 2008), and is beyond the scope of this paper. Specifically as regards ADAS, we\nbelieve it is net beneficial to publish this work. First, this work demonstrates that with the available\nAPI access to powerful FMs, it is easy to program powerful ADAS algorithms, and do so without any\nexp","chunk_id":"dc55f071b95dec721a9820d39cdb3ccd","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"FMS","type":"TECHNOLOGY","description":"Foundation Models (FMs) are used to program the loss function for preference learning in FM alignment training","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"RAFAILOV","type":"PERSON","description":"Rafailov is an author who has worked on FM alignment training in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"EUREKA","type":"TECHNOLOGY","description":"Eureka is a system that enables FMs to write reward functions for reinforcement learning in robotics","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"MA","type":"PERSON","description":"Ma is an author who has worked on Eureka in 2023","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"LANGUAGE-TO-REWARD","type":"TECHNOLOGY","description":"Language-to-reward is a system that enables FMs to write reward functions for reinforcement learning in robotics","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"YU","type":"PERSON","description":"Yu is an author who has worked on language-to-reward in 2023","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"OMNI-EPIC is a system that enables FMs to create robotics learning environments by programming in code","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on OMNI-EPIC in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"ADAS","type":"TECHNOLOGY","description":"Automated Design of Agentic Systems (ADAS) is a new area in AI-GA research that involves learning more components in agentic systems than just prompts","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"OPRO","type":"TECHNOLOGY","description":"OPRO is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"YANG","type":"PERSON","description":"Yang is an author who has worked on OPRO in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"PROMPTBREEDER","type":"TECHNOLOGY","description":"PromptBreeder is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"FERNANDO","type":"PERSON","description":"Fernando is an author who has worked on PromptBreeder in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"SELF-DISCOVER","type":"TECHNOLOGY","description":"Self-Discover is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"ZHOU","type":"PERSON","description":"Zhou is an author who has worked on Agent Symbolic Learning in 2024\nZhou is an author who has worked on Self-Discover in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"EVOAGENT","type":"TECHNOLOGY","description":"EvoAgent is a system that optimizes role definition in the prompt, assigning personas or roles to agents","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"YUAN","type":"PERSON","description":"Yuan is an author who has worked on EvoAgent in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"AGENTVERSE","type":"TECHNOLOGY","description":"AgentVerse is a system that optimizes role definition in the prompt, assigning personas or roles to agents","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"CHEN","type":"PERSON","description":"Chen is an author who has discussed safety concerns when executing untrusted model-generated code in Meta Agent Search in 2021\nChen is an author who has worked on AgentVerse in 2023","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"XU","type":"PERSON","description":"Xu is an author who has shown that assigning personas or roles to agents is beneficial in 2023","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"DYLAN","type":"TECHNOLOGY","description":"DyLAN is a system that starts with a fully connected feed-forward network and uses FMs to score the response quality of nodes in each layer to prune the connections","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"LIU","type":"PERSON","description":"Liu is an author who has worked on DyLAN in 2023","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"DSPY","type":"TECHNOLOGY","description":"DSPy is a system that generates a set of possible nodes and then optimizes across the Cartesian product of these nodes while optimizing the few-shot examples for nodes","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"KHATTAB","type":"PERSON","description":"Khattab is an author who has worked on DSPy in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"GPT-SWARM","type":"TECHNOLOGY","description":"GPT-Swarm is a system that represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the possible connections between nodes while optimizing the prompt for each node in a separate stage","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"ZHUGE","type":"PERSON","description":"Zhuge is an author who has worked on GPT-Swarm in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"AGENTOPTIMIZER","type":"TECHNOLOGY","description":"AgentOptimizer is a system that learns the tools used in agents","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author who has worked on AgentOptimizer in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"AGENT SYMBOLIC LEARNING","type":"TECHNOLOGY","description":"Agent Symbolic Learning is a system that learns prompts, tools, and control flow together in agents","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"TECHNOLOGY"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"dc55f071b95dec721a9820d39cdb3ccd"},{"name":"ROKON","type":"PERSON","description":"Rokon is an author who has discussed safety concerns when executing untrusted model-generated code in 2020","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"YEE","type":"PERSON","description":"Yee is an author who has discussed safety concerns when executing untrusted model-generated code in 2010","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"CLUNE","type":"PERSON","description":"Clune is an author who has discussed the potential of ADAS to contribute to an even faster way to create Artificial General Intelligence (AGI) in 2019","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"BENGIO","type":"PERSON","description":"Bengio is an author who has discussed whether we should pursue AGI and AI-GA in 2024","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"BOSTROM","type":"PERSON","description":"Bostrom is an author who has discussed whether we should pursue AGI and AI-GA in 2002","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"ECOFFET","type":"PERSON","description":"Ecoffet is an author who has discussed whether we should pursue AGI and AI-GA in 2020","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"},{"name":"YUDKOWSKY","type":"PERSON","description":"Yudkowsky is an author who has discussed whether we should pursue AGI and AI-GA in 2008","source_id":"dc55f071b95dec721a9820d39cdb3ccd","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are used to program the loss function for preference learning in FM alignment training<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"RAFAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafailov is an author who has worked on FM alignment training in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Eureka is a system that enables FMs to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ma is an author who has worked on Eureka in 2023<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LANGUAGE-TO-REWARD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language-to-reward is a system that enables FMs to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu is an author who has worked on language-to-reward in 2023<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OMNI-EPIC is a system that enables FMs to create robotics learning environments by programming in code<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on OMNI-EPIC in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Automated Design of Agentic Systems (ADAS) is a new area in AI-GA research that involves learning more components in agentic systems than just prompts<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPRO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">OPRO is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yang is an author who has worked on OPRO in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PROMPTBREEDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">PromptBreeder is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FERNANDO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fernando is an author who has worked on PromptBreeder in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SELF-DISCOVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Discover is a system that adopts FMs to automate prompt engineering for agents, focusing on the phrasing of instructions in the prompt to enhance reasoning capability<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou is an author who has worked on Agent Symbolic Learning in 2024Zhou is an author who has worked on Self-Discover in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EVOAGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">EvoAgent is a system that optimizes role definition in the prompt, assigning personas or roles to agents<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuan is an author who has worked on EvoAgent in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AGENTVERSE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentVerse is a system that optimizes role definition in the prompt, assigning personas or roles to agents<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is an author who has discussed safety concerns when executing untrusted model-generated code in Meta Agent Search in 2021Chen is an author who has worked on AgentVerse in 2023<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is an author who has shown that assigning personas or roles to agents is beneficial in 2023<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DYLAN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DyLAN is a system that starts with a fully connected feed-forward network and uses FMs to score the response quality of nodes in each layer to prune the connections<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author who has worked on DyLAN in 2023<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DSPY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DSPy is a system that generates a set of possible nodes and then optimizes across the Cartesian product of these nodes while optimizing the few-shot examples for nodes<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"KHATTAB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Khattab is an author who has worked on DSPy in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GPT-SWARM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-Swarm is a system that represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the possible connections between nodes while optimizing the prompt for each node in a separate stage<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ZHUGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhuge is an author who has worked on GPT-Swarm in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AGENTOPTIMIZER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentOptimizer is a system that learns the tools used in agents<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author who has worked on AgentOptimizer in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AGENT SYMBOLIC LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agent Symbolic Learning is a system that learns prompts, tools, and control flow together in agents<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/node>    <node id=\"ROKON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rokon is an author who has discussed safety concerns when executing untrusted model-generated code in 2020<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yee is an author who has discussed safety concerns when executing untrusted model-generated code in 2010<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author who has discussed the potential of ADAS to contribute to an even faster way to create Artificial General Intelligence (AGI) in 2019<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BENGIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bengio is an author who has discussed whether we should pursue AGI and AI-GA in 2024<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BOSTROM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bostrom is an author who has discussed whether we should pursue AGI and AI-GA in 2002<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ECOFFET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ecoffet is an author who has discussed whether we should pursue AGI and AI-GA in 2020<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUDKOWSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yudkowsky is an author who has discussed whether we should pursue AGI and AI-GA in 2008<\/data>      <data key=\"d2\">dc55f071b95dec721a9820d39cdb3ccd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <edge source=\"FMS\" target=\"RAFAILOV\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Rafailov has worked on programming the loss function for preference learning in FM alignment training<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"EUREKA\" target=\"MA\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Ma has worked on Eureka, a system that enables FMs to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"LANGUAGE-TO-REWARD\" target=\"YU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yu has worked on language-to-reward, a system that enables FMs to write reward functions for reinforcement learning in robotics<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"OMNI-EPIC\" target=\"FALDOR\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Faldor has worked on OMNI-EPIC, a system that enables FMs to create robotics learning environments by programming in code<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Clune has discussed the potential of ADAS to contribute to an even faster way to create Artificial General Intelligence (AGI)<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"BENGIO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Bengio has discussed whether we should pursue AGI and AI-GA, which includes ADAS<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"BOSTROM\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Bostrom has discussed whether we should pursue AGI and AI-GA, which includes ADAS<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"ECOFFET\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Ecoffet has discussed whether we should pursue AGI and AI-GA, which includes ADAS<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"YUDKOWSKY\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Yudkowsky has discussed whether we should pursue AGI and AI-GA, which includes ADAS<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"OPRO\" target=\"YANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yang has worked on OPRO, a system that adopts FMs to automate prompt engineering for agents<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"PROMPTBREEDER\" target=\"FERNANDO\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Fernando has worked on PromptBreeder, a system that adopts FMs to automate prompt engineering for agents<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"SELF-DISCOVER\" target=\"ZHOU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zhou has worked on Self-Discover, a system that adopts FMs to automate prompt engineering for agents<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"ZHOU\" target=\"AGENT SYMBOLIC LEARNING\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zhou has worked on Agent Symbolic Learning, a system that learns prompts, tools, and control flow together in agents<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"EVOAGENT\" target=\"YUAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Yuan has worked on EvoAgent, a system that optimizes role definition in the prompt<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"AGENTVERSE\" target=\"CHEN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Chen has worked on AgentVerse, a system that optimizes role definition in the prompt<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"AGENTVERSE\" target=\"XU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Xu has shown that assigning personas or roles to agents is beneficial, which is a concept used in AgentVerse<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"META AGENT SEARCH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Chen has discussed safety concerns when executing untrusted model-generated code in Meta Agent Search<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"DYLAN\" target=\"LIU\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Liu has worked on DyLAN, a system that uses FMs to score the response quality of nodes in each layer to prune the connections<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"DSPY\" target=\"KHATTAB\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Khattab has worked on DSPy, a system that generates a set of possible nodes and then optimizes across the Cartesian product of these nodes<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"GPT-SWARM\" target=\"ZHUGE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zhuge has worked on GPT-Swarm, a system that represents an agentic system in a graph with a predefined set of nodes and uses a Reinforcement Learning algorithm to optimize the connections<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"AGENTOPTIMIZER\" target=\"ZHANG\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Zhang has worked on AgentOptimizer, a system that learns the tools used in agents<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ROKON\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Rokon has discussed safety concerns when executing untrusted model-generated code in Meta Agent Search<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"YEE\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Yee has discussed safety concerns when executing untrusted model-generated code in Meta Agent Search<\/data>      <data key=\"d6\">dc55f071b95dec721a9820d39cdb3ccd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6bdf681c0bd9e401ac72344a6a0ae479","chunk":"; Bostrom, 2002; Clune, 2019; Ecoffet et al., 2020;\nYudkowsky et al., 2008), and is beyond the scope of this paper. Specifically as regards ADAS, we\nbelieve it is net beneficial to publish this work. First, this work demonstrates that with the available\nAPI access to powerful FMs, it is easy to program powerful ADAS algorithms, and do so without any\nexpensive hardware like GPUs. We feel it is beneficial to let the community know such algorithms are\npowerful and easy to create, so they can be informed and account for them. Moreover, by sharing this\ninformation, we hope to motivate follow-up work into safe-ADAS, such as algorithms that conduct\nADAS safely during both search itself (e.g. not risking running any harmful code) and that refuse\nto create dishonest, unhelpful, and\/or harmful agents. Such an open-source research approach to\ncreate safe-ADAS could be a better way to create safer AI systems (Caldwell, 2011; Meta, 2024). One\ndirection we find particularly promising is to simply ask the Meta Agent Search algorithm to be safe\nduring training and only create helpful, harmless, honest agents, potentially incorporating ideas such\nas Constitutional AI (Bai et al., 2022).\n12Automated Design of Agentic Systems\nFuture Work. Our work also opens up many future research directions. For example:\n\u2022Higher-order ADAS. Since the meta agent used in ADAS to program new agents in code is also\nan agent, ADAS can become self-referential where the meta agent can be improved through ADAS\nas well. It would be an exciting direction to have a higher order of meta-learning to allow the\nlearning of the meta agent and even the meta-meta agent, etc. (Lu et al., 2023)\n\u2022Seeding ADAS with more existing building blocks. Although we can theoretically allow any\ncomponents in agentic systems to be programmed from scratch in the code space, it is not efficient\nin practice. Therefore, it would be interesting to explore ADAS by standing on the shoulders of\nexisting human efforts, such as search engine tools, RAG (Lewis et al., 2020), or functions from\nexisting agent frameworks like LangChain (LangChainAI, 2022). Additionally, it is interesting\nto support multi-modal capabilities (e.g. vision) in FMs or allow different FMs to be available in\nagentic systems. This will enable the meta agent to choose from different FMs flexibly according to\nthe difficulty of the instruction and whether data privacy is a priority.\n\u2022Multi-objective ADAS. We only consider one objective (i.e., performance) to optimize in this\npaper, but in practice, multiple objectives are often considered, such as cost, latency, and robustness\nof agentic systems (Hu et al., 2021; Huang et al., 2023). Thus, integrating multi-objective search\nalgorithms (Deb et al., 2002) in ADAS could be promising.\n\u2022Novelty search algorithms. In Meta Agent Search, the design of the search algorithm is rel-\natively simple, focusing solely on exploring interesting new designs. A more careful design of\nthe search algorithm can be a promising future direction. For example, one could incorporate\nmore sophisticated ideas from Quality-Diversity (Cully & Demiris, 2017; Mouret & Clune, 2015),\nAI-generating (Clune, 2019), and Open-ended Algorithms (Faldor et al., 2024; Stanley & Lehman,\n2015; Stanley et al., 2019; Zhang et al., 2024a). One could also include more classic approaches\nto balance exploration and exploitation (Liu et al., 2024; Sutton & Barto, 2018).\n\u2022More intelligent evaluation functions. In this work, we simply evaluate discovered agents on the\nevaluation set and use the numerical performance results. However, this approach is both expensive\nand misses a lot of information. A promising future direction is to enable the meta agent to analyze\ndetailed running logs during the evaluation, which contain rich information on the failure and\nsuccess modes for better debugging and improving agentic systems (Zhou et al., 2024b). Also,\nmany tasks involve subjective answer evaluations (Chiang et al., 2024; Lu et al., 2024b) that do\nnot have ground-truth answers. It is also important to design novel evaluation functions in ADAS\nto address these tasks. Finally, in this work, we targeted only one domain during the search. It\nwould be interesting to explore whether ADAS algorithms can design even better generalist agents\nwhen specifically searching for agents capable of performing well across multiple domains.\n\u2022More complex domains. Additionally, we only evaluate Meta Agent Search on single-step QA\ntasks in this paper. It would be interesting to extend the method to more complex domians, such\nas real-world applications involving multi-step interaction with complex environments.\n\u2022Understanding the emergence of complexity from human organizations. Beyond potentially\nsaving researchers\u2019 efforts and improving upon the manual design of agentic systems, the research\nin ADAS is also scientifically intriguing as it sheds light on the origins of complexity emerging from\nhuman organization and society. The agentic system is a machine learning system that operates\nprimarily over natural language\u2014a representation that is interpretable to humans and used by\nhumans in constructing our organization and society. Thus, there is a close connection between\nagentic systems and human organizations, as shown in works incorporating the organizational\nstructure for human companies in agents (Hong et al., 2023) or","chunk_id":"6bdf681c0bd9e401ac72344a6a0ae479","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"CLUNE","type":"PERSON","description":"Clune is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"ECOFFET","type":"PERSON","description":"Ecoffet is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"YUDKOWSKY","type":"PERSON","description":"Yudkowsky is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"ADAS","type":"TECHNOLOGY","description":"ADAS (Automated Design of Agentic Systems) is a technology that allows the programming of powerful algorithms without expensive hardware like GPUs","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"SAFE-ADAS","type":"TECHNOLOGY","description":"Safe-ADAS refers to algorithms that conduct ADAS safely, avoiding harmful code and creating honest, helpful agents","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CALDWELL","type":"PERSON","description":"Caldwell is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"META","type":"ORGANIZATION","description":"Meta is an organization mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"ORGANIZATION"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CONSTITUTIONAL AI","type":"TECHNOLOGY","description":"Constitutional AI is an approach that aims to create AI systems that are safe, honest, and helpful","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"BAI","type":"PERSON","description":"Bai is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"HIGHER-ORDER ADAS","type":"TECHNOLOGY","description":"Higher-order ADAS refers to the concept of improving the meta agent itself through ADAS, allowing for meta-learning of the meta agent and beyond","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"LU","type":"PERSON","description":"Lu is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"RAG","type":"TECHNOLOGY","description":"RAG (Retrieval-Augmented Generation) is a technology mentioned as a potential building block for ADAS","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"LEWIS","type":"PERSON","description":"Lewis is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"LangChain is an existing agent framework mentioned as a potential building block for ADAS","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"LANGCHAINAI","type":"ORGANIZATION","description":"LangChainAI is the organization behind the LangChain framework","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"ORGANIZATION"},{"name":"MULTI-OBJECTIVE ADAS","type":"TECHNOLOGY","description":"Multi-objective ADAS refers to the concept of optimizing multiple objectives such as cost, latency, and robustness in agentic systems","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"HU","type":"PERSON","description":"Hu is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"HUANG","type":"PERSON","description":"Huang is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"DEB","type":"PERSON","description":"Deb is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"NOVELTY SEARCH ALGORITHMS","type":"TECHNOLOGY","description":"Novelty search algorithms are algorithms designed to explore interesting new designs, potentially incorporating ideas from Quality-Diversity and AI-generating algorithms","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"CULLY","type":"PERSON","description":"Cully is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"DEMIRIS","type":"PERSON","description":"Demiris is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"MOURET","type":"PERSON","description":"Mouret is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"ZHANG","type":"PERSON","description":"Zhang is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"LIU","type":"PERSON","description":"Liu is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"SUTTON","type":"PERSON","description":"Sutton is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"BARTO","type":"PERSON","description":"Barto is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"MORE INTELLIGENT EVALUATION FUNCTIONS","type":"TECHNOLOGY","description":"More intelligent evaluation functions refer to advanced methods for evaluating agents, including analyzing detailed running logs and addressing subjective answer evaluations","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"ZHOU","type":"PERSON","description":"Zhou is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"CHIANG","type":"PERSON","description":"Chiang is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479","entity_type":"PERSON"},{"name":"MORE COMPLEX DOMAINS","type":"TECHNOLOGY","description":"More complex domains refer to extending the evaluation of Meta Agent Search to real-world applications involving multi-step interaction with complex environments","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"UNDERSTANDING THE EMERGENCE OF COMPLEXITY FROM HUMAN ORGANIZATIONS","type":"CONCEPT","description":"Understanding the emergence of complexity from human organizations refers to the scientific study of how complexity arises in human organizations and society, and its connection to agentic systems","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"HONG","type":"PERSON","description":"Hong is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"AGENTIC SYSTEMS","type":"TECHNOLOGY","description":"Agentic systems are systems that operate primarily over natural language and are interpretable to humans","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"SEARCH ENGINE TOOLS","type":"TECHNOLOGY","description":"Search engine tools are mentioned as potential building blocks for ADAS","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"MULTI-MODAL CAPABILITIES","type":"TECHNOLOGY","description":"Multi-modal capabilities refer to the ability to support multiple types of data, such as vision, in agentic systems","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"BOSTROM","type":"PERSON","description":"Bostrom is an author mentioned in the text","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"},{"name":"FMS","type":"","description":"","source_id":"6bdf681c0bd9e401ac72344a6a0ae479"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clune is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ECOFFET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ecoffet is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUDKOWSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yudkowsky is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) is a technology that allows the programming of powerful algorithms without expensive hardware like GPUs<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"SAFE-ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Safe-ADAS refers to algorithms that conduct ADAS safely, avoiding harmful code and creating honest, helpful agents<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CALDWELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caldwell is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"META\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Meta is an organization mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CONSTITUTIONAL AI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Constitutional AI is an approach that aims to create AI systems that are safe, honest, and helpful<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"BAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bai is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HIGHER-ORDER ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Higher-order ADAS refers to the concept of improving the meta agent itself through ADAS, allowing for meta-learning of the meta agent and beyond<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is a technology mentioned as a potential building block for ADAS<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lewis is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LangChain is an existing agent framework mentioned as a potential building block for ADAS<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"LANGCHAINAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">LangChainAI is the organization behind the LangChain framework<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"MULTI-OBJECTIVE ADAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multi-objective ADAS refers to the concept of optimizing multiple objectives such as cost, latency, and robustness in agentic systems<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hu is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huang is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Deb is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NOVELTY SEARCH ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Novelty search algorithms are algorithms designed to explore interesting new designs, potentially incorporating ideas from Quality-Diversity and AI-generating algorithms<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"CULLY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cully is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DEMIRIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Demiris is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MOURET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mouret is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhang is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liu is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sutton is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BARTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barto is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MORE INTELLIGENT EVALUATION FUNCTIONS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">More intelligent evaluation functions refer to advanced methods for evaluating agents, including analyzing detailed running logs and addressing subjective answer evaluations<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhou is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chiang is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MORE COMPLEX DOMAINS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">More complex domains refer to extending the evaluation of Meta Agent Search to real-world applications involving multi-step interaction with complex environments<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"UNDERSTANDING THE EMERGENCE OF COMPLEXITY FROM HUMAN ORGANIZATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Understanding the emergence of complexity from human organizations refers to the scientific study of how complexity arises in human organizations and society, and its connection to agentic systems<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hong is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic systems are systems that operate primarily over natural language and are interpretable to humans<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"SEARCH ENGINE TOOLS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search engine tools are mentioned as potential building blocks for ADAS<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"MULTI-MODAL CAPABILITIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multi-modal capabilities refer to the ability to support multiple types of data, such as vision, in agentic systems<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"BOSTROM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bostrom is an author mentioned in the text<\/data>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <node id=\"FMS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/node>    <edge source=\"CLUNE\" target=\"ADAS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Clune is mentioned in relation to ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ECOFFET\" target=\"ADAS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Ecoffet is mentioned in relation to ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"YUDKOWSKY\" target=\"ADAS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Yudkowsky is mentioned in relation to ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SAFE-ADAS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Safe-ADAS is a subset of ADAS focused on conducting ADAS safely and creating honest, helpful agents<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"HIGHER-ORDER ADAS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Higher-order ADAS is a concept within the broader ADAS framework<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"MULTI-OBJECTIVE ADAS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Multi-objective ADAS is a concept within the broader ADAS framework<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"UNDERSTANDING THE EMERGENCE OF COMPLEXITY FROM HUMAN ORGANIZATIONS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Understanding the emergence of complexity from human organizations is a scientific aspect of ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"FMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">FMs are used to program ADAS algorithms without expensive hardware like GPUs<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"AGENTIC SYSTEMS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">ADAS is a type of agentic system<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SEARCH ENGINE TOOLS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Search engine tools are mentioned as potential building blocks for ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"MULTI-MODAL CAPABILITIES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Multi-modal capabilities are mentioned as potential features for ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"RAG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">RAG is mentioned as a potential building block for ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"BOSTROM\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Bostrom is mentioned in relation to ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"SAFE-ADAS\" target=\"CALDWELL\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Caldwell is mentioned in relation to the concept of Safe-ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META\" target=\"META AGENT SEARCH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Meta is mentioned in relation to the Meta Agent Search algorithm<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"HIGHER-ORDER ADAS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Higher-order ADAS is an extension of Meta Agent Search that involves improving the meta agent itself<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"NOVELTY SEARCH ALGORITHMS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Novelty search algorithms are mentioned as a potential future direction for improving Meta Agent Search<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MORE INTELLIGENT EVALUATION FUNCTIONS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">More intelligent evaluation functions are mentioned as a potential future direction for improving Meta Agent Search<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MORE COMPLEX DOMAINS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">More complex domains are mentioned as a potential future direction for extending the evaluation of Meta Agent Search<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"UNDERSTANDING THE EMERGENCE OF COMPLEXITY FROM HUMAN ORGANIZATIONS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Understanding the emergence of complexity from human organizations is a scientific aspect of Meta Agent Search<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"CONSTITUTIONAL AI\" target=\"BAI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Bai is an author associated with Constitutional AI<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"HIGHER-ORDER ADAS\" target=\"LU\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Lu is mentioned in relation to the concept of Higher-order ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"RAG\" target=\"LEWIS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lewis is an author associated with RAG<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"LANGCHAIN\" target=\"LANGCHAINAI\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">LangChain is a framework developed by LangChainAI<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"MULTI-OBJECTIVE ADAS\" target=\"HU\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Hu is mentioned in relation to the concept of Multi-objective ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"MULTI-OBJECTIVE ADAS\" target=\"HUANG\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Huang is mentioned in relation to the concept of Multi-objective ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"MULTI-OBJECTIVE ADAS\" target=\"DEB\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Deb is mentioned in relation to the concept of Multi-objective ADAS<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"CULLY\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Cully is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"DEMIRIS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Demiris is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"MOURET\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Mouret is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"STANLEY\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Stanley is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"LEHMAN\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Lehman is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"ZHANG\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Zhang is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"LIU\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Liu is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"SUTTON\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Sutton is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"NOVELTY SEARCH ALGORITHMS\" target=\"BARTO\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Barto is mentioned in relation to novelty search algorithms<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"MORE INTELLIGENT EVALUATION FUNCTIONS\" target=\"ZHOU\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Zhou is mentioned in relation to more intelligent evaluation functions<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>    <edge source=\"MORE INTELLIGENT EVALUATION FUNCTIONS\" target=\"CHIANG\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Chiang is mentioned in relation to more intelligent evaluation functions<\/data>      <data key=\"d6\">6bdf681c0bd9e401ac72344a6a0ae479<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"7de66b94cf868b37b1df51dc545c415f","chunk":"AS is also scientifically intriguing as it sheds light on the origins of complexity emerging from\nhuman organization and society. The agentic system is a machine learning system that operates\nprimarily over natural language\u2014a representation that is interpretable to humans and used by\nhumans in constructing our organization and society. Thus, there is a close connection between\nagentic systems and human organizations, as shown in works incorporating the organizational\nstructure for human companies in agents (Hong et al., 2023) or simulating a human town with\nagents (Park et al., 2023). Therefore, the study in ADAS may enable us to observe how to create\na simple set of conditions and have an algorithm to bootstrap itself from simplicity to produce\ncomplexity in a system akin to human society.\n13Automated Design of Agentic Systems\n\u2022Towards a Better Understanding of FMs. Works from Neural Architecture Search (Huang et al.,\n2023) show that by observing the emerged architecture, we could gain more insights into Neural\nNetworks. In this paper, we also gained insights about FMs from the results. For example, the\nbest agent with GPT-3.5 involves a complex feedback mechanism, but when we transfer to other\nadvanced models, the agent with a simpler feedback mechanism but more refinement becomes a\nbetter agent (Section 4.3). This shows that GPT-3.5 may have a worse capability in evaluating and\nrefining the answers, so it needs a complex feedback mechanism for better refinement, while other\nadvanced models benefit more from a simpler feedback mechanism.\nConclusion. Inthispaper, weproposeanewresearchproblem, AutomatedDesignofAgenticSystems\n(ADAS), which aims to automatically invent novel building blocks and design powerful agentic systems .\nWe demonstrated that a promising approach to ADAS is to define agents in code, allowing new agents\nto be automatically discovered by a \u201cmeta\u201d agent programming them in code. Following this idea,\nwe propose Meta Agent Search, where the meta agent iteratively builds on previous discoveries\nto program interesting new agents. The experiments show that Meta Agent Search consistently\noutperforms state-of-the-art hand-designed agents across an extensive number of domains, and the\ndiscovered agents transfer well across models and domains. Overall, our work illustrates the potential\nof an exciting new research direction toward full automation in developing powerful agentic systems\nfrom the bottom up.\nAcknowledgments\nThis work was supported by the Vector Institute, the Canada CIFAR AI Chairs program, grants from\nSchmidt Futures and Open Philanthropy, an NSERC Discovery Grant, and a generous donation from\nRafael Cosman. We thank Jenny Zhang, Rach Pradhan, Ruiyu Gou, Nicholas Ioannidis, and Eunjeong\nHwang for insightful discussions and feedback.\n14Automated Design of Agentic Systems\nReferences\nAnthropic. Introducing the next generation of claude. https:\/\/www.anthropic.com\/news\/\nclaude-3-family , March 2024a. Blog post.\nAnthropic. Introducing claude 3.5 sonnet. https:\/\/www.anthropic.com\/news\/\nclaude-3-5-sonnet , June 2024b. Blog post.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073 , 2022.\nYoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah\nHarari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid\nprogress. Science, 384(6698):842\u2013845, 2024.\nN Bostrom. Existential Risks: analyzing human extinction scenarios and related hazards. Journal of\nEvolution and Technology , 9, 2002.\nRobert S Boyer and J Strother Moore. A mechanical proof of the Turing completeness of pure LISP .\nCiteseer, 1983.\nTracey Caldwell. Ethical hackers: putting on the white hat. Network Security , 2011(7):10\u201313, 2011.\nHarrison Chase. What is an agent? https:\/\/blog.langchain.dev\/what-is-an-agent\/ , June\n2024. Blog post.\nBanghao Chen, Zhaofeng Zhang, Nicolas Langren\u00e9, and Shengxin Zhu. Unleashing the poten-\ntial of prompt engineering in large language models: a comprehensive review. arXiv preprint\narXiv:2310.14735 , 2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\nYi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. In The Twelfth International Conference on Learning Representations , 2023b.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angel","chunk_id":"7de66b94cf868b37b1df51dc545c415f","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"AS","type":"CONCEPT","description":"AS refers to a scientifically intriguing concept that sheds light on the origins of complexity emerging from human organization and society","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"AGENTIC SYSTEM","type":"TECHNOLOGY","description":"The agentic system is a machine learning system that operates primarily over natural language, which is interpretable to humans and used by humans in constructing our organization and society","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"HUMAN ORGANIZATIONS","type":"CONCEPT","description":"Human organizations refer to the structured groups of people working together, often mentioned in the context of agentic systems and their connection to human society","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"HONG ET AL.","type":"PERSON","description":"Hong et al. are authors who have worked on incorporating the organizational structure for human companies in agents","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"PARK ET AL.","type":"PERSON","description":"Park et al. are authors who have worked on simulating a human town with agents","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ADAS","type":"CONCEPT","description":"ADAS stands for Automated Design of Agentic Systems, a research problem aimed at automatically inventing novel building blocks and designing powerful agentic systems","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNOLOGY","description":"Neural Architecture Search is a method that involves observing the emerged architecture to gain more insights into Neural Networks","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a model mentioned in the context of agentic systems, noted for involving a complex feedback mechanism","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"TECHNOLOGY"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method where a meta agent iteratively builds on previous discoveries to program new agents, consistently outperforming state-of-the-art hand-designed agents","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"VECTOR INSTITUTE","type":"ORGANIZATION","description":"The Vector Institute is one of the organizations that supported the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"ORGANIZATION"},{"name":"CANADA CIFAR AI CHAIRS PROGRAM","type":"PROGRAM","description":"The Canada CIFAR AI Chairs program is one of the programs that supported the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PROGRAM"},{"name":"SCHMIDT FUTURES","type":"ORGANIZATION","description":"Schmidt Futures is one of the organizations that provided grants for the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"ORGANIZATION"},{"name":"OPEN PHILANTHROPY","type":"ORGANIZATION","description":"Open Philanthropy is one of the organizations that provided grants for the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"ORGANIZATION"},{"name":"NSERC DISCOVERY GRANT","type":"PROGRAM","description":"The NSERC Discovery Grant is one of the grants that supported the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PROGRAM"},{"name":"RAFAEL COSMAN","type":"PERSON","description":"Rafael Cosman is a person who made a generous donation to support the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"RACH PRADHAN","type":"PERSON","description":"Rach Pradhan is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"RUIYU GOU","type":"PERSON","description":"Ruiyu Gou is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"NICHOLAS IOANNIDIS","type":"PERSON","description":"Nicholas Ioannidis is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"EUNJEONG HWANG","type":"PERSON","description":"Eunjeong Hwang is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ANTHROPIC","type":"ORGANIZATION","description":"Anthropic is an organization that published blog posts about the next generation of Claude and Claude 3.5 Sonnet","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"ORGANIZATION"},{"name":"CLAUDE 3","type":"TECHNOLOGY","description":"Claude 3 is a model introduced by Anthropic in a blog post in March 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"TECHNOLOGY"},{"name":"CLAUDE 3.5 SONNET","type":"TECHNOLOGY","description":"Claude 3.5 Sonnet is a model introduced by Anthropic in a blog post in June 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"TECHNOLOGY"},{"name":"YUNTAO BAI","type":"PERSON","description":"Yuntao Bai is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"SAURAV KADAVATH","type":"PERSON","description":"Saurav Kadavath is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"SANDIPAN KUNDU","type":"PERSON","description":"Sandipan Kundu is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"AMANDA ASKELL","type":"PERSON","description":"Amanda Askell is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"JACKSON KERNION","type":"PERSON","description":"Jackson Kernion is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ANDY JONES","type":"PERSON","description":"Andy Jones is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ANNA CHEN","type":"PERSON","description":"Anna Chen is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ANNA GOLDIE","type":"PERSON","description":"Anna Goldie is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"AZALIA MIRHOSEINI","type":"PERSON","description":"Azalia Mirhoseini is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"CAMERON MCKINNON","type":"PERSON","description":"Cameron McKinnon is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YOSHUA BENGIO","type":"PERSON","description":"Yoshua Bengio is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"GEOFFREY HINTON","type":"PERSON","description":"Geoffrey Hinton is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ANDREW YAO","type":"PERSON","description":"Andrew Yao is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"TREVOR DARRELL","type":"PERSON","description":"Trevor Darrell is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YUVAL NOAH HARARI","type":"PERSON","description":"Yuval Noah Harari is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YA-QIN ZHANG","type":"PERSON","description":"Ya-Qin Zhang is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"LAN XUE","type":"PERSON","description":"Lan Xue is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"SHAI SHALEV-SHWARTZ","type":"PERSON","description":"Shai Shalev-Shwartz is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"N BOSTROM","type":"PERSON","description":"N Bostrom is the author of the paper \"Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards\" published in the Journal of Evolution and Technology in 2002","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ROBERT S BOYER","type":"PERSON","description":"Robert S Boyer is one of the authors of the paper \"A Mechanical Proof of the Turing Completeness of Pure LISP\" published in 1983","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"J STROTHER MOORE","type":"PERSON","description":"J Strother Moore is one of the authors of the paper \"A Mechanical Proof of the Turing Completeness of Pure LISP\" published in 1983","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"TRACEY CALDWELL","type":"PERSON","description":"Tracey Caldwell is the author of the paper \"Ethical Hackers: Putting on the White Hat\" published in Network Security in 2011","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"HARRISON CHASE","type":"PERSON","description":"Harrison Chase is the author of the blog post \"What is an Agent?\" published on the LangChain blog in June 2024","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"BANGHAO CHEN","type":"PERSON","description":"Banghao Chen is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ZHAOFENG ZHANG","type":"PERSON","description":"Zhaofeng Zhang is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"NICOLAS LANGREN\u00c9","type":"PERSON","description":"Nicolas Langren\u00e9 is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"SHENGXIN ZHU","type":"PERSON","description":"Shengxin Zhu is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"QIMING YUAN","type":"PERSON","description":"Qiming Yuan is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"HENRIQUE PONDE DE OLIVEIRA PINTO","type":"PERSON","description":"Henrique Ponde De Oliveira Pinto is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"JARED KAPLAN","type":"PERSON","description":"Jared Kaplan is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"HARRI EDWARDS","type":"PERSON","description":"Harri Edwards is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YURI BURDA","type":"PERSON","description":"Yuri Burda is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"NICHOLAS JOSEPH","type":"PERSON","description":"Nicholas Joseph is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"GREG BROCKMAN","type":"PERSON","description":"Greg Brockman is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"WEIZE CHEN","type":"PERSON","description":"Weize Chen is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YUSHENG SU","type":"PERSON","description":"Yusheng Su is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"JINGWEI ZUO","type":"PERSON","description":"Jingwei Zuo is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"CHENG YANG","type":"PERSON","description":"Cheng Yang is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"CHENFEI YUAN","type":"PERSON","description":"Chenfei Yuan is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"CHI-MIN CHAN","type":"PERSON","description":"Chi-Min Chan is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"HEYANG YU","type":"PERSON","description":"Heyang Yu is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"YI-HSIN HUNG","type":"PERSON","description":"Yi-Hsin Hung is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"CHEN QIAN","type":"PERSON","description":"Chen Qian is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"7de66b94cf868b37b1df51dc545c415f","entity_type":"PERSON"},{"name":"ADVANCED MODELS","type":"TECHNOLOGY","description":"Advanced models refer to more recent and sophisticated machine learning models that benefit from simpler feedback mechanisms","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"INSIGHTS","type":"CONCEPT","description":"Insights refer to the understanding gained from observing the results of experiments and studies","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"BUILDING BLOCKS","type":"CONCEPT","description":"Building blocks refer to the fundamental components used in the design of agentic systems","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"ALGORITHM","type":"TECHNOLOGY","description":"An algorithm is a set of rules or processes followed in problem-solving operations, often by a computer","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"EXPERIMENTS","type":"CONCEPT","description":"Experiments refer to the tests conducted to validate the effectiveness of Meta Agent Search in discovering new agents","source_id":"7de66b94cf868b37b1df51dc545c415f"},{"name":"DOMAINS","type":"CONCEPT","description":"Domains refer to the various fields or areas where the discovered agents are tested and applied","source_id":"7de66b94cf868b37b1df51dc545c415f"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AS refers to a scientifically intriguing concept that sheds light on the origins of complexity emerging from human organization and society<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"AGENTIC SYSTEM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The agentic system is a machine learning system that operates primarily over natural language, which is interpretable to humans and used by humans in constructing our organization and society<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"HUMAN ORGANIZATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Human organizations refer to the structured groups of people working together, often mentioned in the context of agentic systems and their connection to human society<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"HONG ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hong et al. are authors who have worked on incorporating the organizational structure for human companies in agents<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"PARK ET AL.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Park et al. are authors who have worked on simulating a human town with agents<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">ADAS stands for Automated Design of Agentic Systems, a research problem aimed at automatically inventing novel building blocks and designing powerful agentic systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural Architecture Search is a method that involves observing the emerged architecture to gain more insights into Neural Networks<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a model mentioned in the context of agentic systems, noted for involving a complex feedback mechanism<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method where a meta agent iteratively builds on previous discoveries to program new agents, consistently outperforming state-of-the-art hand-designed agents<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"VECTOR INSTITUTE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The Vector Institute is one of the organizations that supported the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"CANADA CIFAR AI CHAIRS PROGRAM\">      <data key=\"d0\">PROGRAM<\/data>      <data key=\"d1\">The Canada CIFAR AI Chairs program is one of the programs that supported the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PROGRAM<\/data>    <\/node>    <node id=\"SCHMIDT FUTURES\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Schmidt Futures is one of the organizations that provided grants for the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"OPEN PHILANTHROPY\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Open Philanthropy is one of the organizations that provided grants for the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"NSERC DISCOVERY GRANT\">      <data key=\"d0\">PROGRAM<\/data>      <data key=\"d1\">The NSERC Discovery Grant is one of the grants that supported the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PROGRAM<\/data>    <\/node>    <node id=\"RAFAEL COSMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafael Cosman is a person who made a generous donation to support the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RACH PRADHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rach Pradhan is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUIYU GOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruiyu Gou is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICHOLAS IOANNIDIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Ioannidis is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EUNJEONG HWANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eunjeong Hwang is one of the individuals thanked for insightful discussions and feedback on the work on Automated Design of Agentic Systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANTHROPIC\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Anthropic is an organization that published blog posts about the next generation of Claude and Claude 3.5 Sonnet<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"CLAUDE 3\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude 3 is a model introduced by Anthropic in a blog post in March 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"CLAUDE 3.5 SONNET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Claude 3.5 Sonnet is a model introduced by Anthropic in a blog post in June 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YUNTAO BAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuntao Bai is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAURAV KADAVATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saurav Kadavath is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SANDIPAN KUNDU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sandipan Kundu is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMANDA ASKELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amanda Askell is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JACKSON KERNION\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jackson Kernion is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDY JONES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Jones is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANNA CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anna Chen is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANNA GOLDIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anna Goldie is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AZALIA MIRHOSEINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Azalia Mirhoseini is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAMERON MCKINNON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cameron McKinnon is one of the authors of the paper \"Constitutional AI: Harmlessness from AI Feedback\" published on arXiv in 2022<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YOSHUA BENGIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yoshua Bengio is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GEOFFREY HINTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Geoffrey Hinton is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDREW YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Yao is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TREVOR DARRELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Trevor Darrell is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUVAL NOAH HARARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuval Noah Harari is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YA-QIN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ya-Qin Zhang is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAN XUE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lan Xue is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAI SHALEV-SHWARTZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shai Shalev-Shwartz is one of the authors of the paper \"Managing Extreme AI Risks Amid Rapid Progress\" published in Science in 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"N BOSTROM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N Bostrom is the author of the paper \"Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards\" published in the Journal of Evolution and Technology in 2002<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROBERT S BOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert S Boyer is one of the authors of the paper \"A Mechanical Proof of the Turing Completeness of Pure LISP\" published in 1983<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J STROTHER MOORE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J Strother Moore is one of the authors of the paper \"A Mechanical Proof of the Turing Completeness of Pure LISP\" published in 1983<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TRACEY CALDWELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tracey Caldwell is the author of the paper \"Ethical Hackers: Putting on the White Hat\" published in Network Security in 2011<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARRISON CHASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Chase is the author of the blog post \"What is an Agent?\" published on the LangChain blog in June 2024<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BANGHAO CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Banghao Chen is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHAOFENG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhaofeng Zhang is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICOLAS LANGREN&#201;\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Langren&#233; is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHENGXIN ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengxin Zhu is one of the authors of the paper \"Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review\" published on arXiv in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QIMING YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiming Yuan is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENRIQUE PONDE DE OLIVEIRA PINTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henrique Ponde De Oliveira Pinto is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JARED KAPLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jared Kaplan is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HARRI EDWARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harri Edwards is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YURI BURDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuri Burda is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICHOLAS JOSEPH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Joseph is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GREG BROCKMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Greg Brockman is one of the authors of the paper \"Evaluating Large Language Models Trained on Code\" published on arXiv in 2021<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEIZE CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weize Chen is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUSHENG SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yusheng Su is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JINGWEI ZUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingwei Zuo is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHENG YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng Yang is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHENFEI YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chenfei Yuan is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHI-MIN CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi-Min Chan is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEYANG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Yu is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YI-HSIN HUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Hsin Hung is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Qian is one of the authors of the paper \"Agentverse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADVANCED MODELS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Advanced models refer to more recent and sophisticated machine learning models that benefit from simpler feedback mechanisms<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"INSIGHTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Insights refer to the understanding gained from observing the results of experiments and studies<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"BUILDING BLOCKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Building blocks refer to the fundamental components used in the design of agentic systems<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"ALGORITHM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An algorithm is a set of rules or processes followed in problem-solving operations, often by a computer<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"EXPERIMENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Experiments refer to the tests conducted to validate the effectiveness of Meta Agent Search in discovering new agents<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <node id=\"DOMAINS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Domains refer to the various fields or areas where the discovered agents are tested and applied<\/data>      <data key=\"d2\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/node>    <edge source=\"AGENTIC SYSTEM\" target=\"HUMAN ORGANIZATIONS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The agentic system operates over natural language, which is used by humans in constructing human organizations and society<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"HUMAN ORGANIZATIONS\" target=\"HONG ET AL.\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Hong et al. worked on incorporating the organizational structure for human companies in agents<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"HUMAN ORGANIZATIONS\" target=\"PARK ET AL.\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Park et al. worked on simulating a human town with agents<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"META AGENT SEARCH\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Meta Agent Search is a method proposed as part of the research problem ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"VECTOR INSTITUTE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The Vector Institute supported the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"CANADA CIFAR AI CHAIRS PROGRAM\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The Canada CIFAR AI Chairs program supported the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"SCHMIDT FUTURES\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Schmidt Futures provided grants for the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"OPEN PHILANTHROPY\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Open Philanthropy provided grants for the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"NSERC DISCOVERY GRANT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The NSERC Discovery Grant supported the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"RAFAEL COSMAN\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Rafael Cosman made a generous donation(\"entity\"Rafael Cosman made a generous donation to support the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"JENNY ZHANG\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Jenny Zhang provided insightful discussions and feedback on the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>    <edge source=\"ADAS\" target=\"RACH PRADHAN\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Rach Pradhan provided insightful discussions and feedback on the work on ADAS<\/data>      <data key=\"d6\">7de66b94cf868b37b1df51dc545c415f<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"022e7927d281e80e188f29ea343cc115","chunk":" Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\nYi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. In The Twelfth International Conference on Learning Representations , 2023b.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nHao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An\nopen platform for evaluating llms by human preference, 2024.\nFran\u00e7ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547 , 2019.\nJeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial\nintelligence. arXiv preprint arXiv:1905.10985 , 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168 , 2021.\nAntoineCullyandYiannisDemiris. Qualityanddiversityoptimization: Aunifyingmodularframework.\nIEEE Transactions on Evolutionary Computation , 22(2):245\u2013259, 2017.\n15Automated Design of Agentic Systems\nN. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR\u201905) , volume 1, pp. 886\u2013893\nvol. 1, 2005. doi: 10.1109\/CVPR.2005.177.\nKalyanmoyDeb, AmritPratap, SameerAgarwal, andTAMTMeyarivan. Afastandelitistmultiobjective\ngenetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation , 6(2):182\u2013197, 2002.\nAaron Dharna, Julian Togelius, and Lisa B Soros. Co-generation of game levels and game-playing\nagents. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Enter-\ntainment , volume 16, pp. 203\u2013209, 2020.\nYilun Du, Shuang Li, Antonio Torralba, Joshua BTenenbaum, and Igor Mordatch. Improving factuality\nand reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 ,\n2023.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill\nBurstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pp. 2368\u20132378, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653\/v1\/N19-1246.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL^2: Fast\nreinforcement learning via slow reinforcement learning. In International Conference on Learning\nRepresentations , 2017.\nAdrien Ecoffet, Jeff Clune, and Joel Lehman. Open questions in creating safe open-ended AI: Tensions\nbetween control and creativity. In Conference on Artificial Life , pp. 27\u201335. MIT Press, 2020.\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal\nof Machine Learning Research , 20(55):1\u201321, 2019.\nMaxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. Omni-epic: Open-endedness via\nmodels of human notions of interestingness with environments programmed in code. arXiv preprint\narXiv:2405.15568 , 2024.\nChrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock-\nt\u00e4schel. Promptbreeder: Self-referential self-improvement via prompt evolution, 2024.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on machine learning , pp. 1126\u20131135. PMLR, 2017.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning , pp. 10764\u201310799. PMLR, 2023.\nRyan Greenblatt. Getting 50% sota on arc-agi with gpt-4. https:\/\/redwoodresearch.substack.\ncom\/p\/getting-50-sota-on-arc-agi-with-gpt , July 202","chunk_id":"022e7927d281e80e188f29ea343cc115","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"CHEN","type":"PERSON","description":"Chen is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YUSHENG SU","type":"PERSON","description":"Yusheng Su is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JINGWEI ZUO","type":"PERSON","description":"Jingwei Zuo is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHENG YANG","type":"PERSON","description":"Cheng Yang is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHENFEI YUAN","type":"PERSON","description":"Chenfei Yuan is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHI-MIN CHAN","type":"PERSON","description":"Chi-Min Chan is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HEYANG YU","type":"PERSON","description":"Heyang Yu is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YI-HSIN HUNG","type":"PERSON","description":"Yi-Hsin Hung is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHEN QIAN","type":"PERSON","description":"Chen Qian is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AGENTVERSE","type":"TECHNOLOGY","description":"Agentverse is a system designed to facilitate multi-agent collaboration and explore emergent behaviors, as described in a paper published in The Twelfth International Conference on Learning Representations in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The Twelfth International Conference on Learning Representations is an event where the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" was published in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"WEI-LIN CHIANG","type":"PERSON","description":"Wei-Lin Chiang is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LIANMIN ZHENG","type":"PERSON","description":"Lianmin Zheng is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YING SHENG","type":"PERSON","description":"Ying Sheng is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ANASTASIOS NIKOLAS ANGELOPOULOS","type":"PERSON","description":"Anastasios Nikolas Angelopoulos is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TIANLE LI","type":"PERSON","description":"Tianle Li is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DACHENG LI","type":"PERSON","description":"Dacheng Li is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HAO ZHANG","type":"PERSON","description":"Hao Zhang is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"BANGHUA ZHU","type":"PERSON","description":"Banghua Zhu is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MICHAEL JORDAN","type":"PERSON","description":"Michael Jordan is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOSEPH E. GONZALEZ","type":"PERSON","description":"Joseph E. Gonzalez is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ION STOICA","type":"PERSON","description":"Ion Stoica is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHATBOT ARENA","type":"TECHNOLOGY","description":"Chatbot Arena is an open platform for evaluating large language models (LLMs) by human preference, as described in a paper published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"FRAN\u00c7OIS CHOLLET","type":"PERSON","description":"Fran\u00e7ois Chollet is the author of the paper titled \"On the measure of intelligence\" published on arXiv in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ON THE MEASURE OF INTELLIGENCE","type":"DOCUMENT","description":"The paper \"On the measure of intelligence\" is authored by Fran\u00e7ois Chollet and was published on arXiv in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is the author of the paper titled \"Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence\" published on arXiv in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AI-GAS","type":"TECHNOLOGY","description":"AI-GAs, or AI-generating algorithms, is an alternate paradigm for producing general artificial intelligence, as described in a paper authored by Jeff Clune and published on arXiv in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"KARL COBBE","type":"PERSON","description":"Karl Cobbe is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS","type":"DOCUMENT","description":"The paper \"Training verifiers to solve math word problems\" is authored by Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, and Reiichiro Nakano, and was published on arXiv in 2021","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ANTOINE CULLY","type":"PERSON","description":"Antoine Cully is one of the authors of the paper titled \"Quality and diversity optimization: A unifying modular framework\" published in IEEE Transactions on Evolutionary Computation in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YIANNIS DEMIRIS","type":"PERSON","description":"Yiannis Demiris is one of the authors of the paper titled \"Quality and diversity optimization: A unifying modular framework\" published in IEEE Transactions on Evolutionary Computation in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"QUALITY AND DIVERSITY OPTIMIZATION","type":"TECHNOLOGY","description":"Quality and diversity optimization is a unifying modular framework described in a paper authored by Antoine Cully and Yiannis Demiris, published in IEEE Transactions on Evolutionary Computation in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"N. DALAL","type":"PERSON","description":"N. Dalal is one of the authors of the paper titled \"Histograms of oriented gradients for human detection\" published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"B. TRIGGS","type":"PERSON","description":"B. Triggs is one of the authors of the paper titled \"Histograms of oriented gradients for human detection\" published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HISTOGRAMS OF ORIENTED GRADIENTS FOR HUMAN DETECTION","type":"TECHNOLOGY","description":"Histograms of oriented gradients for human detection is a method described in a paper authored by N. Dalal and B. Triggs, published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AMRIT PRATAP","type":"PERSON","description":"Amrit Pratap is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SAMEER AGARWAL","type":"PERSON","description":"Sameer Agarwal is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TAMT MEYARIVAN","type":"PERSON","description":"TAMT Meyarivan is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"NSGA-II","type":"TECHNOLOGY","description":"NSGA-II is a fast and elitist multiobjective genetic algorithm described in a paper authored by Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan, published in IEEE Transactions on Evolutionary Computation in 2002","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AARON DHARNA","type":"PERSON","description":"Aaron Dharna is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JULIAN TOGELIUS","type":"PERSON","description":"Julian Togelius is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LISA B SOROS","type":"PERSON","description":"Lisa B Soros is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CO-GENERATION OF GAME LEVELS AND GAME-PLAYING AGENTS","type":"TECHNOLOGY","description":"Co-generation of game levels and game-playing agents is a method described in a paper authored by Aaron Dharna, Julian Togelius, and Lisa B Soros, published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YILUN DU","type":"PERSON","description":"Yilun Du is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SHUANG LI","type":"PERSON","description":"Shuang Li is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ANTONIO TORRALBA","type":"PERSON","description":"Antonio Torralba is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOSHUA B TENENBAUM","type":"PERSON","description":"Joshua B Tenenbaum is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"IGOR MORDATCH","type":"PERSON","description":"Igor Mordatch is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"IMPROVING FACTUALITY AND REASONING IN LANGUAGE MODELS THROUGH MULTIAGENT DEBATE","type":"TECHNOLOGY","description":"Improving factuality and reasoning in language models through multiagent debate is a method described in a paper authored by Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch, published on arXiv in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DHEERU DUA","type":"PERSON","description":"Dheeru Dua is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YIZHONG WANG","type":"PERSON","description":"Yizhong Wang is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PRADEEP DASIGI","type":"PERSON","description":"Pradeep Dasigi is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"GABRIEL STANOVSKY","type":"PERSON","description":"Gabriel Stanovsky is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SAMEER SINGH","type":"PERSON","description":"Sameer Singh is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MATT GARDNER","type":"PERSON","description":"Matt Gardner is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DROP","type":"TECHNOLOGY","description":"DROP is a reading comprehension(\"entity\"","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHRISTY DORAN","type":"PERSON","description":"Christy Doran is one of the editors of the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"THAMAR SOLORIO","type":"PERSON","description":"Thamar Solorio is one of the editors of the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YAN DUAN","type":"PERSON","description":"Yan Duan is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOHN SCHULMAN","type":"PERSON","description":"John Schulman is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"XI CHEN","type":"PERSON","description":"Xi Chen is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PETER L. BARTLETT","type":"PERSON","description":"Peter L. Bartlett is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"RL^2","type":"TECHNOLOGY","description":"RL^2 is a method for fast reinforcement learning via slow reinforcement learning, as described in a paper authored by Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel, published in the International Conference on Learning Representations in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"ADRIEN ECOFFET","type":"PERSON","description":"Adrien Ecoffet is one of the authors of the paper titled \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" published in the Conference on Artificial Life in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the paper titled \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" published in the Conference on Artificial Life in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI","type":"DOCUMENT","description":"The paper \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" is authored by Adrien Ecoffet, Jeff Clune, and Joel Lehman, and was published in the Conference on Artificial Life in 2020","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"THOMAS ELSKEN","type":"PERSON","description":"Thomas Elsken is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JAN HENDRIK METZEN","type":"PERSON","description":"Jan Hendrik Metzen is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"FRANK HUTTER","type":"PERSON","description":"Frank Hutter is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"NEURAL ARCHITECTURE SEARCH","type":"TECHNOLOGY","description":"Neural architecture search is a method described in a paper authored by Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter, published in the Journal of Machine Learning Research in 2019","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MAXENCE FALDOR","type":"PERSON","description":"Maxence Faldor is one of the authors of the paper titled \"Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code\" published on arXiv in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is one of the authors of the paper titled \"Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code\" published on arXiv in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"OMNI-EPIC","type":"TECHNOLOGY","description":"Omni-epic is a method for open-endedness via models of human notions of interestingness with environments programmed in code, as described in a paper authored by Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune, published on arXiv in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHRISANTHA FERNANDO","type":"PERSON","description":"Chrisantha Fernando is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"DYLAN SUNIL BANARSE","type":"PERSON","description":"Dylan Sunil Banarse is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"HENRYK MICHALEWSKI","type":"PERSON","description":"Henryk Michalewski is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SIMON OSINDERO","type":"PERSON","description":"Simon Osindero is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"TIM ROCKT\u00c4SCHEL","type":"PERSON","description":"Tim Rockt\u00e4schel is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PROMPTBREEDER","type":"TECHNOLOGY","description":"Promptbreeder is a method for self-referential self-improvement via prompt evolution, as described in a paper authored by Chrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt\u00e4schel, published in 2024","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is one of the authors of the paper titled \"Model-agnostic meta-learning for fast adaptation of deep networks\" published in the International Conference on Machine Learning in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is one of the authors of the paper titled \"Model-agnostic meta-learning for fast adaptation of deep networks\" published in the International Conference on Machine Learning in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"MODEL-AGNOSTIC META-LEARNING","type":"TECHNOLOGY","description":"Model-agnostic meta-learning is a method for fast adaptation of deep networks, as described in a paper authored by Chelsea Finn, Pieter Abbeel, and Sergey Levine, published in the International Conference on Machine Learning in 2017","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"AMAN MADAAN","type":"PERSON","description":"Aman Madaan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"SHUYAN ZHOU","type":"PERSON","description":"Shuyan Zhou is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"PAL","type":"TECHNOLOGY","description":"PAL, or Program-aided language models, is a method described in a paper authored by Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig, published in the International Conference on Machine Learning in 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"RYAN GREENBLATT","type":"PERSON","description":"Ryan Greenblatt is the author of the article titled \"Getting 50% SOTA on ARC-AGI with GPT-4\" published on Redwood Research Substack in July 2023","source_id":"022e7927d281e80e188f29ea343cc115"},{"name":"GETTING 50% SOTA ON ARC-AGI WITH GPT-4","type":"DOCUMENT","description":"The article \"Getting 50% SOTA on ARC-AGI with GPT-4\" is authored by Ryan Greenblatt and was published on Redwood Research Substack in July 2023","source_id":"022e7927d281e80e188f29ea343cc115"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YUSHENG SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yusheng Su is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JINGWEI ZUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingwei Zuo is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHENG YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng Yang is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHENFEI YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chenfei Yuan is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHI-MIN CHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi-Min Chan is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HEYANG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Yu is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YI-HSIN HUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Hsin Hung is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHEN QIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Qian is one of the authors of the paper titled \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AGENTVERSE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentverse is a system designed to facilitate multi-agent collaboration and explore emergent behaviors, as described in a paper published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Twelfth International Conference on Learning Representations is an event where the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" was published in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"WEI-LIN CHIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei-Lin Chiang is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LIANMIN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lianmin Zheng is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YING SHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ying Sheng is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ANASTASIOS NIKOLAS ANGELOPOULOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anastasios Nikolas Angelopoulos is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TIANLE LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianle Li is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DACHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dacheng Li is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Zhang is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"BANGHUA ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Banghua Zhu is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MICHAEL JORDAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Jordan is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOSEPH E. GONZALEZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joseph E. Gonzalez is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ION STOICA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ion Stoica is one of the authors of the paper titled \"Chatbot arena: An open platform for evaluating llms by human preference\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHATBOT ARENA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chatbot Arena is an open platform for evaluating large language models (LLMs) by human preference, as described in a paper published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"FRAN&#199;OIS CHOLLET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fran&#231;ois Chollet is the author of the paper titled \"On the measure of intelligence\" published on arXiv in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ON THE MEASURE OF INTELLIGENCE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"On the measure of intelligence\" is authored by Fran&#231;ois Chollet and was published on arXiv in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is the author of the paper titled \"Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence\" published on arXiv in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AI-GAS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AI-GAs, or AI-generating algorithms, is an alternate paradigm for producing general artificial intelligence, as described in a paper authored by Jeff Clune and published on arXiv in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"KARL COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karl Cobbe is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Training verifiers to solve math word problems\" is authored by Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, and Reiichiro Nakano, and was published on arXiv in 2021<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ANTOINE CULLY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Antoine Cully is one of the authors of the paper titled \"Quality and diversity optimization: A unifying modular framework\" published in IEEE Transactions on Evolutionary Computation in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YIANNIS DEMIRIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiannis Demiris is one of the authors of the paper titled \"Quality and diversity optimization: A unifying modular framework\" published in IEEE Transactions on Evolutionary Computation in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"QUALITY AND DIVERSITY OPTIMIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality and diversity optimization is a unifying modular framework described in a paper authored by Antoine Cully and Yiannis Demiris, published in IEEE Transactions on Evolutionary Computation in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"N. DALAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">N. Dalal is one of the authors of the paper titled \"Histograms of oriented gradients for human detection\" published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"B. TRIGGS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">B. Triggs is one of the authors of the paper titled \"Histograms of oriented gradients for human detection\" published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HISTOGRAMS OF ORIENTED GRADIENTS FOR HUMAN DETECTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Histograms of oriented gradients for human detection is a method described in a paper authored by N. Dalal and B. Triggs, published in the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AMRIT PRATAP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amrit Pratap is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SAMEER AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Agarwal is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TAMT MEYARIVAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">TAMT Meyarivan is one of the authors of the paper titled \"A fast and elitist multiobjective genetic algorithm: NSGA-II\" published in IEEE Transactions on Evolutionary Computation in 2002<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"NSGA-II\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">NSGA-II is a fast and elitist multiobjective genetic algorithm described in a paper authored by Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan, published in IEEE Transactions on Evolutionary Computation in 2002<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AARON DHARNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aaron Dharna is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JULIAN TOGELIUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Togelius is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LISA B SOROS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lisa B Soros is one of the authors of the paper titled \"Co-generation of game levels and game-playing agents\" published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CO-GENERATION OF GAME LEVELS AND GAME-PLAYING AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Co-generation of game levels and game-playing agents is a method described in a paper authored by Aaron Dharna, Julian Togelius, and Lisa B Soros, published in the Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YILUN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Du is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SHUANG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuang Li is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ANTONIO TORRALBA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Antonio Torralba is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOSHUA B TENENBAUM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joshua B Tenenbaum is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"IGOR MORDATCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Igor Mordatch is one of the authors of the paper titled \"Improving factuality and reasoning in language models through multiagent debate\" published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"IMPROVING FACTUALITY AND REASONING IN LANGUAGE MODELS THROUGH MULTIAGENT DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Improving factuality and reasoning in language models through multiagent debate is a method described in a paper authored by Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch, published on arXiv in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DHEERU DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dheeru Dua is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YIZHONG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yizhong Wang is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PRADEEP DASIGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pradeep Dasigi is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"GABRIEL STANOVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Stanovsky is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SAMEER SINGH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Singh is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MATT GARDNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Gardner is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">DROP is a reading comprehension(\"entity\"<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHRISTY DORAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christy Doran is one of the editors of the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"THAMAR SOLORIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thamar Solorio is one of the editors of the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YAN DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yan Duan is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOHN SCHULMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">John Schulman is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"XI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xi Chen is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PETER L. BARTLETT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter L. Bartlett is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is one of the authors of the paper titled \"RL^2: Fast reinforcement learning via slow reinforcement learning\" published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"RL^2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RL^2 is a method for fast reinforcement learning via slow reinforcement learning, as described in a paper authored by Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel, published in the International Conference on Learning Representations in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"ADRIEN ECOFFET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrien Ecoffet is one of the authors of the paper titled \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" published in the Conference on Artificial Life in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the paper titled \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" published in the Conference on Artificial Life in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"OPEN QUESTIONS IN CREATING SAFE OPEN-ENDED AI\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper \"Open questions in creating safe open-ended AI: Tensions between control and creativity\" is authored by Adrien Ecoffet, Jeff Clune, and Joel Lehman, and was published in the Conference on Artificial Life in 2020<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"THOMAS ELSKEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Elsken is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JAN HENDRIK METZEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jan Hendrik Metzen is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"FRANK HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Frank Hutter is one of the authors of the paper titled \"Neural architecture search: A survey\" published in the Journal of Machine Learning Research in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"NEURAL ARCHITECTURE SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Neural architecture search is a method described in a paper authored by Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter, published in the Journal of Machine Learning Research in 2019<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MAXENCE FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maxence Faldor is one of the authors of the paper titled \"Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code\" published on arXiv in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is one of the authors of the paper titled \"Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code\" published on arXiv in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"OMNI-EPIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Omni-epic is a method for open-endedness via models of human notions of interestingness with environments programmed in code, as described in a paper authored by Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune, published on arXiv in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHRISANTHA FERNANDO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chrisantha Fernando is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"DYLAN SUNIL BANARSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dylan Sunil Banarse is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"HENRYK MICHALEWSKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Henryk Michalewski is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SIMON OSINDERO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simon Osindero is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"TIM ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tim Rockt&#228;schel is one of the authors of the paper titled \"Promptbreeder: Self-referential self-improvement via prompt evolution\" published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PROMPTBREEDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Promptbreeder is a method for self-referential self-improvement via prompt evolution, as described in a paper authored by Chrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt&#228;schel, published in 2024<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is one of the authors of the paper titled \"Model-agnostic meta-learning for fast adaptation of deep networks\" published in the International Conference on Machine Learning in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is one of the authors of the paper titled \"Model-agnostic meta-learning for fast adaptation of deep networks\" published in the International Conference on Machine Learning in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"MODEL-AGNOSTIC META-LEARNING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Model-agnostic meta-learning is a method for fast adaptation of deep networks, as described in a paper authored by Chelsea Finn, Pieter Abbeel, and Sergey Levine, published in the International Conference on Machine Learning in 2017<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"AMAN MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"SHUYAN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuyan Zhou is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is one of the authors of the paper titled \"PAL: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"PAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">PAL, or Program-aided language models, is a method described in a paper authored by Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig, published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"RYAN GREENBLATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan Greenblatt is the author of the article titled \"Getting 50% SOTA on ARC-AGI with GPT-4\" published on Redwood Research Substack in July 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <node id=\"GETTING 50% SOTA ON ARC-AGI WITH GPT-4\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The article \"Getting 50% SOTA on ARC-AGI with GPT-4\" is authored by Ryan Greenblatt and was published on Redwood Research Substack in July 2023<\/data>      <data key=\"d2\">022e7927d281e80e188f29ea343cc115<\/data>    <\/node>    <edge source=\"CHEN\" target=\"YUSHENG SU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Yusheng Su co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"JINGWEI ZUO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Jingwei Zuo co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"CHENG YANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Cheng Yang co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"CHENFEI YUAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Chenfei Yuan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"CHI-MIN CHAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Chi-Min Chan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"HEYANG YU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Heyang Yu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"YAXI LU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Yaxi Lu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"YI-HSIN HUNG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Yi-Hsin Hung co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"CHEN\" target=\"CHEN QIAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chen and Chen Qian co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"JINGWEI ZUO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Jingwei Zuo co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"CHENG YANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Cheng Yang co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"CHENFEI YUAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Chenfei Yuan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"CHI-MIN CHAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Chi-Min Chan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"HEYANG YU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Heyang Yu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"YAXI LU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Yaxi Lu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"YI-HSIN HUNG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Yi-Hsin Hung co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"YUSHENG SU\" target=\"CHEN QIAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yusheng Su and Chen Qian co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"CHENG YANG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Cheng Yang co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"CHENFEI YUAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Chenfei Yuan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"CHI-MIN CHAN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Chi-Min Chan co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"HEYANG YU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Heyang Yu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"YAXI LU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Yaxi Lu co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>    <edge source=\"JINGWEI ZUO\" target=\"YI-HSIN HUNG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jingwei Zuo and Yi-Hsin Hung co-authored the paper \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors\" published in The Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d5\">022e7927d281e80e188f29ea343cc115<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6109537356a2ce2339f77c827aa3668e","chunk":", Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning , pp. 10764\u201310799. PMLR, 2023.\nRyan Greenblatt. Getting 50% sota on arc-agi with gpt-4. https:\/\/redwoodresearch.substack.\ncom\/p\/getting-50-sota-on-arc-agi-with-gpt , July 2024. Technical Report.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2021.\n16Automated Design of Agentic Systems\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\nShengran Hu and Jeff Clune. Thought Cloning: Learning to think while acting by imitating human\nthinking. Advances in Neural Information Processing Systems , 36, 2024.\nShengran Hu, Ran Cheng, Cheng He, Zhichao Lu, Jing Wang, and Miao Zhang. Accelerating multi-\nobjective neural architecture search by random-weight evaluation. Complex & Intelligent Systems ,\npp. 1\u201310, 2021.\nShihua Huang, Zhichao Lu, Kalyanmoy Deb, and Vishnu Naresh Boddeti. Revisiting residual networks\nfor adversarial robustness. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 8202\u20138211, 2023.\nFrank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems,\nchallenges . Springer Nature, 2019.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful\nHaq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, et al. Dspy: Compiling\ndeclarative language model calls into state-of-the-art pipelines. In The Twelfth International\nConference on Learning Representations , 2024.\nAlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolutional\nneural networks. Advances in neural information processing systems , 25, 2012.\nAbrahim Ladha. Lecture 11: Turing-completeness. https:\/\/faculty.cc.gatech.edu\/~ladha\/\nS24\/4510\/L11.pdf , 2024. CS 4510 Automata and Complexity, February 21st, 2024, Scribed by\nRishabh Singhal.\nLangChainAI. Langchain: Build context-aware reasoning applications. https:\/\/github.com\/\nlangchain-ai\/langchain , 2022.\nJoel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty\nalone.Evolutionary computation , 19(2):189\u2013223, 2011.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation\nforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems ,33:9459\u20139474,\n2020.\nFei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu\nZhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language\nmodel. In Forty-first International Conference on Machine Learning , 2024.\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent\ncollaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170 , 2023.\nChris Lu, Sebastian Towers, and Jakob Foerster. Arbitrary order meta-learning with simple population-\nbased evolution. In ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life\nConference . MIT Press, 2023.\nChris Lu, Samuel Holt, Claudio Fanconi, Alex J Chan, Jakob Foerster, Mihaela van der Schaar, and\nRobert Tjarko Lange. Discovering preference optimization algorithms with and for large language\nmodels. arXiv preprint arXiv:2406.08414 , 2024a.\n17Automated Design of Agentic Systems\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist:\nTowards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024b.\nCong Lu, Shengran Hu, and Jeff Clune. Intelligent go-explore: Standing on the shoulders of giant\nfoundation models. arXiv preprint arXiv:2405.15143 , 2024c.\nZhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhe","chunk_id":"6109537356a2ce2339f77c827aa3668e","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"JAMIE CALLAN","type":"PERSON","description":"Jamie Callan is one of the authors of the paper titled \"Pal: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"GRAHAM NEUBIG","type":"PERSON","description":"Graham Neubig is one of the authors of the paper titled \"Pal: Program-aided language models\" published in the International Conference on Machine Learning in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"INTERNATIONAL CONFERENCE ON MACHINE LEARNING","type":"EVENT","description":"The International Conference on Machine Learning is an event where the paper \"Pal: Program-aided language models\" was published in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"EVENT"},{"name":"RYAN GREENBLATT","type":"PERSON","description":"Ryan Greenblatt is the author of the technical report titled \"Getting 50% sota on arc-agi with gpt-4\" published on Redwood Research Substack in July 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"REDWOOD RESEARCH SUBSTACK","type":"PLATFORM","description":"Redwood Research Substack is the platform where Ryan Greenblatt published the technical report \"Getting 50% sota on arc-agi with gpt-4\" in July 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PLATFORM"},{"name":"DAN HENDRYCKS","type":"PERSON","description":"Dan Hendrycks is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"COLLIN BURNS","type":"PERSON","description":"Collin Burns is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"STEVEN BASART","type":"PERSON","description":"Steven Basart is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ANDY ZOU","type":"PERSON","description":"Andy Zou is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"MANTAS MAZEIKA","type":"PERSON","description":"Mantas Mazeika is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JACOB STEINHARDT","type":"PERSON","description":"Jacob Steinhardt is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The International Conference on Learning Representations is an event where the paper \"Measuring massive multitask language understanding\" was published in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"EVENT"},{"name":"SIRUI HONG","type":"PERSON","description":"Sirui Hong is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"XIAWU ZHENG","type":"PERSON","description":"Xiawu Zheng is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JONATHAN CHEN","type":"PERSON","description":"Jonathan Chen is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"YUHENG CHENG","type":"PERSON","description":"Yuheng Cheng is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JINLIN WANG","type":"PERSON","description":"Jinlin Wang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"CEYAO ZHANG","type":"PERSON","description":"Ceyao Zhang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ZILI WANG","type":"PERSON","description":"Zili Wang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"STEVEN KA SHING YAU","type":"PERSON","description":"Steven Ka Shing Yau is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ZIJUAN LIN","type":"PERSON","description":"Zijuan Lin is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"LIYANG ZHOU","type":"PERSON","description":"Liyang Zhou is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"METAGPT","type":"TECHNOLOGY","description":"MetaGPT is a meta programming framework for multi-agent collaboration, as described in a paper published on arXiv in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"TECHNOLOGY"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is one of the authors of the paper titled \"Thought Cloning: Learning to think while acting by imitating human thinking\" published in Advances in Neural Information Processing Systems in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"Thought Cloning: Learning to think while acting by imitating human thinking\" published in Advances in Neural Information Processing Systems in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS","type":"EVENT","description":"Advances in Neural Information Processing Systems is an event where the paper \"Thought Cloning: Learning to think while acting by imitating human thinking\" was published in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"EVENT"},{"name":"RAN CHENG","type":"PERSON","description":"Ran Cheng is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex & Intelligent Systems in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"CHENG HE","type":"PERSON","description":"Cheng He is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex & Intelligent Systems in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ZHICHAO LU","type":"PERSON","description":"Zhichao Lu is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex & Intelligent Systems in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JING WANG","type":"PERSON","description":"Jing Wang is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex & Intelligent Systems in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"MIAO ZHANG","type":"PERSON","description":"Miao Zhang is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex & Intelligent Systems in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"COMPLEX & INTELLIGENT SYSTEMS","type":"JOURNAL","description":"Complex & Intelligent Systems is the journal where the paper \"Accelerating multi-objective neural architecture search by random-weight evaluation\" was published in 2021","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"JOURNAL"},{"name":"SHIHUA HUANG","type":"PERSON","description":"Shihua Huang is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"VISHNU NARESH BODDETI","type":"PERSON","description":"Vishnu Naresh Boddeti is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"PROCEEDINGS OF THE IEEE\/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION","type":"EVENT","description":"The Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition is an event where the paper \"Revisiting residual networks for adversarial robustness\" was published in 2023","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"EVENT"},{"name":"FRANK HUTTER","type":"PERSON","description":"Frank Hutter is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"LARS KOTTHOFF","type":"PERSON","description":"Lars Kotthoff is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"JOAQUIN VANSCHOREN","type":"PERSON","description":"Joaquin Vanschoren is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"SPRINGER NATURE","type":"PUBLISHER","description":"Springer Nature is the publisher of the book titled \"Automated machine learning: methods, systems, challenges\" published in 2019","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PUBLISHER"},{"name":"OMAR KHATTAB","type":"PERSON","description":"Omar Khattab is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ARNAV SINGHVI","type":"PERSON","description":"Arnav Singhvi is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"PARIDHI MAHESHWARI","type":"PERSON","description":"Paridhi Maheshwari is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ZHIYUAN ZHANG","type":"PERSON","description":"Zhiyuan Zhang is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"KESHAV SANTHANAM","type":"PERSON","description":"Keshav Santhanam is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"SAIFUL HAQ","type":"PERSON","description":"Saiful Haq is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ASHUTOSH SHARMA","type":"PERSON","description":"Ashutosh Sharma is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"THOMAS T JOSHI","type":"PERSON","description":"Thomas T Joshi is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"HANNA MOAZAM","type":"PERSON","description":"Hanna Moazam is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"HEATHER MILLER","type":"PERSON","description":"Heather Miller is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The Twelfth International Conference on Learning Representations is an event where the paper \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" was published in 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"EVENT"},{"name":"ALEX KRIZHEVSKY","type":"PERSON","description":"Alex Krizhevsky is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ILYA SUTSKEVER","type":"PERSON","description":"Ilya Sutskever is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"GEOFFREY E HINTON","type":"PERSON","description":"Geoffrey E Hinton is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"IMAGENET CLASSIFICATION WITH DEEP CONVOLUTIONAL NEURAL NETWORKS","type":"DOCUMENT","description":"The paper titled \"Imagenet classification with deep convolutional neural networks\" was published in Advances in Neural Information Processing Systems in 2012","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"DOCUMENT"},{"name":"ABRAHIM LADHA","type":"PERSON","description":"Abrahim Ladha is the author of the lecture titled \"Lecture 11: Turing-completeness\" published on the Georgia Tech faculty website in February 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"GEORGIA TECH FACULTY WEBSITE","type":"PLATFORM","description":"The Georgia Tech faculty website is the platform where Abrahim Ladha published the lecture titled \"Lecture 11: Turing-completeness\" in February 2024","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PLATFORM"},{"name":"LANGCHAINAI","type":"ORGANIZATION","description":"LangChainAI is the organization that developed Langchain, a tool for building context-aware reasoning applications, published on GitHub in 2022","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"ORGANIZATION"},{"name":"LANGCHAIN","type":"TECHNOLOGY","description":"Langchain is a tool for building context-aware reasoning applications, developed by LangChainAI and published on GitHub in 2022","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"TECHNOLOGY"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" published in Evolutionary Computation in 2011","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"KENNETH O STANLEY","type":"PERSON","description":"Kenneth O Stanley is one of the authors of the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" published in Evolutionary Computation in 2011","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"EVOLUTIONARY COMPUTATION","type":"JOURNAL","description":"Evolutionary Computation is the journal where the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" was published in 2011","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"JOURNAL"},{"name":"PATRICK LEWIS","type":"PERSON","description":"Patrick Lewis is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ETHAN PEREZ","type":"PERSON","description":"Ethan Perez is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"ALEKSANDRA PIKTUS","type":"PERSON","description":"Aleksandra Piktus is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"FABIO PETRONI","type":"PERSON","description":"Fabio Petroni is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"VLADIMIR KARPUKHIN","type":"PERSON","description":"Vladimir Karpukhin is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"},{"name":"NAMAN GOYAL","type":"PERSON","description":"Naman Goyal is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in","source_id":"6109537356a2ce2339f77c827aa3668e","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"JAMIE CALLAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Callan is one of the authors of the paper titled \"Pal: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GRAHAM NEUBIG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Graham Neubig is one of the authors of the paper titled \"Pal: Program-aided language models\" published in the International Conference on Machine Learning in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INTERNATIONAL CONFERENCE ON MACHINE LEARNING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The International Conference on Machine Learning is an event where the paper \"Pal: Program-aided language models\" was published in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"RYAN GREENBLATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryan Greenblatt is the author of the technical report titled \"Getting 50% sota on arc-agi with gpt-4\" published on Redwood Research Substack in July 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"REDWOOD RESEARCH SUBSTACK\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">Redwood Research Substack is the platform where Ryan Greenblatt published the technical report \"Getting 50% sota on arc-agi with gpt-4\" in July 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"DAN HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Hendrycks is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COLLIN BURNS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Collin Burns is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STEVEN BASART\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Basart is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDY ZOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andy Zou is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MANTAS MAZEIKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mantas Mazeika is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JACOB STEINHARDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Steinhardt is one of the authors of the paper titled \"Measuring massive multitask language understanding\" published in the International Conference on Learning Representations in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The International Conference on Learning Representations is an event where the paper \"Measuring massive multitask language understanding\" was published in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"SIRUI HONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sirui Hong is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAWU ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiawu Zheng is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JONATHAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jonathan Chen is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUHENG CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuheng Cheng is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JINLIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jinlin Wang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CEYAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ceyao Zhang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZILI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zili Wang is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"STEVEN KA SHING YAU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Ka Shing Yau is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZIJUAN LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zijuan Lin is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LIYANG ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Liyang Zhou is one of the authors of the paper titled \"Metagpt: Meta programming for multi-agent collaborative framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"METAGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MetaGPT is a meta programming framework for multi-agent collaboration, as described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is one of the authors of the paper titled \"Thought Cloning: Learning to think while acting by imitating human thinking\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"Thought Cloning: Learning to think while acting by imitating human thinking\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Advances in Neural Information Processing Systems is an event where the paper \"Thought Cloning: Learning to think while acting by imitating human thinking\" was published in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"RAN CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ran Cheng is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex &amp; Intelligent Systems in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHENG HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cheng He is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex &amp; Intelligent Systems in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHICHAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhichao Lu is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex &amp; Intelligent Systems in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JING WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jing Wang is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex &amp; Intelligent Systems in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MIAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Miao Zhang is one of the authors of the paper titled \"Accelerating multi-objective neural architecture search by random-weight evaluation\" published in Complex &amp; Intelligent Systems in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COMPLEX &amp; INTELLIGENT SYSTEMS\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Complex &amp; Intelligent Systems is the journal where the paper \"Accelerating multi-objective neural architecture search by random-weight evaluation\" was published in 2021<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">JOURNAL<\/data>    <\/node>    <node id=\"SHIHUA HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shihua Huang is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VISHNU NARESH BODDETI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishnu Naresh Boddeti is one of the authors of the paper titled \"Revisiting residual networks for adversarial robustness\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PROCEEDINGS OF THE IEEE\/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition is an event where the paper \"Revisiting residual networks for adversarial robustness\" was published in 2023<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"FRANK HUTTER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Frank Hutter is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LARS KOTTHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lars Kotthoff is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOAQUIN VANSCHOREN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joaquin Vanschoren is one of the authors of the book titled \"Automated machine learning: methods, systems, challenges\" published by Springer Nature in 2019<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SPRINGER NATURE\">      <data key=\"d0\">PUBLISHER<\/data>      <data key=\"d1\">Springer Nature is the publisher of the book titled \"Automated machine learning: methods, systems, challenges\" published in 2019<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PUBLISHER<\/data>    <\/node>    <node id=\"OMAR KHATTAB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Khattab is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARNAV SINGHVI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arnav Singhvi is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PARIDHI MAHESHWARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Paridhi Maheshwari is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHIYUAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Zhang is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KESHAV SANTHANAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Keshav Santhanam is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAIFUL HAQ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saiful Haq is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ASHUTOSH SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashutosh Sharma is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THOMAS T JOSHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas T Joshi is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HANNA MOAZAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanna Moazam is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HEATHER MILLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heather Miller is one of the authors of the paper titled \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Twelfth International Conference on Learning Representations is an event where the paper \"Dspy: Compiling declarative language model calls into state-of-the-art pipelines\" was published in 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"ALEX KRIZHEVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex Krizhevsky is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ILYA SUTSKEVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilya Sutskever is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GEOFFREY E HINTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Geoffrey E Hinton is one of the authors of the paper titled \"Imagenet classification with deep convolutional neural networks\" published in Advances in Neural Information Processing Systems in 2012<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IMAGENET CLASSIFICATION WITH DEEP CONVOLUTIONAL NEURAL NETWORKS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Imagenet classification with deep convolutional neural networks\" was published in Advances in Neural Information Processing Systems in 2012<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"ABRAHIM LADHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abrahim Ladha is the author of the lecture titled \"Lecture 11: Turing-completeness\" published on the Georgia Tech faculty website in February 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GEORGIA TECH FACULTY WEBSITE\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">The Georgia Tech faculty website is the platform where Abrahim Ladha published the lecture titled \"Lecture 11: Turing-completeness\" in February 2024<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PLATFORM<\/data>    <\/node>    <node id=\"LANGCHAINAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">LangChainAI is the organization that developed Langchain, a tool for building context-aware reasoning applications, published on GitHub in 2022<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"LANGCHAIN\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Langchain is a tool for building context-aware reasoning applications, developed by LangChainAI and published on GitHub in 2022<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" published in Evolutionary Computation in 2011<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KENNETH O STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O Stanley is one of the authors of the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" published in Evolutionary Computation in 2011<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EVOLUTIONARY COMPUTATION\">      <data key=\"d0\">JOURNAL<\/data>      <data key=\"d1\">Evolutionary Computation is the journal where the paper titled \"Abandoning objectives: Evolution through the search for novelty alone\" was published in 2011<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">JOURNAL<\/data>    <\/node>    <node id=\"PATRICK LEWIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Patrick Lewis is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ETHAN PEREZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ethan Perez is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEKSANDRA PIKTUS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aleksandra Piktus is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FABIO PETRONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fabio Petroni is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VLADIMIR KARPUKHIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vladimir Karpukhin is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in Advances in Neural Information Processing Systems in 2020<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NAMAN GOYAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Naman Goyal is one of the authors of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" published in<\/data>      <data key=\"d2\">6109537356a2ce2339f77c827aa3668e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"1b1399c76420a477c0c97893d258ae69","chunk":" The AI Scientist:\nTowards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024b.\nCong Lu, Shengran Hu, and Jeff Clune. Intelligent go-explore: Standing on the shoulders of giant\nfoundation models. arXiv preprint arXiv:2405.15143 , 2024c.\nZhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, and\nWolfgang Banzhaf. Nsga-net: neural architecture search using multi-objective genetic algorithm.\nInProceedings of the genetic and evolutionary computation conference , pp. 419\u2013427, 2019.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman,\nYuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding\nlarge language models. In The Twelfth International Conference on Learning Representations , 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Processing Systems , 36, 2024.\nMeta. Open source ai is the path forward. https:\/\/about.fb.com\/news\/2024\/07\/\nopen-source-ai-is-the-path-forward\/ , July 2024. News article.\nElliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and\nJoel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint\narXiv:2302.12170 , 2023.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 975\u2013984, 2020.\nJean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint\narXiv:1504.04909 , 2015.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.\nAndrew Ng. Issue 253. https:\/\/www.deeplearning.ai\/the-batch\/issue-253\/ , June 2024.\nNewsletter issue.\nBen Norman and Jeff Clune. First-explore, then exploit: Meta-learning intelligent exploration. arXiv\npreprint arXiv:2307.02276 , 2023.\nOpenAI. Introducing chatgpt. https:\/\/openai.com\/index\/chatgpt\/ , November 2022. Blog\npost.\nOpenAI. Simple evals, 2023. URL https:\/\/github.com\/openai\/simple-evals . Accessed:\n2024-08-10.\nOpenAI. Gpt-4 technical report, 2024.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and technology , pp. 1\u201322, 2023.\n18Automated Design of Agentic Systems\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies , pp. 2080\u20132094, Online,\nJune 2021. Association for Computational Linguistics. doi: 10.18653\/v1\/2021.naacl-main.168.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924 , 2023.\nChen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang,\nZhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration.\narXiv preprint arXiv:2406.07155 , 2024.\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. Tool learning with large language models: A survey. arXiv preprint arXiv:2405.17935 , 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems , ","chunk_id":"1b1399c76420a477c0c97893d258ae69","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"THE AI SCIENTIST","type":"DOCUMENT","description":"The AI Scientist is a preprint titled \"Towards fully automated open-ended scientific discovery\" published on arXiv in 2024","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"First-explore, then exploit: Meta-learning intelligent exploration\" published on arXiv in 2023\nJeff Clune is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024\nJeff Clune is one of the authors of the paper titled \"Illuminating search spaces by mapping elites\" published on arXiv in 2015","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"INTELLIGENT GO-EXPLORE","type":"TECHNOLOGY","description":"Intelligent Go-Explore is a system described in the paper \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"ZHICHAO LU","type":"PERSON","description":"Zhichao Lu is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"IAN WHALEN","type":"PERSON","description":"Ian Whalen is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"VISHNU BODDETI","type":"PERSON","description":"Vishnu Boddeti is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"YASHESH DHEBAR","type":"PERSON","description":"Yashesh Dhebar is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"KALYANMOY DEB","type":"PERSON","description":"Kalyanmoy Deb is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ERIK GOODMAN","type":"PERSON","description":"Erik Goodman is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"WOLFGANG BANZHAF","type":"PERSON","description":"Wolfgang Banzhaf is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"NSGA-NET","type":"TECHNOLOGY","description":"Nsga-net is a neural architecture search method using a multi-objective genetic algorithm described in a paper published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"YECHENG JASON MA","type":"PERSON","description":"Yecheng Jason Ma is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"WILLIAM LIANG","type":"PERSON","description":"William Liang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"DE-AN HUANG","type":"PERSON","description":"De-An Huang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"OSBERT BASTANI","type":"PERSON","description":"Osbert Bastani is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"DINESH JAYARAMAN","type":"PERSON","description":"Dinesh Jayaraman is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"EUREKA","type":"TECHNOLOGY","description":"Eureka is a system for human-level reward design via coding large language models described in a paper published in the Twelfth International Conference on Learning Representations in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"AMAN MADAAN","type":"PERSON","description":"Aman Madaan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"NIKET TANDON","type":"PERSON","description":"Niket Tandon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"PRAKHAR GUPTA","type":"PERSON","description":"Prakhar Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SKYLER HALLINAN","type":"PERSON","description":"Skyler Hallinan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"LUYU GAO","type":"PERSON","description":"Luyu Gao is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SARAH WIEGREFFE","type":"PERSON","description":"Sarah Wiegreffe is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"URI ALON","type":"PERSON","description":"Uri Alon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"NOUHA DZIRI","type":"PERSON","description":"Nouha Dziri is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SHRIMAI PRABHUMOYE","type":"PERSON","description":"Shrimai Prabhumoye is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"YIMING YANG","type":"PERSON","description":"Yiming Yang is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-refine is a system for iterative refinement with self-feedback described in a paper published in Advances in Neural Information Processing Systems in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"META","type":"ORGANIZATION","description":"Meta is the organization that published the news article titled \"Open source AI is the path forward\" in July 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"ORGANIZATION"},{"name":"OPEN SOURCE AI IS THE PATH FORWARD","type":"DOCUMENT","description":"Open source AI is the path forward is a news article published by Meta in July 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"ELLIOT MEYERSON","type":"PERSON","description":"Elliot Meyerson is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"MARK J NELSON","type":"PERSON","description":"Mark J Nelson is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"HERBIE BRADLEY","type":"PERSON","description":"Herbie Bradley is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ADAM GAIER","type":"PERSON","description":"Adam Gaier is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ARASH MORADI","type":"PERSON","description":"Arash Moradi is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"AMY K HOOVER","type":"PERSON","description":"Amy K Hoover is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"LANGUAGE MODEL CROSSOVER","type":"TECHNOLOGY","description":"Language model crossover is a method for variation through few-shot prompting described in a paper published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"SHEN-YUN MIAO","type":"PERSON","description":"Shen-yun Miao is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"CHAO-CHUN LIANG","type":"PERSON","description":"Chao-Chun Liang is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"KEH-YIH SU","type":"PERSON","description":"Keh-Yih Su is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"A DIVERSE CORPUS FOR EVALUATING AND DEVELOPING ENGLISH MATH WORD PROBLEM SOLVERS","type":"DOCUMENT","description":"A diverse corpus for evaluating and developing English math word problem solvers is a paper published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"JEAN-BAPTISTE MOURET","type":"PERSON","description":"Jean-Baptiste Mouret is one of the authors of the paper titled \"Illuminating search spaces by mapping elites\" published on arXiv in 2015","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ILLUMINATING SEARCH SPACES BY MAPPING ELITES","type":"DOCUMENT","description":"Illuminating search spaces by mapping elites is a paper published on arXiv in 2015","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SUCHIR BALAJI","type":"PERSON","description":"Suchir Balaji is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"JEFF WU","type":"PERSON","description":"Jeff Wu is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"LONG OUYANG","type":"PERSON","description":"Long Ouyang is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"CHRISTINA KIM","type":"PERSON","description":"Christina Kim is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"CHRISTOPHER HESSE","type":"PERSON","description":"Christopher Hesse is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"SHANTANU JAIN","type":"PERSON","description":"Shantanu Jain is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"WILLIAM SAUNDERS","type":"PERSON","description":"William Saunders is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"WEBGPT","type":"TECHNOLOGY","description":"WebGPT is a system for browser-assisted question-answering with human feedback described in a paper published on arXiv in 2021","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"ANDREW NG","type":"PERSON","description":"Andrew Ng is the author of the newsletter issue titled \"Issue 253\" published on the DeepLearning.AI website in June 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"ISSUE 253","type":"DOCUMENT","description":"Issue 253 is a newsletter issue authored by Andrew Ng and published on the DeepLearning.AI website in June 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"BEN NORMAN","type":"PERSON","description":"Ben Norman is one of the authors of the paper titled \"First-explore, then exploit: Meta-learning intelligent exploration\" published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"FIRST-EXPLORE, THEN EXPLOIT","type":"TECHNOLOGY","description":"First-explore, then exploit is a method for meta-learning intelligent exploration described in a paper published on arXiv in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that published the blog post titled \"Introducing ChatGPT\" in November 2022","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"ORGANIZATION"},{"name":"INTRODUCING CHATGPT","type":"DOCUMENT","description":"Introducing ChatGPT is a blog post published by OpenAI in November 2022","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"SIMPLE EVALS","type":"TECHNOLOGY","description":"Simple evals is a system described by OpenAI and accessible via GitHub as of August 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"TECHNOLOGY"},{"name":"GPT-4 TECHNICAL REPORT","type":"DOCUMENT","description":"The GPT-4 technical report is a document published by OpenAI in 2024","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"DOCUMENT"},{"name":"JOON SUNG PARK","type":"PERSON","description":"Joon Sung Park is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023","source_id":"1b1399c76420a477c0c97893d258ae69","entity_type":"PERSON"},{"name":"JOSEPH O(\"ENTITY\"","type":"SHRAN HU","description":"PERSON","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"JOSEPH O'BRIEN","type":"PERSON","description":"Joseph O'Brien is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"CARRIE JUN CAI","type":"PERSON","description":"Carrie Jun Cai is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"MEREDITH RINGEL MORRIS","type":"PERSON","description":"Meredith Ringel Morris is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023","source_id":"1b1399c76420a477c0c97893d258ae69"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023","source_id":"1b1399c76420a477c0c97893d258ae69"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"THE AI SCIENTIST\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The AI Scientist is a preprint titled \"Towards fully automated open-ended scientific discovery\" published on arXiv in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"First-explore, then exploit: Meta-learning intelligent exploration\" published on arXiv in 2023Jeff Clune is one of the authors of the paper titled \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024Jeff Clune is one of the authors of the paper titled \"Illuminating search spaces by mapping elites\" published on arXiv in 2015<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Intelligent Go-Explore is a system described in the paper \"Intelligent go-explore: Standing on the shoulders of giant foundation models\" published on arXiv in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"ZHICHAO LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhichao Lu is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"IAN WHALEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ian Whalen is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VISHNU BODDETI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishnu Boddeti is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YASHESH DHEBAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yashesh Dhebar is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KALYANMOY DEB\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kalyanmoy Deb is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERIK GOODMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erik Goodman is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WOLFGANG BANZHAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wolfgang Banzhaf is one of the authors of the paper titled \"Nsga-net: neural architecture search using multi-objective genetic algorithm\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NSGA-NET\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Nsga-net is a neural architecture search method using a multi-objective genetic algorithm described in a paper published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YECHENG JASON MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yecheng Jason Ma is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WILLIAM LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William Liang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DE-AN HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">De-An Huang is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"OSBERT BASTANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Osbert Bastani is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DINESH JAYARAMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dinesh Jayaraman is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is one of the authors of the paper titled \"Eureka: Human-level reward design via coding large language models\" published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EUREKA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Eureka is a system for human-level reward design via coding large language models described in a paper published in the Twelfth International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AMAN MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aman Madaan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NIKET TANDON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Niket Tandon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PRAKHAR GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Prakhar Gupta is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SKYLER HALLINAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Skyler Hallinan is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUYU GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luyu Gao is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SARAH WIEGREFFE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sarah Wiegreffe is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"URI ALON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Uri Alon is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NOUHA DZIRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nouha Dziri is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHRIMAI PRABHUMOYE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shrimai Prabhumoye is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIMING YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiming Yang is one of the authors of the paper titled \"Self-refine: Iterative refinement with self-feedback\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-refine is a system for iterative refinement with self-feedback described in a paper published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"META\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Meta is the organization that published the news article titled \"Open source AI is the path forward\" in July 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"OPEN SOURCE AI IS THE PATH FORWARD\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Open source AI is the path forward is a news article published by Meta in July 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"ELLIOT MEYERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Elliot Meyerson is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MARK J NELSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark J Nelson is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HERBIE BRADLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Herbie Bradley is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADAM GAIER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adam Gaier is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARASH MORADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arash Moradi is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AMY K HOOVER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amy K Hoover is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the paper titled \"Language model crossover: Variation through few-shot prompting\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LANGUAGE MODEL CROSSOVER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Language model crossover is a method for variation through few-shot prompting described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SHEN-YUN MIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shen-yun Miao is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHAO-CHUN LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chao-Chun Liang is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KEH-YIH SU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Keh-Yih Su is one of the authors of the paper titled \"A diverse corpus for evaluating and developing English math word problem solvers\" published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"A DIVERSE CORPUS FOR EVALUATING AND DEVELOPING ENGLISH MATH WORD PROBLEM SOLVERS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A diverse corpus for evaluating and developing English math word problem solvers is a paper published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics in 2020<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"JEAN-BAPTISTE MOURET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jean-Baptiste Mouret is one of the authors of the paper titled \"Illuminating search spaces by mapping elites\" published on arXiv in 2015<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ILLUMINATING SEARCH SPACES BY MAPPING ELITES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Illuminating search spaces by mapping elites is a paper published on arXiv in 2015<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SUCHIR BALAJI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suchir Balaji is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JEFF WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Wu is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LONG OUYANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Long Ouyang is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHRISTINA KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christina Kim is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHRISTOPHER HESSE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher Hesse is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHANTANU JAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shantanu Jain is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WILLIAM SAUNDERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William Saunders is one of the authors of the paper titled \"WebGPT: Browser-assisted question-answering with human feedback\" published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WEBGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">WebGPT is a system for browser-assisted question-answering with human feedback described in a paper published on arXiv in 2021<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ANDREW NG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Ng is the author of the newsletter issue titled \"Issue 253\" published on the DeepLearning.AI website in June 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ISSUE 253\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Issue 253 is a newsletter issue authored by Andrew Ng and published on the DeepLearning.AI website in June 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"BEN NORMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ben Norman is one of the authors of the paper titled \"First-explore, then exploit: Meta-learning intelligent exploration\" published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FIRST-EXPLORE, THEN EXPLOIT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">First-explore, then exploit is a method for meta-learning intelligent exploration described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that published the blog post titled \"Introducing ChatGPT\" in November 2022<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"INTRODUCING CHATGPT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Introducing ChatGPT is a blog post published by OpenAI in November 2022<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"SIMPLE EVALS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Simple evals is a system described by OpenAI and accessible via GitHub as of August 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The GPT-4 technical report is a document published by OpenAI in 2024<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"JOON SUNG PARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joon Sung Park is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOSEPH O(&quot;ENTITY&quot;\">      <data key=\"d0\">SHRAN HU<\/data>      <data key=\"d1\">PERSON<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"JOSEPH O'BRIEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joseph O'Brien is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"CARRIE JUN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carrie Jun Cai is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"MEREDITH RINGEL MORRIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meredith Ringel Morris is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is one of the authors of the paper titled \"Generative agents: Interactive simulacra of human behavior\" published in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology in 2023<\/data>      <data key=\"d2\">1b1399c76420a477c0c97893d258ae69<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"34d0bb2211fc795fe1096442e086a2b3","chunk":"qiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. Tool learning with large language models: A survey. arXiv preprint arXiv:2405.17935 , 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems , 36, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark,\n2023.\nToran Bruce Richards. Autogpt. https:\/\/github.com\/Significant-Gravitas\/AutoGPT ,\n2023. GitHub repository.\nTim Rockt\u00e4schel. Artificial Intelligence: 10 Things You Should Know . Seven Dials, September 2024.\nISBN 978-1399626521.\nMd Omar Faruk Rokon, Risul Islam, Ahmad Darki, Evangelos E Papalexakis, and Michalis Faloutsos.\n{SourceFinder}: Finding malware {Source-Code}from publicly available repositories in {GitHub}.\nIn23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020) , pp.\n149\u2013163, 2020.\nBernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan\nKumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al.\nMathematical discoveries from program search with large language models. Nature, 625(7995):\n468\u2013475, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems ,\n2023. URL https:\/\/openreview.net\/forum?id=Yacmpz84TH .\nSander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si,\nYinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, et al. The prompt report: A systematic\nsurvey of prompting techniques. arXiv preprint arXiv:2406.06608 , 2024.\nXuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, and Yanzhi Wang. Deepmad:\nMathematical architecture design for deep convolutional neural network. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition , pp. 6163\u20136173, 2023.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\n19Automated Design of Agentic Systems\nare multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning\nRepresentations , 2023.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36, 2023.\nKenneth O Stanley and Joel Lehman. Why greatness cannot be planned: The myth of the objective .\nSpringer, 2015.\nKenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks\nthrough neuroevolution. Nature Machine Intelligence , 1(1):24\u201335, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.\nSai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics:\nDesign principles and model abilities. Technical Report MSR-TR-2023-8, Microsoft,\nFebruary 2023. URL https:\/\/www.microsoft.com\/en-us\/research\/publication\/\nchatgpt-for-robotics-design-principles-and-model-abilities\/ .\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291 , 2023a.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763 , 2016.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\nFrontiers of Computer Science ,","chunk_id":"34d0bb2211fc795fe1096442e086a2b3","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"QIANG WANG","type":"PERSON","description":"Qiang Wang is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DAWEI YIN","type":"PERSON","description":"Dawei Yin is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JUN XU","type":"PERSON","description":"Jun Xu is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JI-RONG WEN","type":"PERSON","description":"Ji-Rong Wen is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RAFAEL RAFAILOV","type":"PERSON","description":"Rafael Rafailov is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ARCHIT SHARMA","type":"PERSON","description":"Archit Sharma is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ERIC MITCHELL","type":"PERSON","description":"Eric Mitchell is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHRISTOPHER D MANNING","type":"PERSON","description":"Christopher D Manning is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"STEFANO ERMON","type":"PERSON","description":"Stefano Ermon is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHELSEA FINN","type":"PERSON","description":"Chelsea Finn is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DAVID REIN","type":"PERSON","description":"David Rein is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"BETTY LI HOU","type":"PERSON","description":"Betty Li Hou is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASA COOPER STICKLAND","type":"PERSON","description":"Asa Cooper Stickland is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JACKSON PETTY","type":"PERSON","description":"Jackson Petty is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RICHARD YUANZHE PANG","type":"PERSON","description":"Richard Yuanzhe Pang is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JULIEN DIRANI","type":"PERSON","description":"Julien Dirani is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JULIAN MICHAEL","type":"PERSON","description":"Julian Michael is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SAMUEL R. BOWMAN","type":"PERSON","description":"Samuel R. Bowman is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&A benchmark\" published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TORAN BRUCE RICHARDS","type":"PERSON","description":"Toran Bruce Richards is the author of the project \"AutoGPT\" available on GitHub in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AUTOGPT","type":"TECHNOLOGY","description":"AutoGPT is a project available on GitHub, created by Toran Bruce Richards in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TIM ROCKT\u00c4SCHEL","type":"PERSON","description":"Tim Rockt\u00e4schel is the author of the book \"Artificial Intelligence: 10 Things You Should Know\" published by Seven Dials in September 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ARTIFICIAL INTELLIGENCE: 10 THINGS YOU SHOULD KNOW","type":"DOCUMENT","description":"Artificial Intelligence: 10 Things You Should Know is a book authored by Tim Rockt\u00e4schel and published by Seven Dials in September 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MD OMAR FARUK ROKON","type":"PERSON","description":"Md Omar Faruk Rokon is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RISUL ISLAM","type":"PERSON","description":"Risul Islam is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AHMAD DARKI","type":"PERSON","description":"Ahmad Darki is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"EVANGELOS E PAPALEXAKIS","type":"PERSON","description":"Evangelos E Papalexakis is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MICHALIS FALOUTSOS","type":"PERSON","description":"Michalis Faloutsos is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SOURCEFINDER","type":"TECHNOLOGY","description":"SourceFinder is a system designed to find malware source code from publicly available repositories in GitHub, as described in a paper published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"BERNARDINO ROMERA-PAREDES","type":"PERSON","description":"Bernardino Romera-Paredes is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MOHAMMADAMIN BAREKATAIN","type":"PERSON","description":"Mohammadamin Barekatain is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ALEXANDER NOVIKOV","type":"PERSON","description":"Alexander Novikov is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATEJ BALOG","type":"PERSON","description":"Matej Balog is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"M PAWAN KUMAR","type":"PERSON","description":"M Pawan Kumar is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"EMILIEN DUPONT","type":"PERSON","description":"Emilien Dupont is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FRANCISCO JR RUIZ","type":"PERSON","description":"Francisco JR Ruiz is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JORDAN S ELLENBERG","type":"PERSON","description":"Jordan S Ellenberg is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"PENGMING WANG","type":"PERSON","description":"Pengming Wang is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"OMAR FAWZI","type":"PERSON","description":"Omar Fawzi is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATHEMATICAL DISCOVERIES FROM PROGRAM SEARCH WITH LARGE LANGUAGE MODELS","type":"DOCUMENT","description":"The paper titled \"Mathematical discoveries from program search with large language models\" was published in Nature in 2024 and authored by Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, and Omar Fawzi","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TIMO SCHICK","type":"PERSON","description":"Timo Schick is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JANE DWIVEDI-YU","type":"PERSON","description":"Jane Dwivedi-Yu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROBERTO DESSI","type":"PERSON","description":"Roberto Dessi is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROBERTA RAILEANU","type":"PERSON","description":"Roberta Raileanu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MARIA LOMELI","type":"PERSON","description":"Maria Lomeli is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ERIC HAMBRO","type":"PERSON","description":"Eric Hambro is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LUKE ZETTLEMOYER","type":"PERSON","description":"Luke Zettlemoyer is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NICOLA CANCEDDA","type":"PERSON","description":"Nicola Cancedda is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"THOMAS SCIALOM","type":"PERSON","description":"Thomas Scialom is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"TOOLFORMER","type":"TECHNOLOGY","description":"Toolformer is a system described in a paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SANDER SCHULHOFF","type":"PERSON","description":"Sander Schulhoff is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MICHAEL ILIE","type":"PERSON","description":"Michael Ilie is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NISHANT BALEPUR","type":"PERSON","description":"Nishant Balepur is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KONSTANTINE KAHADZE","type":"PERSON","description":"Konstantine Kahadze is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AMANDA LIU","type":"PERSON","description":"Amanda Liu is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHENGLEI SI","type":"PERSON","description":"Chenglei Si is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YINHENG LI","type":"PERSON","description":"Yinheng Li is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AAYUSH GUPTA","type":"PERSON","description":"Aayush Gupta is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HYOJUNG HAN","type":"PERSON","description":"HyoJung Han is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SEVIEN SCHULHOFF","type":"PERSON","description":"Sevien Schulhoff is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"THE PROMPT REPORT","type":"DOCUMENT","description":"The paper titled \"The prompt report: A systematic survey of prompting techniques\" was published on arXiv in 2024 and authored by Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, and Sevien Schulhoff","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XUAN SHEN","type":"PERSON","description":"Xuan Shen is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YAOHUA WANG","type":"PERSON","description":"Yaohua Wang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MING LIN","type":"PERSON","description":"Ming Lin is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YILUN HUANG","type":"PERSON","description":"Yilun Huang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HAO TANG","type":"PERSON","description":"Hao Tang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XIUYU SUN","type":"PERSON","description":"Xiuyu Sun is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YANZHI WANG","type":"PERSON","description":"Yanzhi Wang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DEEPMAD","type":"TECHNOLOGY","description":"Deepmad is a system for mathematical architecture design for deep convolutional neural networks, as described in a paper published in the Proceedings of the IEEE\/CVF","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RAID 2020","type":"EVENT","description":"The 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020) is an event where the paper \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" was published","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NEURAL INFORMATION PROCESSING SYSTEMS","type":"EVENT","description":"Advances in Neural Information Processing Systems is a conference where the paper \"Direct preference optimization: Your language model is secretly a reward model\" was published in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NATURE","type":"DOCUMENT","description":"Nature is a journal where the paper \"Mathematical discoveries from program search with large language models\" was published in 2024","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"IEEE\/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION","type":"EVENT","description":"The IEEE\/CVF Conference on Computer Vision and Pattern Recognition is an event where the paper \"Deepmad: Mathematical architecture design for deep convolutional neural network\" was published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The Eleventh International Conference on Learning Representations is an event where the paper \"Language models are multilingual chain-of-thought reasoners\" was published in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FREDA SHI","type":"PERSON","description":"Freda Shi is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MIRAC SUZGUN","type":"PERSON","description":"Mirac Suzgun is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MARKUS FREITAG","type":"PERSON","description":"Markus Freitag is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SURAJ SRIVATS","type":"PERSON","description":"Suraj Srivats is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SOROUSH VOSOUGHI","type":"PERSON","description":"Soroush Vosoughi is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SEBASTIAN RUDER","type":"PERSON","description":"Sebastian Ruder is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DIPANJAN DAS","type":"PERSON","description":"Dipanjan Das is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NOAH SHINN","type":"PERSON","description":"Noah Shinn is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FEDERICO CASSANO","type":"PERSON","description":"Federico Cassano is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASHWIN GOPINATH","type":"PERSON","description":"Ashwin Gopinath is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KARTHIK NARASIMHAN","type":"PERSON","description":"Karthik Narasimhan is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SHUNYU YAO","type":"PERSON","description":"Shunyu Yao is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"KENNETH O STANLEY","type":"PERSON","description":"Kenneth O Stanley is one of the authors of the book titled \"Why greatness cannot be planned: The myth of the objective\" published by Springer in 2015","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the book titled \"Why greatness cannot be planned: The myth of the objective\" published by Springer in 2015","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SPRINGER","type":"ORGANIZATION","description":"Springer is the publisher of the book titled \"Why greatness cannot be planned: The myth of the objective\" authored by Kenneth O Stanley and Joel Lehman in 2015","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"Designing neural networks through neuroevolution\" published in Nature Machine Intelligence in 2019","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RISTO MIIKKULAINEN","type":"PERSON","description":"Risto Miikkulainen is one of the authors of the paper titled \"Designing neural networks through neuroevolution\" published in Nature Machine Intelligence in 2019","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"NATURE MACHINE INTELLIGENCE","type":"DOCUMENT","description":"Nature Machine Intelligence is a journal where the paper titled \"Designing neural networks through neuroevolution\" was published in 2019","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"RICHARD S SUTTON","type":"PERSON","description":"Richard S Sutton is one of the authors of the book titled \"Reinforcement learning: An introduction\" published by MIT Press in 2018","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ANDREW G BARTO","type":"PERSON","description":"Andrew G Barto is one of the authors of the book titled \"Reinforcement learning: An introduction\" published by MIT Press in 2018","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MIT PRESS","type":"ORGANIZATION","description":"MIT Press is the publisher of the book titled \"Reinforcement learning: An introduction\" authored by Richard S Sutton and Andrew G Barto in 2018","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"SAI VEMPRALA","type":"PERSON","description":"Sai Vemprala is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ROGERIO BONATTI","type":"PERSON","description":"Rogerio Bonatti is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ARTHUR BUCKER","type":"PERSON","description":"Arthur Bucker is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ASHISH KAPOOR","type":"PERSON","description":"Ashish Kapoor is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is the organization that published the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" in February 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"GUANZHI WANG","type":"PERSON","description":"Guanzhi Wang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUQI XIE","type":"PERSON","description":"Yuqi Xie is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUNFAN JIANG","type":"PERSON","description":"Yunfan Jiang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"AJAY MANDLEKAR","type":"PERSON","description":"Ajay Mandlekar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHAOWEI XIAO","type":"PERSON","description":"Chaowei Xiao is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YUKE ZHU","type":"PERSON","description":"Yuke Zhu is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LINXI FAN","type":"PERSON","description":"Linxi Fan is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ANIMA ANANDKUMAR","type":"PERSON","description":"Anima Anandkumar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"VOYAGER","type":"TECHNOLOGY","description":"Voyager is an open-ended embodied agent with large language models, as described in a paper published on arXiv in 2023","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JANE X WANG","type":"PERSON","description":"Jane X Wang is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZEB KURTH-NELSON","type":"PERSON","description":"Zeb Kurth-Nelson is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DHRUVA TIRUMALA","type":"PERSON","description":"Dhruva Tirumala is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HUBERT SOYER","type":"PERSON","description":"Hubert Soyer is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JOEL Z LEIBO","type":"PERSON","description":"Joel Z Leibo is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"REMI MUNOS","type":"PERSON","description":"Remi Munos is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHARLES BLUNDELL","type":"PERSON","description":"Charles Blundell is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"DHARSHAN KUMARAN","type":"PERSON","description":"Dharshan Kumaran is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"MATT BOTVINICK","type":"PERSON","description":"Matt Botvinick is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"LEI WANG","type":"PERSON","description":"Lei Wang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XUEYANG FENG","type":"PERSON","description":"Xueyang Feng is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZEYU ZHANG","type":"PERSON","description":"Zeyu Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"HAO YANG","type":"PERSON","description":"Hao Yang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JINGSEN ZHANG","type":"PERSON","description":"Jingsen Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"ZHIYUAN CHEN","type":"PERSON","description":"Zhiyuan Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"JIAKAI TANG","type":"PERSON","description":"Jiakai Tang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"XU CHEN","type":"PERSON","description":"Xu Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science","source_id":"34d0bb2211fc795fe1096442e086a2b3"},{"name":"FRONTIERS OF COMPUTER SCIENCE","type":"DOCUMENT","description":"Frontiers of Computer Science is a journal where the paper titled \"A survey on large language model based autonomous agents\" was published","source_id":"34d0bb2211fc795fe1096442e086a2b3"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"QIANG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiang Wang is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DAWEI YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawei Yin is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JUN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jun Xu is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JI-RONG WEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ji-Rong Wen is one of the authors of the paper titled \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RAFAEL RAFAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rafael Rafailov is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ARCHIT SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Archit Sharma is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ERIC MITCHELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Mitchell is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHRISTOPHER D MANNING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Christopher D Manning is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"STEFANO ERMON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stefano Ermon is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHELSEA FINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chelsea Finn is one of the authors of the paper titled \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DAVID REIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Rein is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"BETTY LI HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Betty Li Hou is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASA COOPER STICKLAND\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Asa Cooper Stickland is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JACKSON PETTY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jackson Petty is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RICHARD YUANZHE PANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard Yuanzhe Pang is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JULIEN DIRANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julien Dirani is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JULIAN MICHAEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Julian Michael is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SAMUEL R. BOWMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel R. Bowman is one of the authors of the paper titled \"Gpqa: A graduate-level google-proof Q&amp;A benchmark\" published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TORAN BRUCE RICHARDS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Toran Bruce Richards is the author of the project \"AutoGPT\" available on GitHub in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AUTOGPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AutoGPT is a project available on GitHub, created by Toran Bruce Richards in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TIM ROCKT&#196;SCHEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tim Rockt&#228;schel is the author of the book \"Artificial Intelligence: 10 Things You Should Know\" published by Seven Dials in September 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ARTIFICIAL INTELLIGENCE: 10 THINGS YOU SHOULD KNOW\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Artificial Intelligence: 10 Things You Should Know is a book authored by Tim Rockt&#228;schel and published by Seven Dials in September 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MD OMAR FARUK ROKON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Md Omar Faruk Rokon is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RISUL ISLAM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Risul Islam is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AHMAD DARKI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmad Darki is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"EVANGELOS E PAPALEXAKIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Evangelos E Papalexakis is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MICHALIS FALOUTSOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michalis Faloutsos is one of the authors of the paper titled \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SOURCEFINDER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">SourceFinder is a system designed to find malware source code from publicly available repositories in GitHub, as described in a paper published in the 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"BERNARDINO ROMERA-PAREDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernardino Romera-Paredes is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MOHAMMADAMIN BAREKATAIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammadamin Barekatain is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ALEXANDER NOVIKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander Novikov is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATEJ BALOG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matej Balog is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"M PAWAN KUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">M Pawan Kumar is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"EMILIEN DUPONT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emilien Dupont is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FRANCISCO JR RUIZ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Francisco JR Ruiz is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JORDAN S ELLENBERG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jordan S Ellenberg is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"PENGMING WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengming Wang is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"OMAR FAWZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Omar Fawzi is one of the authors of the paper titled \"Mathematical discoveries from program search with large language models\" published in Nature in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATHEMATICAL DISCOVERIES FROM PROGRAM SEARCH WITH LARGE LANGUAGE MODELS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Mathematical discoveries from program search with large language models\" was published in Nature in 2024 and authored by Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, and Omar Fawzi<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TIMO SCHICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timo Schick is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JANE DWIVEDI-YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane Dwivedi-Yu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROBERTO DESSI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberto Dessi is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROBERTA RAILEANU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Roberta Raileanu is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MARIA LOMELI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maria Lomeli is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ERIC HAMBRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Hambro is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LUKE ZETTLEMOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luke Zettlemoyer is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NICOLA CANCEDDA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicola Cancedda is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"THOMAS SCIALOM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Scialom is one of the authors of the paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"TOOLFORMER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Toolformer is a system described in a paper titled \"Toolformer: Language models can teach themselves to use tools\" published in the Thirty-seventh Conference on Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SANDER SCHULHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sander Schulhoff is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MICHAEL ILIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Ilie is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NISHANT BALEPUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nishant Balepur is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KONSTANTINE KAHADZE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Konstantine Kahadze is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AMANDA LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amanda Liu is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHENGLEI SI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chenglei Si is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YINHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yinheng Li is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AAYUSH GUPTA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aayush Gupta is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HYOJUNG HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">HyoJung Han is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SEVIEN SCHULHOFF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sevien Schulhoff is one of the authors of the paper titled \"The prompt report: A systematic survey of prompting techniques\" published on arXiv in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"THE PROMPT REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"The prompt report: A systematic survey of prompting techniques\" was published on arXiv in 2024 and authored by Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, and Sevien Schulhoff<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XUAN SHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuan Shen is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YAOHUA WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaohua Wang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MING LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ming Lin is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YILUN HUANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Huang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HAO TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Tang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XIUYU SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiuyu Sun is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yanzhi Wang is one of the authors of the paper titled \"Deepmad: Mathematical architecture design for deep convolutional neural network\" published in the Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DEEPMAD\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Deepmad is a system for mathematical architecture design for deep convolutional neural networks, as described in a paper published in the Proceedings of the IEEE\/CVF<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RAID 2020\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020) is an event where the paper \"SourceFinder: Finding malware Source-Code from publicly available repositories in GitHub\" was published<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NEURAL INFORMATION PROCESSING SYSTEMS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">Advances in Neural Information Processing Systems is a conference where the paper \"Direct preference optimization: Your language model is secretly a reward model\" was published in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NATURE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Nature is a journal where the paper \"Mathematical discoveries from program search with large language models\" was published in 2024<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"IEEE\/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The IEEE\/CVF Conference on Computer Vision and Pattern Recognition is an event where the paper \"Deepmad: Mathematical architecture design for deep convolutional neural network\" was published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"THE ELEVENTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Eleventh International Conference on Learning Representations is an event where the paper \"Language models are multilingual chain-of-thought reasoners\" was published in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FREDA SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Freda Shi is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MIRAC SUZGUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mirac Suzgun is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MARKUS FREITAG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Markus Freitag is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SURAJ SRIVATS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suraj Srivats is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SOROUSH VOSOUGHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Soroush Vosoughi is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SEBASTIAN RUDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Ruder is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DIPANJAN DAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dipanjan Das is one of the authors of the paper titled \"Language models are multilingual chain-of-thought reasoners\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NOAH SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah Shinn is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FEDERICO CASSANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Federico Cassano is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASHWIN GOPINATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashwin Gopinath is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KARTHIK NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik Narasimhan is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SHUNYU YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shunyu Yao is one of the authors of the paper titled \"Reflexion: Language agents with verbal reinforcement learning\" published in Advances in Neural Information Processing Systems in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"KENNETH O STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O Stanley is one of the authors of the book titled \"Why greatness cannot be planned: The myth of the objective\" published by Springer in 2015<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the book titled \"Why greatness cannot be planned: The myth of the objective\" published by Springer in 2015<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SPRINGER\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Springer is the publisher of the book titled \"Why greatness cannot be planned: The myth of the objective\" authored by Kenneth O Stanley and Joel Lehman in 2015<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"Designing neural networks through neuroevolution\" published in Nature Machine Intelligence in 2019<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RISTO MIIKKULAINEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Risto Miikkulainen is one of the authors of the paper titled \"Designing neural networks through neuroevolution\" published in Nature Machine Intelligence in 2019<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"NATURE MACHINE INTELLIGENCE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Nature Machine Intelligence is a journal where the paper titled \"Designing neural networks through neuroevolution\" was published in 2019<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"RICHARD S SUTTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Richard S Sutton is one of the authors of the book titled \"Reinforcement learning: An introduction\" published by MIT Press in 2018<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ANDREW G BARTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew G Barto is one of the authors of the book titled \"Reinforcement learning: An introduction\" published by MIT Press in 2018<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MIT PRESS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">MIT Press is the publisher of the book titled \"Reinforcement learning: An introduction\" authored by Richard S Sutton and Andrew G Barto in 2018<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"SAI VEMPRALA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sai Vemprala is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ROGERIO BONATTI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rogerio Bonatti is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ARTHUR BUCKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Bucker is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ASHISH KAPOOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashish Kapoor is one of the authors of the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" published by Microsoft in February 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is the organization that published the technical report titled \"ChatGPT for robotics: Design principles and model abilities\" in February 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"GUANZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanzhi Wang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUQI XIE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuqi Xie is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUNFAN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Jiang is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"AJAY MANDLEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ajay Mandlekar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHAOWEI XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chaowei Xiao is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YUKE ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuke Zhu is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LINXI FAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxi Fan is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ANIMA ANANDKUMAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anima Anandkumar is one of the authors of the paper titled \"Voyager: An open-ended embodied agent with large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"VOYAGER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Voyager is an open-ended embodied agent with large language models, as described in a paper published on arXiv in 2023<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JANE X WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jane X Wang is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZEB KURTH-NELSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeb Kurth-Nelson is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DHRUVA TIRUMALA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dhruva Tirumala is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HUBERT SOYER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hubert Soyer is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JOEL Z LEIBO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Z Leibo is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"REMI MUNOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Remi Munos is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHARLES BLUNDELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charles Blundell is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"DHARSHAN KUMARAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dharshan Kumaran is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"MATT BOTVINICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Botvinick is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"LEI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lei Wang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XUEYANG FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xueyang Feng is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"HAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Yang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JINGSEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingsen Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"ZHIYUAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"JIAKAI TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiakai Tang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"XU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <node id=\"FRONTIERS OF COMPUTER SCIENCE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Frontiers of Computer Science is a journal where the paper titled \"A survey on large language model based autonomous agents\" was published<\/data>      <data key=\"d2\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/node>    <edge source=\"QIANG WANG\" target=\"DAWEI YIN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qiang Wang and Dawei Yin co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"QIANG WANG\" target=\"JUN XU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qiang Wang and Jun Xu co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"QIANG WANG\" target=\"JI-RONG WEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Qiang Wang and Ji-Rong Wen co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"DAWEI YIN\" target=\"JUN XU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dawei Yin and Jun Xu co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"DAWEI YIN\" target=\"JI-RONG WEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dawei Yin and Ji-Rong Wen co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"JUN XU\" target=\"JI-RONG WEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Jun Xu and Ji-Rong Wen co-authored the paper \"Tool learning with large language models: A survey\" published on arXiv in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"RAFAEL RAFAILOV\" target=\"ARCHIT SHARMA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rafael Rafailov and Archit Sharma co-authored the paper \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>    <edge source=\"RAFAEL RAFAILOV\" target=\"ERIC MITCHELL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Rafael Rafailov and Eric Mitchell co-authored the paper \"Direct preference optimization: Your language model is secretly a reward model\" published in Advances in Neural Information Processing Systems in 2024<\/data>      <data key=\"d5\">34d0bb2211fc795fe1096442e086a2b3<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"2600a1ed94ad2d3675ea80575c39cbd1","chunk":"shan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763 , 2016.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\nFrontiers of Computer Science , 18(6):186345, 2024.\nRui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Poet: open-ended coevolution of\nenvironments and their optimized solutions. In Proceedings of the Genetic and Evolutionary Compu-\ntation Conference , GECCO \u201919, pp. 142\u2013151, New York, NY, USA, 2019. Association for Computing\nMachinery. ISBN 9781450361118. doi: 10.1145\/3321707.3321799.\nRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley.\nEnhanced poet: Open-ended reinforcement learning through unbounded invention of learning\nchallenges and their solutions. In International conference on machine learning , pp. 9940\u20139951.\nPMLR, 2020.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations , 2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems , 35:24824\u201324837, 2022.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent\nconversation framework. arXiv preprint arXiv:2308.08155 , 2023.\n20Automated Design of Agentic Systems\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao.\nExpertprompting: Instructing large language models to be distinguished experts. arXiv preprint\narXiv:2305.14688 , 2023.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.\nLarge language models as optimizers. In The Twelfth International Conference on Learning Represen-\ntations, 2024. URL https:\/\/openreview.net\/forum?id=Bb4VGOWELI .\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In The Eleventh International\nConference on Learning Representations , 2023. URL https:\/\/openreview.net\/forum?id=WE_\nvluYUL-X .\nBennet Yee, David Sehr, Gregory Dardyk, J Bradley Chen, Robert Muth, Tavis Ormandy, Shiki Okasaka,\nNeha Narula, and Nicholas Fullagar. Native client: A sandbox for portable, untrusted x86 native\ncode.Communications of the ACM , 53(1):91\u201399, 2010.\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez\nArenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to\nrewards for robotic skill synthesis. In Conference on Robot Learning , pp. 374\u2013404. PMLR, 2023.\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards\nautomatic multi-agent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228 ,\n2024.\nEliezer Yudkowsky et al. Artificial Intelligence as a positive and negative factor in global risk. Global\ncatastrophic risks , 1(303):184, 2008.\nMatei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts,\nJames Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift\nfrom models to compound ai systems. https:\/\/bair.berkeley.edu\/blog\/2024\/02\/18\/\ncompound-ai-systems\/ , 2024.\nJennyZhang, JoelLehman, KennethStanley, andJeffClune. OMNI:Open-endednessviamodelsofhu-\nman notions of interestingness. In The Twelfth International Conference on Learning Representations ,\n2024a. URL https:\/\/openreview.net\/forum?id=AgM3MzT99c .\nShaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang","chunk_id":"2600a1ed94ad2d3675ea80575c39cbd1","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"SHAN KUMARAN","type":"PERSON","description":"Shan Kumaran is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"MATT BOTVINICK","type":"PERSON","description":"Matt Botvinick is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"LEI WANG","type":"PERSON","description":"Lei Wang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"XUEYANG FENG","type":"PERSON","description":"Xueyang Feng is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ZEYU ZHANG","type":"PERSON","description":"Zeyu Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"HAO YANG","type":"PERSON","description":"Hao Yang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JINGSEN ZHANG","type":"PERSON","description":"Jingsen Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ZHIYUAN CHEN","type":"PERSON","description":"Zhiyuan Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JIAKAI TANG","type":"PERSON","description":"Jiakai Tang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"XU CHEN","type":"PERSON","description":"Xu Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"RUI WANG","type":"PERSON","description":"Rui Wang is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"KENNETH O. STANLEY","type":"PERSON","description":"Kenneth O. Stanley is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ADITYA RAWAL","type":"PERSON","description":"Aditya Rawal is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JIALE ZHI","type":"PERSON","description":"Jiale Zhi is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YULUN LI","type":"PERSON","description":"Yulun Li is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"XUEZHI WANG","type":"PERSON","description":"Xuezhi Wang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"DALE SCHUURMANS","type":"PERSON","description":"Dale Schuurmans is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ED H. CHI","type":"PERSON","description":"Ed H. Chi is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"SHARAN NARANG","type":"PERSON","description":"Sharan Narang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"MAARTEN BOSMA","type":"PERSON","description":"Maarten Bosma is one of the authors of the paper titled \"Chain-of-thought prompting elicits reasoning in large language models\" published in Advances in Neural Information Processing Systems in 2022","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"FEI XIA","type":"PERSON","description":"Fei Xia is one of the authors of the paper titled \"Chain-of-thought prompting elicits reasoning in large language models\" published in Advances in Neural Information Processing Systems in 2022","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"GAGAN BANSAL","type":"PERSON","description":"Gagan Bansal is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YIRAN WU","type":"PERSON","description":"Yiran Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ERKANG ZHU","type":"PERSON","description":"Erkang Zhu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"BEIBIN LI","type":"PERSON","description":"Beibin Li is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"LI JIANG","type":"PERSON","description":"Li Jiang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"XIAOYUN ZHANG","type":"PERSON","description":"Xiaoyun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"BENFENG XU","type":"PERSON","description":"Benfeng Xu is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"AN YANG","type":"PERSON","description":"An Yang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JUNYANG LIN","type":"PERSON","description":"Junyang Lin is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"QUAN WANG","type":"PERSON","description":"Quan Wang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"CHANG ZHOU","type":"PERSON","description":"Chang Zhou is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YONGDONG ZHANG","type":"PERSON","description":"Yongdong Zhang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ZHENDONG MAO","type":"PERSON","description":"Zhendong Mao is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"CHENGRUN YANG","type":"PERSON","description":"Chengrun Yang is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YIFENG LU","type":"PERSON","description":"Yifeng Lu is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"HANXIAO LIU","type":"PERSON","description":"Hanxiao Liu is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"XINYUN CHEN","type":"PERSON","description":"Xinyun Chen is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"SHUNYU YAO","type":"PERSON","description":"Shunyu Yao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"JEFFREY ZHAO","type":"PERSON","description":"Jeffrey Zhao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"DIAN YU","type":"PERSON","description":"Dian Yu is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"NAN DU","type":"PERSON","description":"Nan Du is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"IZHAK SHAFRAN","type":"PERSON","description":"Izhak Shafran is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"KARTHIK R NARASIMHAN","type":"PERSON","description":"Karthik R Narasimhan is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"YUAN CAO","type":"PERSON","description":"Yuan Cao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"BENNET YEE","type":"PERSON","description":"Bennet Yee is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"DAVID SEHR","type":"PERSON","description":"David Sehr is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"GREGORY DARDYK","type":"PERSON","description":"Gregory Dardyk is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"J BRADLEY CHEN","type":"PERSON","description":"J Bradley Chen is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"ROBERT MUTH","type":"PERSON","description":"Robert Muth is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"TAVIS ORMANDY","type":"PERSON","description":"Tavis Ormandy is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"SHIKI OKASAKA","type":"PERSON","description":"Shiki Okasaka is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"NEHA NARULA","type":"PERSON","description":"Neha Narula is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"NICHOLAS FULLAGAR","type":"PERSON","description":"Nicholas Fullagar is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"WENHAO YU","type":"PERSON","description":"Wenhao Yu is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"NIMROD GILEADI","type":"PERSON","description":"Nimrod Gileadi is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"CHUYUAN FU","type":"PERSON","description":"Chuyuan Fu is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"},{"name":"SEAN KIRMANI","type":"PERSON","description":"Sean Kirmani is one of the authors of the paper","source_id":"2600a1ed94ad2d3675ea80575c39cbd1","entity_type":"PERSON"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SHAN KUMARAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shan Kumaran is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MATT BOTVINICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Botvinick is one of the authors of the paper titled \"Learning to reinforcement learn\" published on arXiv in 2016<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lei Wang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUEYANG FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xueyang Feng is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAO YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Yang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JINGSEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jingsen Zhang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHIYUAN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAKAI TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiakai Tang is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Chen is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is one of the authors of the paper titled \"A survey on large language model based autonomous agents\" published in Frontiers of Computer Science in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Wang is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KENNETH O. STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth O. Stanley is one of the authors of the paper titled \"Poet: open-ended coevolution of environments and their optimized solutions\" published in the Proceedings of the Genetic and Evolutionary Computation Conference in 2019<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ADITYA RAWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aditya Rawal is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIALE ZHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Zhi is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YULUN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yulun Li is one of the authors of the paper titled \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\" published in the International Conference on Machine Learning in 2020<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUEZHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuezhi Wang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DALE SCHUURMANS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dale Schuurmans is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED H. CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H. Chi is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHARAN NARANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sharan Narang is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is one of the authors of the paper titled \"Self-consistency improves chain of thought reasoning in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAARTEN BOSMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maarten Bosma is one of the authors of the paper titled \"Chain-of-thought prompting elicits reasoning in large language models\" published in Advances in Neural Information Processing Systems in 2022<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEI XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Xia is one of the authors of the paper titled \"Chain-of-thought prompting elicits reasoning in large language models\" published in Advances in Neural Information Processing Systems in 2022<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GAGAN BANSAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gagan Bansal is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIRAN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERKANG ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erkang Zhu is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BEIBIN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Beibin Li is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LI JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Jiang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOYUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is one of the authors of the paper titled \"Autogen: Enabling next-gen LLM applications via multi-agent conversation framework\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BENFENG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Benfeng Xu is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">An Yang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JUNYANG LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junyang Lin is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quan Wang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHANG ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chang Zhou is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YONGDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yongdong Zhang is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENDONG MAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhendong Mao is one of the authors of the paper titled \"Expertprompting: Instructing large language models to be distinguished experts\" published on arXiv in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHENGRUN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chengrun Yang is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIFENG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifeng Lu is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HANXIAO LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hanxiao Liu is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XINYUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyun Chen is one of the authors of the paper titled \"Large language models as optimizers\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHUNYU YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shunyu Yao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JEFFREY ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeffrey Zhao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DIAN YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dian Yu is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NAN DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nan Du is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"IZHAK SHAFRAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Izhak Shafran is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KARTHIK R NARASIMHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karthik R Narasimhan is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YUAN CAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuan Cao is one of the authors of the paper titled \"React: Synergizing reasoning and acting in language models\" published in The Eleventh International Conference on Learning Representations in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BENNET YEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bennet Yee is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DAVID SEHR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Sehr is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GREGORY DARDYK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gregory Dardyk is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J BRADLEY CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J Bradley Chen is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROBERT MUTH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Robert Muth is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TAVIS ORMANDY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tavis Ormandy is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIKI OKASAKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shiki Okasaka is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NEHA NARULA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neha Narula is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NICHOLAS FULLAGAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicholas Fullagar is one of the authors of the paper titled \"Native client: A sandbox for portable, untrusted x86 native code\" published in Communications of the ACM in 2010<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WENHAO YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenhao Yu is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NIMROD GILEADI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nimrod Gileadi is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHUYUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chuyuan Fu is one of the authors of the paper titled \"Language to rewards for robotic skill synthesis\" published in the Conference on Robot Learning in 2023<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SEAN KIRMANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sean Kirmani is one of the authors of the paper<\/data>      <data key=\"d2\">2600a1ed94ad2d3675ea80575c39cbd1<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"cc802d9b841fde55e9c0c2ba0ef7869d","chunk":"ai-systems\/ , 2024.\nJennyZhang, JoelLehman, KennethStanley, andJeffClune. OMNI:Open-endednessviamodelsofhu-\nman notions of interestingness. In The Twelfth International Conference on Learning Representations ,\n2024a. URL https:\/\/openreview.net\/forum?id=AgM3MzT99c .\nShaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun\nWu. Offline training of language model agents with functions as learnable weights. In Forty-first\nInternational Conference on Machine Learning , 2024b.\nZeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and\nJi-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv\npreprint arXiv:2404.13501 , 2024c.\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and\nDenny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. arXiv\npreprint arXiv:2310.06117 , 2023.\nPei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou,\nSwaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose\nreasoning structures. arXiv preprint arXiv:2402.03620 , 2024a.\n21Automated Design of Agentic Systems\nWangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen,\nShuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents.\narXiv preprint arXiv:2406.18532 , 2024b.\nMingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and J\u00fcrgen\nSchmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International\nConference on Machine Learning , 2024.\nLuisaZintgraf,SebastianSchulze,CongLu,LeoFeng,MaximilianIgl,KyriacosShiarlis,YarinGal,Katja\nHofmann, and Shimon Whiteson. Varibad: Variational bayes-adaptive deep rl via meta-learning.\nJournal of Machine Learning Research , 22(289):1\u201339, 2021a.\nLuisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and\nShimon Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning.\nInInternational Conference on Machine Learning , pp. 12991\u201313001. PMLR, 2021b.\n22Automated Design of Agentic Systems\nSupplementary Material\nTable of Contents\nA Prompts 24\nB Framework Code 26\nC Experiment Details for ARC Challenge 30\nD Experiment Details for Reasoning and Problem-Solving Domains 33\nE Baselines 35\nF Example Agents 36\nG Cost of Experiments 39\n23Automated Design of Agentic Systems\nA. Prompts\nWe use the following prompts for the meta agent in Meta Agent Search. Variables in the prompts\nthat vary depending on domains and iterations are highlighted. All detailed prompts are available at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nWe use the following system prompt for every query in the meta agent.\nSystem prompt for the meta agent.\nYou are a helpful assistant. Make sure to return in a WELL-FORMED JSON object.\nWe use the following prompt for the meta agent to design the new agent based on the archive of\npreviously discovered agents.\nMain prompt for the meta agent.\nYou are an expert machine learning researcher testing various agentic systems. Your objective is to design\nbuilding blocks such as prompts and control flows within these systems to solve complex tasks. Your aim\nis to design an optimal agent performing well on [BriefDescriptionoftheDomain].\n[FrameworkCode]\n[OutputInstructionsandExamples]\n[DiscoveredAgentArchive] (initializedwithbaselines,updatedateveryiteration)\n# Your task\nYou are deeply familiar with prompting techniques and the agent works from the literature. Your goal is\nto maximize the specified performance metrics by proposing interestingly new agents.\nObserve the discovered agents carefully and think about what insights, lessons, or stepping stones can be\nlearned from them.\nBe creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration\nfrom related agent papers or academic papers from other research areas.\nUse the knowledge from the archive and inspiration from academic literature to propose the next\ninteresting agentic system design.\nTHINK OUTSIDE THE BOX.\nThe domain descriptions are available in Appendices C and D and the framework code is available\nin Appendix B. We use the following prompt to instruct and format the output of the meta agent.\nHere, we collect and present some common mistakes that the meta agent may make in the prompt.\nWe found it effective in improving the quality of the generated code.\nOutput Instruction and Example.\n# Output Instruction and Example:\nThe first key should be (\u201cthought\u201d), and it should capture your thought process for designing the\nnext function. In","chunk_id":"cc802d9b841fde55e9c0c2ba0ef7869d","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"JENNY ZHANG","type":"PERSON","description":"Jenny Zhang is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JOEL LEHMAN","type":"PERSON","description":"Joel Lehman is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"KENNETH STANLEY","type":"PERSON","description":"Kenneth Stanley is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JEFF CLUNE","type":"PERSON","description":"Jeff Clune is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS","type":"EVENT","description":"The Twelfth International Conference on Learning Representations is an event where the paper \"OMNI: Open-endedness via models of human notions of interestingness\" was published in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JIALE LIU","type":"PERSON","description":"Jiale Liu is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LINXIN SONG","type":"PERSON","description":"Linxin Song is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"RANJAY KRISHNA","type":"PERSON","description":"Ranjay Krishna is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"FORTY-FIRST INTERNATIONAL CONFERENCE ON MACHINE LEARNING","type":"EVENT","description":"The Forty-first International Conference on Machine Learning is an event where the paper \"Offline training of language model agents with functions as learnable weights\" was published in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"EVENT"},{"name":"ZEYU ZHANG","type":"PERSON","description":"Zeyu Zhang is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"XIAOHE BO","type":"PERSON","description":"Xiaohe Bo is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"CHEN MA","type":"PERSON","description":"Chen Ma is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"RUI LI","type":"PERSON","description":"Rui Li is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"XU CHEN","type":"PERSON","description":"Xu Chen is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"QUANYU DAI","type":"PERSON","description":"Quanyu Dai is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JIEMING ZHU","type":"PERSON","description":"Jieming Zhu is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"ZHENHUA DONG","type":"PERSON","description":"Zhenhua Dong is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JI-RONG WEN","type":"PERSON","description":"Ji-Rong Wen is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"HUAIXIU STEVEN ZHENG","type":"PERSON","description":"Huaixiu Steven Zheng is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024\nSwaroop Mishra is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"XINYUN CHEN","type":"PERSON","description":"Xinyun Chen is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024\nXinyun Chen is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"HENG-TZE CHENG","type":"PERSON","description":"Heng-Tze Cheng is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024\nHeng-Tze Cheng is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"ED H CHI","type":"PERSON","description":"Ed H Chi is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023\nEd H Chi is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023\nQuoc V Le is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024\nDenny Zhou is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"PEI ZHOU","type":"PERSON","description":"Pei Zhou is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JAY PUJARA","type":"PERSON","description":"Jay Pujara is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"XIANG REN","type":"PERSON","description":"Xiang Ren is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"WANGCHUNSHU ZHOU","type":"PERSON","description":"Wangchunshu Zhou is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"YIXIN OU","type":"PERSON","description":"Yixin Ou is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SHENGWEI DING","type":"PERSON","description":"Shengwei Ding is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LONG LI","type":"PERSON","description":"Long Li is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JIALONG WU","type":"PERSON","description":"Jialong Wu is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"TIANNAN WANG","type":"PERSON","description":"Tiannan Wang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JIAMIN CHEN","type":"PERSON","description":"Jiamin Chen is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SHUAI WANG","type":"PERSON","description":"Shuai Wang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"XIAOHUA XU","type":"PERSON","description":"Xiaohua Xu is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"NINGYU ZHANG","type":"PERSON","description":"Ningyu Zhang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"MINGCHEN ZHUGE","type":"PERSON","description":"Mingchen Zhuge is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"WENYI WANG","type":"PERSON","description":"Wenyi Wang is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LOUIS KIRSCH","type":"PERSON","description":"Louis Kirsch is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"FRANCESCO FACCIO","type":"PERSON","description":"Francesco Faccio is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"DMITRII KHIZBULLIN","type":"PERSON","description":"Dmitrii Khizbullin is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"J\u00dcRGEN SCHMIDHUBER","type":"PERSON","description":"J\u00fcrgen Schmidhuber is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LUISA ZINTGRAF","type":"PERSON","description":"Luisa Zintgraf is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SEBASTIAN SCHULZE","type":"PERSON","description":"Sebastian Schulze is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"CONG LU","type":"PERSON","description":"Cong Lu is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"LEO FENG","type":"PERSON","description":"Leo Feng is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"MAXIMILIAN IGL","type":"PERSON","description":"Maximilian Igl is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"KYRIACOS SHIARLIS","type":"PERSON","description":"Kyriacos Shiarlis is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"YARIN GAL","type":"PERSON","description":"Yarin Gal is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"KATJA HOFMANN","type":"PERSON","description":"Katja Hofmann is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"SHIMON WHITESON","type":"PERSON","description":"Shimon Whiteson is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PERSON"},{"name":"JOURNAL OF MACHINE LEARNING RESEARCH","type":"PUBLICATION","description":"The Journal of Machine Learning Research is a publication where the paper \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" was published in 2021","source_id":"cc802d9b841fde55e9c0c2ba0ef7869d","entity_type":"PUBLICATION"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"JENNY ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jenny Zhang is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JOEL LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Lehman is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"KENNETH STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kenneth Stanley is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JEFF CLUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeff Clune is one of the authors of the paper titled \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"THE TWELFTH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Twelfth International Conference on Learning Representations is an event where the paper \"OMNI: Open-endedness via models of human notions of interestingness\" was published in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIALE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Liu is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LINXIN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linxin Song is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RANJAY KRISHNA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ranjay Krishna is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is one of the authors of the paper titled \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FORTY-FIRST INTERNATIONAL CONFERENCE ON MACHINE LEARNING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The Forty-first International Conference on Machine Learning is an event where the paper \"Offline training of language model agents with functions as learnable weights\" was published in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">EVENT<\/data>    <\/node>    <node id=\"ZEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeyu Zhang is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOHE BO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaohe Bo is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHEN MA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Ma is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"RUI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rui Li is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu Chen is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUANYU DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quanyu Dai is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIEMING ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieming Zhu is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ZHENHUA DONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenhua Dong is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JI-RONG WEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ji-Rong Wen is one of the authors of the paper titled \"A survey on the memory mechanism of large language model based agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HUAIXIU STEVEN ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Huaixiu Steven Zheng is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024Swaroop Mishra is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XINYUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyun Chen is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024Xinyun Chen is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HENG-TZE CHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heng-Tze Cheng is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024Heng-Tze Cheng is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ED H CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H Chi is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023Ed H Chi is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023Quoc V Le is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024Denny Zhou is one of the authors of the paper titled \"Take a step back: Evoking reasoning via abstraction in large language models\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PEI ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pei Zhou is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JAY PUJARA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jay Pujara is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIANG REN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiang Ren is one of the authors of the paper titled \"Self-discover: Large language models self-compose reasoning structures\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WANGCHUNSHU ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wangchunshu Zhou is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YIXIN OU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Ou is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHENGWEI DING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengwei Ding is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LONG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Long Li is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIALONG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jialong Wu is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TIANNAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tiannan Wang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAMIN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiamin Chen is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHUAI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuai Wang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOHUA XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaohua Xu is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"NINGYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ningyu Zhang is one of the authors of the paper titled \"Symbolic learning enables self-evolving agents\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MINGCHEN ZHUGE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mingchen Zhuge is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WENYI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenyi Wang is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LOUIS KIRSCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Louis Kirsch is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FRANCESCO FACCIO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Francesco Faccio is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DMITRII KHIZBULLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dmitrii Khizbullin is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"J&#220;RGEN SCHMIDHUBER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">J&#252;rgen Schmidhuber is one of the authors of the paper titled \"GPTSwarm: Language agents as optimizable graphs\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUISA ZINTGRAF\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luisa Zintgraf is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SEBASTIAN SCHULZE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Schulze is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cong Lu is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LEO FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Leo Feng is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MAXIMILIAN IGL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maximilian Igl is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KYRIACOS SHIARLIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kyriacos Shiarlis is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YARIN GAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yarin Gal is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KATJA HOFMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Katja Hofmann is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIMON WHITESON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shimon Whiteson is one of the authors of the paper titled \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" published in the Journal of Machine Learning Research in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JOURNAL OF MACHINE LEARNING RESEARCH\">      <data key=\"d0\">PUBLICATION<\/data>      <data key=\"d1\">The Journal of Machine Learning Research is a publication where the paper \"Varibad: Variational bayes-adaptive deep RL via meta-learning\" was published in 2021<\/data>      <data key=\"d2\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>      <data key=\"d3\">PUBLICATION<\/data>    <\/node>    <edge source=\"JENNY ZHANG\" target=\"JOEL LEHMAN\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Jenny Zhang and Joel Lehman co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JENNY ZHANG\" target=\"KENNETH STANLEY\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Jenny Zhang and Kenneth Stanley co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JENNY ZHANG\" target=\"JEFF CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Jenny Zhang and Jeff Clune co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JOEL LEHMAN\" target=\"KENNETH STANLEY\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Joel Lehman and Kenneth Stanley co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"JOEL LEHMAN\" target=\"JEFF CLUNE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Joel Lehman and Jeff Clune co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"KENNETH STANLEY\" target=\"JEFF CLUNE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Kenneth Stanley and Jeff Clune co-authored the paper \"OMNI: Open-endedness via models of human notions of interestingness\" published in The Twelfth International Conference on Learning Representations in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"SHAOKUN ZHANG\" target=\"JIEYU ZHANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shaokun Zhang and Jieyu Zhang co-authored the paper \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"SHAOKUN ZHANG\" target=\"JIALE LIU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shaokun Zhang and Jiale Liu co-authored the paper \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>    <edge source=\"SHAOKUN ZHANG\" target=\"LINXIN SONG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Shaokun Zhang and Linxin Song co-authored the paper \"Offline training of language model agents with functions as learnable weights\" published in the Forty-first International Conference on Machine Learning in 2024<\/data>      <data key=\"d6\">cc802d9b841fde55e9c0c2ba0ef7869d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"282313a8340c6792e8c35f53ed157cd0","chunk":" available in Appendices C and D and the framework code is available\nin Appendix B. We use the following prompt to instruct and format the output of the meta agent.\nHere, we collect and present some common mistakes that the meta agent may make in the prompt.\nWe found it effective in improving the quality of the generated code.\nOutput Instruction and Example.\n# Output Instruction and Example:\nThe first key should be (\u201cthought\u201d), and it should capture your thought process for designing the\nnext function. In the \u201cthought\u201d section, first reason about what the next interesting agent to try\nshould be, then describe your reasoning and the overall concept behind the agent design, and\nfinally detail the implementation steps. The second key (\u201cname\u201d) corresponds to the name of\nyour next agent architecture. Finally, the last key (\u201ccode\u201d) corresponds to the exact \u201cforward()\u201d\nfunction in Python code that you would like to try. You must write COMPLETE CODE in \u201ccode\u201d:\nYourcodewillbepartoftheentireproject, sopleaseimplementcomplete, reliable, reusablecodesnippets.\n24Automated Design of Agentic Systems\nHere is an example of the output format for the next agent:\n{\u201cthought\u201d: \u201c**Insights:** Your insights on what should be the next interesting agent. **Overall Idea:**\nyour reasoning and the overall concept behind the agent design. **Implementation:** describe the\nimplementation step by step.\u201d,\n\u201cname\u201d: \u201cName of your proposed agent\u201d,\n\u201ccode\u201d: \u201cdef forward(self, taskInfo): # Your code here\u201d}\n## WRONG Implementation examples:\n[Examplesofpotentialmistakesthemetaagentmaymakeinimplementation]\nAfter the first response from the meta agent, we perform two rounds of self-reflection to make the\ngenerated agent novel and error-free (Madaan et al., 2024; Shinn et al., 2023).\nPrompt for self-reflection round 1.\n[GeneratedAgentfromPreviousIteration]\nCarefully review the proposed new architecture and reflect on the following points:\n1. **Interestingness**: Assess whether your proposed architecture is interesting or innovative compared\nto existing methods in the archive. If you determine that the proposed architecture is not interesting,\nsuggest a new architecture that addresses these shortcomings.\n- Make sure to check the difference between the proposed architecture and previous attempts.\n- Compare the proposal and the architectures in the archive CAREFULLY, including their actual differences\nin the implementation.\n- Decide whether the current architecture is innovative.\n- USE CRITICAL THINKING!\n2. **Implementation Mistakes**: Identify any mistakes you may have made in the implementation.\nReview the code carefully, debug any issues you find, and provide a corrected version. REMEMBER\nchecking \"## WRONG Implementation examples\" in the prompt.\n3. **Improvement**: Based on the proposed architecture, suggest improvements in the detailed\nimplementation that could increase its performance or effectiveness. In this step, focus on refining and\noptimizing the existing implementation without altering the overall design framework, except if you\nwant to propose a different architecture if the current is not interesting.\n- Observe carefully about whether the implementation is actually doing what it is supposed to do.\n- Check if there is redundant code or unnecessary steps in the implementation. Replace them with\neffective implementation.\n- Try to avoid the implementation being too similar to the previous agent.\nAnd then, you need to improve or revise the implementation, or implement the new proposed architecture\nbased on the reflection.\nYour response should be organized as follows:\n\"reflection\": Provide your thoughts on the interestingness of the architecture, identify any mistakes in the\nimplementation, and suggest improvements.\n\"thought\": Revise your previous proposal or propose a new architecture if necessary, using the same\nformat as the example response.\n\"name\": Provide a name for the revised or new architecture. (Don\u2019t put words like \"new\" or \"improved\"\nin the name.)\n\"code\": Provide the corrected code or an improved implementation. Make sure you actually implement\nyour fix and improvement in this code.\n25Automated Design of Agentic Systems\nPrompt for self-reflection round 2.\nUsing the tips in \u201c## WRONG Implementation examples\u201d section, further revise the code.\nYour response should be organized as follows:\nInclude your updated reflections in the \u201creflection\u201d. Repeat the previous \u201cthought\u201d and \u201cname\u201d. Update\nthe corrected version of the code in the \u201ccode\u201d section.\nWhen an error is encountered during the execution of the generated code, we conduct a reflection\nand re-run the code. This process is repeated up to five times if errors persist. Here is the prompt we\nuse to self-reflect any runtime error:\nPrompt for self-reflection when a runtime error occurs.\nError during evaluation:\n[Runtimeerrors]\nCarefully consider where you went wrong in your latest implementation. Using insights from previous\nattempts, try to debug the current code to implement the same thought. Repeat your previous thought in\n\u201cthought\u201d, and put your thinking for debugging in \u201cdebug_thought\u201d.\nB. Framework Code\nIn this paper, we provide the meta agent with a simple framework to implement basic functions,\nsuch as querying Foundation Models (FMs) and formatting prompts. The framework consists of\nfewer than 100 lines of code (excluding comments). In this framework, we encapsulate every\npiece of information into a namedtuple Info object, making it easy to combine different types of\ninformation (e.g., FM responses, results from tool function calls, task descriptions) and facilitate\ncommunicationbetweendifferentmodules. Additionally,intheFMmodule,weautomaticallyconstruct\nthe prompt by concatenating all input Info objects into a structured format, with each Info titled by\nits metadata (e.g., name, author). Throughout the appendix, we renamed some variables in the\ncode to match the terminologies used in the main text. The full framework code is available at","chunk_id":"282313a8340c6792e8c35f53ed157cd0","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"APPENDIX C","type":"DOCUMENT SECTION","description":"Appendix C is a section in the document that contains additional information relevant to the main text","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"APPENDIX D","type":"DOCUMENT SECTION","description":"Appendix D is a section in the document that contains additional information relevant to the main text","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"APPENDIX B","type":"DOCUMENT SECTION","description":"Appendix B is a section in the document that contains the framework code","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"META AGENT","type":"TECHNOLOGY","description":"The meta agent is a system used to generate and improve code by following specific prompts and instructions","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"PROMPT","type":"TECHNOLOGY","description":"The prompt is a set of instructions given to the meta agent to guide its output and improve the quality of the generated code","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"OUTPUT INSTRUCTION AND EXAMPLE","type":"DOCUMENT SECTION","description":"The Output Instruction and Example section provides guidelines and examples for the meta agent's output format","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"SELF-REFLECTION","type":"PROCESS","description":"Self-reflection is a process where the meta agent reviews and improves its generated code to make it novel and error-free","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"MADAAN","type":"PERSON","description":"Madaan is an author who contributed to the concept of self-reflection in 2024","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"SHINN","type":"PERSON","description":"Shinn is an author who contributed to the concept of self-reflection in 2023","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FOUNDATION MODELS (FMS)","type":"TECHNOLOGY","description":"Foundation Models (FMs) are models queried by the meta agent to perform tasks and format prompts","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"NAMEDTUPLE INFO OBJECT","type":"TECHNOLOGY","description":"The namedtuple Info object is used in the framework to encapsulate and combine different types of information","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FM MODULE","type":"TECHNOLOGY","description":"The FM module is a part of the framework that constructs prompts by concatenating input Info objects into a structured format","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FRAMEWORK CODE","type":"TECHNOLOGY","description":"The framework code is a simple codebase provided to the meta agent to implement basic functions and facilitate communication between different modules","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FIGURE 3A","type":"DOCUMENT SECTION","description":"Figure 3a is a figure in the document that shows the results and analysis of Meta Agent Search","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"ARCHIVE","type":"TECHNOLOGY","description":"The archive is a collection of previous methods and architectures used for comparison and improvement in Meta Agent Search","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"TASKINFO","type":"TECHNOLOGY","description":"TaskInfo is a parameter used in the \"forward()\" function in the Python code for agent implementation","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"AGENTIC SYSTEMS","type":"TECHNOLOGY","description":"Agentic Systems are systems designed to perform tasks autonomously, as discussed in the document","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"RUNTIME ERROR","type":"TECHNOLOGY","description":"A runtime error is an error encountered during the execution of the generated code, prompting self-reflection and debugging","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"DEBUG_THOUGHT","type":"TECHNOLOGY","description":"Debug_thought is a section in the prompt where the meta agent provides its thinking for debugging the current code","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"INFO OBJECT","type":"TECHNOLOGY","description":"Info object is a namedtuple used to encapsulate and combine different types of information in the framework","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"MAIN TEXT","type":"DOCUMENT SECTION","description":"The main text is the primary content of the document, to which the appendices provide additional information","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"TERMINOLOGIES","type":"TECHNOLOGY","description":"Terminologies are specific terms used in the main text and matched in the code throughout the appendix","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"PROJECT","type":"TECHNOLOGY","description":"The project refers to the entire endeavor of designing and implementing agentic systems as discussed in the document","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"IMPLEMENTATION","type":"PROCESS","description":"Implementation refers to the process of coding and executing the agent designs as described in the document","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"INSIGHTS","type":"PROCESS","description":"Insights refer to the thoughts and reasoning behind the design of the next agent, as captured in the \"thought\" section","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"OVERALL IDEA","type":"PROCESS","description":"Overall Idea refers to the reasoning and concept behind the agent design, as described in the \"thought\" section","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"IMPROVEMENT","type":"PROCESS","description":"Improvement refers to the suggestions and refinements made to the agent's implementation to enhance performance or effectiveness","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"INTERESTINGNESS","type":"PROCESS","description":"Interestingness is the assessment of whether the proposed architecture is innovative compared to existing methods","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"IMPLEMENTATION MISTAKES","type":"PROCESS","description":"Implementation Mistakes refer to errors identified in the code during the self-reflection process","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"REFLECTION","type":"PROCESS","description":"Reflection is the process of reviewing the proposed architecture, identifying mistakes, and suggesting improvements","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"RESPONSE","type":"PROCESS","description":"Response refers to the meta agent's output after performing self-reflection and making improvements","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"ERROR DURING EVALUATION","type":"PROCESS","description":"Error during evaluation refers to the runtime errors encountered and the subsequent debugging process","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"APPENDIX E","type":"","description":"","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"META AGENT SEARCH","type":"","description":"","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"FORWARD FUNCTION","type":"","description":"","source_id":"282313a8340c6792e8c35f53ed157cd0"},{"name":"THOUGHT","type":"","description":"","source_id":"282313a8340c6792e8c35f53ed157cd0"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"APPENDIX C\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Appendix C is a section in the document that contains additional information relevant to the main text<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"APPENDIX D\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Appendix D is a section in the document that contains additional information relevant to the main text<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"APPENDIX B\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Appendix B is a section in the document that contains the framework code<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The meta agent is a system used to generate and improve code by following specific prompts and instructions<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The prompt is a set of instructions given to the meta agent to guide its output and improve the quality of the generated code<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"OUTPUT INSTRUCTION AND EXAMPLE\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">The Output Instruction and Example section provides guidelines and examples for the meta agent's output format<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Self-reflection is a process where the meta agent reviews and improves its generated code to make it novel and error-free<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"MADAAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Madaan is an author who contributed to the concept of self-reflection in 2024<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"SHINN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shinn is an author who contributed to the concept of self-reflection in 2023<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FOUNDATION MODELS (FMS)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Foundation Models (FMs) are models queried by the meta agent to perform tasks and format prompts<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"NAMEDTUPLE INFO OBJECT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The namedtuple Info object is used in the framework to encapsulate and combine different types of information<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FM MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The FM module is a part of the framework that constructs prompts by concatenating input Info objects into a structured format<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FRAMEWORK CODE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The framework code is a simple codebase provided to the meta agent to implement basic functions and facilitate communication between different modules<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FIGURE 3A\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">Figure 3a is a figure in the document that shows the results and analysis of Meta Agent Search<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"ARCHIVE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The archive is a collection of previous methods and architectures used for comparison and improvement in Meta Agent Search<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">TaskInfo is a parameter used in the \"forward()\" function in the Python code for agent implementation<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"AGENTIC SYSTEMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic Systems are systems designed to perform tasks autonomously, as discussed in the document<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"RUNTIME ERROR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A runtime error is an error encountered during the execution of the generated code, prompting self-reflection and debugging<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"DEBUG_THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Debug_thought is a section in the prompt where the meta agent provides its thinking for debugging the current code<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"INFO OBJECT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Info object is a namedtuple used to encapsulate and combine different types of information in the framework<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"MAIN TEXT\">      <data key=\"d0\">DOCUMENT SECTION<\/data>      <data key=\"d1\">The main text is the primary content of the document, to which the appendices provide additional information<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"TERMINOLOGIES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Terminologies are specific terms used in the main text and matched in the code throughout the appendix<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"PROJECT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The project refers to the entire endeavor of designing and implementing agentic systems as discussed in the document<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"IMPLEMENTATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Implementation refers to the process of coding and executing the agent designs as described in the document<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"INSIGHTS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Insights refer to the thoughts and reasoning behind the design of the next agent, as captured in the \"thought\" section<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"OVERALL IDEA\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Overall Idea refers to the reasoning and concept behind the agent design, as described in the \"thought\" section<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"IMPROVEMENT\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Improvement refers to the suggestions and refinements made to the agent's implementation to enhance performance or effectiveness<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"INTERESTINGNESS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Interestingness is the assessment of whether the proposed architecture is innovative compared to existing methods<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"IMPLEMENTATION MISTAKES\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Implementation Mistakes refer to errors identified in the code during the self-reflection process<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"REFLECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Reflection is the process of reviewing the proposed architecture, identifying mistakes, and suggesting improvements<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"RESPONSE\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Response refers to the meta agent's output after performing self-reflection and making improvements<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"ERROR DURING EVALUATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Error during evaluation refers to the runtime errors encountered and the subsequent debugging process<\/data>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"APPENDIX E\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"FORWARD FUNCTION\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <node id=\"THOUGHT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/node>    <edge source=\"APPENDIX C\" target=\"APPENDIX D\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both Appendix C and Appendix D are sections in the document that contain additional information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX C\" target=\"APPENDIX E\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both Appendix E and Appendix C are sections in the document that contain additional information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX C\" target=\"MAIN TEXT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Appendix C provides additional information relevant to the main text<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX D\" target=\"APPENDIX E\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both Appendix E and Appendix D are sections in the document that contain additional information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX D\" target=\"MAIN TEXT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Appendix D provides additional information relevant to the main text<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX B\" target=\"FRAMEWORK CODE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Appendix B contains the framework code<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX B\" target=\"APPENDIX E\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both Appendix E and Appendix B are sections in the document that contain additional information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"APPENDIX B\" target=\"MAIN TEXT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Appendix B provides additional information relevant to the main text<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"PROMPT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The meta agent uses the prompt to guide its output and improve the quality of the generated code<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"SELF-REFLECTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent performs self-reflection to review and improve its generated code<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"FOUNDATION MODELS (FMS)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent queries Foundation Models (FMs) to perform tasks and format prompts<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"AGENTIC SYSTEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent is part of the automated design of agentic systems<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"RUNTIME ERROR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent performs self-reflection and debugging when a runtime error is encountered<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"PROJECT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent is part of the project of designing and implementing agentic systems<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"IMPLEMENTATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent is responsible for the implementation of agent designs<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"IMPROVEMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent suggests improvements to the agent's implementation<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"RESPONSE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The response is the meta agent's output after performing self-reflection and making improvements<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"ERROR DURING EVALUATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The meta agent performs self-reflection and debugging when an error is encountered during evaluation<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"MADAAN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Madaan contributed to the concept of self-reflection in 2024<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"SHINN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Shinn contributed to the concept of self-reflection in 2023<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"INTERESTINGNESS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Interestingness is assessed during the self-reflection process<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"IMPLEMENTATION MISTAKES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Implementation Mistakes are identified during the self-reflection process<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"REFLECTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Reflection is part of the self-reflection process<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"NAMEDTUPLE INFO OBJECT\" target=\"FRAMEWORK CODE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The namedtuple Info object is used in the framework code to encapsulate and combine different types of information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"FRAMEWORK CODE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The FM module is part of the framework code that constructs prompts by concatenating input Info objects<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"FRAMEWORK CODE\" target=\"INFO OBJECT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Info object is used in the framework code to encapsulate and combine different types of information<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"FIGURE 3A\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Figure 3a shows the results and analysis of Meta Agent Search<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"ARCHIVE\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The archive is used for comparison and improvement in Meta Agent Search<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"TASKINFO\" target=\"FORWARD FUNCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TaskInfo is a parameter used in the \"forward()\" function in the Python code for agent implementation<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"RUNTIME ERROR\" target=\"DEBUG_THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Debug_thought is used to provide the meta agent's thinking for debugging the current code after a runtime error<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"MAIN TEXT\" target=\"TERMINOLOGIES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Terminologies used in the main text are matched in the code throughout the appendix<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"INSIGHTS\" target=\"THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Insights are captured in the \"thought\" section of the meta agent's output<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>    <edge source=\"OVERALL IDEA\" target=\"THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Overall Idea is described in the \"thought\" section of the meta agent's output<\/data>      <data key=\"d5\">282313a8340c6792e8c35f53ed157cd0<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"d66dc9ce4a9545b44f7486ea057b5937","chunk":"\ninformation (e.g., FM responses, results from tool function calls, task descriptions) and facilitate\ncommunicationbetweendifferentmodules. Additionally,intheFMmodule,weautomaticallyconstruct\nthe prompt by concatenating all input Info objects into a structured format, with each Info titled by\nits metadata (e.g., name, author). Throughout the appendix, we renamed some variables in the\ncode to match the terminologies used in the main text. The full framework code is available at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nCode 1|The simple framework used in Meta-Agent Search.\n1# Named tuple for holding task information\n2Info = namedtuple (\u2019Info \u2019, [\u2019name \u2019, \u2019author \u2019, \u2019content \u2019, \u2019\niteration_idx \u2019])\n3\n4# Format instructions for FM response\n5FORMAT_INST = lambda request_keys : f\" Reply EXACTLY with the\nfollowing JSON format .\\n{str( request_keys )}\\ nDO NOT MISS ANY\nFIELDS AND MAKE SURE THE JSON FORMAT IS CORRECT !\\n\"\n6\n7# Description of the role of the FM Module\n8ROLE_DESC = lambda role : f\"You are a { role }.\"\n9\n10@backoff . on_exception ( backoff .expo , openai . RateLimitError )\n11def get_json_response_from_gpt (msg , model , system_message ,\ntemperature ):\n12 \\\"\"\"\n13 Function to get JSON response from GPT model .\n14\n15 Args :\n16 - msg (str ): The user message .\n26Automated Design of Agentic Systems\n17 - model (str ): The model to use .\n18 - system_message (str ): The system message .\n19 - temperature ( float ): Sampling temperature .\n20\n21 Returns :\n22 - dict : The JSON response .\n23 \\\"\"\"\n24 ...\n25 return json_dict\n26\n27class FM_Module :\n28 \\\"\"\"\n29 Base class for an FM module .\n30\n31 Attributes :\n32 - output_fields ( list ): Fields expected in the output .\n33 - name (str ): Name of the FM module .\n34 - role (str ): Role description for the FM module .\n35 - model (str ): Model to be used .\n36 - temperature ( float ): Sampling temperature .\n37 - id (str ): Unique identifier for the FM module instance .\n38 \\\"\"\"\n39\n40 def __init__ (self , output_fields : list , name : str , role =\u2019helpful\nassistant \u2019, model =\u2019gpt -3.5 - turbo -0125 \u2019, temperature =0.5) ->\nNone :\n41 ...\n42\n43 def generate_prompt (self , input_infos , instruction ) -> str:\n44 \\\"\"\"\n45 Generates a prompt for the FM.\n46\n47 Args :\n48 - input_infos ( list ): List of input information .\n49 - instruction (str ): Instruction for the task .\n50\n51 Returns :\n52 - tuple : System prompt and user prompt .\n53\n54 An example of generated prompt :\n55 \"\"\n56 You are a helpful assistant .\n57\n58 # Output Format :\n59 Reply EXACTLY with the following JSON format .\n60 ...\n61\n62 # Your Task :\n63 You will given some number of paired example inputs and\noutputs . The outputs ...\n64\n65 ### thinking #1 by Chain -of - Thought hkFo ( yourself ):\n66 ...\n67\n68 # Instruction :\n69 Please think step by step and then solve the task by writing\n27Automated Design of Agentic Systems\nthe code .\n70 \"\"\n71 \\\"\"\"\n72 ...\n73 return system_prompt , prompt\n74\n75 def query (self , input_infos : list , instruction , iteration_idx\n= -1) -> list [ Info ]:\n76 \\\"\"\"\n77 Queries the FM with provided input information and\ninstruction .\n78\n79 Args :\n80 - input_infos ( list ): List of input information .\n81 - instruction (str ): Instruction for the task .\n82 - iteration_idx (int ): Iteration index for the task .\n83\n84 Returns :\n85 - output_infos ( list [ Info ]): Output information .\n86 \\\"\"\"\n87 ...\n88 return output_infos\n89\n90 def __repr__ ( self ):\n91 return f\"{ self . agent_name } { self .id}\"\n92\n93 def __call__ (self , input_infos : list , instruction , iteration_idx\n= -1):\n94 return self . query ( input_infos , instruction , iteration_idx =\niteration_idx )\n95\n96class AgentSystem :\n97 def forward (self , taskInfo ) -> Union [Info , str ]:\n98 \\\"\"\"\n99 Placeholder method for processing task information .\n100\n101 Args :\n102 - taskInfo ( Info ): Task information .\n103\n104 Returns :\n105 - Answer ( Union [Info , str ]): Your FINAL Answer . Return\neither a namedtuple Info or a string for the answer .\n106 \\\"\"\"\n107 pass\nWith the provided framework, an agent can be easily defined with a \u201cforward\u201d function. Here we\nshow an example of implementing self-reflection using the framework.\nCode 2|Self-Reflection implementation example\n1def forward (self , taskInfo ):\n2 # Instruction for initial reasoning\n3 cot_initial_instruction = \" Please think step by step and then\nsolve the task .\"\n4\n5 # Instruction for reflecting on previous attempts and feedback\n28Automated Design of Agentic Systems\nto improve\n6 cot_reflect_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n7 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\n\u2019)\n8\n9 # Instruction for providing feedback and correcting the answer\n10 critic_instruction = \" Please review the answer above and\ncriticize","chunk_id":"d66dc9ce4a9545b44f7486ea057b5937","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"FM MODULE","type":"TECHNOLOGY","description":"The FM Module is a component that automatically constructs prompts by concatenating all input Info objects into a structured format, with each Info titled by its metadata","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"INFO","type":"DATA STRUCTURE","description":"Info is a named tuple used for holding task information, including attributes like name, author, content, and iteration index","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses various baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is the author who has made the full framework code available on GitHub","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"GITHUB","type":"PLATFORM","description":"GitHub is the platform where the full framework code for the project is available","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"ADAS","type":"PROJECT","description":"ADAS stands for Automated Design of Agentic Systems, and it is the project for which the framework code is provided","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"FORMAT_INST","type":"FUNCTION","description":"FORMAT_INST is a lambda function that formats instructions for FM responses to ensure the JSON format is correct","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"ROLE_DESC","type":"FUNCTION","description":"ROLE_DESC is a lambda function that describes the role of the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"GET_JSON_RESPONSE_FROM_GPT","type":"FUNCTION","description":"get_json_response_from_gpt is a function to get a JSON response from a GPT model, handling rate limit errors using backoff","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"FM_MODULE","type":"CLASS","description":"FM_Module is a base class for an FM module, with attributes like output fields, name, role, model, temperature, and a unique identifier","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"AGENT SYSTEM","type":"CLASS","description":"AgentSystem is a class with a forward method for processing task information and returning either a namedtuple Info or a string as the answer","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"SELF-REFLECTION","type":"TECHNIQUE","description":"Self-Reflection is a technique implemented using the framework to improve task performance by reflecting on previous attempts and feedback","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNIQUE","description":"Chain-of-Thought is a technique used in the FM Module for step-by-step reasoning to solve tasks","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"COT_INITIAL_INSTRUCTION","type":"INSTRUCTION","description":"cot_initial_instruction is an instruction for initial reasoning, asking the agent to think step by step to solve the task","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"COT_REFLECT_INSTRUCTION","type":"INSTRUCTION","description":"cot_reflect_instruction is an instruction for reflecting on previous attempts and feedback to improve task performance","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"CRITIC_INSTRUCTION","type":"INSTRUCTION","description":"critic_instruction is an instruction for providing feedback and correcting the answer","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"CODE 1","type":"DOCUMENT","description":"Code 1 is the simple framework used in Meta-Agent Search, including definitions for named tuples, lambda functions, and the FM_Module class","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"CODE 2","type":"DOCUMENT","description":"Code 2 is an example of implementing self-reflection using the provided framework","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization associated with the GPT model used in the get_json_response_from_gpt function","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"TASK DESCRIPTIONS","type":"DATA STRUCTURE","description":"Task descriptions are pieces of information that describe the tasks to be performed by the system","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"FM RESPONSES","type":"DATA STRUCTURE","description":"FM responses are the results generated by the FM Module in response to the input information and instructions","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"TOOL FUNCTION CALLS","type":"DATA STRUCTURE","description":"Tool function calls are invocations of specific functions within the system to perform certain tasks","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"PROMPT","type":"DATA STRUCTURE","description":"The prompt is a structured format constructed by concatenating all input Info objects, each titled by its metadata","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"APPENDIX","type":"DOCUMENT","description":"The appendix contains additional details and renamed variables to match the terminologies used in the main text","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"FORWARD FUNCTION","type":"FUNCTION","description":"The forward function is a method in the AgentSystem class for processing task information and returning either a namedtuple Info or a string as the answer","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"BACKOFF","type":"TECHNOLOGY","description":"Backoff is a technique used to handle rate limit errors by retrying the function call with exponential backoff","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"RATE LIMIT ERROR","type":"DATA STRUCTURE","description":"Rate Limit Error is an error encountered when the rate limit for API calls is exceeded","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"SYSTEM PROMPT","type":"DATA STRUCTURE","description":"The system prompt is part of the generated prompt that provides context and instructions for the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"USER PROMPT","type":"DATA STRUCTURE","description":"The user prompt is part of the generated prompt that includes the task instructions for the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"OUTPUT INFOS","type":"DATA STRUCTURE","description":"Output Infos are the results generated by the FM Module in response to the input information and instructions","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"INSTRUCTION","type":"DATA STRUCTURE","description":"Instruction is a piece of information that guides the FM Module on how to perform a task","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"NAMED TUPLE","type":"DATA STRUCTURE","description":"Named tuple is a data structure used to hold task information in a structured format","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"JSON RESPONSE","type":"DATA STRUCTURE","description":"JSON response is the output generated by the GPT model in response to a query, formatted as JSON","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"MODEL","type":"DATA STRUCTURE","description":"Model refers to the specific version of the GPT model used in the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"TEMPERATURE","type":"DATA STRUCTURE","description":"Temperature is a parameter that controls the randomness of the GPT model's output","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"UNIQUE IDENTIFIER","type":"DATA STRUCTURE","description":"Unique identifier is a string that uniquely identifies an instance of the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"OUTPUT FIELDS","type":"DATA STRUCTURE","description":"Output fields are the fields expected in the output generated by the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"ROLE","type":"DATA STRUCTURE","description":"Role is a description of the function or position of the FM Module","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"TASK INFO","type":"DATA STRUCTURE","description":"Task Info is a named tuple containing information about a task, including attributes like name, author, content, and iteration index","source_id":"d66dc9ce4a9545b44f7486ea057b5937"},{"name":"COT MODULE","type":"CLASS","description":"cot_module is an instance of the FM_Module class used for Chain-of-Thought reasoning","source_id":"d66dc9ce4a9545b44f7486ea057b5937"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"FM MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The FM Module is a component that automatically constructs prompts by concatenating all input Info objects into a structured format, with each Info titled by its metadata<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"INFO\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Info is a named tuple used for holding task information, including attributes like name, author, content, and iteration index<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses various baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is the author who has made the full framework code available on GitHub<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"GITHUB\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">GitHub is the platform where the full framework code for the project is available<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">PROJECT<\/data>      <data key=\"d1\">ADAS stands for Automated Design of Agentic Systems, and it is the project for which the framework code is provided<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"FORMAT_INST\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">FORMAT_INST is a lambda function that formats instructions for FM responses to ensure the JSON format is correct<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"ROLE_DESC\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">ROLE_DESC is a lambda function that describes the role of the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"GET_JSON_RESPONSE_FROM_GPT\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">get_json_response_from_gpt is a function to get a JSON response from a GPT model, handling rate limit errors using backoff<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">CLASS<\/data>      <data key=\"d1\">FM_Module is a base class for an FM module, with attributes like output fields, name, role, model, temperature, and a unique identifier<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"AGENT SYSTEM\">      <data key=\"d0\">CLASS<\/data>      <data key=\"d1\">AgentSystem is a class with a forward method for processing task information and returning either a namedtuple Info or a string as the answer<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"SELF-REFLECTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Reflection is a technique implemented using the framework to improve task performance by reflecting on previous attempts and feedback<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought is a technique used in the FM Module for step-by-step reasoning to solve tasks<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"COT_INITIAL_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">cot_initial_instruction is an instruction for initial reasoning, asking the agent to think step by step to solve the task<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"COT_REFLECT_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">cot_reflect_instruction is an instruction for reflecting on previous attempts and feedback to improve task performance<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"CRITIC_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">critic_instruction is an instruction for providing feedback and correcting the answer<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"CODE 1\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Code 1 is the simple framework used in Meta-Agent Search, including definitions for named tuples, lambda functions, and the FM_Module class<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"CODE 2\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Code 2 is an example of implementing self-reflection using the provided framework<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization associated with the GPT model used in the get_json_response_from_gpt function<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"TASK DESCRIPTIONS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Task descriptions are pieces of information that describe the tasks to be performed by the system<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"FM RESPONSES\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">FM responses are the results generated by the FM Module in response to the input information and instructions<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"TOOL FUNCTION CALLS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Tool function calls are invocations of specific functions within the system to perform certain tasks<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The prompt is a structured format constructed by concatenating all input Info objects, each titled by its metadata<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"APPENDIX\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The appendix contains additional details and renamed variables to match the terminologies used in the main text<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"FORWARD FUNCTION\">      <data key=\"d0\">FUNCTION<\/data>      <data key=\"d1\">The forward function is a method in the AgentSystem class for processing task information and returning either a namedtuple Info or a string as the answer<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"BACKOFF\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Backoff is a technique used to handle rate limit errors by retrying the function call with exponential backoff<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"RATE LIMIT ERROR\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Rate Limit Error is an error encountered when the rate limit for API calls is exceeded<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"SYSTEM PROMPT\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The system prompt is part of the generated prompt that provides context and instructions for the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"USER PROMPT\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">The user prompt is part of the generated prompt that includes the task instructions for the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"OUTPUT INFOS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Output Infos are the results generated by the FM Module in response to the input information and instructions<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"INSTRUCTION\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Instruction is a piece of information that guides the FM Module on how to perform a task<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"NAMED TUPLE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Named tuple is a data structure used to hold task information in a structured format<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"JSON RESPONSE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">JSON response is the output generated by the GPT model in response to a query, formatted as JSON<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Model refers to the specific version of the GPT model used in the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"TEMPERATURE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Temperature is a parameter that controls the randomness of the GPT model's output<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"UNIQUE IDENTIFIER\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Unique identifier is a string that uniquely identifies an instance of the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"OUTPUT FIELDS\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Output fields are the fields expected in the output generated by the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"ROLE\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Role is a description of the function or position of the FM Module<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"TASK INFO\">      <data key=\"d0\">DATA STRUCTURE<\/data>      <data key=\"d1\">Task Info is a named tuple containing information about a task, including attributes like name, author, content, and iteration index<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <node id=\"COT MODULE\">      <data key=\"d0\">CLASS<\/data>      <data key=\"d1\">cot_module is an instance of the FM_Module class used for Chain-of-Thought reasoning<\/data>      <data key=\"d2\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/node>    <edge source=\"FM MODULE\" target=\"INFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM Module constructs prompts by concatenating all input Info objects<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"TASK DESCRIPTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM Module facilitates communication between different modules by using task descriptions<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"FM RESPONSES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM Module generates FM responses based on the input information and instructions<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"TOOL FUNCTION CALLS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM Module uses tool function calls to perform specific tasks<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"PROMPT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM Module constructs the prompt by concatenating all input Info objects<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"OUTPUT INFOS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Output Infos are the results generated by the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction guides the FM Module on how to perform a task<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Model refers to the specific version of the GPT model used in the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"TEMPERATURE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Temperature is a parameter that controls the randomness of the GPT model's output in the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"UNIQUE IDENTIFIER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Unique identifier is a string that uniquely identifies an instance of the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"OUTPUT FIELDS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Output fields are the fields expected in the output generated by the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM MODULE\" target=\"ROLE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Role is a description of the function or position of the FM Module<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"INFO\" target=\"NAMED TUPLE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Info is a named tuple used for holding task information<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"INFO\" target=\"TASK INFO\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Task Info is a named tuple containing information about a task<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"GITHUB\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Shengran Hu has made the full framework code available on GitHub<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"GITHUB\" target=\"ADAS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The full framework code for ADAS is available on GitHub<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FORMAT_INST\" target=\"FM_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM_Module uses the FORMAT_INST lambda function to format instructions for FM responses<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"ROLE_DESC\" target=\"FM_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM_Module uses the ROLE_DESC lambda function to describe its role<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"GET_JSON_RESPONSE_FROM_GPT\" target=\"FM_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The FM_Module can use the get_json_response_from_gpt function to get JSON responses from a GPT model<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"GET_JSON_RESPONSE_FROM_GPT\" target=\"OPENAI\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The get_json_response_from_gpt function uses a GPT model associated with OpenAI<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"GET_JSON_RESPONSE_FROM_GPT\" target=\"JSON RESPONSE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The get_json_response_from_gpt function generates a JSON response<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The FM_Module uses the Chain-of-Thought technique for step-by-step reasoning<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"CODE 1\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Code 1 includes the definition of the FM_Module class<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"AGENT SYSTEM\" target=\"FORWARD FUNCTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The forward function is a method in the AgentSystem class<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Self-Reflection implementation example uses the Chain-of-Thought technique<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"COT_INITIAL_INSTRUCTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Reflection implementation example includes the cot_initial_instruction for initial reasoning<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"COT_REFLECT_INSTRUCTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Reflection implementation example includes the cot_reflect_instruction for reflecting on previous attempts and feedback<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"CRITIC_INSTRUCTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Self-Reflection implementation example includes the critic_instruction for providing feedback and correcting the answer<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"SELF-REFLECTION\" target=\"CODE 2\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Code 2 is an example of implementing self-reflection using the provided framework<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT\" target=\"COT MODULE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">cot_module is an instance of the FM_Module class used for Chain-of-Thought reasoning<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"APPENDIX\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">The appendix contains details about how the prompt is constructed<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"SYSTEM PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The system prompt is part of the generated prompt<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"USER PROMPT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user prompt is part of the generated prompt<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>    <edge source=\"BACKOFF\" target=\"RATE LIMIT ERROR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Backoff is used to handle Rate Limit Errors by retrying the function call<\/data>      <data key=\"d5\">d66dc9ce4a9545b44f7486ea057b5937<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"4b43decac6833d1515992f8869ecada7","chunk":"entic Systems\nto improve\n6 cot_reflect_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n7 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\n\u2019)\n8\n9 # Instruction for providing feedback and correcting the answer\n10 critic_instruction = \" Please review the answer above and\ncriticize on where might be wrong . If you are absolutely sure\nit is correct , output \u2019True \u2019 in \u2019correct \u2019.\"\n11 critic_module = FM_Module ([ \u2019feedback \u2019, \u2019correct \u2019], \u2019Critic \u2019)\n12\n13 N_max = 5 # Maximum number of attempts\n14\n15 # Initial attempt\n16 cot_inputs = [ taskInfo ]\n17 thinking , answer = cot_module ( cot_inputs ,\ncot_initial_instruction , 0)\n18\n19 for i in range ( N_max ):\n20 # Get feedback and correct status from the critic\n21 feedback , correct = critic_module ([ taskInfo , thinking ,\nanswer ], critic_instruction , i)\n22 if correct . content == \u2019True \u2019:\n23 break\n24\n25 # Add feedback to the inputs for the next iteration\n26 cot_inputs . extend ([ thinking , answer , feedback ])\n27\n28 # Reflect on previous attemps and refine the answer\n29 thinking , answer = cot_module ( cot_inputs ,\ncot_reflect_instruction , i + 1)\n30 return answer\n29Automated Design of Agentic Systems\nExample Input -output grid #1\nExample Input -output grid #2\nTest grid\nAnswer\nFigure 4|An example task from the ARC challenge (Chollet, 2019). Given the input-output grid\nexamples, the AI system is asked to learn the transformation rules and then apply these learned rules\nto the test grid to predict the final answer.\nC. Experiment Details for ARC Challenge\nAn example task from the ARC challenge is shown in Figure 4. In the ARC challenge experiments\n(Section 4.1), we represent the grids as strings of 2-D arrays, where each color is represented by an\ninteger. Weinstructthemetaagenttodesignagentsthatgeneratecodeassolutionsratherthandirectly\noutputting answers. Additionally, we provide two tool functions within the framework: (1) to test\nwhetherthegeneratedcodecansolvetheexamplegridsand(2)toobtainthetask\u2019sanswerbyapplying\nthe generated code to the test grid. The accuracy rate is calculated by the Exact Match between the\nreference solution and the predicted answer. The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI,\n2024), while discovered agents and baselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI,\n2022) to reduce compute cost.\nThe domain description of ARC for the meta agent is shown below:\nDescription of ARC for the meta agent.\nYour aim is to find an optimal agent performing well on the ARC (Abstraction and Reasoning Corpus)\nchallenge.\nIn this challenge, each task consists of three demonstration examples, and one test example. Each\nExample consists of an \u201cinput grid\u201d and an \u201coutput grid\u201d. Test-takers need to use the transformation rule\nlearned from the examples to predict the output grid for the test example.\n30Automated Design of Agentic Systems\n# An example task from ARC challenge:\n## Task Overview:\nYou will be given some number of paired example inputs and outputs grids. The outputs were produced\nby applying a transformation rule to the input grids. In addition to the paired example inputs and\noutputs, there is also one test input without a known output.\nThe inputs and outputs are each \u201cgrids\u201d. A grid is a rectangular matrix of integers between 0 and 9\n(inclusive). Each number corresponds to a color. 0 is black.\nYour task is to determine the transformation rule from examples and find out the answer, involving\ndetermining the size of the output grid for the test and correctly filling each cell of the grid with the\nappropriate color or number.\nThe transformation only needs to be unambiguous and applicable to the example inputs and the test\ninput. It doesn\u2019t need to work for all possible inputs. Observe the examples carefully, imagine the grid\nvisually, and try to find the pattern.\n## Examples:\n### Example 0:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,0,5,0,0,0,0], [0,0,0,4,5,0,0,0,0], [0,0,0,4,5,4,4,0,0],\n[0,0,3,3,5,0,0,0,0], [0,0,0,3,5,0,0,0,0], [0,0,0,3,5,3,3,3,0], [0,0,0,3,5,0,0,0,0], [0,0,0,0,5,0,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,0], [0,0,0,4], [0,0,4,4], [0,0,3,3], [0,0,0,3], [0,3,3,3], [0,0,0,3], [0,0","chunk_id":"4b43decac6833d1515992f8869ecada7","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"COT_MODULE","type":"TECHNOLOGY","description":"A module named FM_Module that handles 'thinking' and 'answer' processes, referred to as 'Chain-of-Thought'","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CRITIC_INSTRUCTION","type":"INSTRUCTION","description":"The instruction for providing feedback and correcting the answer, which involves reviewing the answer and criticizing where it might be wrong, or confirming it is correct by outputting 'True' in 'correct'","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CRITIC_MODULE","type":"TECHNOLOGY","description":"A module named FM_Module that handles 'feedback' and 'correct' processes, referred to as 'Critic'","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"N_MAX","type":"PARAMETER","description":"The maximum number of attempts allowed, set to 5","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"COT_INPUTS","type":"DATA","description":"The initial inputs for the Chain-of-Thought module, which include task information","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TASKINFO","type":"DATA","description":"Information about the task that is used as input for the Chain-of-Thought module","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"THINKING","type":"DATA","description":"The thought process generated by the Chain-of-Thought module","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"ANSWER","type":"DATA","description":"The answer generated by the Chain-of-Thought module","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"FEEDBACK","type":"DATA","description":"The feedback provided by the Critic module","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CORRECT","type":"DATA","description":"The correctness status provided by the Critic module, which can be 'True' if the answer is correct","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"ARC CHALLENGE","type":"EVENT","description":"A challenge where the AI system is asked to learn transformation rules from input-output grid examples and apply these rules to predict the final answer for a test grid","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"META AGENT","type":"TECHNOLOGY","description":"An agent designed to generate code as solutions for the ARC challenge rather than directly outputting answers","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GPT-4O-2024-05-13","type":"TECHNOLOGY","description":"A version of OpenAI's GPT-4 model used by the meta agent in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"A version of OpenAI's GPT-3.5 model used to evaluate discovered agents and baselines in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXACT MATCH","type":"METRIC","description":"The accuracy rate calculated by comparing the reference solution and the predicted answer in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"INPUT GRID","type":"DATA","description":"A rectangular matrix of integers between 0 and 9 representing colors, used as input in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"OUTPUT GRID","type":"DATA","description":"A rectangular matrix of integers between 0 and 9 representing colors, used as output in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TRANSFORMATION RULE","type":"CONCEPT","description":"The rule learned from input-output grid examples to predict the output grid for the test example in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXAMPLE 0","type":"EXAMPLE","description":"An example task from the ARC challenge with specific input and output grids","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"FM_MODULE","type":"TECHNOLOGY","description":"FM_Module is a module used for different processes such as 'thinking', 'answer', 'feedback', and 'correct'","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CHAIN-OF-THOUGHT","type":"TECHNOLOGY","description":"Chain-of-Thought is a process handled by the FM_Module that involves generating a thought process and an answer","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"CRITIC","type":"TECHNOLOGY","description":"Critic is a process handled by the FM_Module that involves providing feedback and correctness status","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"ARC","type":"EVENT","description":"ARC (Abstraction and Reasoning Corpus) is a challenge where the AI system learns transformation rules from input-output grid examples to predict the final answer for a test grid","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that developed the GPT-4 and GPT-3.5 models used in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a version of OpenAI's language model used by the meta agent in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"GPT-3.5","type":"TECHNOLOGY","description":"GPT-3.5 is a version of OpenAI's language model used to evaluate discovered agents and baselines in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXAMPLE INPUT-OUTPUT GRID #1","type":"EXAMPLE","description":"An example input-output grid used in the ARC challenge to demonstrate the transformation rules","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXAMPLE INPUT-OUTPUT GRID #2","type":"EXAMPLE","description":"Another example input-output grid used in the ARC challenge to demonstrate the transformation rules","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TEST GRID","type":"DATA","description":"The grid used in the ARC challenge to test the AI system's ability to apply learned transformation rules to predict the final answer","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXPERIMENT DETAILS FOR ARC CHALLENGE","type":"DOCUMENT","description":"A document providing detailed information about the experiments conducted for the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"TASK OVERVIEW","type":"DOCUMENT","description":"A document providing an overview of the tasks involved in the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"},{"name":"EXAMPLES","type":"DOCUMENT","description":"A document providing examples of tasks from the ARC challenge","source_id":"4b43decac6833d1515992f8869ecada7"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"COT_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A module named FM_Module that handles 'thinking' and 'answer' processes, referred to as 'Chain-of-Thought'<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CRITIC_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">The instruction for providing feedback and correcting the answer, which involves reviewing the answer and criticizing where it might be wrong, or confirming it is correct by outputting 'True' in 'correct'<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CRITIC_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A module named FM_Module that handles 'feedback' and 'correct' processes, referred to as 'Critic'<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"N_MAX\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">The maximum number of attempts allowed, set to 5<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"COT_INPUTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The initial inputs for the Chain-of-Thought module, which include task information<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Information about the task that is used as input for the Chain-of-Thought module<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"THINKING\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The thought process generated by the Chain-of-Thought module<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The answer generated by the Chain-of-Thought module<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The feedback provided by the Critic module<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CORRECT\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The correctness status provided by the Critic module, which can be 'True' if the answer is correct<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"ARC CHALLENGE\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">A challenge where the AI system is asked to learn transformation rules from input-output grid examples and apply these rules to predict the final answer for a test grid<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">An agent designed to generate code as solutions for the ARC challenge rather than directly outputting answers<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GPT-4O-2024-05-13\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A version of OpenAI's GPT-4 model used by the meta agent in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A version of OpenAI's GPT-3.5 model used to evaluate discovered agents and baselines in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXACT MATCH\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The accuracy rate calculated by comparing the reference solution and the predicted answer in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"INPUT GRID\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A rectangular matrix of integers between 0 and 9 representing colors, used as input in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"OUTPUT GRID\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A rectangular matrix of integers between 0 and 9 representing colors, used as output in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TRANSFORMATION RULE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The rule learned from input-output grid examples to predict the output grid for the test example in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXAMPLE 0\">      <data key=\"d0\">EXAMPLE<\/data>      <data key=\"d1\">An example task from the ARC challenge with specific input and output grids<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM_Module is a module used for different processes such as 'thinking', 'answer', 'feedback', and 'correct'<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Chain-of-Thought is a process handled by the FM_Module that involves generating a thought process and an answer<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"CRITIC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Critic is a process handled by the FM_Module that involves providing feedback and correctness status<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">ARC (Abstraction and Reasoning Corpus) is a challenge where the AI system learns transformation rules from input-output grid examples to predict the final answer for a test grid<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that developed the GPT-4 and GPT-3.5 models used in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a version of OpenAI's language model used by the meta agent in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5 is a version of OpenAI's language model used to evaluate discovered agents and baselines in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXAMPLE INPUT-OUTPUT GRID #1\">      <data key=\"d0\">EXAMPLE<\/data>      <data key=\"d1\">An example input-output grid used in the ARC challenge to demonstrate the transformation rules<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXAMPLE INPUT-OUTPUT GRID #2\">      <data key=\"d0\">EXAMPLE<\/data>      <data key=\"d1\">Another example input-output grid used in the ARC challenge to demonstrate the transformation rules<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TEST GRID\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">The grid used in the ARC challenge to test the AI system's ability to apply learned transformation rules to predict the final answer<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXPERIMENT DETAILS FOR ARC CHALLENGE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A document providing detailed information about the experiments conducted for the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"TASK OVERVIEW\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A document providing an overview of the tasks involved in the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <node id=\"EXAMPLES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">A document providing examples of tasks from the ARC challenge<\/data>      <data key=\"d2\">4b43decac6833d1515992f8869ecada7<\/data>    <\/node>    <edge source=\"COT_MODULE\" target=\"THINKING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Chain-of-Thought module generates the thought process<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"COT_MODULE\" target=\"ANSWER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Chain-of-Thought module generates the answer<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_INSTRUCTION\" target=\"CRITIC_MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Critic module uses the critic instruction to provide feedback and correctness status<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_MODULE\" target=\"FEEDBACK\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Critic module provides feedback<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"CRITIC_MODULE\" target=\"CORRECT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Critic module provides the correctness status<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"COT_INPUTS\" target=\"TASKINFO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Task information is part of the initial inputs for the Chain-of-Thought module<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"META AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent is designed to generate code solutions for the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The GPT-3.5-turbo-0125 model is used to evaluate discovered agents and baselines in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXACT MATCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Exact Match is the metric used to calculate the accuracy rate in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"INPUT GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Input grids are used as part of the tasks in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"OUTPUT GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Output grids are used as part of the tasks in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"TRANSFORMATION RULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The transformation rule is learned from input-output grid examples to predict the output grid for the test example in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXAMPLE 0\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Example 0 is a specific task from the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"ARC\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ARC is the full name of the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXAMPLE INPUT-OUTPUT GRID #1\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Example Input-Output Grid #1 is used in the ARC challenge to demonstrate transformation rules<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXAMPLE INPUT-OUTPUT GRID #2\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Example Input-Output Grid #2 is used in the ARC challenge to demonstrate transformation rules<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"TEST GRID\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Test Grid is used in the ARC challenge to test the AI system's ability to apply learned transformation rules<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXPERIMENT DETAILS FOR ARC CHALLENGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Experiment Details for ARC Challenge document provides detailed information about the experiments conducted for the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"TASK OVERVIEW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Task Overview document provides an overview of the tasks involved in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"ARC CHALLENGE\" target=\"EXAMPLES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Examples document provides examples of tasks from the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"GPT-4O-2024-05-13\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The meta agent uses the GPT-4o-2024-05-13 model<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"GPT-4O-2024-05-13\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4o-2024-05-13 is a specific version of the GPT-4 model<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"GPT-3.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-3.5-turbo-0125 is a specific version of the GPT-3.5 model<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"CHAIN-OF-THOUGHT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Chain-of-Thought is a process handled by the FM_Module<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"CRITIC\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Critic is a process handled by the FM_Module<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"OPENAI\" target=\"GPT-4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">OpenAI developed the GPT-4 model used in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>    <edge source=\"OPENAI\" target=\"GPT-3.5\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">OpenAI developed the GPT-3.5 model used in the ARC challenge<\/data>      <data key=\"d5\">4b43decac6833d1515992f8869ecada7<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"449db721e37968e073e3579b59e023b2","chunk":",0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,0], [0,0,0,4], [0,0,4,4], [0,0,3,3], [0,0,0,3], [0,3,3,3], [0,0,0,3], [0,0,0,0],\n[0,0,0,0]]\n### Example 1:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,2,5,0,0,0,0], [0,0,0,2,5,2,6,0,0], [0,0,0,2,5,0,0,0,0],\n[0,0,0,2,5,2,2,2,0], [0,0,6,6,5,6,0,0,0], [0,0,0,2,5,0,0,0,0], [0,2,2,0,5,2,0,0,0], [0,0,0,2,5,0,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput = [[0,0,0,0], [0,0,0,2], [0,0,6,2], [0,0,0,2], [0,2,2,2], [0,0,6,6], [0,0,0,2], [0,2,2,2], [0,0,0,2],\n[0,0,0,0]]\n### Example 2:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,0,5,7,0,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,8,5,0,0,0,0],\n[0,7,8,8,5,0,0,0,0], [0,0,0,0,5,8,8,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,8,5,0,0,0,0], [0,0,0,0,5,8,7,0,0],\n[0,0,0,0,5,0,0,0,0]]\noutput= [[0,0,0,0], [0,0,0,7], [0,0,0,8], [0,0,0,8], [0,7,8,8], [0,0,8,8], [0,0,0,8], [0,0,0,8], [0,0,7,8],\n[0,0,0,0]]\n### Test Problem:\ninput = [[0,0,0,0,5,0,0,0,0], [0,0,0,1,5,0,0,0,0], [0,0,0,1,5,1,0,0,0], [0,1,1,1,5,1,1,1,6],\n[0,0,0,6,5,6,6,0,0], [0,0,0,0,5,1,1,1,0], [0,0,0,1,5,0,0,0,0], [0,0,0,1,5,1,6,0,0], [0,0,0,0,5,6,0,0,0],\n[0,0,0,0,5,0,0,0,0]]\nAnalyze the transformation rules based on the provided Examples and determine what the output should\nbe for the Test Problem.\nHere we present the best agent on ARC discovered by Meta Agent Search. All agents from the\nexperiment can be found at https:\/\/github.com\/ShengranHu\/ADAS .\nCode 3|The best agent on ARC discovered by Meta Agent Search\n1# Structured Feedback and Ensemble Agent\n2def forward (self , taskInfo ):\n3 # Step 1: Generate initial candidate solutions using multiple FM\nModules\n31Automated Design of Agentic Systems\n4 initial_instruction = \u2019Please think step by step and then solve\nthe task by writing the code .\u2019\n5 num_candidates = 5 # Number of initial candidates\n6 initial_module = [ FM_Module ([ \u2019thinking \u2019, \u2019code \u2019], \u2019Initial\nSolution \u2019, temperature =0.8) for _ in range ( num_candidates )]\n7\n8 initial_solutions = []\n9 for i in range ( num_candidates ):\n10 thoughts = initial_module [i ]([ taskInfo ], initial_instruction\n)\n11 thinking , code = thoughts [0] , thoughts [1]\n12 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( code )\n13 if len ( correct_examples ) > 0: # Only consider solutions\nthat passed at least one example\n14 initial_solutions . append ({ \u2019thinking \u2019: thinking , \u2019code \u2019:\ncode , \u2019feedback \u2019: feedback , \u2019 correct_count \u2019: len (\ncorrect_examples )","chunk_id":"449db721e37968e073e3579b59e023b2","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"ARC","type":"DATASET","description":"ARC (Abstraction and Reasoning Corpus) is a dataset used to evaluate the performance of agents discovered by Meta Agent Search","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is associated with the GitHub repository where all agents from the experiment can be found","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"GITHUB","type":"PLATFORM","description":"GitHub is the platform where the repository containing all agents from the experiment is hosted","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"ADAS","type":"REPOSITORY","description":"ADAS is the GitHub repository where all agents from the experiment can be found","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"FM_MODULE","type":"TECHNOLOGY","description":"FM_Module is a module used to generate initial candidate solutions by thinking and writing code","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"INITIAL SOLUTION","type":"TECHNOLOGY","description":"Initial Solution is the output generated by the FM_Module based on the initial instruction","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"TASKINFO","type":"DATA","description":"TaskInfo is the input data provided to the FM_Module to generate initial candidate solutions","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"THINKING","type":"PROCESS","description":"Thinking is the process of generating thoughts as part of the initial candidate solutions","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"CODE","type":"PROCESS","description":"Code is the process of writing the solution as part of the initial candidate solutions","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"FEEDBACK","type":"PROCESS","description":"Feedback is the process of evaluating the generated code by running examples and getting feedback","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"CORRECT EXAMPLES","type":"DATA","description":"Correct Examples are the examples that the generated code passed during the feedback process","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"WRONG EXAMPLES","type":"DATA","description":"Wrong Examples are the examples that the generated code failed during the feedback process","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"EXAMPLE 2","type":"DATA","description":"Example 2 is another test case provided to analyze the transformation rules based on the provided examples","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"TEST PROBLEM","type":"DATA","description":"Test Problem is a test case provided to analyze the transformation rules based on the provided examples","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"RESULTS AND ANALYSIS","type":"PROCESS","description":"Results and Analysis is the section where the effectiveness of Meta Agent Search is discussed","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"FIGURE 3A","type":"DATA","description":"Figure 3a is a visual representation showing the effectiveness of Meta Agent Search","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"INITIAL INSTRUCTION","type":"DATA","description":"Initial Instruction is the instruction given to the FM_Module to generate initial candidate solutions","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"NUM_CANDIDATES","type":"DATA","description":"Num_Candidates is the number of initial candidates generated by the FM_Module","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"THOUGHTS","type":"DATA","description":"Thoughts are the intermediate outputs generated by the FM_Module during the initial candidate solution process","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"CORRECT_COUNT","type":"DATA","description":"Correct_Count is the number of correct examples passed by the generated code during the feedback process","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"EXAMPLE 1","type":"","description":"","source_id":"449db721e37968e073e3579b59e023b2"},{"name":"OUTPUT","type":"","description":"","source_id":"449db721e37968e073e3579b59e023b2"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ARC (Abstraction and Reasoning Corpus) is a dataset used to evaluate the performance of agents discovered by Meta Agent Search<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is associated with the GitHub repository where all agents from the experiment can be found<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"GITHUB\">      <data key=\"d0\">PLATFORM<\/data>      <data key=\"d1\">GitHub is the platform where the repository containing all agents from the experiment is hosted<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"ADAS\">      <data key=\"d0\">REPOSITORY<\/data>      <data key=\"d1\">ADAS is the GitHub repository where all agents from the experiment can be found<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM_Module is a module used to generate initial candidate solutions by thinking and writing code<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"INITIAL SOLUTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Initial Solution is the output generated by the FM_Module based on the initial instruction<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">TaskInfo is the input data provided to the FM_Module to generate initial candidate solutions<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"THINKING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Thinking is the process of generating thoughts as part of the initial candidate solutions<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"CODE\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Code is the process of writing the solution as part of the initial candidate solutions<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Feedback is the process of evaluating the generated code by running examples and getting feedback<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"CORRECT EXAMPLES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Correct Examples are the examples that the generated code passed during the feedback process<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"WRONG EXAMPLES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Wrong Examples are the examples that the generated code failed during the feedback process<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"EXAMPLE 2\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Example 2 is another test case provided to analyze the transformation rules based on the provided examples<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"TEST PROBLEM\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Test Problem is a test case provided to analyze the transformation rules based on the provided examples<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"RESULTS AND ANALYSIS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Results and Analysis is the section where the effectiveness of Meta Agent Search is discussed<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"FIGURE 3A\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Figure 3a is a visual representation showing the effectiveness of Meta Agent Search<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"INITIAL INSTRUCTION\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Initial Instruction is the instruction given to the FM_Module to generate initial candidate solutions<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"NUM_CANDIDATES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Num_Candidates is the number of initial candidates generated by the FM_Module<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"THOUGHTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Thoughts are the intermediate outputs generated by the FM_Module during the initial candidate solution process<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"CORRECT_COUNT\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Correct_Count is the number of correct examples passed by the generated code during the feedback process<\/data>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"EXAMPLE 1\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <node id=\"OUTPUT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">449db721e37968e073e3579b59e023b2<\/data>    <\/node>    <edge source=\"META AGENT SEARCH\" target=\"ARC\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search is used to discover the best agent on the ARC dataset<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"RESULTS AND ANALYSIS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Results and Analysis discusses the effectiveness of Meta Agent Search<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"SHENGRAN HU\" target=\"GITHUB\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shengran Hu is associated with the GitHub platform where the repository is hosted<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"GITHUB\" target=\"ADAS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">ADAS is the repository hosted on GitHub<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"INITIAL SOLUTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">FM_Module generates the Initial Solution based on the initial instruction<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"TASKINFO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">TaskInfo is the input data provided to the FM_Module<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"THINKING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thinking is part of the process generated by the FM_Module<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"CODE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Code is part of the process generated by the FM_Module<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"FEEDBACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Feedback is part of the process generated by the FM_Module<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"INITIAL INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Initial Instruction is given to the FM_Module to generate initial candidate solutions<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"NUM_CANDIDATES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Num_Candidates is the number of initial candidates generated by the FM_Module<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thoughts are generated by the FM_Module during the initial candidate solution process<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"CORRECT EXAMPLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Correct Examples are evaluated during the feedback process<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"WRONG EXAMPLES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Wrong Examples are evaluated during the feedback process<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"CORRECT_COUNT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Correct_Count is the number of correct examples passed by the generated code during the feedback process<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"EXAMPLE 2\" target=\"OUTPUT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Example 2 is used to generate a specific output based on the transformation rules<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"TEST PROBLEM\" target=\"OUTPUT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Test Problem is used to generate a specific output based on the transformation rules<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"RESULTS AND ANALYSIS\" target=\"FIGURE 3A\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Figure 3a is part of the Results and Analysis section<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>    <edge source=\"EXAMPLE 1\" target=\"OUTPUT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Example 1 is used to generate a specific output based on the transformation rules<\/data>      <data key=\"d5\">449db721e37968e073e3579b59e023b2<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"84317ae35cc75d612287186d93461447","chunk":" ]([ taskInfo ], initial_instruction\n)\n11 thinking , code = thoughts [0] , thoughts [1]\n12 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( code )\n13 if len ( correct_examples ) > 0: # Only consider solutions\nthat passed at least one example\n14 initial_solutions . append ({ \u2019thinking \u2019: thinking , \u2019code \u2019:\ncode , \u2019feedback \u2019: feedback , \u2019 correct_count \u2019: len (\ncorrect_examples )})\n15\n16 # Step 2: Simulate human - like feedback for each candidate\nsolution\n17 human_like_feedback_module = FM_Module ([ \u2019thinking \u2019, \u2019feedback \u2019],\n\u2019Human - like Feedback \u2019, temperature =0.5)\n18 human_feedback_instruction = \u2019Please provide human - like feedback\nfor the code , focusing on common mistakes , heuristic\ncorrections , and best practices .\u2019\n19\n20 for sol in initial_solutions :\n21 thoughts = human_like_feedback_module ([ taskInfo , sol[\u2019\nthinking \u2019], sol[\u2019code \u2019]], human_feedback_instruction )\n22 human_thinking , human_feedback = thoughts [0] , thoughts [1]\n23 sol [\u2019 human_feedback \u2019] = human_feedback\n24\n25 # Step 3: Assign expert advisors to evaluate and provide\ntargeted feedback\n26 expert_roles = [\u2019Efficiency Expert \u2019, \u2019 Readability Expert \u2019, \u2019\nSimplicity Expert \u2019]\n27 expert_advisors = [ FM_Module ([ \u2019thinking \u2019, \u2019feedback \u2019], role ,\ntemperature =0.6) for role in expert_roles ]\n28 expert_instruction = \u2019Please evaluate the given code and provide\ntargeted feedback for improvement .\u2019\n29\n30 for sol in initial_solutions :\n31 sol_feedback = {}\n32 for advisor in expert_advisors :\n33 thoughts = advisor ([ taskInfo , sol[\u2019thinking \u2019], sol[\u2019code\n\u2019]], expert_instruction )\n34 thinking , feedback = thoughts [0] , thoughts [1]\n35 sol_feedback [ advisor . role ] = feedback\n36 sol [\u2019 expert_feedback \u2019] = sol_feedback\n37\n38 # Step 4: Parse and structure the feedback to avoid redundancy\nand refine the solutions iteratively\n39 max_refinement_iterations = 3\n40 refinement_module = FM_Module ([ \u2019thinking \u2019, \u2019code \u2019], \u2019Refinement\nModule \u2019, temperature =0.5)\n32Automated Design of Agentic Systems\n41 refined_solutions = []\n42\n43 for sol in initial_solutions :\n44 for i in range ( max_refinement_iterations ):\n45 combined_feedback = sol[\u2019feedback \u2019]. content + sol [\u2019\nhuman_feedback \u2019]. content + \u2019\u2019. join ([ fb. content for fb\nin sol [\u2019 expert_feedback \u2019]. values () ])\n46 structured_feedback = \u2019 \u2019. join (set( combined_feedback .\nsplit ())) # Avoid redundancy\n47 refinement_instruction = \u2019Using the structured feedback ,\nrefine the solution to improve its performance .\u2019\n48 thoughts = refinement_module ([ taskInfo , sol[\u2019thinking \u2019],\nsol[\u2019code \u2019], Info (\u2019feedback \u2019, \u2019Structured Feedback \u2019,\nstructured_feedback , i)], refinement_instruction , i)\n49 refinement_thinking , refined_code = thoughts [0] ,\nthoughts [1]\n50 feedback , correct_examples , wrong_examples = self .\nrun_examples_and_get_feedback ( refined_code )\n51 if len ( correct_examples ) > 0:\n52 sol. update ({ \u2019thinking \u2019: refinement_thinking , \u2019code \u2019:\nrefined_code , \u2019feedback \u2019: feedback , \u2019\ncorrect_count \u2019: len( correct_examples )})\n53 refined_solutions . append ( sol)\n54\n55 # Step 5: Select the best - performing solutions and make a final\ndecision using an ensemble approach\n56 sorted_solutions = sorted ( refined_solutions , key= lambda x: x[\u2019\ncorrect_count \u2019], reverse = True )\n57 top_solutions = sorted_solutions [:3] # Select the top 3\nsolutions\n58\n59 final_decision_instruction = \u2019Given all the above solutions ,\nreason over them carefully and provide a final answer by\nwriting the code .\u2019\n60 final_decision_module = refinement_module ([ \u2019thinking \u2019, \u2019code \u2019],\n\u2019Final Decision Module \u2019, temperature =0.1)\n61 final_inputs = [ taskInfo ] + [ item for solution in top_solutions\nfor item in [ solution [\u2019thinking \u2019], solution [\u2019code \u2019], solution\n[\u2019feedback \u2019]]]\n62 final_thoughts = final_decision_module ( final_inputs ,\nfinal_decision_instruction )\n63 final_thinking , final_code = final_thoughts [0] , final_thoughts\n[1]\n64 answer = self . get_test_output_from_code ( final_code )\n65 return answer\nD. Experiment Details for Reasoning and Problem-Solving Domains\nTo reduce costs during search and evaluation, we sample subsets of data from each domain. For GPQA\n(Science), the validation set consists of 32 questions, while the remaining 166 questions form the\ntest set. For the other domains, the validation and test sets are sampled with 128 and 800 questions,\nrespectively. We evaluate agents five times for GPQA and once for the other domains to maintain a\nconsistent total number of evaluations. Each domain uses zero-shot style questions, except DROP\n(Reading Comprehension), which uses one-shot style questions following the practice in (OpenAI,\n33Automated Design of Agentic Systems\n2023). The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI, 2024), while discovered agents and\nbaselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI, 2022) to reduce compute cost","chunk_id":"84317ae35cc75d612287186d93461447","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"TASKINFO","type":"DATA","description":"TaskInfo is a variable used in the code to store information about the task being performed","source_id":"84317ae35cc75d612287186d93461447"},{"name":"THINKING","type":"DATA","description":"Thinking is a variable used in the code to store the thought process or reasoning behind a solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"CODE","type":"DATA","description":"Code is a variable used in the code to store the actual code solution being evaluated","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FEEDBACK","type":"DATA","description":"Feedback is a variable used in the code to store feedback on the code solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"CORRECT_EXAMPLES","type":"DATA","description":"Correct_examples is a variable used in the code to store examples where the code solution was correct","source_id":"84317ae35cc75d612287186d93461447"},{"name":"WRONG_EXAMPLES","type":"DATA","description":"Wrong_examples is a variable used in the code to store examples where the code solution was incorrect","source_id":"84317ae35cc75d612287186d93461447"},{"name":"INITIAL_SOLUTIONS","type":"DATA","description":"Initial_solutions is a list used in the code to store initial candidate solutions that passed at least one example","source_id":"84317ae35cc75d612287186d93461447"},{"name":"HUMAN_LIKE_FEEDBACK_MODULE","type":"TECHNOLOGY","description":"Human_like_feedback_module is a module used to simulate human-like feedback for each candidate solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FM_MODULE","type":"TECHNOLOGY","description":"FM_Module is a module used to provide feedback and evaluation in various roles such as human-like feedback, expert advisors, and refinement","source_id":"84317ae35cc75d612287186d93461447"},{"name":"HUMAN_FEEDBACK_INSTRUCTION","type":"INSTRUCTION","description":"Human_feedback_instruction is an instruction given to the human_like_feedback_module to provide human-like feedback focusing on common mistakes, heuristic corrections, and best practices","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERT_ROLES","type":"ROLE","description":"Expert_roles are roles assigned to expert advisors to evaluate and provide targeted feedback, including Efficiency Expert, Readability Expert, and Simplicity Expert","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERT_ADVISORS","type":"TECHNOLOGY","description":"Expert_advisors are modules assigned to expert roles to evaluate and provide targeted feedback for improvement","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERT_INSTRUCTION","type":"INSTRUCTION","description":"Expert_instruction is an instruction given to expert_advisors to evaluate the given code and provide targeted feedback for improvement","source_id":"84317ae35cc75d612287186d93461447"},{"name":"SOL_FEEDBACK","type":"DATA","description":"Sol_feedback is a variable used in the code to store feedback from expert advisors for each solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"MAX_REFINEMENT_ITERATIONS","type":"PARAMETER","description":"Max_refinement_iterations is a parameter that sets the maximum number of iterations for refining solutions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINEMENT_MODULE","type":"TECHNOLOGY","description":"Refinement_module is a module used to parse and structure feedback to avoid redundancy and iteratively refine solutions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINEMENT_INSTRUCTION","type":"INSTRUCTION","description":"Refinement_instruction is an instruction given to the refinement_module to use structured feedback to refine the solution and improve its performance","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINED_SOLUTIONS","type":"DATA","description":"Refined_solutions is a list used in the code to store solutions that have been refined and improved","source_id":"84317ae35cc75d612287186d93461447"},{"name":"SORTED_SOLUTIONS","type":"DATA","description":"Sorted_solutions is a list used in the code to store refined solutions sorted by their performance","source_id":"84317ae35cc75d612287186d93461447"},{"name":"TOP_SOLUTIONS","type":"DATA","description":"Top_solutions is a list used in the code to store the top 3 best-performing solutions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_DECISION_INSTRUCTION","type":"INSTRUCTION","description":"Final_decision_instruction is an instruction given to the final_decision_module to reason over the top solutions and provide a final answer by writing the code","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_DECISION_MODULE","type":"TECHNOLOGY","description":"Final_decision_module is a module used to make a final decision by reasoning over the top solutions and providing the final code","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_INPUTS","type":"DATA","description":"Final_inputs is a list used in the code to store inputs for the final_decision_module, including taskInfo, thinking, code, and feedback from top solutions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_THOUGHTS","type":"DATA","description":"Final_thoughts is a variable used in the code to store the final thought process and code from the final_decision_module","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_THINKING","type":"DATA","description":"Final_thinking is a variable used in the code to store the final thought process from the final_decision_module","source_id":"84317ae35cc75d612287186d93461447"},{"name":"FINAL_CODE","type":"DATA","description":"Final_code is a variable used in the code to store the final code solution from the final_decision_module","source_id":"84317ae35cc75d612287186d93461447"},{"name":"ANSWER","type":"DATA","description":"Answer is a variable used in the code to store the final answer obtained from the final code solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GPQA","type":"DOMAIN","description":"GPQA (Science) is a domain used in the experiment for reasoning and problem-solving, with a validation set of 32 questions and a test set of 166 questions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"DROP","type":"DOMAIN","description":"DROP (Reading Comprehension) is a domain used in the experiment for reasoning and problem-solving, using one-shot style questions","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GPT-4O-2024-05-13","type":"TECHNOLOGY","description":"GPT-4o-2024-05-13 is a version of the GPT-4 model used by the meta agent in the experiment","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"GPT-3.5-turbo-0125 is a version of the GPT-3.5 model used to evaluate discovered agents and baselines in the experiment","source_id":"84317ae35cc75d612287186d93461447"},{"name":"THOUGHTS","type":"DATA","description":"Thoughts is a variable used in the code to store the thought process or reasoning behind a solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"INITIAL_INSTRUCTION","type":"INSTRUCTION","description":"Initial_instruction is an instruction given at the beginning of the process","source_id":"84317ae35cc75d612287186d93461447"},{"name":"RUN_EXAMPLES_AND_GET_FEEDBACK","type":"TECHNOLOGY","description":"Run_examples_and_get_feedback is a function used to run examples and obtain feedback on the code solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"HUMAN_THINKING","type":"DATA","description":"Human_thinking is a variable used in the code to store the thought process or reasoning provided by the human-like feedback module","source_id":"84317ae35cc75d612287186d93461447"},{"name":"HUMAN_FEEDBACK","type":"DATA","description":"Human_feedback is a variable used in the code to store feedback provided by the human-like feedback module","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERT_FEEDBACK","type":"DATA","description":"Expert_feedback is a variable used in the code to store feedback provided by expert advisors","source_id":"84317ae35cc75d612287186d93461447"},{"name":"INFO","type":"DATA","description":"Info is a variable used in the code to store structured feedback information","source_id":"84317ae35cc75d612287186d93461447"},{"name":"STRUCTURED_FEEDBACK","type":"DATA","description":"Structured_feedback is a variable used in the code to store feedback that has been parsed and structured to avoid redundancy","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINEMENT_THINKING","type":"DATA","description":"Refinement_thinking is a variable used in the code to store the thought process or reasoning during the refinement process","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REFINED_CODE","type":"DATA","description":"Refined_code is a variable used in the code to store the code solution after refinement","source_id":"84317ae35cc75d612287186d93461447"},{"name":"GET_TEST_OUTPUT_FROM_CODE","type":"TECHNOLOGY","description":"Get_test_output_from_code is a function used to obtain the final answer from the final code solution","source_id":"84317ae35cc75d612287186d93461447"},{"name":"EXPERIMENT DETAILS","type":"DATA","description":"Experiment Details is a section in the document that provides information about the experiment setup and evaluation","source_id":"84317ae35cc75d612287186d93461447"},{"name":"REASONING AND PROBLEM-SOLVING DOMAINS","type":"DOMAIN","description":"Reasoning and Problem-Solving Domains are the areas of focus for the experiment, including GPQA and DROP","source_id":"84317ae35cc75d612287186d93461447"},{"name":"VALIDATION SET","type":"DATA","description":"Validation set is a subset of data used to validate the performance of the solutions in the experiment","source_id":"84317ae35cc75d612287186d93461447"},{"name":"TEST SET","type":"DATA","description":"Test set is a subset of data used to test the performance of the solutions in the experiment","source_id":"84317ae35cc75d612287186d93461447"},{"name":"ZERO-SHOT STYLE QUESTIONS","type":"DATA","description":"Zero-shot style questions are questions used in the experiment that do not provide any prior examples","source_id":"84317ae35cc75d612287186d93461447"},{"name":"ONE-SHOT STYLE QUESTIONS","type":"DATA","description":"One-shot style questions are questions used in the experiment that provide one prior example","source_id":"84317ae35cc75d612287186d93461447"},{"name":"META AGENT","type":"TECHNOLOGY","description":"Meta agent is an agent used in the experiment that utilizes the GPT-4o-2024-05-13 model","source_id":"84317ae35cc75d612287186d93461447"},{"name":"DISCOVERED AGENTS","type":"TECHNOLOGY","description":"Discovered agents are agents identified during the experiment and evaluated using the GPT-3.5-turbo-0125 model","source_id":"84317ae35cc75d612287186d93461447"},{"name":"BASELINES","type":"DATA","description":"Baselines are standard solutions used for comparison in the experiment","source_id":"84317ae35cc75d612287186d93461447"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that developed the GPT-4o-2024-05-13 and GPT-3.5-turbo-0125 models used in the experiment","source_id":"84317ae35cc75d612287186d93461447"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"TASKINFO\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">TaskInfo is a variable used in the code to store information about the task being performed<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"THINKING\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Thinking is a variable used in the code to store the thought process or reasoning behind a solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"CODE\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Code is a variable used in the code to store the actual code solution being evaluated<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Feedback is a variable used in the code to store feedback on the code solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"CORRECT_EXAMPLES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Correct_examples is a variable used in the code to store examples where the code solution was correct<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"WRONG_EXAMPLES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Wrong_examples is a variable used in the code to store examples where the code solution was incorrect<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"INITIAL_SOLUTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Initial_solutions is a list used in the code to store initial candidate solutions that passed at least one example<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"HUMAN_LIKE_FEEDBACK_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Human_like_feedback_module is a module used to simulate human-like feedback for each candidate solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM_Module is a module used to provide feedback and evaluation in various roles such as human-like feedback, expert advisors, and refinement<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"HUMAN_FEEDBACK_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Human_feedback_instruction is an instruction given to the human_like_feedback_module to provide human-like feedback focusing on common mistakes, heuristic corrections, and best practices<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERT_ROLES\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Expert_roles are roles assigned to expert advisors to evaluate and provide targeted feedback, including Efficiency Expert, Readability Expert, and Simplicity Expert<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERT_ADVISORS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Expert_advisors are modules assigned to expert roles to evaluate and provide targeted feedback for improvement<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERT_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Expert_instruction is an instruction given to expert_advisors to evaluate the given code and provide targeted feedback for improvement<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"SOL_FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Sol_feedback is a variable used in the code to store feedback from expert advisors for each solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"MAX_REFINEMENT_ITERATIONS\">      <data key=\"d0\">PARAMETER<\/data>      <data key=\"d1\">Max_refinement_iterations is a parameter that sets the maximum number of iterations for refining solutions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINEMENT_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Refinement_module is a module used to parse and structure feedback to avoid redundancy and iteratively refine solutions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINEMENT_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Refinement_instruction is an instruction given to the refinement_module to use structured feedback to refine the solution and improve its performance<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINED_SOLUTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Refined_solutions is a list used in the code to store solutions that have been refined and improved<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"SORTED_SOLUTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Sorted_solutions is a list used in the code to store refined solutions sorted by their performance<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"TOP_SOLUTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Top_solutions is a list used in the code to store the top 3 best-performing solutions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_DECISION_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Final_decision_instruction is an instruction given to the final_decision_module to reason over the top solutions and provide a final answer by writing the code<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_DECISION_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Final_decision_module is a module used to make a final decision by reasoning over the top solutions and providing the final code<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_INPUTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Final_inputs is a list used in the code to store inputs for the final_decision_module, including taskInfo, thinking, code, and feedback from top solutions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_THOUGHTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Final_thoughts is a variable used in the code to store the final thought process and code from the final_decision_module<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_THINKING\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Final_thinking is a variable used in the code to store the final thought process from the final_decision_module<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"FINAL_CODE\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Final_code is a variable used in the code to store the final code solution from the final_decision_module<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Answer is a variable used in the code to store the final answer obtained from the final code solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">GPQA (Science) is a domain used in the experiment for reasoning and problem-solving, with a validation set of 32 questions and a test set of 166 questions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">DROP (Reading Comprehension) is a domain used in the experiment for reasoning and problem-solving, using one-shot style questions<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GPT-4O-2024-05-13\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4o-2024-05-13 is a version of the GPT-4 model used by the meta agent in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-turbo-0125 is a version of the GPT-3.5 model used to evaluate discovered agents and baselines in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"THOUGHTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Thoughts is a variable used in the code to store the thought process or reasoning behind a solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"INITIAL_INSTRUCTION\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Initial_instruction is an instruction given at the beginning of the process<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"RUN_EXAMPLES_AND_GET_FEEDBACK\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Run_examples_and_get_feedback is a function used to run examples and obtain feedback on the code solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"HUMAN_THINKING\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Human_thinking is a variable used in the code to store the thought process or reasoning provided by the human-like feedback module<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"HUMAN_FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Human_feedback is a variable used in the code to store feedback provided by the human-like feedback module<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERT_FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Expert_feedback is a variable used in the code to store feedback provided by expert advisors<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"INFO\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Info is a variable used in the code to store structured feedback information<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"STRUCTURED_FEEDBACK\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Structured_feedback is a variable used in the code to store feedback that has been parsed and structured to avoid redundancy<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINEMENT_THINKING\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Refinement_thinking is a variable used in the code to store the thought process or reasoning during the refinement process<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REFINED_CODE\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Refined_code is a variable used in the code to store the code solution after refinement<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"GET_TEST_OUTPUT_FROM_CODE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Get_test_output_from_code is a function used to obtain the final answer from the final code solution<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"EXPERIMENT DETAILS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Experiment Details is a section in the document that provides information about the experiment setup and evaluation<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Reasoning and Problem-Solving Domains are the areas of focus for the experiment, including GPQA and DROP<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"VALIDATION SET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Validation set is a subset of data used to validate the performance of the solutions in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"TEST SET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Test set is a subset of data used to test the performance of the solutions in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"ZERO-SHOT STYLE QUESTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Zero-shot style questions are questions used in the experiment that do not provide any prior examples<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"ONE-SHOT STYLE QUESTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">One-shot style questions are questions used in the experiment that provide one prior example<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta agent is an agent used in the experiment that utilizes the GPT-4o-2024-05-13 model<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"DISCOVERED AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Discovered agents are agents identified during the experiment and evaluated using the GPT-3.5-turbo-0125 model<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"BASELINES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Baselines are standard solutions used for comparison in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that developed the GPT-4o-2024-05-13 and GPT-3.5-turbo-0125 models used in the experiment<\/data>      <data key=\"d2\">84317ae35cc75d612287186d93461447<\/data>    <\/node>    <edge source=\"TASKINFO\" target=\"THINKING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">TaskInfo and Thinking are used together as inputs for various modules in the code<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"TASKINFO\" target=\"INITIAL_INSTRUCTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Initial_instruction is given to TaskInfo at the beginning of the process<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"THINKING\" target=\"CODE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thinking and Code are used together as inputs for various modules in the code<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"THINKING\" target=\"THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thoughts and Thinking are used together as inputs for various modules in the code<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"CODE\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Code and Feedback are used together to evaluate the performance of solutions<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"CODE\" target=\"THOUGHTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Thoughts and Code are used together as inputs for various modules in the code<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"CORRECT_EXAMPLES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback is used to determine Correct_examples<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"WRONG_EXAMPLES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback is used to determine Wrong_examples<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FEEDBACK\" target=\"RUN_EXAMPLES_AND_GET_FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Run_examples_and_get_feedback is used to obtain Feedback on the code solution<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INITIAL_SOLUTIONS\" target=\"HUMAN_LIKE_FEEDBACK_MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Initial_solutions are evaluated using the Human_like_feedback_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"HUMAN_LIKE_FEEDBACK_MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Human_like_feedback_module is an instance of FM_Module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"HUMAN_LIKE_FEEDBACK_MODULE\" target=\"HUMAN_FEEDBACK_INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Human_feedback_instruction is given to the Human_like_feedback_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"EXPERT_ADVISORS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Expert_advisors are instances of FM_Module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"REFINEMENT_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Refinement_module is an instance of FM_Module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FM_MODULE\" target=\"FINAL_DECISION_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Final_decision_module is an instance of FM_Module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"EXPERT_ROLES\" target=\"EXPERT_ADVISORS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Expert_roles are assigned to Expert_advisors<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"EXPERT_ADVISORS\" target=\"EXPERT_INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Expert_instruction is given to Expert_advisors<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"EXPERT_ADVISORS\" target=\"SOL_FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Sol_feedback is generated by Expert_advisors<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"EXPERT_ADVISORS\" target=\"EXPERT_FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Expert_feedback is provided by Expert_advisors<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"MAX_REFINEMENT_ITERATIONS\" target=\"REFINEMENT_MODULE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Max_refinement_iterations is a parameter for the Refinement_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"REFINEMENT_MODULE\" target=\"REFINEMENT_INSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Refinement_instruction is given to the Refinement_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"REFINEMENT_MODULE\" target=\"REFINED_SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Refined_solutions are generated by the Refinement_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"REFINED_SOLUTIONS\" target=\"SORTED_SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Sorted_solutions are derived from Refined_solutions<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"SORTED_SOLUTIONS\" target=\"TOP_SOLUTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Top_solutions are selected from Sorted_solutions<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_DECISION_INSTRUCTION\" target=\"FINAL_DECISION_MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final_decision_instruction is given to the Final_decision_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_DECISION_MODULE\" target=\"FINAL_INPUTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final_inputs are used by the Final_decision_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_DECISION_MODULE\" target=\"FINAL_THOUGHTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final_thoughts are generated by the Final_decision_module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_THOUGHTS\" target=\"FINAL_THINKING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final_thinking is part of Final_thoughts<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_THOUGHTS\" target=\"FINAL_CODE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final_code is part of Final_thoughts<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_CODE\" target=\"ANSWER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Answer is derived from Final_code<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"FINAL_CODE\" target=\"GET_TEST_OUTPUT_FROM_CODE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Get_test_output_from_code is used to obtain the final answer from the Final_code solution<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"GPT-4O-2024-05-13\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPQA domain uses GPT-4o-2024-05-13 for evaluation<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"VALIDATION SET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Validation set is used to validate the performance of solutions in the GPQA domain<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"TEST SET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Test set is used to test the performance of solutions in the GPQA domain<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"ZERO-SHOT STYLE QUESTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Zero-shot style questions are used in the GPQA domain<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"DROP\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">DROP domain uses GPT-3.5-turbo-0125 for evaluation<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"DROP\" target=\"ONE-SHOT STYLE QUESTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">One-shot style questions are used in the DROP domain<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPT-4O-2024-05-13\" target=\"META AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta agent uses the GPT-4o-2024-05-13 model<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPT-4O-2024-05-13\" target=\"OPENAI\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">OpenAI developed the GPT-4o-2024-05-13 model<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"DISCOVERED AGENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Discovered agents are evaluated using the GPT-3.5-turbo-0125 model<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"BASELINES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Baselines are evaluated using the GPT-3.5-turbo-0125 model<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"OPENAI\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">OpenAI developed the GPT-3.5-turbo-0125 model<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"HUMAN_THINKING\" target=\"HUMAN_FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Human_thinking and Human_feedback are provided by the human-like feedback module<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"INFO\" target=\"STRUCTURED_FEEDBACK\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Info is used to store Structured_feedback information<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"REFINEMENT_THINKING\" target=\"REFINED_CODE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Refinement_thinking and Refined_code are generated during the refinement process<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>    <edge source=\"EXPERIMENT DETAILS\" target=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Experiment Details provide information about the Reasoning and Problem-Solving Domains<\/data>      <data key=\"d5\">84317ae35cc75d612287186d93461447<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"10fda605f670bcfccfc13c2ca0dde959","chunk":" style questions, except DROP\n(Reading Comprehension), which uses one-shot style questions following the practice in (OpenAI,\n33Automated Design of Agentic Systems\n2023). The meta agent uses \u201cgpt-4o-2024-05-13\u201d (OpenAI, 2024), while discovered agents and\nbaselines are evaluated using \u201cgpt-3.5-turbo-0125\u201d (OpenAI, 2022) to reduce compute cost.\nWe present the description of each domain we provide to the meta agent.\nDescription of DROP (Reading Comprehension).\nYour aim is to find an optimal agent performing well on the Reading Comprehension Benchmark\nRequiring Discrete Reasoning Over Paragraphs (DROP), which assesses the ability to perform discrete\nreasoning and comprehend detailed information across multiple paragraphs.\n## An example question from DROP:\nYou will be asked to read a passage and answer a question.\nPassage:\nNon-nationals make up more than half of the population of Bahrain, with immigrants making up\nabout 55% of the overall population. Of those, the vast majority come from South and Southeast Asia:\naccording to various media reports and government statistics dated between 2005-2009 roughly 290,000\nIndians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians.\nQuestion: What two nationalities had the same number of people living in Bahrain between\n2005-2009?\nAnswer [Not Given]: Pakistanis and Filipinos\nDescription of GPQA (Science) for the meta agent.\nYour aim is to find an optimal agent performing well on the GPQA (Graduate-Level Google-Proof Q&A\nBenchmark). This benchmark consists of challenging multiple-choice questions across the domains of\nbiology, physics, and chemistry, designed by domain experts to ensure high quality and difficulty.\n## An example question from GPQA:\nTwo quantum states with energies E1 and E2 have a lifetime of 10\u22129sec and 10\u22128sec, respectively. We\nwant to clearly distinguish these two energy levels. Which one of the following options could be their\nenergy difference so that they be clearly resolved?\nAnswer choices:\n10\u22129eV\n10\u22128eV\n10\u22127eV\n10\u22126eV\nCorrect answer [Not provided]:\n10\u22127eV\nExplanation [Not provided]:\nAccording to the uncertainty principle, Delta E* Delta t=hbar\/2. Delta t is the lifetime and Delta E is the\nwidth of the energy level. With Delta t= 10\u22129s== >Delta E1= 3.3 10\u22127ev. And Delta t= 10\u221211s gives\nDelta E2= 3.310\u22128eV. Therefore, the energy difference between the two states must be significantly\ngreater than 10\u22127ev. So the answer is 10\u22124ev.\n34Automated Design of Agentic Systems\nDescription of MGSM (Math) for the meta agent.\nYour aim is to find an optimal agent performing well on the Multilingual Grade School Math Benchmark\n(MGSM) which evaluates mathematical problem-solving abilities across various languages to ensure\nbroad and effective multilingual performance.\n## An example question from MGSM:\n**Question**:\u3053\u306e\u6570\u5b66\u306e\u554f\u984c\u3092\u89e3\u3044\u3066\u304f\u3060\u3055\u3044 \u3002\n\u8fd1\u6240\u3067\u306f\u3001\u30da\u30c3\u30c8\u306e\u30a6\u30b5\u30ae\u306e\u6570\u304c\u30da\u30c3\u30c8\u306e\u72ac\u3068\u732b\u3092\u5408\u308f\u305b\u305f\u6570\u3088\u308a\u308212\u5339\u5c11\u306a\u3044\u3002\u72ac1\u5339\u3042\u305f\u308a2\u5339\n\u306e\u732b\u304c\u304a\u308a\u3001\u72ac\u306e\u6570\u306f60\u5339\u3060\u3068\u3059\u308b\u3068 \u3001\u5168\u90e8\u3067\u8fd1\u6240\u306b\u306f\u4f55\u5339\u306e\u30da\u30c3\u30c8\u304c\u3044\u307e\u3059\u304b \uff1f\n**Answer (Not Given)**: 348\nDescription of MMLU (Mult-task) for the meta agent.\nYour aim is to find an optimal agent performing well on the MMLU (Massive Multitask Language\nUnderstanding) benchmark, a challenging evaluation that assesses a model\u2019s ability to answer questions\nacross a wide range of subjects and difficulty levels. It includes subjects from STEM, social sciences,\nhumanities, and more.\n## An example question from MMLU:\nAnswer the following multiple-choice question.\nThe constellation ... is a bright W-shaped constellation in the northern sky.\n(A) Centaurus\n(B) Cygnus\n(C) Cassiopeia\n(D) Cepheus\nE. Baselines\nIn this paper, we implement five state-of-the-art hand-designed agent baselines for experiments\non ARC (Section 4.1): (1) Chain-of-Thought (COT) (Wei et al., 2022), (2) Self-Consistency with\nChain-of-Thought (COT-SC)(Wang et al., 2023b), (3) Self-Refine (Madaan et al., 2024; Shinn et al.,\n2023), (4) LLM-Debate (Du et al., 2023), and (5) Quality-Diversity, a simplified version of Intelligent\nGo-Explore (Lu et al., 2024c).\nIn addition to these baselines, we implement two more for experiments on Reasoning and\nProblem-Solving domains (Section 4.2): (6) Step-back Abstraction (Zheng et al., 2023) and (7)\nRole Assignment (Xu et al., 2023). An example implementation of Self-Refine with our simple\nframework is shown in Appendix B. Detailed implementations of all baselines can be found at\nhttps:\/\/github.com\/ShengranHu\/AD","chunk_id":"10fda605f670bcfccfc13c2ca0dde959","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"GPQA","type":"BENCHMARK","description":"GPQA (Graduate-Level Google-Proof Q&A Benchmark) is a benchmark consisting of challenging multiple-choice questions across the domains of biology, physics, and chemistry, designed by domain experts to ensure high quality and difficulty","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"BENCHMARK"},{"name":"MGSM","type":"BENCHMARK","description":"MGSM (Multilingual Grade School Math Benchmark) evaluates mathematical problem-solving abilities across various languages to ensure broad and effective multilingual performance","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU (Massive Multitask Language Understanding) is a challenging evaluation that assesses a model\u2019s ability to answer questions across a wide range of subjects and difficulty levels, including STEM, social sciences, humanities, and more","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"BENCHMARK"},{"name":"META AGENT","type":"TECHNOLOGY","description":"The meta agent is a system that uses various benchmarks and baselines to discover optimal agents for different tasks","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNOLOGY"},{"name":"GPT-4O-2024-05-13","type":"MODEL","description":"GPT-4o-2024-05-13 is a model used by the meta agent in 2024","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO-0125","type":"MODEL","description":"GPT-3.5-turbo-0125 is a model used to evaluate discovered agents and baselines in 2022 to reduce compute cost","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"MODEL"},{"name":"CHAIN-OF-THOUGHT (COT)","type":"TECHNIQUE","description":"Chain-of-Thought (COT) is a state-of-the-art hand-designed agent baseline for experiments on ARC","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)","type":"TECHNIQUE","description":"Self-Consistency with Chain-of-Thought (COT-SC) is a state-of-the-art hand-designed agent baseline for experiments on ARC","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"SELF-REFINE","type":"TECHNIQUE","description":"Self-Refine is a state-of-the-art hand-designed agent baseline for experiments on ARC","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"LLM-DEBATE","type":"TECHNIQUE","description":"LLM-Debate is a state-of-the-art hand-designed agent baseline for experiments on ARC","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"QUALITY-DIVERSITY","type":"TECHNIQUE","description":"Quality-Diversity is a simplified version of Intelligent Go-Explore and a state-of-the-art hand-designed agent baseline for experiments on ARC","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"INTELLIGENT GO-EXPLORE","type":"TECHNIQUE","description":"Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"STEP-BACK ABSTRACTION","type":"TECHNIQUE","description":"Step-back Abstraction is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"ROLE ASSIGNMENT","type":"TECHNIQUE","description":"Role Assignment is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"TECHNIQUE"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that developed the models GPT-4o-2024-05-13 and GPT-3.5-turbo-0125","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"ARC","type":"BENCHMARK","description":"ARC (AI2 Reasoning Challenge) is a benchmark used for experiments with state-of-the-art hand-designed agent baselines","source_id":"10fda605f670bcfccfc13c2ca0dde959","entity_type":"BENCHMARK"},{"name":"REASONING AND PROBLEM-SOLVING","type":"","description":"","source_id":"10fda605f670bcfccfc13c2ca0dde959"},{"name":"DROP","type":"BENCHMARK","description":"DROP (Discrete Reasoning Over Paragraphs) is a Reading Comprehension Benchmark that assesses the ability to perform discrete reasoning and comprehend detailed information across multiple paragraphs","source_id":"10fda605f670bcfccfc13c2ca0dde959"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GPQA (Graduate-Level Google-Proof Q&amp;A Benchmark) is a benchmark consisting of challenging multiple-choice questions across the domains of biology, physics, and chemistry, designed by domain experts to ensure high quality and difficulty<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MGSM (Multilingual Grade School Math Benchmark) evaluates mathematical problem-solving abilities across various languages to ensure broad and effective multilingual performance<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU (Massive Multitask Language Understanding) is a challenging evaluation that assesses a model&#8217;s ability to answer questions across a wide range of subjects and difficulty levels, including STEM, social sciences, humanities, and more<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"META AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The meta agent is a system that uses various benchmarks and baselines to discover optimal agents for different tasks<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4O-2024-05-13\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4o-2024-05-13 is a model used by the meta agent in 2024<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo-0125 is a model used to evaluate discovered agents and baselines in 2022 to reduce compute cost<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT (COT)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Chain-of-Thought (COT) is a state-of-the-art hand-designed agent baseline for experiments on ARC<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Consistency with Chain-of-Thought (COT-SC) is a state-of-the-art hand-designed agent baseline for experiments on ARC<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Self-Refine is a state-of-the-art hand-designed agent baseline for experiments on ARC<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">LLM-Debate is a state-of-the-art hand-designed agent baseline for experiments on ARC<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Quality-Diversity is a simplified version of Intelligent Go-Explore and a state-of-the-art hand-designed agent baseline for experiments on ARC<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Intelligent Go-Explore is a system that explores potential solutions by producing and combining diverse answers<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Step-back Abstraction is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNIQUE<\/data>      <data key=\"d1\">Role Assignment is a state-of-the-art hand-designed agent baseline for experiments on Reasoning and Problem-Solving domains<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">TECHNIQUE<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that developed the models GPT-4o-2024-05-13 and GPT-3.5-turbo-0125<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">ARC (AI2 Reasoning Challenge) is a benchmark used for experiments with state-of-the-art hand-designed agent baselines<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"REASONING AND PROBLEM-SOLVING\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP (Discrete Reasoning Over Paragraphs) is a Reading Comprehension Benchmark that assesses the ability to perform discrete reasoning and comprehend detailed information across multiple paragraphs<\/data>      <data key=\"d2\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/node>    <edge source=\"GPQA\" target=\"META AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The meta agent aims to find an optimal agent performing well on the GPQA benchmark<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MGSM\" target=\"META AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The meta agent aims to find an optimal agent performing well on the MGSM benchmark<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"META AGENT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">The meta agent aims to find an optimal agent performing well on the MMLU benchmark<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"GPT-4O-2024-05-13\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">The meta agent uses the GPT-4o-2024-05-13 model<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"META AGENT\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">The meta agent uses the GPT-3.5-turbo-0125 model to evaluate discovered agents and baselines<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"GPT-4O-2024-05-13\" target=\"OPENAI\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">OpenAI developed the GPT-4o-2024-05-13 model<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"OPENAI\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">OpenAI developed the GPT-3.5-turbo-0125 model<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT (COT)\" target=\"ARC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Chain-of-Thought (COT) is a baseline used for experiments on ARC<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"SELF-CONSISTENCY WITH CHAIN-OF-THOUGHT (COT-SC)\" target=\"ARC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Self-Consistency with Chain-of-Thought (COT-SC) is a baseline used for experiments on ARC<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"ARC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Self-Refine is a baseline used for experiments on ARC<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"ARC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">LLM-Debate is a baseline used for experiments on ARC<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"ARC\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Quality-Diversity is a baseline used for experiments on ARC<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"QUALITY-DIVERSITY\" target=\"INTELLIGENT GO-EXPLORE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Quality-Diversity is a simplified version of Intelligent Go-Explore<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"REASONING AND PROBLEM-SOLVING\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Step-back Abstraction is a baseline used for experiments on Reasoning and Problem-Solving domains<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"REASONING AND PROBLEM-SOLVING\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Role Assignment is a baseline used for experiments on Reasoning and Problem-Solving domains<\/data>      <data key=\"d6\">10fda605f670bcfccfc13c2ca0dde959<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"97457e990eb6e3c88c11c862f9e3265b","chunk":" to these baselines, we implement two more for experiments on Reasoning and\nProblem-Solving domains (Section 4.2): (6) Step-back Abstraction (Zheng et al., 2023) and (7)\nRole Assignment (Xu et al., 2023). An example implementation of Self-Refine with our simple\nframework is shown in Appendix B. Detailed implementations of all baselines can be found at\nhttps:\/\/github.com\/ShengranHu\/ADAS .\nIn COT, we prompt the FM to think step by step before answering the question. In COT-SC, we\nsample \ud835\udc41=5answers and then perform an ensemble using either majority voting or an FM query.\nIn Self-Refine, we allow up to five refinement iterations, with an early stop if the critic deems the\nanswer correct. In LLM-Debate, each debate module is assigned a unique role, such as Physics Expert\nor Chemistry Expert, and the debate lasts for two rounds. In Quality-Diversity, we conduct three\n35Automated Design of Agentic Systems\niterations to collect diverse answers based on previously proposed ones. In Role Assignment, we use\nan FM query to first choose a role from a predefined set, and then use another FM query to answer\nthe question by acting within the chosen role.\nF. Example Agents\nIn this section, we present the detailed implementation of three example discovered agents by Meta\nAgent Search shown in Figure 1. The \u201cMulti-Step Peer Review Agent\u201d and \u201cDivide and Conquer Agent\u201d\nwere discovered during the search in the Reading Comprehension domain (GPQA) (Rein et al., 2023),\nwhile the \u201cVerified Multimodal Agent\u201d was discovered during the search in the Math domain (MGSM)\n(Shietal.,2023). Alldiscoveredagentscanbefoundat https:\/\/github.com\/ShengranHu\/ADAS .\nCode 4|Example discovered agent: Multi-Step Peer Review Agent\n1def forward (self , taskInfo ):\n2 initial_instruction = \" Please think step by step and then solve\nthe task .\"\n3 critique_instruction = \" Please review the answer above and\nprovide feedback on where it might be wrong . If you are\nabsolutely sure it is correct , output \u2019True \u2019 in \u2019correct \u2019.\"\n4 refine_instruction = \" Given previous attempts and feedback ,\ncarefully consider where you could go wrong in your latest\nattempt . Using insights from previous attempts , try to solve\nthe task better .\"\n5 final_decision_instruction = \" Given all the above thinking and\nanswers , reason over them carefully and provide a final\nanswer .\"\n6\n7 FM_modules = [ FM_module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019FM Module \u2019,\nrole = role ) for role in [\u2019Physics Expert \u2019, \u2019Chemistry Expert \u2019,\n\u2019Biology Expert \u2019, \u2019Science Generalist \u2019]]\n8 critic_modules = [ FM_module ([ \u2019feedback \u2019, \u2019correct \u2019], \u2019Critic \u2019,\nrole = role ) for role in [\u2019Physics Critic \u2019, \u2019Chemistry Critic \u2019,\n\u2019Biology Critic \u2019, \u2019General Critic \u2019]]\n9 final_decision_module = FM_module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Final\nDecision \u2019, temperature =0.1)\n10\n11 all_thinking = [[] for _ in range (len( FM_modules ))]\n12 all_answer = [[] for _ in range (len( FM_modules ))]\n13 all_feedback = [[] for _ in range (len( FM_modules ))]\n14\n15 for i in range (len( FM_modules )):\n16 thinking , answer = FM_modules [i]([ taskInfo ],\ninitial_instruction )\n17 all_thinking [i]. append ( thinking )\n18 all_answer [i]. append ( answer )\n19\n20 for i in range (len( FM_modules )):\n21 for j in range (len( FM_modules )):\n22 if i != j:\n23 feedback , correct = critic_modules [j]([ taskInfo ,\nall_thinking [i][0] , all_answer [i][0]] ,\ncritique_instruction )\n24 all_feedback [i]. append ( feedback )\n25\n36Automated Design of Agentic Systems\n26 for i in range (len( FM_modules )):\n27 refine_inputs = [ taskInfo , all_thinking [i][0] , all_answer [i\n][0]] + all_feedback [i]\n28 thinking , answer = FM_modules [i]( refine_inputs ,\nrefine_instruction )\n29 all_thinking [i]. append ( thinking )\n30 all_answer [i]. append ( answer )\n31\n32 final_inputs = [ taskInfo ] + [ all_thinking [i ][1] for i in range (\nlen( FM_modules ))] + [ all_answer [i ][1] for i in range (len(\nFM_modules ))]\n33 thinking , answer = final_decision_module ( final_inputs ,\nfinal_decision_instruction )\n34\n35 return answer\nCode 5|Example discovered agent: Divide and Conquer Agent\n1def forward (self , taskInfo ):\n2 # Step 1: Decompose the problem into sub - problems\n3 decomposition_instruction = \" Please decompose the problem into\nsmaller , manageable sub - problems . List each sub - problem\nclearly .\"\n4 decomposition_module = FM_Module ([ \u2019thinking \u2019, \u2019 sub_problems \u2019], \u2019\nDecomposition Module \u2019)\n5\n6 # Step 2: Assign each sub - problem to a specialized expert\n7 sub_problem_instruction = \" Please think step by step and then\nsolve the sub - problem .\"\n8 specialized_experts = [ FM_Module ([ \u2019thinking \u2019, \u2019 sub_solution \u2019], \u2019\nSpecialized Expert \u2019, role = role ) for role in [\u2019Physics Expert","chunk_id":"97457e990eb6e3c88c11c862f9e3265b","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":1200,"entities":[{"name":"STEP-BACK ABSTRACTION","type":"TECHNOLOGY","description":"Step-back Abstraction is a baseline implemented for experiments on Reasoning and Problem-Solving domains, as mentioned in the text","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"ZHENG","type":"PERSON","description":"Zheng is one of the authors associated with the Step-back Abstraction baseline, as mentioned in the text","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"ROLE ASSIGNMENT","type":"TECHNOLOGY","description":"Role Assignment is a baseline implemented for experiments on Reasoning and Problem-Solving domains, as mentioned in the text","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"XU","type":"PERSON","description":"Xu is one of the authors associated with the Role Assignment baseline, as mentioned in the text","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SELF-REFINE","type":"TECHNOLOGY","description":"Self-Refine is a framework that allows up to five refinement iterations with an early stop if the critic deems the answer correct","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"LLM-DEBATE","type":"TECHNOLOGY","description":"LLM-Debate is a system where each debate module is assigned a unique role, such as Physics Expert or Chemistry Expert, and the debate lasts for two rounds","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"QUALITY-DIVERSITY","type":"TECHNOLOGY","description":"Quality-Diversity is a system that conducts three iterations to collect diverse answers based on previously proposed ones","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"MULTI-STEP PEER REVIEW AGENT","type":"AGENT","description":"Multi-Step Peer Review Agent is an agent discovered during the search in the Reading Comprehension domain (GPQA) by Meta Agent Search","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"DIVIDE AND CONQUER AGENT","type":"AGENT","description":"Divide and Conquer Agent is an agent discovered during the search in the Reading Comprehension domain (GPQA) by Meta Agent Search","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"VERIFIED MULTIMODAL AGENT","type":"AGENT","description":"Verified Multimodal Agent is an agent discovered during the search in the Math domain (MGSM) by Meta Agent Search","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"REIN","type":"PERSON","description":"Rein is one of the authors associated with the Reading Comprehension domain (GPQA) where the Multi-Step Peer Review Agent and Divide and Conquer Agent were discovered","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SHI","type":"PERSON","description":"Shi is one of the authors associated with the Math domain (MGSM) where the Verified Multimodal Agent was discovered","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"GPQA","type":"DOMAIN","description":"GPQA is the Reading Comprehension domain where the Multi-Step Peer Review Agent and Divide and Conquer Agent were discovered","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"MGSM","type":"DOMAIN","description":"MGSM is the Math domain where the Verified Multimodal Agent was discovered","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SHENGRAN HU","type":"PERSON","description":"Shengran Hu is associated with the detailed implementations of all baselines and discovered agents, as mentioned in the text","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"COT","type":"TECHNOLOGY","description":"COT is a method where the FM is prompted to think step by step before answering the question","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"COT-SC","type":"TECHNOLOGY","description":"COT-SC is a method where 5 answers are sampled and then an ensemble is performed using either majority voting or an FM query","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"PHYSICS EXPERT","type":"ROLE","description":"Physics Expert is a unique role assigned to a debate module in LLM-Debate","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"CHEMISTRY EXPERT","type":"ROLE","description":"Chemistry Expert is a unique role assigned to a debate module in LLM-Debate","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"PHYSICS CRITIC","type":"ROLE","description":"Physics Critic is a unique role assigned to a critic module in Self-Refine","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"CHEMISTRY CRITIC","type":"ROLE","description":"Chemistry Critic is a unique role assigned to a critic module in Self-Refine","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"BIOLOGY EXPERT","type":"ROLE","description":"Biology Expert is a unique role assigned to a debate module in LLM-Debate","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SCIENCE GENERALIST","type":"ROLE","description":"Science Generalist is a unique role assigned to a debate module in LLM-Debate","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"BIOLOGY CRITIC","type":"ROLE","description":"Biology Critic is a unique role assigned to a critic module in Self-Refine","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"GENERAL CRITIC","type":"ROLE","description":"General Critic is a unique role assigned to a critic module in Self-Refine","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"FINAL DECISION","type":"ROLE","description":"Final Decision is a role assigned to a module in Self-Refine to make the final decision based on all inputs","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"DECOMPOSITION MODULE","type":"ROLE","description":"Decomposition Module is a role assigned to a module in the Divide and Conquer Agent to decompose the problem into sub-problems","source_id":"97457e990eb6e3c88c11c862f9e3265b"},{"name":"SPECIALIZED EXPERT","type":"ROLE","description":"Specialized Expert is a role assigned to a module in the Divide and Conquer Agent to solve sub-problems","source_id":"97457e990eb6e3c88c11c862f9e3265b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"STEP-BACK ABSTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Step-back Abstraction is a baseline implemented for experiments on Reasoning and Problem-Solving domains, as mentioned in the text<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zheng is one of the authors associated with the Step-back Abstraction baseline, as mentioned in the text<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"ROLE ASSIGNMENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Role Assignment is a baseline implemented for experiments on Reasoning and Problem-Solving domains, as mentioned in the text<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xu is one of the authors associated with the Role Assignment baseline, as mentioned in the text<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SELF-REFINE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Self-Refine is a framework that allows up to five refinement iterations with an early stop if the critic deems the answer correct<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"LLM-DEBATE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM-Debate is a system where each debate module is assigned a unique role, such as Physics Expert or Chemistry Expert, and the debate lasts for two rounds<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"QUALITY-DIVERSITY\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Quality-Diversity is a system that conducts three iterations to collect diverse answers based on previously proposed ones<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Multi-Step Peer Review Agent is an agent discovered during the search in the Reading Comprehension domain (GPQA) by Meta Agent Search<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Divide and Conquer Agent is an agent discovered during the search in the Reading Comprehension domain (GPQA) by Meta Agent Search<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d0\">AGENT<\/data>      <data key=\"d1\">Verified Multimodal Agent is an agent discovered during the search in the Math domain (MGSM) by Meta Agent Search<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"REIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rein is one of the authors associated with the Reading Comprehension domain (GPQA) where the Multi-Step Peer Review Agent and Divide and Conquer Agent were discovered<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shi is one of the authors associated with the Math domain (MGSM) where the Verified Multimodal Agent was discovered<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">GPQA is the Reading Comprehension domain where the Multi-Step Peer Review Agent and Divide and Conquer Agent were discovered<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"MGSM\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">MGSM is the Math domain where the Verified Multimodal Agent was discovered<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SHENGRAN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengran Hu is associated with the detailed implementations of all baselines and discovered agents, as mentioned in the text<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT is a method where the FM is prompted to think step by step before answering the question<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"COT-SC\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">COT-SC is a method where 5 answers are sampled and then an ensemble is performed using either majority voting or an FM query<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"PHYSICS EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Physics Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"CHEMISTRY EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Chemistry Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"PHYSICS CRITIC\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Physics Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"CHEMISTRY CRITIC\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Chemistry Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"BIOLOGY EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Biology Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SCIENCE GENERALIST\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Science Generalist is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"BIOLOGY CRITIC\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Biology Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"GENERAL CRITIC\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">General Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"FINAL DECISION\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Final Decision is a role assigned to a module in Self-Refine to make the final decision based on all inputs<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"DECOMPOSITION MODULE\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Decomposition Module is a role assigned to a module in the Divide and Conquer Agent to decompose the problem into sub-problems<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <node id=\"SPECIALIZED EXPERT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">Specialized Expert is a role assigned to a module in the Divide and Conquer Agent to solve sub-problems<\/data>      <data key=\"d2\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/node>    <edge source=\"STEP-BACK ABSTRACTION\" target=\"ZHENG\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Zheng is one of the authors associated with the Step-back Abstraction baseline<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"ROLE ASSIGNMENT\" target=\"XU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Xu is one of the authors associated with the Role Assignment baseline<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"PHYSICS CRITIC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Physics Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"CHEMISTRY CRITIC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chemistry Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"BIOLOGY CRITIC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Biology Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"GENERAL CRITIC\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">General Critic is a unique role assigned to a critic module in Self-Refine<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SELF-REFINE\" target=\"FINAL DECISION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Final Decision is a role assigned to a module in Self-Refine to make the final decision based on all inputs<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"PHYSICS EXPERT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Physics Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"CHEMISTRY EXPERT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chemistry Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"BIOLOGY EXPERT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Biology Expert is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"LLM-DEBATE\" target=\"SCIENCE GENERALIST\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Science Generalist is a unique role assigned to a debate module in LLM-Debate<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"MULTI-STEP PEER REVIEW AGENT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search discovered the Multi-Step Peer Review Agent during the search in the Reading Comprehension domain (GPQA)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"DIVIDE AND CONQUER AGENT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search discovered the Divide and Conquer Agent during the search in the Reading Comprehension domain (GPQA)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Meta Agent Search discovered the Verified Multimodal Agent during the search in the Math domain (MGSM)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"SHENGRAN HU\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Shengran Hu is associated with the detailed implementations of all baselines and discovered agents by Meta Agent Search<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"MULTI-STEP PEER REVIEW AGENT\" target=\"GPQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Multi-Step Peer Review Agent was discovered during the search in the Reading Comprehension domain (GPQA)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"DIVIDE AND CONQUER AGENT\" target=\"GPQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Divide and Conquer Agent was discovered during the search in the Reading Comprehension domain (GPQA)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"DIVIDE AND CONQUER AGENT\" target=\"DECOMPOSITION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Decomposition Module is a role assigned to a module in the Divide and Conquer Agent to decompose the problem into sub-problems<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"DIVIDE AND CONQUER AGENT\" target=\"SPECIALIZED EXPERT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Specialized Expert is a role assigned to a module in the Divide and Conquer Agent to solve sub-problems<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"VERIFIED MULTIMODAL AGENT\" target=\"MGSM\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent was discovered during the search in the Math domain (MGSM)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"REIN\" target=\"GPQA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Rein is one of the authors associated with the Reading Comprehension domain (GPQA)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"SHI\" target=\"MGSM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Shi is one of the authors associated with the Math domain (MGSM)<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>    <edge source=\"COT\" target=\"COT-SC\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both COT and COT-SC are methods used to prompt the FM to think step by step before answering the question<\/data>      <data key=\"d5\">97457e990eb6e3c88c11c862f9e3265b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ef75d2c866bee783577ed9f65707cf13","chunk":"4 decomposition_module = FM_Module ([ \u2019thinking \u2019, \u2019 sub_problems \u2019], \u2019\nDecomposition Module \u2019)\n5\n6 # Step 2: Assign each sub - problem to a specialized expert\n7 sub_problem_instruction = \" Please think step by step and then\nsolve the sub - problem .\"\n8 specialized_experts = [ FM_Module ([ \u2019thinking \u2019, \u2019 sub_solution \u2019], \u2019\nSpecialized Expert \u2019, role = role ) for role in [\u2019Physics Expert \u2019\n, \u2019Chemistry Expert \u2019, \u2019Biology Expert \u2019, \u2019General Expert \u2019]]\n9\n10 # Step 3: Integrate the sub - problem solutions into the final\nanswer\n11 integration_instruction = \" Given the solutions to the sub -\nproblems , integrate them to provide a final answer to the\noriginal problem .\"\n12 integration_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019\nIntegration Module \u2019, temperature =0.1)\n13\n14 # Decompose the problem\n15 thinking , sub_problems = decomposition_module ([ taskInfo ],\ndecomposition_instruction )\n16\n17 # Ensure sub_problems is a string and split into individual sub -\nproblems\n18 sub_problems_list = sub_problems . content . split (\u2019\\n\u2019) if\nisinstance ( sub_problems . content , str) else []\n19\n20 # Solve each sub - problem\n21 sub_solutions = []\n22 for i, sub_problem in enumerate ( sub_problems_list ):\n23 sub_problem_info = Info (\u2019 sub_problem \u2019, decomposition_module .\n__repr__ () , sub_problem , i)\n24 sub_thinking , sub_solution = specialized_experts [i % len (\n37Automated Design of Agentic Systems\nspecialized_experts )]([ sub_problem_info ],\nsub_problem_instruction )\n25 sub_solutions . append ( sub_solution )\n26\n27 # Integrate the sub - problem solutions\n28 integration_inputs = [ taskInfo ] + sub_solutions\n29 thinking , answer = integration_module ( integration_inputs ,\nintegration_instruction )\n30\n31 return answer\nCode 6|Example discovered agent: Verified Multimodal Agent\n1def forward (self , taskInfo ):\n2 # Instruction for generating visual representation of the\nproblem\n3 visual_instruction = \" Please create a visual representation (e.g\n., diagram , graph ) of the given problem .\"\n4\n5 # Instruction for verifying the visual representation\n6 verification_instruction = \" Please verify the accuracy and\nrelevance of the visual representation . Provide feedback and\nsuggestions for improvement if necessary .\"\n7\n8 # Instruction for solving the problem using the verified visual\naid\n9 cot_instruction = \" Using the provided visual representation ,\nthink step by step and solve the problem .\"\n10\n11 # Instantiate the visual representation module , verification\nmodule , and Chain -of - Thought module\n12 visual_module = FM_Module ([ \u2019visual \u2019], \u2019Visual Representation\nModule \u2019)\n13 verification_module = FM_Module ([ \u2019feedback \u2019, \u2019 verified_visual \u2019],\n\u2019 Verification Module \u2019)\n14 cot_module = FM_Module ([ \u2019thinking \u2019, \u2019answer \u2019], \u2019Chain -of - Thought\nModule \u2019)\n15\n16 # Generate the visual representation of the problem\n17 visual_output = visual_module ([ taskInfo ], visual_instruction )\n18 visual_representation = visual_output [0] # Using Info object\ndirectly\n19\n20 # Verify the visual representation\n21 feedback , verified_visual = verification_module ([ taskInfo ,\nvisual_representation ], verification_instruction )\n22\n23 # Use the verified visual representation to solve the problem\n24 thinking , answer = cot_module ([ taskInfo , verified_visual ],\ncot_instruction )\n25 return answer\n38Automated Design of Agentic Systems\nG. Cost of Experiments\nA single run of search and evaluation on ARC (Section 4.1) costs approximately $500 USD in OpenAI\nAPI costs, while a run within the reasoning and problem-solving domains (Section 4.2) costs about\n$300 USD.\nThe primary expense comes from querying the \u201cgpt-3.5-turbo-0125\u201d model during the evaluation\nof discovered agents. Notably, the latest GPT-4 model, \u201cgpt-4o-mini,\u201d is less than one-third the price\nof \u201cgpt-3.5-turbo-0125\u201d and offers better performance, suggesting that we could achieve improved\nresults with Meta Agent Search at just one-third of the cost. Additionally, as discussed in Section 6, the\ncurrent naive evaluation function is both expensive and overlooks valuable information. We anticipate\nthat future work adopting more sophisticated evaluation functions could significantly reduce the cost\nof ADAS algorithms.\n39","chunk_id":"ef75d2c866bee783577ed9f65707cf13","document_ids":["59656444e942d1316dac3a6c8f1827a5"],"n_tokens":964,"entities":[{"name":"DECOMPOSITION MODULE","type":"TECHNOLOGY","description":"The Decomposition Module is a system designed to break down a problem into sub-problems for easier solving","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SPECIALIZED EXPERT","type":"TECHNOLOGY","description":"Specialized Expert is a system that assigns each sub-problem to a specialized expert in fields such as Physics, Chemistry, Biology, or General knowledge","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"INTEGRATION MODULE","type":"TECHNOLOGY","description":"The Integration Module is a system that integrates the solutions of sub-problems to provide a final answer to the original problem","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VERIFIED MULTIMODAL AGENT","type":"TECHNOLOGY","description":"Verified Multimodal Agent is an agent that uses visual representation, verification, and Chain-of-Thought modules to solve problems","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VISUAL REPRESENTATION MODULE","type":"TECHNOLOGY","description":"The Visual Representation Module is a system that generates visual representations of problems","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VERIFICATION MODULE","type":"TECHNOLOGY","description":"The Verification Module is a system that verifies the accuracy and relevance of visual representations and provides feedback","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"CHAIN-OF-THOUGHT MODULE","type":"TECHNOLOGY","description":"The Chain-of-Thought Module is a system that uses verified visual representations to solve problems step by step","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"ARC","type":"DATASET","description":"ARC (AI2 Reasoning Challenge) is a dataset used for evaluating search and evaluation algorithms","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization providing the API services used in the experiments","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"GPT-3.5-TURBO-0125","type":"TECHNOLOGY","description":"GPT-3.5-turbo-0125 is a model used during the evaluation of discovered agents, known for its high cost","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"GPT-4O-MINI","type":"TECHNOLOGY","description":"GPT-4o-mini is a newer model that is less expensive and offers better performance compared to GPT-3.5-turbo-0125","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses various modules and models to discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"ADAS ALGORITHMS","type":"TECHNOLOGY","description":"ADAS (Automated Design of Agentic Systems) algorithms are used for designing and evaluating agentic systems","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"FM_MODULE","type":"TECHNOLOGY","description":"FM_Module is a system used to create various modules such as Decomposition Module, Specialized Expert, Integration Module, Visual Representation Module, Verification Module, and Chain-of-Thought Module","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"TASKINFO","type":"DATASET","description":"TaskInfo is the input data used in various modules for problem-solving","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"INFO","type":"TECHNOLOGY","description":"Info is a system used to encapsulate information about sub-problems and their solutions","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB_PROBLEM","type":"DATASET","description":"Sub_problem is a part of the original problem that is broken down for easier solving","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"SUB_SOLUTION","type":"DATASET","description":"Sub_solution is the solution to a sub-problem","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VISUAL_OUTPUT","type":"DATASET","description":"Visual_output is the output generated by the Visual Representation Module","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"VISUAL_REPRESENTATION","type":"DATASET","description":"Visual_representation is the visual depiction of a problem generated by the Visual Representation Module","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"FEEDBACK","type":"DATASET","description":"Feedback is the response provided by the Verification Module regarding the accuracy and relevance of the visual representation","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"ANSWER","type":"DATASET","description":"Answer is the final solution to the original problem after integrating sub-problem solutions","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"COST OF EXPERIMENTS","type":"DATASET","description":"Cost of Experiments refers to the financial expense incurred during the search and evaluation processes","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"REASONING AND PROBLEM-SOLVING DOMAINS","type":"DATASET","description":"Reasoning and Problem-Solving Domains are areas where the experiments are conducted, costing about $300 USD","source_id":"ef75d2c866bee783577ed9f65707cf13"},{"name":"EVALUATION FUNCTION","type":"TECHNOLOGY","description":"Evaluation Function is a system used to assess the performance of discovered agents","source_id":"ef75d2c866bee783577ed9f65707cf13"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"DECOMPOSITION MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Decomposition Module is a system designed to break down a problem into sub-problems for easier solving<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SPECIALIZED EXPERT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Specialized Expert is a system that assigns each sub-problem to a specialized expert in fields such as Physics, Chemistry, Biology, or General knowledge<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"INTEGRATION MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Integration Module is a system that integrates the solutions of sub-problems to provide a final answer to the original problem<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VERIFIED MULTIMODAL AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Verified Multimodal Agent is an agent that uses visual representation, verification, and Chain-of-Thought modules to solve problems<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VISUAL REPRESENTATION MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Visual Representation Module is a system that generates visual representations of problems<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VERIFICATION MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Verification Module is a system that verifies the accuracy and relevance of visual representations and provides feedback<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"CHAIN-OF-THOUGHT MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Chain-of-Thought Module is a system that uses verified visual representations to solve problems step by step<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">ARC (AI2 Reasoning Challenge) is a dataset used for evaluating search and evaluation algorithms<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization providing the API services used in the experiments<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO-0125\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-turbo-0125 is a model used during the evaluation of discovered agents, known for its high cost<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"GPT-4O-MINI\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4o-mini is a newer model that is less expensive and offers better performance compared to GPT-3.5-turbo-0125<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses various modules and models to discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"ADAS ALGORITHMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">ADAS (Automated Design of Agentic Systems) algorithms are used for designing and evaluating agentic systems<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"FM_MODULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">FM_Module is a system used to create various modules such as Decomposition Module, Specialized Expert, Integration Module, Visual Representation Module, Verification Module, and Chain-of-Thought Module<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"TASKINFO\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">TaskInfo is the input data used in various modules for problem-solving<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"INFO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Info is a system used to encapsulate information about sub-problems and their solutions<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB_PROBLEM\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Sub_problem is a part of the original problem that is broken down for easier solving<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"SUB_SOLUTION\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Sub_solution is the solution to a sub-problem<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VISUAL_OUTPUT\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Visual_output is the output generated by the Visual Representation Module<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"VISUAL_REPRESENTATION\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Visual_representation is the visual depiction of a problem generated by the Visual Representation Module<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"FEEDBACK\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Feedback is the response provided by the Verification Module regarding the accuracy and relevance of the visual representation<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"ANSWER\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Answer is the final solution to the original problem after integrating sub-problem solutions<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"COST OF EXPERIMENTS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Cost of Experiments refers to the financial expense incurred during the search and evaluation processes<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Reasoning and Problem-Solving Domains are areas where the experiments are conducted, costing about $300 USD<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <node id=\"EVALUATION FUNCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Evaluation Function is a system used to assess the performance of discovered agents<\/data>      <data key=\"d2\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/node>    <edge source=\"DECOMPOSITION MODULE\" target=\"SPECIALIZED EXPERT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Decomposition Module assigns each sub-problem to a Specialized Expert<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"DECOMPOSITION MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Decomposition Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"DECOMPOSITION MODULE\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TaskInfo is used as input data in the Decomposition Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"SPECIALIZED EXPERT\" target=\"INTEGRATION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Specialized Experts solve sub-problems, and the Integration Module integrates these solutions<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"SPECIALIZED EXPERT\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Specialized Expert<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INTEGRATION MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Integration Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INTEGRATION MODULE\" target=\"ANSWER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Answer is generated by the Integration Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFIED MULTIMODAL AGENT\" target=\"VISUAL REPRESENTATION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent uses the Visual Representation Module to generate visual representations of problems<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFIED MULTIMODAL AGENT\" target=\"VERIFICATION MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent uses the Verification Module to verify visual representations<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFIED MULTIMODAL AGENT\" target=\"CHAIN-OF-THOUGHT MODULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Verified Multimodal Agent uses the Chain-of-Thought Module to solve problems using verified visual representations<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VISUAL REPRESENTATION MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Visual Representation Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VISUAL REPRESENTATION MODULE\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TaskInfo is used as input data in the Visual Representation Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFICATION MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Verification Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFICATION MODULE\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TaskInfo is used as input data in the Verification Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFICATION MODULE\" target=\"VISUAL_REPRESENTATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Visual_representation is verified by the Verification Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VERIFICATION MODULE\" target=\"FEEDBACK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Feedback is provided by the Verification Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT MODULE\" target=\"FM_MODULE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">FM_Module is used to create the Chain-of-Thought Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"CHAIN-OF-THOUGHT MODULE\" target=\"TASKINFO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">TaskInfo is used as input data in the Chain-of-Thought Module<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"ARC\" target=\"META AGENT SEARCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">ARC is a dataset used for evaluating the performance of Meta Agent Search<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"OPENAI\" target=\"GPT-3.5-TURBO-0125\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">OpenAI provides the GPT-3.5-turbo-0125 model used in the experiments<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"OPENAI\" target=\"GPT-4O-MINI\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">OpenAI provides the GPT-4o-mini model, which is less expensive and offers better performance<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"OPENAI\" target=\"COST OF EXPERIMENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Cost of Experiments is incurred due to querying OpenAI's models<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses the GPT-3.5-turbo-0125 model during the evaluation of discovered agents<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO-0125\" target=\"COST OF EXPERIMENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cost of Experiments is high due to the use of GPT-3.5-turbo-0125<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"GPT-4O-MINI\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search could achieve improved results using the GPT-4o-mini model at a lower cost<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"GPT-4O-MINI\" target=\"COST OF EXPERIMENTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Cost of Experiments could be reduced by using GPT-4o-mini<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"ADAS ALGORITHMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search is a method used within ADAS algorithms<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"EVALUATION FUNCTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Evaluation Function is used to assess the performance of Meta Agent Search<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INFO\" target=\"SUB_PROBLEM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Info encapsulates information about sub-problems<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"INFO\" target=\"SUB_SOLUTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Info encapsulates information about sub-problem solutions<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"VISUAL_OUTPUT\" target=\"VISUAL_REPRESENTATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Visual_output contains the visual representation of a problem<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>    <edge source=\"COST OF EXPERIMENTS\" target=\"REASONING AND PROBLEM-SOLVING DOMAINS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Reasoning and Problem-Solving Domains incur a cost of about $300 USD<\/data>      <data key=\"d5\">ef75d2c866bee783577ed9f65707cf13<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"6fe27f9eb76cf2ddf712a2cee5783d1c","chunk":"AgentInstruct:\nToward Generative Teaching with Agentic\nFlows\nArindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan,\nDany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos,\nCorby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, Ahmed Awadallah\nMicrosoft Research\nAbstract\nSynthetic data is becoming increasingly important for accelerating the development of\nlanguage models, both large and small. Despite several successful use cases, researchers\nalso raised concerns around model collapse and drawbacks of imitating other models. This\ndiscrepancy can be attributed to the fact that synthetic data varies in quality and diversity.\nEffective use of synthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data by powerful\nmodels to teach a new skill or behavior to another model, we refer to this setting as Generative\nTeaching . We introduce AgentInstruct, an extensible agentic framework for automatically\ncreating large amounts of diverse and high-quality synthetic data. AgentInstruct can create\nboth the prompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset\nof 25M pairs to teach language models different skills, such as text editing, creative writing,\ntool usage, coding, reading comprehension, etc. The dataset can be used for instruction\ntuning of any base model. We post-train Mistral-7b with the data. When comparing\nthe resulting model (Orca-3) to Mistral-7b-Instruct (which uses the same base model), we\nobserve significant improvements across many benchmarks. For example, 40% improvement\non AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms\nother models such as LLAMA-8B-instruct and GPT-3.5-turbo.\n \n 0102030405060AGIEVAL\n010203040506070MMLU\n010203040506070BBH\n0102030405060708090GSM8K\n0510152025ALPACA\nEVAL\n0102030405060708090FOFO\n010203040506070MIRAGE -\nRAG\n+40.2%  +19.4 % \n+38.3 % +53.7%  \n+45.0% +38.3%  \n+46.6% \nMistral -Instruct -7B Mistra l-7B + AgentInstruct (Orca -3) \nFigure 1: Effect of using AgentInstruct data for post-training Mistral-7BarXiv:2407.03502v1  [cs.AI]  3 Jul 20241 Introduction\nSynthetic data accelerated the development of LLMS : The rise of synthetic data in\nthe training of Large Language Models (LLMs) has been a significant development of the\nlast year. Synthetic data was used to significantly accelerate the progress of model training\n(especially SLMs) in all stages of training from pre-training (e.g., [ 1]), to instruction-tuning\n(e.g., [21, 36]) and RLHF(e.g., [12, 28]).\nGenerating high quality synthetic data is hard : On the other hand, research has also\nshown that pre-training models on synthetic data generated by other models can lead to\nmodel collapse [ 29], leading to models gradually degenerating as a result. Similar arguments\nhave been made against using synthetic data for pos-training, which could amount to an\nimitation process that could teach the trained model to pick only stylistic characteristics\nand not real capabilities [ 8]. This discrepancy could be explained by the observation that\ncreating high-quality and diverse synthetic data is hard [ 17]. Successful use of synthetic data\ninvolved significant human effort in curating and filtering the data to ensure high quality. If\nwe focus on post-training synthetic data, we will see the most widely used approach includes\nstarting with a set of prompts and using a powerful model such as GPT-4 [ 22] to generate\nresponses to these prompts [ 24] or of an expanded set of the prompts [ 36]. This recipe\nwas further improved by eliciting explanations or step-by-step instructions from the teacher\nmodel [20] or using more complex prompting techniques to elicit higher quality answers [ 18].\nSynthetic data meets Agents : Another major development we witnessed last year is the\nrise of Agentic (especially multiagent) workflows [ 33,13]. Agentic workflows can generate\nhigh quality data, that surpasses the capabilities of the underlying LLMs, by using flows\nwith reflection and iteration, where agents can look back at solutions, generate critiques and\nimprove solutions. They can also use tools (e.g. search apis, calculator, code interpreters)\naddressing limitations of LLMs. Multi-agent workflows bring in additional benefits such\nas simulating scenarios where we can generate both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nte","chunk_id":"6fe27f9eb76cf2ddf712a2cee5783d1c","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data, using raw data sources like text documents and code files as seeds","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"LUCIANO DEL CORRO","type":"PERSON","description":"Luciano Del Corro is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GUOQING ZHENG","type":"PERSON","description":"Guoqing Zheng is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SHWETI MAHAJAN","type":"PERSON","description":"Shweti Mahajan is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"DANY ROUHANA","type":"PERSON","description":"Dany Rouhana is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ANDRES CODAS","type":"PERSON","description":"Andres Codas is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"YADONG LU","type":"PERSON","description":"Yadong Lu is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"WEI-GE CHEN","type":"PERSON","description":"Wei-ge Chen is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"OLGA VROUSGOS","type":"PERSON","description":"Olga Vrousgos is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"FILLIPE SILVA","type":"PERSON","description":"Fillipe Silva is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"HAMED KHANPOUR","type":"PERSON","description":"Hamed Khanpour is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"YASH LARA","type":"PERSON","description":"Yash Lara is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MICROSOFT RESEARCH","type":"ORGANIZATION","description":"Microsoft Research is the organization where the authors of the paper \"AgentInstruct: Toward Generative Teaching with Agentic Flows\" are affiliated","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SYNTHETIC DATA","type":"TECHNOLOGY","description":"Synthetic data is artificially generated data used to accelerate the development of language models, both large and small","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GENERATIVE TEACHING","type":"TECHNOLOGY","description":"Generative Teaching is a setting where synthetic data is created by powerful models to teach a new skill or behavior to another model","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MISTRAL-7B","type":"TECHNOLOGY","description":"Mistral-7b is a base language model that was post-trained using data generated by AgentInstruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ORCA-3","type":"TECHNOLOGY","description":"Orca-3 is the resulting model from post-training Mistral-7b with data generated by AgentInstruct, showing significant improvements across many benchmarks","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 40% improvement over Mistral-7b-Instruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 19% improvement over Mistral-7b-Instruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 54% improvement over Mistral-7b-Instruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 38% improvement over Mistral-7b-Instruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 45% improvement over Mistral-7b-Instruct","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GPT-3.5-TURBO","type":"TECHNOLOGY","description":"GPT-3.5-turbo is a language model that was outperformed by Orca-3 in various benchmarks","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"LLMS","type":"TECHNOLOGY","description":"LLMs, or Large Language Models, are advanced language models that have been significantly developed using synthetic data","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SLMS","type":"TECHNOLOGY","description":"SLMs, or Small Language Models, are smaller-scale language models that also benefit from synthetic data in their training","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"RLHF","type":"TECHNOLOGY","description":"RLHF, or Reinforcement Learning from Human Feedback, is a technique used in the training of language models","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a powerful language model used to generate responses to prompts for creating synthetic data","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"SEARCH APIS","type":"TECHNOLOGY","description":"Search APIs are tools used in agentic workflows to enhance the capabilities of language models","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CALCULATOR","type":"TECHNOLOGY","description":"Calculator is a tool used in agentic workflows to enhance the capabilities of language models","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"CODE INTERPRETERS","type":"TECHNOLOGY","description":"Code interpreters are tools used in agentic workflows to enhance the capabilities of language models","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MULTI-AGENT WORKFLOWS","type":"TECHNOLOGY","description":"Multi-agent workflows are agentic workflows that involve multiple agents to generate high-quality data and automate data generation tasks","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"MISTRAL-7B-INSTRUCT","type":"TECHNOLOGY","description":"Mistral-7b-Instruct is a version of the Mistral-7b model used for instruction tuning, which was outperformed by Orca-3","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"},{"name":"LLAMA-8B-INSTRUCT","type":"","description":"","source_id":"6fe27f9eb76cf2ddf712a2cee5783d1c"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data, using raw data sources like text documents and code files as seeds<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"LUCIANO DEL CORRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luciano Del Corro is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GUOQING ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guoqing Zheng is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SHWETI MAHAJAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shweti Mahajan is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"DANY ROUHANA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dany Rouhana is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ANDRES CODAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andres Codas is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"YADONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yadong Lu is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"WEI-GE CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wei-ge Chen is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"OLGA VROUSGOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olga Vrousgos is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"FILLIPE SILVA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fillipe Silva is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"HAMED KHANPOUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamed Khanpour is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"YASH LARA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yash Lara is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is one of the authors of the paper titled \"AgentInstruct: Toward Generative Teaching with Agentic Flows\"<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MICROSOFT RESEARCH\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft Research is the organization where the authors of the paper \"AgentInstruct: Toward Generative Teaching with Agentic Flows\" are affiliated<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Synthetic data is artificially generated data used to accelerate the development of language models, both large and small<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Generative Teaching is a setting where synthetic data is created by powerful models to teach a new skill or behavior to another model<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Mistral-7b is a base language model that was post-trained using data generated by AgentInstruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca-3 is the resulting model from post-training Mistral-7b with data generated by AgentInstruct, showing significant improvements across many benchmarks<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 40% improvement over Mistral-7b-Instruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 19% improvement over Mistral-7b-Instruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 54% improvement over Mistral-7b-Instruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 38% improvement over Mistral-7b-Instruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of language models, where Orca-3 showed a 45% improvement over Mistral-7b-Instruct<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-turbo is a language model that was outperformed by Orca-3 in various benchmarks<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"LLMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLMs, or Large Language Models, are advanced language models that have been significantly developed using synthetic data<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SLMS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">SLMs, or Small Language Models, are smaller-scale language models that also benefit from synthetic data in their training<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"RLHF\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RLHF, or Reinforcement Learning from Human Feedback, is a technique used in the training of language models<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a powerful language model used to generate responses to prompts for creating synthetic data<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"SEARCH APIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search APIs are tools used in agentic workflows to enhance the capabilities of language models<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CALCULATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Calculator is a tool used in agentic workflows to enhance the capabilities of language models<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"CODE INTERPRETERS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Code interpreters are tools used in agentic workflows to enhance the capabilities of language models<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MULTI-AGENT WORKFLOWS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multi-agent workflows are agentic workflows that involve multiple agents to generate high-quality data and automate data generation tasks<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Mistral-7b-Instruct is a version of the Mistral-7b model used for instruction tuning, which was outperformed by Orca-3<\/data>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/node>    <edge source=\"AGENTINSTRUCT\" target=\"GENERATIVE TEACHING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">AgentInstruct is used to create synthetic data for Generative Teaching<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MISTRAL-7B\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct was used to generate data for post-training Mistral-7b<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ARINDAM MITRA\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Arindam Mitra is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"LUCIANO DEL CORRO\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Luciano Del Corro is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"GUOQING ZHENG\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Guoqing Zheng is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SHWETI MAHAJAN\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Shweti Mahajan is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"DANY ROUHANA\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Dany Rouhana is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ANDRES CODAS\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Andres Codas is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"YADONG LU\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yadong Lu is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"WEI-GE CHEN\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wei-ge Chen is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"OLGA VROUSGOS\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Olga Vrousgos is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"CORBY ROSSET\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Corby Rosset is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"FILLIPE SILVA\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Fillipe Silva is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"HAMED KHANPOUR\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hamed Khanpour is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"YASH LARA\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Yash Lara is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"AHMED AWADALLAH\" target=\"MICROSOFT RESEARCH\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Ahmed Awadallah is affiliated with Microsoft Research<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"LLMS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Synthetic data has been used to significantly accelerate the development of Large Language Models (LLMs)<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"SLMS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Synthetic data has been used to significantly accelerate the development of Small Language Models (SLMs)<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"RLHF\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Synthetic data has been used in the training of language models through Reinforcement Learning from Human Feedback (RLHF)<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SYNTHETIC DATA\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used to generate responses to prompts for creating synthetic data<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 is the model resulting from post-training Mistral-7b with data generated by AgentInstruct<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 showed a 40% improvement over Mistral-7b-Instruct on the AGIEval benchmark<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 showed a 19% improvement over Mistral-7b-Instruct on the MMLU benchmark<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 showed a 54% improvement over Mistral-7b-Instruct on the GSM8K benchmark<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 showed a 38% improvement over Mistral-7b-Instruct on the BBH benchmark<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3 showed a 45% improvement over Mistral-7b-Instruct on the AlpacaEval benchmark<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 consistently outperformed LLAMA-8B-instruct in various benchmarks<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5-TURBO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 consistently outperformed GPT-3.5-turbo in various benchmarks<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Orca-3 showed significant improvements over Mistral-7b-Instruct across many benchmarks<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"SEARCH APIS\" target=\"MULTI-AGENT WORKFLOWS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Search APIs are tools used in multi-agent workflows to enhance the capabilities of language models<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"CALCULATOR\" target=\"MULTI-AGENT WORKFLOWS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Calculator is a tool used in multi-agent workflows to enhance the capabilities of language models<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>    <edge source=\"CODE INTERPRETERS\" target=\"MULTI-AGENT WORKFLOWS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Code interpreters are tools used in multi-agent workflows to enhance the capabilities of language models<\/data>      <data key=\"d5\">6fe27f9eb76cf2ddf712a2cee5783d1c<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"b88745a13b69cecbc0ee9c3af41389bf","chunk":" both new prompts and the corresponding\nresponses. They also enable automation of the data generation workflows reducing or\neliminating need for human intervention on some tasks.\nGenerative Teaching & Orca AgentInstruct : Generating synthetic data for post-\ntraining often relies on an existing prompt set that is used as is or used as seeds for\ngenerating more instructions. In this work, we generalize the problem settings to a broader\nobjective of generating abundant amounts of diverse, challenging and high-quality data to\nteach a particular skill to an AI model, we refer to this setting as Generative Teaching.\nAgentInstruct is an agentic solution for Generative Teaching. AgentInstruct focuses on\ncreating demonstration and feedback data and requires only raw documents as input. When\ngeneric data is used as seeds, AgentInstruct can be used to teach an LLM a general capability\n(e.g. Math, Reasoning, RAG, etc.). Domain specific data (e.g. gaming, finance) can also be\nused as seeds to improve the model in a certain specialization. AgentInstruct can create:\n1.High-quality data: using powerful models like GPT-4, coupled with tools like search\nand code interpreters.\n2.Diverse data: AgentInstruct generates both prompts and responses. It uses a large\nnumber of agents (equipped with powerful LLMs, tools and reflection flows) and a\ntaxonomy (of over 100 subcategories) to create diverse and high quality prompts\nand responses,\n3.Large quantities of data: AgentInstruct can run autonomously and can apply flows\nfor verification and data filtering. It does not require seed prompts and uses raw\ndocuments for seeding.\nUsing raw data (unstructured text documents or source code) as seeds has two benefits.\nFirst, this data is available in abundance enabling the use of AgentInstruct to create large\namounts of diverse data. Additionally, using raw data as seeds, and hence, avoiding using\nexisting prompts, as is or after paraphrasing, can promote learning more general capabilities\nas opposed to benchmark-specific ones.\nWe demonstrate the utility of AgentInstruct by creating a comprehensive synthetic post-\ntraining dataset of 25 million prompt and response pairs. The dataset covers a wide array\n2of skills including creative writing, reasoning, math, RAG, tool use, etc. To assess the\nvalue of the data, we use it to finetune Mistral-7B[11] model. The finetuned Mistral model\n(Orca-3) shows significant improvement over other instruction-tuned models using the same\nbase model. For example, compared to Mistral-Instruct-7B, it shows 40% improvement on\nAGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement\non BBH, 45% improvement on AlpacaEval and 31.34% reduction on hallucination across\nmultiple summarization benchmarks. Additionally, it outperforms other models such as\nLLAMA-8B-instruct and GPT-3.5 on multiple benchmarks. Note that the only seed data\nused is publicly available raw materials and no task-specific or benchmark data has been\nused as seeds.\nWhilewedemonstratetheutilityofAgentInstructbycreatingagenericpost-trainingsynthetic\ndataset, we believe that agents can enable the creation of Synthetic-Data-Generation-As-A-\nService where we start with raw materials (e.g. web data for general model training or domain\nspecific data for specialized models), and we generate data for post-training and finetuning,\nhence enabling continual learning and improvement of any base LLM. Additionally, we\nbelieve that the AgentInstruct approach can be used for self-improvement of larger, more\ncapable models because of: (1) the ability to generate new prompts and (2) the ability to\ngenerate responses that exceed the quality of the LLM used in the agentic flow (because of\nthe use of tools, reflection, etc.).\n2 Generative Teaching: AgentInstruct\nCreating synthetic datasets for supervised fine-tuning and instruction-tuning has seen\nsignificant progress over the last year. The quality of these datasets has been steadily\nimproving. High quality can be achieved by using powerful frontier models (or agenetic flows\nbased on these models) to generate responses. However, when creating synthetic data, in\naddition to quality, we also need to consider several other fundamental questions:\n1. How can we create a vast amount of data?\n2. How can we ensure that the generated data is diverse?\n3. How can we generate complex or nuanced data points?\nIn the AgentInstruct methodology, we outline a structured approach to tackle these challenges\nas follows:\nFigure 2: Concise Summary of the AgentInstruct Methodology\n1. Assemble a collection of raw seeds (e.g., textbook chapters, web articles, code\nsnippets).\n2.foreach seed in the collection do\n3. Transform the seed with the aid of one or more content transformation Agents (\nContent Transformation Flow).\n4. Route it through a series of instruction creation Agents to create a diverse set of\ninstructions (Seed Instruction Creation Flow).\n5. Utilize another group of Refinement Agents to iteratively refine the complexity\nand quality of the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and","chunk_id":"b88745a13b69cecbc0ee9c3af41389bf","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"GENERATIVE TEACHING","type":"METHODOLOGY, CONCEPT","description":"Generative Teaching is a methodology aimed at generating abundant amounts of diverse, challenging, and high-quality data to teach a particular skill to an AI model","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY, SYSTEM","description":"AgentInstruct is an agentic solution for Generative Teaching that focuses on creating demonstration and feedback data using raw documents as input. It can generate high-quality, diverse, and large quantities of data autonomously","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GPT-4","type":"TECHNOLOGY, MODEL","description":"GPT-4 is a powerful model used by AgentInstruct to generate high-quality data, coupled with tools like search and code interpreters","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"MISTRAL-7B","type":"MODEL","description":"Mistral-7B is a model that was fine-tuned using the synthetic post-training dataset created by AgentInstruct, resulting in the Orca-3 model","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is the fine-tuned version of the Mistral-7B model, showing significant improvements over other instruction-tuned models using the same base model","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 40% improvement compared to Mistral-Instruct-7B","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 19% improvement compared to Mistral-Instruct-7B","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 54% improvement compared to Mistral-Instruct-7B","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 38% improvement compared to Mistral-Instruct-7B","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 45% improvement compared to Mistral-Instruct-7B","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"BENCHMARK"},{"name":"LLAMA-8B-INSTRUCT","type":"MODEL","description":"LLAMA-8B-Instruct is a model that Orca-3 outperformed on multiple benchmarks","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"GPT-3.5","type":"MODEL","description":"GPT-3.5 is a model that Orca-3 outperformed on multiple benchmarks","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE","type":"SERVICE, CONCEPT","description":"Synthetic-Data-Generation-As-A-Service is a concept where agents start with raw materials and generate data for post-training and fine-tuning, enabling continual learning and improvement of any base LLM","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SERVICE, CONCEPT"},{"name":"CONTENT TRANSFORMATION AGENTS","type":"TECHNOLOGY, SYSTEM","description":"Content Transformation Agents are used in the AgentInstruct methodology to transform raw seeds into diverse sets of instructions","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"TECHNOLOGY, SYSTEM"},{"name":"REFINEMENT AGENTS","type":"TECHNOLOGY, SYSTEM","description":"Refinement Agents are used in the AgentInstruct methodology to iteratively refine the complexity and quality of seed instructions","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"TECHNOLOGY, SYSTEM"},{"name":"RAW SEEDS","type":"DATA, INPUT","description":"Raw seeds are unstructured text documents or source code used as input in the AgentInstruct methodology to generate diverse and high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"DATA, INPUT"},{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"Mistral-Instruct-7B is a model used as a comparison benchmark, where Orca-3 showed significant improvements","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"MODEL"},{"name":"POST-TRAINING DATASET","type":"DATASET","description":"The post-training dataset created by AgentInstruct consists of 25 million prompt and response pairs covering a wide array of skills","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"DATASET"},{"name":"INSTRUCTION CREATION AGENTS","type":"TECHNOLOGY, SYSTEM","description":"Instruction Creation Agents are used in the AgentInstruct methodology to create a diverse set of instructions from raw seeds","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"TECHNOLOGY, SYSTEM"},{"name":"REFLECTION FLOWS","type":"TECHNOLOGY, SYSTEM","description":"Reflection Flows are used in AgentInstruct to enhance the quality of generated responses by leveraging tools and iterative refinement","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"TECHNOLOGY, SYSTEM"},{"name":"RAG","type":"SKILL, CAPABILITY","description":"RAG (Retrieval-Augmented Generation) is one of the general capabilities that can be taught to an LLM using AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SKILL, CAPABILITY"},{"name":"CREATIVE WRITING","type":"SKILL, CAPABILITY","description":"Creative Writing is one of the skills covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SKILL, CAPABILITY"},{"name":"REASONING","type":"SKILL, CAPABILITY","description":"Reasoning is one of the skills covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SKILL, CAPABILITY"},{"name":"MATH","type":"SKILL, CAPABILITY","description":"Math is one of the skills covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SKILL, CAPABILITY"},{"name":"TOOL USE","type":"SKILL, CAPABILITY","description":"Tool Use is one of the skills covered in the synthetic post-training dataset created by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf","entity_type":"SKILL, CAPABILITY"},{"name":"DATA GENERATION WORKFLOWS","type":"PROCESS","description":"Data generation workflows are processes that can be automated to reduce or eliminate the need for human intervention on some tasks","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"PROMPT SET","type":"DATA","description":"A prompt set is an existing collection of prompts used as seeds for generating more instructions in the context of Generative Teaching","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DEMONSTRATION DATA","type":"DATA","description":"Demonstration data is created by AgentInstruct to teach an AI model specific skills or capabilities","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"FEEDBACK DATA","type":"DATA","description":"Feedback data is created by AgentInstruct to provide responses and evaluations to improve AI model performance","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DOMAIN SPECIFIC DATA","type":"DATA","description":"Domain specific data refers to specialized data (e.g., gaming, finance) used as seeds to improve an AI model in a certain specialization","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"GENERAL CAPABILITY","type":"SKILL, CAPABILITY","description":"General capability refers to broad skills such as Math, Reasoning, and RAG that can be taught to an LLM using AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"RAW DOCUMENTS","type":"DATA, INPUT","description":"Raw documents are unstructured text documents used as input for AgentInstruct to generate diverse and high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"VERIFICATION FLOWS","type":"TECHNOLOGY, SYSTEM","description":"Verification flows are processes used by AgentInstruct to verify and filter generated data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"DATA FILTERING","type":"PROCESS","description":"Data filtering is a process used by AgentInstruct to ensure the quality and relevance of generated data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"UNSTRUCTURED TEXT DOCUMENTS","type":"DATA, INPUT","description":"Unstructured text documents are raw data sources used as seeds in the AgentInstruct methodology","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"SOURCE CODE","type":"DATA, INPUT","description":"Source code is a type of raw data used as seeds in the AgentInstruct methodology to generate diverse and high-quality data","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"CONTINUAL LEARNING","type":"PROCESS","description":"Continual learning is a process enabled by Synthetic-Data-Generation-As-A-Service, allowing for ongoing improvement of AI models","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"BASE LLM","type":"MODEL","description":"Base LLM refers to the initial language model that can be continually improved using data generated by AgentInstruct","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"SUPERVISED FINE-TUNING","type":"PROCESS","description":"Supervised fine-tuning is a process of improving AI models using high-quality synthetic datasets","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"INSTRUCTION-TUNING","type":"PROCESS","description":"Instruction-tuning is a process of improving AI models using high-quality synthetic datasets to follow instructions better","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"Content Transformation Flow is a process in the AgentInstruct methodology that transforms raw seeds into diverse sets of instructions","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"SEED INSTRUCTION CREATION FLOW","type":"PROCESS","description":"Seed Instruction Creation Flow is a process in the AgentInstruct methodology that creates a diverse set of instructions from raw seeds","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"REFINEMENT FLOW","type":"PROCESS","description":"Refinement Flow is a process in the AgentInstruct methodology that iteratively refines the complexity and quality of seed instructions","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"WEB DATA","type":"DATA, INPUT","description":"Web data is a type of raw material used as seeds for general model training in Synthetic-Data-Generation-As-A-Service","source_id":"b88745a13b69cecbc0ee9c3af41389bf"},{"name":"TAXONOMY","type":"SYSTEM","description":"A taxonomy of over 100 subcategories is used by AgentInstruct to create diverse and high-quality prompts and responses","source_id":"b88745a13b69cecbc0ee9c3af41389bf"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">METHODOLOGY, CONCEPT<\/data>      <data key=\"d1\">Generative Teaching is a methodology aimed at generating abundant amounts of diverse, challenging, and high-quality data to teach a particular skill to an AI model<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">AgentInstruct is an agentic solution for Generative Teaching that focuses on creating demonstration and feedback data using raw documents as input. It can generate high-quality, diverse, and large quantities of data autonomously<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY, MODEL<\/data>      <data key=\"d1\">GPT-4 is a powerful model used by AgentInstruct to generate high-quality data, coupled with tools like search and code interpreters<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"MISTRAL-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B is a model that was fine-tuned using the synthetic post-training dataset created by AgentInstruct, resulting in the Orca-3 model<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is the fine-tuned version of the Mistral-7B model, showing significant improvements over other instruction-tuned models using the same base model<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 40% improvement compared to Mistral-Instruct-7B<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 19% improvement compared to Mistral-Instruct-7B<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 54% improvement compared to Mistral-Instruct-7B<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 38% improvement compared to Mistral-Instruct-7B<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of AI models, where Orca-3 showed a 45% improvement compared to Mistral-Instruct-7B<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"LLAMA-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA-8B-Instruct is a model that Orca-3 outperformed on multiple benchmarks<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5 is a model that Orca-3 outperformed on multiple benchmarks<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE\">      <data key=\"d0\">SERVICE, CONCEPT<\/data>      <data key=\"d1\">Synthetic-Data-Generation-As-A-Service is a concept where agents start with raw materials and generate data for post-training and fine-tuning, enabling continual learning and improvement of any base LLM<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SERVICE, CONCEPT<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">Content Transformation Agents are used in the AgentInstruct methodology to transform raw seeds into diverse sets of instructions<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">TECHNOLOGY, SYSTEM<\/data>    <\/node>    <node id=\"REFINEMENT AGENTS\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">Refinement Agents are used in the AgentInstruct methodology to iteratively refine the complexity and quality of seed instructions<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">TECHNOLOGY, SYSTEM<\/data>    <\/node>    <node id=\"RAW SEEDS\">      <data key=\"d0\">DATA, INPUT<\/data>      <data key=\"d1\">Raw seeds are unstructured text documents or source code used as input in the AgentInstruct methodology to generate diverse and high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">DATA, INPUT<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-Instruct-7B is a model used as a comparison benchmark, where Orca-3 showed significant improvements<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"POST-TRAINING DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The post-training dataset created by AgentInstruct consists of 25 million prompt and response pairs covering a wide array of skills<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">DATASET<\/data>    <\/node>    <node id=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">Instruction Creation Agents are used in the AgentInstruct methodology to create a diverse set of instructions from raw seeds<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">TECHNOLOGY, SYSTEM<\/data>    <\/node>    <node id=\"REFLECTION FLOWS\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">Reflection Flows are used in AgentInstruct to enhance the quality of generated responses by leveraging tools and iterative refinement<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">TECHNOLOGY, SYSTEM<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is one of the general capabilities that can be taught to an LLM using AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SKILL, CAPABILITY<\/data>    <\/node>    <node id=\"CREATIVE WRITING\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">Creative Writing is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SKILL, CAPABILITY<\/data>    <\/node>    <node id=\"REASONING\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">Reasoning is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SKILL, CAPABILITY<\/data>    <\/node>    <node id=\"MATH\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">Math is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SKILL, CAPABILITY<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">Tool Use is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>      <data key=\"d3\">SKILL, CAPABILITY<\/data>    <\/node>    <node id=\"DATA GENERATION WORKFLOWS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Data generation workflows are processes that can be automated to reduce or eliminate the need for human intervention on some tasks<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"PROMPT SET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A prompt set is an existing collection of prompts used as seeds for generating more instructions in the context of Generative Teaching<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DEMONSTRATION DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Demonstration data is created by AgentInstruct to teach an AI model specific skills or capabilities<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"FEEDBACK DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Feedback data is created by AgentInstruct to provide responses and evaluations to improve AI model performance<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DOMAIN SPECIFIC DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Domain specific data refers to specialized data (e.g., gaming, finance) used as seeds to improve an AI model in a certain specialization<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"GENERAL CAPABILITY\">      <data key=\"d0\">SKILL, CAPABILITY<\/data>      <data key=\"d1\">General capability refers to broad skills such as Math, Reasoning, and RAG that can be taught to an LLM using AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"RAW DOCUMENTS\">      <data key=\"d0\">DATA, INPUT<\/data>      <data key=\"d1\">Raw documents are unstructured text documents used as input for AgentInstruct to generate diverse and high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"VERIFICATION FLOWS\">      <data key=\"d0\">TECHNOLOGY, SYSTEM<\/data>      <data key=\"d1\">Verification flows are processes used by AgentInstruct to verify and filter generated data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"DATA FILTERING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Data filtering is a process used by AgentInstruct to ensure the quality and relevance of generated data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"UNSTRUCTURED TEXT DOCUMENTS\">      <data key=\"d0\">DATA, INPUT<\/data>      <data key=\"d1\">Unstructured text documents are raw data sources used as seeds in the AgentInstruct methodology<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"SOURCE CODE\">      <data key=\"d0\">DATA, INPUT<\/data>      <data key=\"d1\">Source code is a type of raw data used as seeds in the AgentInstruct methodology to generate diverse and high-quality data<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"CONTINUAL LEARNING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Continual learning is a process enabled by Synthetic-Data-Generation-As-A-Service, allowing for ongoing improvement of AI models<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"BASE LLM\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Base LLM refers to the initial language model that can be continually improved using data generated by AgentInstruct<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"SUPERVISED FINE-TUNING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Supervised fine-tuning is a process of improving AI models using high-quality synthetic datasets<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"INSTRUCTION-TUNING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction-tuning is a process of improving AI models using high-quality synthetic datasets to follow instructions better<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Content Transformation Flow is a process in the AgentInstruct methodology that transforms raw seeds into diverse sets of instructions<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Creation Flow is a process in the AgentInstruct methodology that creates a diverse set of instructions from raw seeds<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Refinement Flow is a process in the AgentInstruct methodology that iteratively refines the complexity and quality of seed instructions<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"WEB DATA\">      <data key=\"d0\">DATA, INPUT<\/data>      <data key=\"d1\">Web data is a type of raw material used as seeds for general model training in Synthetic-Data-Generation-As-A-Service<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <node id=\"TAXONOMY\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">A taxonomy of over 100 subcategories is used by AgentInstruct to create diverse and high-quality prompts and responses<\/data>      <data key=\"d2\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/node>    <edge source=\"GENERATIVE TEACHING\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">AgentInstruct is an agentic solution for the Generative Teaching methodology<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"GPT-4\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AgentInstruct uses GPT-4 to generate high-quality data<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"SYNTHETIC-DATA-GENERATION-AS-A-SERVICE\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AgentInstruct can enable Synthetic-Data-Generation-As-A-Service by generating data for post-training and fine-tuning<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CONTENT TRANSFORMATION AGENTS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Content Transformation Agents are used in the AgentInstruct methodology<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REFINEMENT AGENTS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Refinement Agents are used in the AgentInstruct methodology<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RAW SEEDS\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Raw seeds are used as input in the AgentInstruct methodology<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"POST-TRAINING DATASET\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">The post-training dataset was created by AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"INSTRUCTION CREATION AGENTS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Instruction Creation Agents are used in the AgentInstruct methodology<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REFLECTION FLOWS\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Reflection Flows are used in AgentInstruct to enhance the quality of generated responses<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RAG\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">RAG is one of the general capabilities that can be taught to an LLM using AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CREATIVE WRITING\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Creative Writing is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"REASONING\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Reasoning is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MATH\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">Math is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TOOL USE\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Tool Use is one of the skills covered in the synthetic post-training dataset created by AgentInstruct<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"MISTRAL-7B\" target=\"ORCA-3\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Orca-3 is the fine-tuned version of the Mistral-7B model<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 showed a 40% improvement on the AGIEval benchmark compared to Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 showed a 19% improvement on the MMLU benchmark compared to Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 showed a 54% improvement on the GSM8K benchmark compared to Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 showed a 38% improvement on the BBH benchmark compared to Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 showed a 45% improvement on the AlpacaEval benchmark compared to Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LLAMA-8B-INSTRUCT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Orca-3 outperformed LLAMA-8B-Instruct on multiple benchmarks<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-3.5\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Orca-3 outperformed GPT-3.5 on multiple benchmarks<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Orca-3 showed significant improvements over Mistral-Instruct-7B<\/data>      <data key=\"d6\">b88745a13b69cecbc0ee9c3af41389bf<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"f7eb89a70f544664546a510e46d5febd","chunk":" the seed instructions (Refinement Flow).\n6.end for\nWe use agentic flows to automate the generation process and leverage raw articles as seeds to\nfoster diversity and ensure that problems generated in different iterations (line 2 in Figure 1)\nare distinct and of broad coverage. This enables us to create data at scale (benefiting from\nautomation of agentic flows), with high diversity (based on the broad and diverse seeds) and\nof varying complexity (benefiting from the iterative and refinement patterns supported by\nagentic flows). AgentInstruct defines three different flows:\nContent Transformation Flow converts the raw seed into an intermediate representation\nthat simplifies the creation of instructions tailored to specific objectives. It comprises of\n31\nContent Transformation Flow\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026Raw text \ndocuments or \nsource code \nfiles are used \nas seedsTransformed \ncontent ( e.g.  \nargument passage, \nmeeting transcript, \nlist of APIs,\u2026.)\nSeed Instruction Generation Flow\n\u2026\u2026Transformed \ncontent provide \ndiversity is easier \nto use to generate \ninstructions Seed instructions \nare generated \nfollowing  a \ncomprehensive \ntaxonomy\u2026...\u2026...\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026...\u2026...\u2026...\n3\nInstruction Refinement Flow\nIteratively refine the instructions \nto boost quality, diversity and \ncomplexity\n2Figure 3: Figure provides a thematic overview of the roles played by different groups of\nagents. Content Transformation Flow converts the seed into an intermediate representation\nthat makes it easier to create high quality and diverse data. Seed Instruction Generation\nFlow creates instances of the target tasks following a taxonomy. Refinement Flow explores\nthe space further by starting from these initial data points and exploring the neighborhood.\nThe expectation is that by picking a random seed we will be able to cover the entire region\nof data points.\nmultiple agents and is often instrumental in the generation of high-quality data and serve as\nan additional means to introduce diversity.\nSeed Instruction Generation Flow comprising of multiple agents takes as input the\ntransformed seed from the Content Transformation Flow and generates a set of diverse\ninstructions. The only goal of the Seed Instruction Flow is to introduce diversity for which\nit often relies on a pre-defined, but extensible, taxonomy.\nInstruction Refinement Flow takes as input the instructions from the Seed Instruction\nFlow and iteratively enhances their complexity and quality. Towards this we use the concept\nof Suggester-Editor Agents[ 19]. Suggester agents initially propose various approaches to\nincrease the intricacy of the initial instructions (making them more complex, unsolvable,\nor tricky), after which the Editor agents modify the instructions in accordance with these\nsuggestions.\nEach flow consists of a number of agents. We use a generic definition of an agent, where an\nagent is powered by an LLM and can optionally have the ability to use tools such as search\nAPIs, code interpreter or a calculator. Each agent has a specific role and set of instructions\nspecified as part of the underlying LLM system message.\nWe implemented these flows for 17 different skills, each having multiple subcategories.\nThe skills include reading comprehension, question answering, coding, retrieval augmented\ngeneration, creative writing, tool\/API use and Web control. The full list is provided in\nTable 1\nWe explain how the workflows work with case studies of generating data for the following\nthree skills:\nReading Comprehension: The ability to understand, process, and interpret written text.\nText Modification: The process of altering text to suit different purposes.\nTool Use: The employment of functions or APIs to perform tasks or solve problems.\n4Reading Comprehension: Reading comprehension is a critical skill involving processing and un-\nderstanding text, which is necessary for learning and encompasses decoding, fluency, and vocabulary\nknowledge. Reading comprehension tests typically present text passages of varying lengths and\nsubjects, followed by questions that assess the reader\u2019s understanding.\nOpen Domain Question Answering: Open domain question answering involves generating\nresponses to questions over a wide range of topics, without being restricted to a specific domain.\nText Modification: Text modification involves changing existing text to improve its quality,\nmodify its tone, or to fit a specific context or audience. It is a common task in content creation and\nediting.\nWeb Agent: A web agent is a software program that autonomously performs tasks on the web,\nsuch as where to click, how much to scroll.\nBrain Teaser A brain teaser is a problem or puzzle, typically requiring thought to solve, often for\namusement but also used for training logical thinking and problem-solving skills.\nAnalytical Reasoning Analytical reasoning involves the ability to look at information, be it\nqualitative or quantitative in nature, and discern patterns within the information. It\u2019s a process\nthat includes understanding a system of relationships and draw logical conclusions about those\nrelationships.\nMultiple Choice Questions Multiple choice questions are a form of assessment where respondents\nare asked to select the best possible answer (or answers) out of the choices from a list. They are\ncommon in standardized tests, quizzes, and surveys.\nData To Text Data-to-text refers to human-readable textual summaries from source data. These\ncould be used for generating reports, explanations, or narratives from structured data.\nFermiFermi problems are estimation problems which seek quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText","chunk_id":"f7eb89a70f544664546a510e46d5febd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AGENTIC FLOWS","type":"TECHNOLOGY","description":"Agentic flows are automated processes used to generate data at scale, ensuring high diversity and varying complexity by leveraging raw articles as seeds","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CONTENT TRANSFORMATION FLOW","type":"PROCESS","description":"Content Transformation Flow converts raw seeds into intermediate representations that simplify the creation of instructions tailored to specific objectives","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SEED INSTRUCTION GENERATION FLOW","type":"PROCESS","description":"Seed Instruction Generation Flow generates diverse instructions from transformed content, following a comprehensive taxonomy","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"Instruction Refinement Flow iteratively enhances the complexity and quality of instructions generated by the Seed Instruction Generation Flow","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SUGGESTER-EDITOR AGENTS","type":"TECHNOLOGY","description":"Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions to increase their intricacy and quality","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM (Large Language Model) is a type of AI model that powers agents in the described flows, capable of using tools like search APIs, code interpreters, or calculators","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is the ability to understand, process, and interpret written text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"QUESTION ANSWERING","type":"SKILL","description":"Question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TEXT MODIFICATION","type":"SKILL","description":"Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TOOL USE","type":"SKILL","description":"Tool use involves employing functions or APIs to perform tasks or solve problems","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"WEB AGENT","type":"TECHNOLOGY","description":"A web agent is a software program that autonomously performs tasks on the web, such as clicking, scrolling, and navigating","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"BRAIN TEASER","type":"ACTIVITY","description":"A brain teaser is a problem or puzzle requiring thought to solve, often used for amusement or training logical thinking and problem-solving skills","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"ANALYTICAL REASONING","type":"SKILL","description":"Analytical reasoning involves the ability to look at information, discern patterns, and draw logical conclusions about relationships within the information","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"ASSESSMENT","description":"Multiple choice questions are a form of assessment where respondents select the best possible answer from a list of choices","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"DATA TO TEXT","type":"TECHNOLOGY","description":"Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"FERMI PROBLEMS","type":"ACTIVITY","description":"Fermi problems are estimation problems that seek quick, rough estimates of quantities, often requiring justified guesses or assumptions","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CODING","type":"SKILL","description":"Coding involves writing, understanding, debugging, and testing code following instructions","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TEXT EXTRACTION","type":"TECHNOLOGY","description":"Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition and keyword extraction","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct defines three different flows: Content Transformation Flow, Seed Instruction Generation Flow, and Instruction Refinement Flow to automate the generation process and ensure diversity and complexity in the generated data","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"RAW ARTICLES","type":"DOCUMENT","description":"Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INTERMEDIATE REPRESENTATION","type":"DOCUMENT","description":"Intermediate representation is the transformed content produced by the Content Transformation Flow, simplifying the creation of instructions tailored to specific objectives","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SEED INSTRUCTIONS","type":"DOCUMENT","description":"Seed instructions are generated from transformed content in the Seed Instruction Generation Flow, following a comprehensive taxonomy to ensure diversity","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"INSTRUCTIONS","type":"DOCUMENT","description":"Instructions are iteratively refined in the Instruction Refinement Flow to boost their quality, diversity, and complexity","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"SEARCH APIS","type":"TECHNOLOGY","description":"Search APIs are tools that can be used by agents powered by LLMs to perform specific tasks","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CODE INTERPRETER","type":"TECHNOLOGY","description":"Code interpreter is a tool that can be used by agents powered by LLMs to execute and interpret code","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CALCULATOR","type":"TECHNOLOGY","description":"Calculator is a tool that can be used by agents powered by LLMs to perform mathematical calculations","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TABLE 1","type":"DOCUMENT","description":"Table 1 provides a full list of the 17 different skills implemented in the described workflows","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CREATIVE WRITING","type":"SKILL","description":"Creative writing is a skill involving the creation of original written content, often requiring imagination and storytelling abilities","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"RETRIEVAL AUGMENTED GENERATION","type":"SKILL","description":"Retrieval augmented generation is a skill that involves generating content by retrieving and incorporating relevant information from external sources","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"WEB CONTROL","type":"SKILL","description":"Web control is a skill involving the use of web agents to autonomously perform tasks on the web","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CASE STUDIES","type":"DOCUMENT","description":"Case studies are used to explain how the workflows work for generating data for specific skills like Reading Comprehension, Text Modification, and Tool Use","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"DECODING","type":"SKILL","description":"Decoding is a sub-skill of reading comprehension involving the ability to interpret written text into meaningful language","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"FLUENCY","type":"SKILL","description":"Fluency is a sub-skill of reading comprehension involving the ability to read text smoothly and accurately","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"VOCABULARY KNOWLEDGE","type":"SKILL","description":"Vocabulary knowledge is a sub-skill of reading comprehension involving the understanding and use of a wide range of words","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"TEXT PASSAGES","type":"DOCUMENT","description":"Text passages are used in reading comprehension tests to assess the reader\u2019s understanding","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"QUESTIONS","type":"DOCUMENT","description":"Questions are used in reading comprehension tests to assess the reader\u2019s understanding of the text passages","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"OPEN DOMAIN QUESTION ANSWERING","type":"SKILL","description":"Open domain question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"CONTENT CREATION","type":"SKILL","description":"Content creation involves generating original written, visual, or audio content for various purposes","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"EDITING","type":"SKILL","description":"Editing involves modifying existing content to improve its quality, tone, or fit for a specific context or audience","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"REPORTS","type":"DOCUMENT","description":"Reports are human-readable textual summaries generated from source data, often used in data-to-text tasks","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"EXPLANATIONS","type":"DOCUMENT","description":"Explanations are human-readable textual summaries generated from source data, often used in data-to-text tasks","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"NARRATIVES","type":"DOCUMENT","description":"Narratives are human-readable textual summaries generated from source data, often used in data-to-text tasks","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"ENRICO FERMI","type":"PERSON","description":"Enrico Fermi was a physicist after whom Fermi problems are named, involving estimation problems that seek quick, rough estimates of quantities","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"NAMED ENTITY RECOGNITION","type":"TECHNOLOGY","description":"Named entity recognition is a task in text extraction involving identifying and classifying named entities in text","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"KEYWORD EXTRACTION","type":"TECHNOLOGY","description":"Keyword extraction is a task in text extraction involving identifying and extracting important keywords from text","source_id":"f7eb89a70f544664546a510e46d5febd"},{"name":"DATA FIELDS","type":"TECHNOLOGY","description":"Data fields are specific pieces of information extracted from unstructured text in text extraction tasks","source_id":"f7eb89a70f544664546a510e46d5febd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic flows are automated processes used to generate data at scale, ensuring high diversity and varying complexity by leveraging raw articles as seeds<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Content Transformation Flow converts raw seeds into intermediate representations that simplify the creation of instructions tailored to specific objectives<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Generation Flow generates diverse instructions from transformed content, following a comprehensive taxonomy<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction Refinement Flow iteratively enhances the complexity and quality of instructions generated by the Seed Instruction Generation Flow<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions to increase their intricacy and quality<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM (Large Language Model) is a type of AI model that powers agents in the described flows, capable of using tools like search APIs, code interpreters, or calculators<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is the ability to understand, process, and interpret written text, necessary for learning and encompassing decoding, fluency, and vocabulary knowledge<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"QUESTION ANSWERING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Text modification involves changing existing text to improve its quality, modify its tone, or fit a specific context or audience<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Tool use involves employing functions or APIs to perform tasks or solve problems<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"WEB AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A web agent is a software program that autonomously performs tasks on the web, such as clicking, scrolling, and navigating<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"BRAIN TEASER\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">A brain teaser is a problem or puzzle requiring thought to solve, often used for amusement or training logical thinking and problem-solving skills<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"ANALYTICAL REASONING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Analytical reasoning involves the ability to look at information, discern patterns, and draw logical conclusions about relationships within the information<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">ASSESSMENT<\/data>      <data key=\"d1\">Multiple choice questions are a form of assessment where respondents select the best possible answer from a list of choices<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"DATA TO TEXT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Data-to-text refers to generating human-readable textual summaries from source data, used for reports, explanations, or narratives<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"FERMI PROBLEMS\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Fermi problems are estimation problems that seek quick, rough estimates of quantities, often requiring justified guesses or assumptions<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Coding involves writing, understanding, debugging, and testing code following instructions<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition and keyword extraction<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct defines three different flows: Content Transformation Flow, Seed Instruction Generation Flow, and Instruction Refinement Flow to automate the generation process and ensure diversity and complexity in the generated data<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"RAW ARTICLES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Raw articles are used as seeds in agentic flows to foster diversity and ensure broad coverage of generated problems<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INTERMEDIATE REPRESENTATION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Intermediate representation is the transformed content produced by the Content Transformation Flow, simplifying the creation of instructions tailored to specific objectives<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SEED INSTRUCTIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Seed instructions are generated from transformed content in the Seed Instruction Generation Flow, following a comprehensive taxonomy to ensure diversity<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"INSTRUCTIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Instructions are iteratively refined in the Instruction Refinement Flow to boost their quality, diversity, and complexity<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"SEARCH APIS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Search APIs are tools that can be used by agents powered by LLMs to perform specific tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CODE INTERPRETER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Code interpreter is a tool that can be used by agents powered by LLMs to execute and interpret code<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CALCULATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Calculator is a tool that can be used by agents powered by LLMs to perform mathematical calculations<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TABLE 1\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Table 1 provides a full list of the 17 different skills implemented in the described workflows<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CREATIVE WRITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Creative writing is a skill involving the creation of original written content, often requiring imagination and storytelling abilities<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Retrieval augmented generation is a skill that involves generating content by retrieving and incorporating relevant information from external sources<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"WEB CONTROL\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Web control is a skill involving the use of web agents to autonomously perform tasks on the web<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CASE STUDIES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Case studies are used to explain how the workflows work for generating data for specific skills like Reading Comprehension, Text Modification, and Tool Use<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"DECODING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Decoding is a sub-skill of reading comprehension involving the ability to interpret written text into meaningful language<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"FLUENCY\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Fluency is a sub-skill of reading comprehension involving the ability to read text smoothly and accurately<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"VOCABULARY KNOWLEDGE\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Vocabulary knowledge is a sub-skill of reading comprehension involving the understanding and use of a wide range of words<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"TEXT PASSAGES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Text passages are used in reading comprehension tests to assess the reader&#8217;s understanding<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"QUESTIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Questions are used in reading comprehension tests to assess the reader&#8217;s understanding of the text passages<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Open domain question answering involves generating responses to questions over a wide range of topics, without being restricted to a specific domain<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"CONTENT CREATION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Content creation involves generating original written, visual, or audio content for various purposes<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"EDITING\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Editing involves modifying existing content to improve its quality, tone, or fit for a specific context or audience<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"REPORTS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Reports are human-readable textual summaries generated from source data, often used in data-to-text tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"EXPLANATIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Explanations are human-readable textual summaries generated from source data, often used in data-to-text tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"NARRATIVES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Narratives are human-readable textual summaries generated from source data, often used in data-to-text tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"ENRICO FERMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Enrico Fermi was a physicist after whom Fermi problems are named, involving estimation problems that seek quick, rough estimates of quantities<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"NAMED ENTITY RECOGNITION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Named entity recognition is a task in text extraction involving identifying and classifying named entities in text<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"KEYWORD EXTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Keyword extraction is a task in text extraction involving identifying and extracting important keywords from text<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <node id=\"DATA FIELDS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Data fields are specific pieces of information extracted from unstructured text in text extraction tasks<\/data>      <data key=\"d2\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/node>    <edge source=\"AGENTIC FLOWS\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Content Transformation Flow is one of the agentic flows used to convert raw seeds into intermediate representations<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Seed Instruction Generation Flow is one of the agentic flows used to generate diverse instructions from transformed content<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTIC FLOWS\" target=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction Refinement Flow is one of the agentic flows used to iteratively enhance the complexity and quality of instructions<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Seed Instruction Generation Flow takes transformed content from the Content Transformation Flow to generate diverse instructions<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Content Transformation Flow is one of the three flows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"RAW ARTICLES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Raw articles are used as seeds in the Content Transformation Flow<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction Refinement Flow takes instructions from the Seed Instruction Generation Flow to enhance their complexity and quality<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Seed Instruction Generation Flow is one of the three flows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"INTERMEDIATE REPRESENTATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Intermediate representation is the input for the Seed Instruction Generation Flow<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER-EDITOR AGENTS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Suggester-Editor Agents are used in the Instruction Refinement Flow to propose and modify instructions<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instruction Refinement Flow is one of the three flows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SEED INSTRUCTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Seed instructions are the input for the Instruction Refinement Flow<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"INSTRUCTIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Instructions are iteratively refined in the Instruction Refinement Flow<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"SUGGESTER-EDITOR AGENTS\" target=\"LLM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Suggester-Editor Agents are powered by LLMs and can use tools like search APIs, code interpreters, or calculators<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"LLM\" target=\"SEARCH APIS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Search APIs are tools that can be used by agents powered by LLMs<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CODE INTERPRETER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Code interpreter is a tool that can be used by agents powered by LLMs<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"LLM\" target=\"CALCULATOR\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Calculator is a tool that can be used by agents powered by LLMs<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"QUESTION ANSWERING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Reading Comprehension and Question Answering are skills related to understanding and processing text<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Multiple Choice Questions are often used to assess Reading Comprehension skills<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"DECODING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Decoding is a sub-skill of reading comprehension<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"FLUENCY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Fluency is a sub-skill of reading comprehension<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"VOCABULARY KNOWLEDGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Vocabulary knowledge is a sub-skill of reading comprehension<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"TEXT PASSAGES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Text passages are used in reading comprehension tests<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"QUESTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Questions are used in reading comprehension tests<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"QUESTION ANSWERING\" target=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Open domain question answering is a type of question answering<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"TOOL USE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Text Modification and Tool Use are skills related to altering or employing text and tools to perform tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"CONTENT CREATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Content creation is related to text modification as both involve generating or altering text<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION\" target=\"EDITING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Editing is related to text modification as both involve altering text to improve its quality or fit a specific context<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"WEB AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">A web agent is a type of tool used in Tool Use to perform tasks on the web<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TOOL USE\" target=\"CODING\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Coding is a skill that often involves the use of tools or APIs to perform tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"BRAIN TEASER\" target=\"ANALYTICAL REASONING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Brain Teasers and Analytical Reasoning involve problem-solving and logical thinking skills<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"ANALYTICAL REASONING\" target=\"FERMI PROBLEMS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Fermi Problems require Analytical Reasoning to make justified guesses or assumptions<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"TEXT EXTRACTION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both Data to Text and Text Extraction involve processing and summarizing information from text<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"REPORTS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Reports are a type of document generated in data-to-text tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"EXPLANATIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Explanations are a type of document generated in data-to-text tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"DATA TO TEXT\" target=\"NARRATIVES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Narratives are a type of document generated in data-to-text tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"FERMI PROBLEMS\" target=\"ENRICO FERMI\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Fermi problems are named after physicist Enrico Fermi<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"NAMED ENTITY RECOGNITION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Named entity recognition is a task in text extraction<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"KEYWORD EXTRACTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Keyword extraction is a task in text extraction<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"DATA FIELDS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Data fields are specific pieces of information extracted in text extraction tasks<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"TABLE 1\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Table 1 provides a full list of the 17 different skills implemented in the workflows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CREATIVE WRITING\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Creative writing is one of the 17 different skills implemented in the workflows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Retrieval augmented generation is one of the 17 different skills implemented in the workflows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"WEB CONTROL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Web control is one of the 17 different skills implemented in the workflows defined by AgentInstruct<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"CASE STUDIES\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Case studies are used to explain how the workflows defined by AgentInstruct work for generating data for specific skills<\/data>      <data key=\"d5\">f7eb89a70f544664546a510e46d5febd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0c212c1467564ad33330b1f655a8e27e","chunk":" quick, rough estimates of quantities\nwhich can be difficult to measure. Named after physicist Enrico Fermi, these problems often require\nmaking justified guesses or assumptions to reach a solution.\nCoding Coding involves writing code following instructions, understanding code, debugging code,\ntracing or writing test cases.\nText Extraction Text extraction is the process of retrieving relevant information from a larger text\ndocument. This can include tasks like named entity recognition, keyword extraction, or extracting\nspecific data fields from unstructured text.\nText Classification Text classification is a type of machine learning task where text documents are\nautomatically classified into predefined categories. This can be used for spam detection, sentiment\nanalysis, and topic labelling among others.\nRetrieval Augmented Generation Retrieval Augmented Generation (RAG) is a method used\nin natural language processing that combines retrieval-based and generative models to generate\nresponses. It first retrieves relevant documents and then uses these documents to generate a response.\nTool Use Tool use involves the manipulation of tools to achieve goals. In AI, this refers to the\nability of an AI system to use available resources or auxiliary systems to solve complex tasks.\nCreative Content Generation Creative content generation involves the creation of original\ncontent, often involving elements of novelty, value, and surprise. In AI, this could refer to generating\ntext, music, or images that are not only new but also meaningful and interesting.\nFew Shot Reasoning Few-shot reasoning refers to the ability of a machine learning model to\nunderstand new concepts, patterns, or tasks with minimal examples or guidance. It\u2019s a desired trait\nin AI, mimicking the human ability to learn quickly from few examples.\nConversation Conversation refers to conversational agents or chatbots that interact with humans\nin a natural, human-like manner.\nTable 1: List of 17 capabilities for which we implemented AgentInstruct Flows\n52.1 AgentInstruct Flow for Reading Comprehension\nReading comprehension is a critical skill involving processing and understanding text, which\nis necessary for learning and encompasses decoding, fluency, and vocabulary knowledge.\nReading comprehension questions range from asking for explicit information (literal com-\nprehension) to requiring inferences, understanding vocabulary in context, analyzing text\nstructure and argumentation, critically evaluating the content, and synthesizing informa-\ntion from different parts of or multiple texts. Reading comprehension is a very important\ncapability and can enable scenarios like question answering, search, grounded reasoning, etc.\nContent Transformation Flow Web crawls encompass an extensive collection of human-\ngenerated text, which holds potential for generating reading comprehension materials.\nHowever, these sources are not inherently structured to facilitate the teaching of read-\ning comprehension skills. Consequently, they do not support the consistent generation of\ndiverse question types required for comprehensive reading comprehension evaluation. For\ninstance, the LSAT Logical Reasoning test features specialized question categories, including\nassumption, strengthening\/weakening, flaw, and inference questions. Crafting such questions\nnecessitates passages with a particular stylistic and logical framework. The objective of\nContent Transformation Flow is to transform arbitrary articles into well-crafted pieces that\nare conducive to the formulation of a wide array of reading comprehension question types.\nOur current flow for Reading Comprehension encompasses a suite of nine content transfor-\nmation agents for generating argument passages, debates and conversations, long passages,\nmeeting transcripts, poems, satarical content, etc. Detailed description is provided in Ap-\npendix A. Given a seed article, the flow will randomly pick one of the Content Transformation\nAgents to assess the seed article and attempt to generate the text passages. The following\nprovides an example for the Argument Passage Generator .\nEXAMPLE: Content Transformation Flow\nRandom Seed\nUric acid is a substance produced naturally by the breakdown of purine (a type of dietary\nprotein). When it is in excess in the body, crystals composed of these substances are formed.\nThese crystals are deposited in various parts of the body, mainly in the joints and kidneys,\ncausing pain and other aggravations. The lack or excess of uric acid in the body is caused by\nsome diseases (such as leukemia, obesity, kidney diseases, and anemia) and factors related\nto lifestyle (consumption of alcohol and processed foods, for example).\nContents\n\u2022Where does purine come from?\n\u2022Where is uric acid found?\n\u2022What is high uric acid?\n\u2022What can high uric acid cause?\n\u2022What is low uric acid?\n\u2022Tests: how to know uric acid levels?\nWhere does purine come from?\nPurine is produced and released into the bloodstream when an amino acid is broken down\nby digestion. Therefore, it is produced naturally in the body. ... (Truncated)\n6Transformed Seed (Argument Passage Generator)\nRecent studies have indicated that high levels of uric acid in the blood, known as hyper-\nuricemia, may increase the risk of cardiovascular disease. Uric acid is a byproduct of purine\nmetabolism, and while it is naturally produced in the body, excessive amounts can lead to\nhealth complications. Hyperuricemia is typically defined as levels above 6 mg\/dL in women\nand 7 mg\/dL in men. This condition can result from increased production of uric acid or\ninsufficient elimination through urine. Factors contributing to high uric acid include a diet\nrich in purines, found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal","chunk_id":"0c212c1467564ad33330b1f655a8e27e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ENRICO FERMI","type":"PERSON","description":"Enrico Fermi was a physicist after whom Fermi problems, which involve making justified guesses or assumptions to reach a solution, are named","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FERMI PROBLEMS","type":"CONCEPT","description":"Fermi problems are quick, rough estimates of quantities that can be difficult to measure, often requiring justified guesses or assumptions to reach a solution","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CODING","type":"ACTIVITY","description":"Coding involves writing code following instructions, understanding code, debugging code, and tracing or writing test cases","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT EXTRACTION","type":"PROCESS","description":"Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TEXT CLASSIFICATION","type":"PROCESS","description":"Text classification is a machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"RETRIEVAL AUGMENTED GENERATION","type":"METHOD","description":"Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TOOL USE","type":"ACTIVITY","description":"Tool use involves the manipulation of tools to achieve goals, and in AI, it refers to the ability of an AI system to use available resources or auxiliary systems to solve complex tasks","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CREATIVE CONTENT GENERATION","type":"ACTIVITY","description":"Creative content generation involves the creation of original content, often involving elements of novelty, value, and surprise, and in AI, it refers to generating text, music, or images that are new, meaningful, and interesting","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FEW SHOT REASONING","type":"CONCEPT","description":"Few-shot reasoning refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONVERSATION","type":"ACTIVITY","description":"Conversation refers to conversational agents or chatbots that interact with humans in a natural, human-like manner","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"AGENTINSTRUCT FLOW","type":"TECHNOLOGY","description":"AgentInstruct Flow is a system implemented for various capabilities, including reading comprehension, to process and understand text for learning and other applications","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"READING COMPREHENSION","type":"SKILL","description":"Reading comprehension is a critical skill involving processing and understanding text, necessary for learning, and encompasses decoding, fluency, and vocabulary knowledge","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CONTENT TRANSFORMATION FLOW","type":"TECHNOLOGY","description":"Content Transformation Flow is a system designed to transform arbitrary articles into well-crafted pieces conducive to the formulation of a wide array of reading comprehension question types","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"ARGUMENT PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Argument Passage Generator is a component of the Content Transformation Flow that generates argument passages from seed articles","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"URIC ACID","type":"SUBSTANCE","description":"Uric acid is a substance produced naturally by the breakdown of purine, and excessive amounts can lead to health complications such as hyperuricemia","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPERURICEMIA","type":"CONDITION","description":"Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which may increase the risk of cardiovascular disease","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"HYPOURICEMIA","type":"CONDITION","description":"Hypouricemia is a condition characterized by low levels of uric acid, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"PURINE","type":"SUBSTANCE","description":"Purine is a type of dietary protein that, when broken down, produces uric acid","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LSAT LOGICAL REASONING TEST","type":"TEST","description":"The LSAT Logical Reasoning test features specialized question categories, including assumption, strengthening\/weakening, flaw, and inference questions","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"WEB CRAWLS","type":"PROCESS","description":"Web crawls encompass an extensive collection of human-generated text, which holds potential for generating reading comprehension materials","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LABORATORY BLOOD AND URINE TESTS","type":"PROCESS","description":"Laboratory blood and urine tests are used to diagnose conditions like hyperuricemia and hypouricemia by measuring uric acid levels","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"NAMED ENTITY RECOGNITION","type":"PROCESS","description":"Named entity recognition is a task in text extraction that involves identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations, etc.","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"KEYWORD EXTRACTION","type":"PROCESS","description":"Keyword extraction is a task in text extraction that involves identifying and extracting important words or phrases from a larger text document","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"SPAM DETECTION","type":"PROCESS","description":"Spam detection is a type of text classification task where text documents, such as emails, are automatically classified as spam or not spam","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"SENTIMENT ANALYSIS","type":"PROCESS","description":"Sentiment analysis is a type of text classification task where text documents are automatically classified based on the sentiment they express, such as positive, negative, or neutral","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"TOPIC LABELING","type":"PROCESS","description":"Topic labeling is a type of text classification task where text documents are automatically classified into predefined topics or categories","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"QUESTION ANSWERING","type":"PROCESS","description":"Question answering is a task in reading comprehension that involves providing answers to questions based on a given text","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"SEARCH","type":"PROCESS","description":"Search is a task in reading comprehension that involves finding specific information within a text or across multiple texts","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"GROUNDED REASONING","type":"PROCESS","description":"Grounded reasoning is a task in reading comprehension that involves making inferences and drawing conclusions based on evidence from the text","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"ASSUMPTION QUESTIONS","type":"QUESTION TYPE","description":"Assumption questions are a type of question in the LSAT Logical Reasoning test that require identifying assumptions made in an argument","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"STRENGTHENING\/WEAKENING QUESTIONS","type":"QUESTION TYPE","description":"Strengthening\/weakening questions are a type of question in the LSAT Logical Reasoning test that require identifying how an argument can be strengthened or weakened","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"FLAW QUESTIONS","type":"QUESTION TYPE","description":"Flaw questions are a type of question in the LSAT Logical Reasoning test that require identifying flaws in an argument","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"INFERENCE QUESTIONS","type":"QUESTION TYPE","description":"Inference questions are a type of question in the LSAT Logical Reasoning test that require drawing inferences from the given information","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"DEBATES AND CONVERSATIONS","type":"CONTENT TYPE","description":"Debates and conversations are types of content generated by the Content Transformation Flow for reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"LONG PASSAGES","type":"CONTENT TYPE","description":"Long passages are types of content generated by the Content Transformation Flow for reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"MEETING TRANSCRIPTS","type":"CONTENT TYPE","description":"Meeting transcripts are types of content generated by the Content Transformation Flow for reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"POEMS","type":"CONTENT TYPE","description":"Poems are types of content generated by the Content Transformation Flow for reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"SATIRICAL CONTENT","type":"CONTENT TYPE","description":"Satirical content is a type of content generated by the Content Transformation Flow for reading comprehension","source_id":"0c212c1467564ad33330b1f655a8e27e"},{"name":"CARDIOVASCULAR DISEASE","type":"CONDITION","description":"Cardiovascular disease is a health condition that may be associated with high levels of uric acid in the blood","source_id":"0c212c1467564ad33330b1f655a8e27e"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ENRICO FERMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Enrico Fermi was a physicist after whom Fermi problems, which involve making justified guesses or assumptions to reach a solution, are named<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FERMI PROBLEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Fermi problems are quick, rough estimates of quantities that can be difficult to measure, often requiring justified guesses or assumptions to reach a solution<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CODING\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Coding involves writing code following instructions, understanding code, debugging code, and tracing or writing test cases<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT EXTRACTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text extraction is the process of retrieving relevant information from a larger text document, including tasks like named entity recognition, keyword extraction, or extracting specific data fields from unstructured text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TEXT CLASSIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text classification is a machine learning task where text documents are automatically classified into predefined categories, used for spam detection, sentiment analysis, and topic labeling among others<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"RETRIEVAL AUGMENTED GENERATION\">      <data key=\"d0\">METHOD<\/data>      <data key=\"d1\">Retrieval Augmented Generation (RAG) is a method in natural language processing that combines retrieval-based and generative models to generate responses by first retrieving relevant documents and then using these documents to generate a response<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Tool use involves the manipulation of tools to achieve goals, and in AI, it refers to the ability of an AI system to use available resources or auxiliary systems to solve complex tasks<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CREATIVE CONTENT GENERATION\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Creative content generation involves the creation of original content, often involving elements of novelty, value, and surprise, and in AI, it refers to generating text, music, or images that are new, meaningful, and interesting<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FEW SHOT REASONING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Few-shot reasoning refers to the ability of a machine learning model to understand new concepts, patterns, or tasks with minimal examples or guidance, mimicking the human ability to learn quickly from few examples<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONVERSATION\">      <data key=\"d0\">ACTIVITY<\/data>      <data key=\"d1\">Conversation refers to conversational agents or chatbots that interact with humans in a natural, human-like manner<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"AGENTINSTRUCT FLOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct Flow is a system implemented for various capabilities, including reading comprehension, to process and understand text for learning and other applications<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"READING COMPREHENSION\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">Reading comprehension is a critical skill involving processing and understanding text, necessary for learning, and encompasses decoding, fluency, and vocabulary knowledge<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Content Transformation Flow is a system designed to transform arbitrary articles into well-crafted pieces conducive to the formulation of a wide array of reading comprehension question types<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Argument Passage Generator is a component of the Content Transformation Flow that generates argument passages from seed articles<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"URIC ACID\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">Uric acid is a substance produced naturally by the breakdown of purine, and excessive amounts can lead to health complications such as hyperuricemia<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which may increase the risk of cardiovascular disease<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"HYPOURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Hypouricemia is a condition characterized by low levels of uric acid, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"PURINE\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">Purine is a type of dietary protein that, when broken down, produces uric acid<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LSAT LOGICAL REASONING TEST\">      <data key=\"d0\">TEST<\/data>      <data key=\"d1\">The LSAT Logical Reasoning test features specialized question categories, including assumption, strengthening\/weakening, flaw, and inference questions<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"WEB CRAWLS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Web crawls encompass an extensive collection of human-generated text, which holds potential for generating reading comprehension materials<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Laboratory blood and urine tests are used to diagnose conditions like hyperuricemia and hypouricemia by measuring uric acid levels<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"NAMED ENTITY RECOGNITION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Named entity recognition is a task in text extraction that involves identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations, etc.<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"KEYWORD EXTRACTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Keyword extraction is a task in text extraction that involves identifying and extracting important words or phrases from a larger text document<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"SPAM DETECTION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Spam detection is a type of text classification task where text documents, such as emails, are automatically classified as spam or not spam<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"SENTIMENT ANALYSIS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Sentiment analysis is a type of text classification task where text documents are automatically classified based on the sentiment they express, such as positive, negative, or neutral<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"TOPIC LABELING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Topic labeling is a type of text classification task where text documents are automatically classified into predefined topics or categories<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"QUESTION ANSWERING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Question answering is a task in reading comprehension that involves providing answers to questions based on a given text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"SEARCH\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Search is a task in reading comprehension that involves finding specific information within a text or across multiple texts<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"GROUNDED REASONING\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Grounded reasoning is a task in reading comprehension that involves making inferences and drawing conclusions based on evidence from the text<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"ASSUMPTION QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Assumption questions are a type of question in the LSAT Logical Reasoning test that require identifying assumptions made in an argument<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"STRENGTHENING\/WEAKENING QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Strengthening\/weakening questions are a type of question in the LSAT Logical Reasoning test that require identifying how an argument can be strengthened or weakened<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"FLAW QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Flaw questions are a type of question in the LSAT Logical Reasoning test that require identifying flaws in an argument<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"INFERENCE QUESTIONS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Inference questions are a type of question in the LSAT Logical Reasoning test that require drawing inferences from the given information<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"DEBATES AND CONVERSATIONS\">      <data key=\"d0\">CONTENT TYPE<\/data>      <data key=\"d1\">Debates and conversations are types of content generated by the Content Transformation Flow for reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"LONG PASSAGES\">      <data key=\"d0\">CONTENT TYPE<\/data>      <data key=\"d1\">Long passages are types of content generated by the Content Transformation Flow for reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"MEETING TRANSCRIPTS\">      <data key=\"d0\">CONTENT TYPE<\/data>      <data key=\"d1\">Meeting transcripts are types of content generated by the Content Transformation Flow for reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"POEMS\">      <data key=\"d0\">CONTENT TYPE<\/data>      <data key=\"d1\">Poems are types of content generated by the Content Transformation Flow for reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"SATIRICAL CONTENT\">      <data key=\"d0\">CONTENT TYPE<\/data>      <data key=\"d1\">Satirical content is a type of content generated by the Content Transformation Flow for reading comprehension<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Cardiovascular disease is a health condition that may be associated with high levels of uric acid in the blood<\/data>      <data key=\"d2\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/node>    <edge source=\"ENRICO FERMI\" target=\"FERMI PROBLEMS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Fermi problems are named after physicist Enrico Fermi<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"NAMED ENTITY RECOGNITION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Named entity recognition is a task in text extraction<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT EXTRACTION\" target=\"KEYWORD EXTRACTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Keyword extraction is a task in text extraction<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"SPAM DETECTION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Spam detection is a type of text classification task<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"SENTIMENT ANALYSIS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Sentiment analysis is a type of text classification task<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"TEXT CLASSIFICATION\" target=\"TOPIC LABELING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Topic labeling is a type of text classification task<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"QUESTION ANSWERING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Question answering is a task in reading comprehension<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"SEARCH\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Search is a task in reading comprehension<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"READING COMPREHENSION\" target=\"GROUNDED REASONING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Grounded reasoning is a task in reading comprehension<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Argument Passage Generator is a component of the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"WEB CRAWLS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Content Transformation Flow uses web crawls to generate reading comprehension materials<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"DEBATES AND CONVERSATIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Debates and conversations are types of content generated by the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"LONG PASSAGES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Long passages are types of content generated by the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"MEETING TRANSCRIPTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meeting transcripts are types of content generated by the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"POEMS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Poems are types of content generated by the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"SATIRICAL CONTENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Satirical content is a type of content generated by the Content Transformation Flow<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"HYPERURICEMIA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Hyperuricemia is a condition characterized by high levels of uric acid in the blood<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"HYPOURICEMIA\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Hypouricemia is a condition characterized by low levels of uric acid<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"PURINE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Uric acid is produced by the breakdown of purine<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Laboratory blood and urine tests are used to diagnose hyperuricemia<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Hyperuricemia may increase the risk of cardiovascular disease<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"LABORATORY BLOOD AND URINE TESTS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Laboratory blood and urine tests are used to diagnose hypouricemia<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"ASSUMPTION QUESTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Assumption questions are a type of question in the LSAT Logical Reasoning test<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"STRENGTHENING\/WEAKENING QUESTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Strengthening\/weakening questions are a type of question in the LSAT Logical Reasoning test<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"FLAW QUESTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Flaw questions are a type of question in the LSAT Logical Reasoning test<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>    <edge source=\"LSAT LOGICAL REASONING TEST\" target=\"INFERENCE QUESTIONS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Inference questions are a type of question in the LSAT Logical Reasoning test<\/data>      <data key=\"d5\">0c212c1467564ad33330b1f655a8e27e<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"1d8835c0ce90e56be22873bcf2740a5d","chunk":" found in red meat and seafood, as well as lifestyle choices such as alcohol\nconsumption and physical inactivity. Conversely, low levels of uric acid, or hypouricemia,\nare less common and usually do not present symptoms. Nonetheless, they can be indicative\nof underlying kidney or liver issues. Diagnosing either condition requires laboratory blood\nand urine tests. It is important to note that while high uric acid levels are associated with\nan increased risk of cardiovascular disease, the causal relationship remains to be conclusively\nestablished.\nSeed Instruction Generation Flow We have currently compiled a collection of 43\nreading comprehension questions types. The list includes literal comprehension questions,\ncritical comprehension questions, evaluative comprehension questions, reasoning, identifying\nassumptions, identifying information that strengthens\/weakens an argument, ordering events,\netc.\nA list some of these types is in Appendix A.\nWe have defined multiple Agents targeting these categories. Each of these agent receives as\ninput a piece of text and generates a list of questions based on a predefined question type.\nThe orchestration process will engage a subset of these agents, determined by the content\ntransformation agent in the preceding step. Upon completion of this phase, we will obtain a\ncollection of (passage, question) pairs that will constitute the input for the subsequent stage.\nThe following figure provides an example of Strengthen type question that is generated from\nthe running example.\nEXAMPLE: Seed Instruction\nWhich one of the following, if true, most strengthens the argument that high lev-\nels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid levels in the\nblood.\n(C) Dietary supplements have been shown to reduce uric acid levels in the blood.\n(D) A significant number of patients with cardiovascular disease have been found to have\nuric acid levels above the defined thresholds for hyperuricemia.\n(E) Individuals with a family history of cardiovascular disease are more likely to have high\nuric acid levels.\nInstruction Refinement Flow The refinement flow contains multiple suggester-editor\nagents that will go through each of the (passage, question) pairs and create more such pairs\nwith the following goals: (1) Modify the passage to make the question unanswerable, (2)\nModify the passage to alter the answer, if possible, in opposite direction, or (3) Modify the\nquestions or answer choices (if applicable) to make them complex.\nHere is an example where the suggester agent that tries to add a difficult distractor to\nconfuse the test taker.\n7EXAMPLE: Refinement Flow\nSUGGESTER\n\u2022Suggestion 1 Introduce a hypothetical study or finding that could potentially\nstrengthen the argument, requiring the test-taker to infer its impact on the rela-\ntionship between uric acid levels and cardiovascular disease.\n\u2022Suggestion 2 Add a layer of complexity by suggesting a genetic predisposition\nto hyperuricemia and its correlation with increased cardiovascular events, which\nwould require the test-taker to consider both genetic and physiological factors.\n\u2022Suggestion 3 Include a distractor option that seems to strengthen the argument\nbut upon closer examination, it does not directly relate to the causal relationship\nbetween uric acid levels and cardiovascular disease, thus testing the test-taker\u2019s\nability to discern relevant from irrelevant information.\nEDITOR\n\u2022Modification 1,2 (omitted for brevity)\n\u2022Modification 3 Which one of the following, if true, most strengthens the argument\nthat high levels of uric acid increase the risk of cardiovascular disease?\n(A) Some individuals with hyperuricemia do not develop cardiovascular disease.\n(B) Physical activity has been found to have no significant impact on uric acid\nlevels in the blood.\n(C) A placebo-controlled trial shows that individuals taking uric acid-lowering\nmedication do not exhibit a reduced incidence of cardiovascular disease compared\nto the placebo group.\n(D) A significant number of patients with cardiovascular disease have been found\nto have uric acid levels above the defined thresholds for hyperuricemia.\n(E) A cross-sectional study finds that high uric acid levels are associated with other\nrisk factors for cardiovascular disease, such as hypertension and diabetes, but does\nnot establish a direct causal link.\n2.2 AgentInstruct Flow for Text Modification\nText modification is the process of editing and refining written content to enhance its quality\nand effectiveness or alter its attributes. This involves correcting spelling and grammar,\nclarifyingideas, reorganizingcontentforbetterflow, adjustingtone, ensuringstyleconsistency,\nfact-checking, removingredundancies, formatting, developingcontent, andadaptingtospecific\naudiences. It is a useful skill for LLMs that help in content writing. While several content\ntransformation agents can be introduced to intentionally modify the text as described earlier,\nwe focus here on showing the how the instructions are created and refined.\nSeed Instruction Generation Flow We have currently compiled a collection of 18types\nof text modification tasks including paraphrasing, expansion, simplification, redacting or\nremoving content, styling, code switching, etc. The full list is in Appendix A.\nWe define an Agent for each of the task type. Each agent takes as input a piece of text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have","chunk_id":"1d8835c0ce90e56be22873bcf2740a5d","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"URIC ACID","type":"SUBSTANCE","description":"Uric acid is a chemical found in red meat and seafood, and its levels can be influenced by lifestyle choices such as alcohol consumption and physical inactivity","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPERURICEMIA","type":"CONDITION","description":"Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which can be associated with cardiovascular disease\nHyperuricemia refers to high levels of uric acid in the blood, which is associated with an increased risk of cardiovascular disease","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONDITION"},{"name":"HYPOURICEMIA","type":"CONDITION","description":"Hypouricemia refers to low levels of uric acid in the blood, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CARDIOVASCULAR DISEASE","type":"CONDITION","description":"Cardiovascular disease is a condition that may be associated with high levels of uric acid, although the causal relationship is not conclusively established","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"LABORATORY TESTS","type":"PROCEDURE","description":"Laboratory blood and urine tests are required to diagnose conditions related to uric acid levels","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"SEED INSTRUCTION GENERATION FLOW","type":"PROCESS","description":"Seed Instruction Generation Flow is a process that compiles a collection of reading comprehension question types and defines agents to generate questions based on predefined types","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"AGENT","type":"TECHNOLOGY","description":"Agents are defined to target specific categories of reading comprehension questions and generate questions based on a piece of text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CONTENT TRANSFORMATION AGENT","type":"TECHNOLOGY","description":"Content Transformation Agent determines the subset of agents to engage in the orchestration process for generating (passage, question) pairs","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"PROCESS","description":"Instruction Refinement Flow involves suggester-editor agents that refine (passage, question) pairs to create more complex or unanswerable questions","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"SUGGESTER AGENT","type":"TECHNOLOGY","description":"Suggester Agent provides suggestions to modify passages or questions to add complexity or create difficult distractors","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"EDITOR AGENT","type":"TECHNOLOGY","description":"Editor Agent makes modifications to passages or questions based on suggestions to refine the content","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"AGENTINSTRUCT FLOW","type":"PROCESS","description":"AgentInstruct Flow is a process for text modification that involves editing and refining written content to enhance its quality and effectiveness","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"TEXT MODIFICATION","type":"PROCESS","description":"Text modification involves editing written content to correct spelling and grammar, clarify ideas, reorganize content, adjust tone, ensure style consistency, fact-check, remove redundancies, format, develop content, and adapt to specific audiences","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PARAPHRASING AGENT","type":"TECHNOLOGY","description":"Paraphrasing Agent is an agent that takes a piece of text and creates paraphrased versions as part of text modification tasks","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"READING COMPREHENSION QUESTIONS","type":"CONCEPT","description":"Reading comprehension questions are questions designed to assess understanding of a text, including types like literal, critical, evaluative, reasoning, and identifying assumptions","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"HYPOTHETICAL STUDY","type":"CONCEPT","description":"A hypothetical study is a suggested addition to a passage that could potentially strengthen an argument, requiring inference of its impact on relationships within the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"GENETIC PREDISPOSITION","type":"CONCEPT","description":"Genetic predisposition refers to the likelihood of developing a condition based on genetic factors, which can add complexity to questions about relationships between conditions","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"DISTRACTOR OPTION","type":"CONCEPT","description":"A distractor option is a misleading answer choice that seems relevant but does not directly relate to the question, testing the ability to discern relevant from irrelevant information","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"TEXT MODIFICATION TASKS","type":"CONCEPT","description":"Text modification tasks include activities like paraphrasing, expansion, simplification, redacting, styling, and code switching to alter text attributes","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"PASSAGE","type":"CONCEPT","description":"A passage is a piece of text used as input for generating questions or for text modification tasks","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"QUESTION","type":"CONCEPT","description":"A question is an inquiry generated based on a passage to assess comprehension or to be refined for complexity","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"ANSWER CHOICES","type":"CONCEPT","description":"Answer choices are the options provided for a question, which can be modified to add complexity or create distractors","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONCEPT"},{"name":"CARDIOVASCULAR EVENTS","type":"CONDITION","description":"Cardiovascular events are incidents related to heart disease, which can be influenced by factors like genetic predisposition and uric acid levels","source_id":"1d8835c0ce90e56be22873bcf2740a5d","entity_type":"CONDITION"},{"name":"RED MEAT","type":"SUBSTANCE","description":"Red meat is a type of food that contains uric acid","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"SEAFOOD","type":"SUBSTANCE","description":"Seafood is a type of food that contains uric acid","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"ALCOHOL CONSUMPTION","type":"LIFESTYLE CHOICE","description":"Alcohol consumption is a lifestyle choice that can influence uric acid levels","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PHYSICAL INACTIVITY","type":"LIFESTYLE CHOICE","description":"Physical inactivity is a lifestyle choice that can influence uric acid levels","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"KIDNEY ISSUES","type":"CONDITION","description":"Kidney issues can be indicated by low levels of uric acid","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"LIVER ISSUES","type":"CONDITION","description":"Liver issues can be indicated by low levels of uric acid","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"LITERAL COMPREHENSION QUESTIONS","type":"CONCEPT","description":"Literal comprehension questions are a type of reading comprehension question that assesses understanding of explicit information in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CRITICAL COMPREHENSION QUESTIONS","type":"CONCEPT","description":"Critical comprehension questions are a type of reading comprehension question that assesses the ability to evaluate and analyze the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"EVALUATIVE COMPREHENSION QUESTIONS","type":"CONCEPT","description":"Evaluative comprehension questions are a type of reading comprehension question that assesses the ability to make judgments about the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"REASONING QUESTIONS","type":"CONCEPT","description":"Reasoning questions are a type of reading comprehension question that assesses the ability to draw inferences and conclusions from the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"IDENTIFYING ASSUMPTIONS","type":"CONCEPT","description":"Identifying assumptions is a type of reading comprehension question that assesses the ability to recognize underlying assumptions in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"IDENTIFYING INFORMATION THAT STRENGTHENS\/WEAKENS AN ARGUMENT","type":"CONCEPT","description":"Identifying information that strengthens or weakens an argument is a type of reading comprehension question that assesses the ability to evaluate the strength of arguments in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"ORDERING EVENTS","type":"CONCEPT","description":"Ordering events is a type of reading comprehension question that assesses the ability to sequence events in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"STRENGTHEN TYPE QUESTION","type":"CONCEPT","description":"Strengthen type question is a type of question that asks which information, if true, would most strengthen an argument presented in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"MODIFY THE PASSAGE","type":"CONCEPT","description":"Modify the passage is a task in the instruction refinement flow that involves changing the passage to make the question unanswerable or alter the answer","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"MODIFY THE QUESTIONS OR ANSWER CHOICES","type":"CONCEPT","description":"Modify the questions or answer choices is a task in the instruction refinement flow that involves changing the questions or answer choices to add complexity","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"PARAPHRASING","type":"CONCEPT","description":"Paraphrasing is a text modification task that involves rephrasing the text while retaining its original meaning","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"EXPANSION","type":"CONCEPT","description":"Expansion is a text modification task that involves adding more information to the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"SIMPLIFICATION","type":"CONCEPT","description":"Simplification is a text modification task that involves making the text easier to understand","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"REDACTING OR REMOVING CONTENT","type":"CONCEPT","description":"Redacting or removing content is a text modification task that involves deleting parts of the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"STYLING","type":"CONCEPT","description":"Styling is a text modification task that involves changing the style of the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"CODE SWITCHING","type":"CONCEPT","description":"Code switching is a text modification task that involves changing the language or dialect used in the text","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"FINANCE","type":"CONCEPT","description":"Finance is a field that deals with the study of investments, money, and revenue management","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"SOCIAL IMPACT","type":"CONCEPT","description":"Social impact refers to the effect of an activity on the social fabric of the community and well-being of individuals and families","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"FINANCIAL DISCOURSES","type":"CONCEPT","description":"Financial discourses refer to conversations and discussions about financial topics","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"MARKETS","type":"CONCEPT","description":"Markets refer to systems or environments where commercial dealings are conducted","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"ACTORS","type":"CONCEPT","description":"Actors in finance refer to individuals or entities that participate in financial markets","source_id":"1d8835c0ce90e56be22873bcf2740a5d"},{"name":"INSTITUTIONS","type":"CONCEPT","description":"Institutions in finance refer to organizations that influence or participate in financial markets","source_id":"1d8835c0ce90e56be22873bcf2740a5d"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"URIC ACID\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">Uric acid is a chemical found in red meat and seafood, and its levels can be influenced by lifestyle choices such as alcohol consumption and physical inactivity<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPERURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Hyperuricemia is a condition characterized by high levels of uric acid in the blood, which can be associated with cardiovascular diseaseHyperuricemia refers to high levels of uric acid in the blood, which is associated with an increased risk of cardiovascular disease<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONDITION<\/data>    <\/node>    <node id=\"HYPOURICEMIA\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Hypouricemia refers to low levels of uric acid in the blood, which is less common and usually does not present symptoms but can indicate underlying kidney or liver issues<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CARDIOVASCULAR DISEASE\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Cardiovascular disease is a condition that may be associated with high levels of uric acid, although the causal relationship is not conclusively established<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"LABORATORY TESTS\">      <data key=\"d0\">PROCEDURE<\/data>      <data key=\"d1\">Laboratory blood and urine tests are required to diagnose conditions related to uric acid levels<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Generation Flow is a process that compiles a collection of reading comprehension question types and defines agents to generate questions based on predefined types<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agents are defined to target specific categories of reading comprehension questions and generate questions based on a piece of text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Content Transformation Agent determines the subset of agents to engage in the orchestration process for generating (passage, question) pairs<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction Refinement Flow involves suggester-editor agents that refine (passage, question) pairs to create more complex or unanswerable questions<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"SUGGESTER AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Suggester Agent provides suggestions to modify passages or questions to add complexity or create difficult distractors<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"EDITOR AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Editor Agent makes modifications to passages or questions based on suggestions to refine the content<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"AGENTINSTRUCT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">AgentInstruct Flow is a process for text modification that involves editing and refining written content to enhance its quality and effectiveness<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"TEXT MODIFICATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text modification involves editing written content to correct spelling and grammar, clarify ideas, reorganize content, adjust tone, ensure style consistency, fact-check, remove redundancies, format, develop content, and adapt to specific audiences<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PARAPHRASING AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Paraphrasing Agent is an agent that takes a piece of text and creates paraphrased versions as part of text modification tasks<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"READING COMPREHENSION QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reading comprehension questions are questions designed to assess understanding of a text, including types like literal, critical, evaluative, reasoning, and identifying assumptions<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"HYPOTHETICAL STUDY\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A hypothetical study is a suggested addition to a passage that could potentially strengthen an argument, requiring inference of its impact on relationships within the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"GENETIC PREDISPOSITION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Genetic predisposition refers to the likelihood of developing a condition based on genetic factors, which can add complexity to questions about relationships between conditions<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DISTRACTOR OPTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A distractor option is a misleading answer choice that seems relevant but does not directly relate to the question, testing the ability to discern relevant from irrelevant information<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"TEXT MODIFICATION TASKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Text modification tasks include activities like paraphrasing, expansion, simplification, redacting, styling, and code switching to alter text attributes<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"PASSAGE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A passage is a piece of text used as input for generating questions or for text modification tasks<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A question is an inquiry generated based on a passage to assess comprehension or to be refined for complexity<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"ANSWER CHOICES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Answer choices are the options provided for a question, which can be modified to add complexity or create distractors<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"CARDIOVASCULAR EVENTS\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Cardiovascular events are incidents related to heart disease, which can be influenced by factors like genetic predisposition and uric acid levels<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>      <data key=\"d3\">CONDITION<\/data>    <\/node>    <node id=\"RED MEAT\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">Red meat is a type of food that contains uric acid<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"SEAFOOD\">      <data key=\"d0\">SUBSTANCE<\/data>      <data key=\"d1\">Seafood is a type of food that contains uric acid<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"ALCOHOL CONSUMPTION\">      <data key=\"d0\">LIFESTYLE CHOICE<\/data>      <data key=\"d1\">Alcohol consumption is a lifestyle choice that can influence uric acid levels<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PHYSICAL INACTIVITY\">      <data key=\"d0\">LIFESTYLE CHOICE<\/data>      <data key=\"d1\">Physical inactivity is a lifestyle choice that can influence uric acid levels<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"KIDNEY ISSUES\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Kidney issues can be indicated by low levels of uric acid<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"LIVER ISSUES\">      <data key=\"d0\">CONDITION<\/data>      <data key=\"d1\">Liver issues can be indicated by low levels of uric acid<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"LITERAL COMPREHENSION QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Literal comprehension questions are a type of reading comprehension question that assesses understanding of explicit information in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CRITICAL COMPREHENSION QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Critical comprehension questions are a type of reading comprehension question that assesses the ability to evaluate and analyze the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Evaluative comprehension questions are a type of reading comprehension question that assesses the ability to make judgments about the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"REASONING QUESTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Reasoning questions are a type of reading comprehension question that assesses the ability to draw inferences and conclusions from the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"IDENTIFYING ASSUMPTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Identifying assumptions is a type of reading comprehension question that assesses the ability to recognize underlying assumptions in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"IDENTIFYING INFORMATION THAT STRENGTHENS\/WEAKENS AN ARGUMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Identifying information that strengthens or weakens an argument is a type of reading comprehension question that assesses the ability to evaluate the strength of arguments in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"ORDERING EVENTS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Ordering events is a type of reading comprehension question that assesses the ability to sequence events in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"STRENGTHEN TYPE QUESTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Strengthen type question is a type of question that asks which information, if true, would most strengthen an argument presented in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"MODIFY THE PASSAGE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Modify the passage is a task in the instruction refinement flow that involves changing the passage to make the question unanswerable or alter the answer<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"MODIFY THE QUESTIONS OR ANSWER CHOICES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Modify the questions or answer choices is a task in the instruction refinement flow that involves changing the questions or answer choices to add complexity<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"PARAPHRASING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Paraphrasing is a text modification task that involves rephrasing the text while retaining its original meaning<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"EXPANSION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Expansion is a text modification task that involves adding more information to the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"SIMPLIFICATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Simplification is a text modification task that involves making the text easier to understand<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"REDACTING OR REMOVING CONTENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Redacting or removing content is a text modification task that involves deleting parts of the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"STYLING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Styling is a text modification task that involves changing the style of the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"CODE SWITCHING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Code switching is a text modification task that involves changing the language or dialect used in the text<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"FINANCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Finance is a field that deals with the study of investments, money, and revenue management<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"SOCIAL IMPACT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Social impact refers to the effect of an activity on the social fabric of the community and well-being of individuals and families<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"FINANCIAL DISCOURSES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Financial discourses refer to conversations and discussions about financial topics<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"MARKETS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Markets refer to systems or environments where commercial dealings are conducted<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"ACTORS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Actors in finance refer to individuals or entities that participate in financial markets<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <node id=\"INSTITUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Institutions in finance refer to organizations that influence or participate in financial markets<\/data>      <data key=\"d2\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/node>    <edge source=\"URIC ACID\" target=\"HYPERURICEMIA\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">High levels of uric acid in the blood lead to hyperuricemia<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"HYPOURICEMIA\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Low levels of uric acid in the blood lead to hypouricemia<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"RED MEAT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Uric acid is found in red meat<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"SEAFOOD\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Uric acid is found in seafood<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"ALCOHOL CONSUMPTION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Uric acid levels can be influenced by alcohol consumption<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"URIC ACID\" target=\"PHYSICAL INACTIVITY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Uric acid levels can be influenced by physical inactivity<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"CARDIOVASCULAR DISEASE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Hyperuricemia is associated with an increased risk of cardiovascular disease<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPERURICEMIA\" target=\"LABORATORY TESTS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Laboratory blood and urine tests are required to diagnose hyperuricemia<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"LABORATORY TESTS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Laboratory blood and urine tests are required to diagnose hypouricemia<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"KIDNEY ISSUES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Hypouricemia can indicate underlying kidney issues<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"HYPOURICEMIA\" target=\"LIVER ISSUES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Hypouricemia can indicate underlying liver issues<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"AGENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow defines agents to generate reading comprehension questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"LITERAL COMPREHENSION QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes literal comprehension questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"CRITICAL COMPREHENSION QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes critical comprehension questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"EVALUATIVE COMPREHENSION QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes evaluative comprehension questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"REASONING QUESTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes reasoning questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"IDENTIFYING ASSUMPTIONS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes identifying assumptions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"IDENTIFYING INFORMATION THAT STRENGTHENS\/WEAKENS AN ARGUMENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes identifying information that strengthens or weakens an argument<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION GENERATION FLOW\" target=\"ORDERING EVENTS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Seed Instruction Generation Flow includes ordering events<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENT\" target=\"READING COMPREHENSION QUESTIONS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Agents generate reading comprehension questions based on predefined types<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENT\" target=\"CONTENT TRANSFORMATION AGENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Content Transformation Agent determines the subset of agents to engage in the orchestration process<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"SUGGESTER AGENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Instruction Refinement Flow involves suggester agents to provide suggestions for refining questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"EDITOR AGENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Instruction Refinement Flow involves editor agents to make modifications based on suggestions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"MODIFY THE PASSAGE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Instruction Refinement Flow includes tasks to modify the passage<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"INSTRUCTION REFINEMENT FLOW\" target=\"MODIFY THE QUESTIONS OR ANSWER CHOICES\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Instruction Refinement Flow includes tasks to modify the questions or answer choices<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SUGGESTER AGENT\" target=\"HYPOTHETICAL STUDY\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Suggester Agent may introduce a hypothetical study to strengthen an argument<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SUGGESTER AGENT\" target=\"GENETIC PREDISPOSITION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Suggester Agent may suggest genetic predisposition to add complexity to questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"SUGGESTER AGENT\" target=\"DISTRACTOR OPTION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Suggester Agent may include a distractor option to test the ability to discern relevant from irrelevant information<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"TEXT MODIFICATION\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">AgentInstruct Flow is a process for text modification<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"PARAPHRASING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes paraphrasing as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"EXPANSION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes expansion as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"SIMPLIFICATION\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes simplification as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"REDACTING OR REMOVING CONTENT\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes redacting or removing content as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"STYLING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes styling as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"CODE SWITCHING\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">AgentInstruct Flow includes code switching as a text modification task<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"PARAPHRASING AGENT\" target=\"TEXT MODIFICATION TASKS\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">Paraphrasing Agent creates paraphrased versions of text as part of text modification tasks<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"GENETIC PREDISPOSITION\" target=\"CARDIOVASCULAR EVENTS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Cardiovascular events can be influenced by genetic predisposition<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"PASSAGE\" target=\"QUESTION\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">A passage is used as input for generating questions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"ANSWER CHOICES\">      <data key=\"d4\">9.0<\/data>      <data key=\"d5\">Questions have answer choices that can be modified to add complexity<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"FINANCE\" target=\"SOCIAL IMPACT\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Finance has an increasing social impact<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"FINANCE\" target=\"FINANCIAL DISCOURSES\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Finance involves financial discourses<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"FINANCE\" target=\"MARKETS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Finance involves markets<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"FINANCE\" target=\"ACTORS\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">Finance involves actors<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>    <edge source=\"FINANCE\" target=\"INSTITUTIONS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">Finance involves institutions<\/data>      <data key=\"d6\">1d8835c0ce90e56be22873bcf2740a5d<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"427e98b00e49b6a8f8649054122dd45b","chunk":" text and\ncreates several text modification tasks of the associated type. Here we provide an example\ninput and a task created by the Paraphrasing Agent.\n8EXAMPLE: Seed Instruction\nRandom Seed\nApril 6-8, 2017, University of Iowa, Iowa City, USA. Abstracts due December 1, 2016.\nFinance is hard to escape. In recent years, the increasing social impact and interconnection\nof financial discourses, markets, actors, and institutions have been understood under the\nbroad concept of financialization. Natascha van der Zwan identifies three distinct research\nstreams that have approached financialization as 1) a regime of accumulation, 2) the influence\nof financial markets and instruments on non-financial corporations as well as the banking\nand finance industry, and 3) a discourse of risk-taking, self-management and self-fulfillment\nthat is transforming people into investing subjects. Some anthropological skeptics, however,\nargue that finance has a far longer genealogy than the financialization literature has to date\nrecognized. For example, in the context of a lengthy human history of creating hierarchy,\nfinancialization may simply be a new technology serving an old purpose.\n... (omitted for brevity)... \u00b7The supply chains of financial products connect different places\nand political projects across the globe. How do such financial instruments transform social\nlife?\nAbstract deadline is December 1, 2016.\n1. Go to americananthro.org and log in. If you don\u2019t have a login id and password, create\none (you do not need to join the American Anthropological Association).\n2. Once you are logged in, look to the left hand column, click on Meeting registration.\n3. Click on register under the SEA 2017 Annual Meeting then follow online prompts to\nregister for the meeting (if we do not accept your abstract and you decide not to attend,\nyou may request that your registration fee be refunded and we would be happy to do so).\n4. Once you are registered, AAA will automatically send you an email inviting you to submit\nan abstract. Click the link and follow the instructions.\nTask:\nRewrite the event details (date, location, abstract deadline) in a more casual tone.\nGiven a random seed, the seed instruction creation flow randomly picks one of the 18agents\nand uses that to obtain a seed (text, text modification instruction) pair.\nInstruction Refinement Flow The refinement flow consists of only one Suggester-Editor\npair. The Suggester-Editor duo increase the complexity of the generated instructions while\ntaking as input the text and the task modification instruction as input. Here we show three\nsuggestions and edits as proposed by the suggester-editor pair.\nEXAMPLE: IRefinement Flow\nSuggestions\nSuggestion 1: Incorporate a fictional narrative. Use a conversational style with colloquial\nlanguage and include a humorous element.\nSuggestion 2: Translate the event details into a poetic format. Maintain accurate infor-\nmation while using rhyming couplets and ensure the tone remains light and\nengaging.\nSuggestion 3: Frame the event details as a social media post. Use internet slang and\nemojis. Keep the message within 280 characters.\nModified Instructions\nInstruction 1: Rewrite the event details (date, location, abstract deadline) as if you\u2019re\ntelling a funny story to a friend using casual and colloquial language,\nwhile incorporating a fictional narrative that still conveys the necessary\ninformation.\nInstruction 2: Transform the event details (date, location, abstract deadline) into a light-\nheartedpoemwithrhymingcouplets, ensuringthattheessentialinformation\nis accurately conveyed in a poetic format.\nInstruction 3: Craft a social media post that includes the event details (date, location,\nabstract deadline) using internet slang, emojis, and a casual tone, while\nkeeping the message concise and within 280 characters.\n92.3 AgentInstruct Flow for Tool Use\nThe task of tool use or API use for LLMs involves enabling models to interact with external\ntools or services; via APIs. This capability allows AI systems to extend their functionality,\naccess external data, and perform actions beyond their native capabilities.\nContent Transformation Flow We use source code snippets or an API description [ 26] as\nthe random seed. If source code snippets has been used as the seed, a content transformation\nagent is used to synthesize an API description from the code snippet. The goal of the\nContent Transformation Flow is to synthesize list of APIs from the random seed. API lists\nare created by either: (1) using an API retrieval agent that iteratively searches for similar\ncode to expand the API list or (2) the agent uses the LLM to hypothesize other APIs present\nin the library.\nThe following figure provides an example of the library reconstruction scenario.\nEXAMPLE: Content Transformation Flow [Library Reconstruction]\nSeed\n{\n\"name\": \"View All Food Items\",\n\"description\": \"The request enables clients to obtain a detailed list of food items,\ncomplete with nutritional profiles such as calorie count, protein, fat, ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"limit\": {\n\"type\": \"NUMBER\",\n\"description\": \"limit the length of response\"\n}\n},\n\"required\": []\n}\n}\nReconstructed Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n","chunk_id":"427e98b00e49b6a8f8649054122dd45b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"NATASCHA VAN DER ZWAN","type":"PERSON","description":"Natascha van der Zwan is a researcher who identifies three distinct research streams that have approached financialization","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"FINANCIALIZATION","type":"CONCEPT","description":"Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"UNIVERSITY OF IOWA","type":"ORGANIZATION","description":"The University of Iowa is the location where an event took place from April 6-8, 2017","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"AMERICAN ANTHROPOLOGICAL ASSOCIATION","type":"ORGANIZATION","description":"The American Anthropological Association is an organization that manages the registration and abstract submission process for the SEA 2017 Annual Meeting","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SEA 2017 ANNUAL MEETING","type":"EVENT","description":"The SEA 2017 Annual Meeting is an event held at the University of Iowa from April 6-8, 2017, with an abstract deadline of December 1, 2016","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SUGGESTER-EDITOR","type":"TECHNOLOGY","description":"The Suggester-Editor is a duo that increases the complexity of generated instructions by providing suggestions and edits","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"AGENTINSTRUCT FLOW","type":"TECHNOLOGY","description":"AgentInstruct Flow is a process that enables models to interact with external tools or services via APIs","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"CONTENT TRANSFORMATION FLOW","type":"TECHNOLOGY","description":"Content Transformation Flow is a process that synthesizes a list of APIs from a random seed, such as source code snippets or an API description","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"API RETRIEVAL AGENT","type":"TECHNOLOGY","description":"The API Retrieval Agent is a system that iteratively searches for similar code to expand an API list","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"LLM","type":"TECHNOLOGY","description":"LLM stands for Large Language Model, which is used in various tasks such as debating, content transformation, and API synthesis","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"VIEW ALL FOOD ITEMS","type":"API","description":"View All Food Items is an API that enables clients to obtain a detailed list of food items, complete with nutritional profiles","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"SEARCH FOOD ITEMS","type":"API","description":"Search Food Items is an API that allows clients to search for food items by name and retrieve a list of matching items","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"IOWA CITY","type":"LOCATION","description":"Iowa City is the location where the SEA 2017 Annual Meeting was held","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"DECEMBER 1, 2016","type":"DATE","description":"December 1, 2016, is the abstract submission deadline for the SEA 2017 Annual Meeting","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"FINANCE","type":"CONCEPT","description":"Finance refers to the management of money, including activities such as investing, borrowing, lending, budgeting, and saving","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"RANDOM SEED","type":"CONCEPT","description":"A random seed is a value or set of values used to initialize a random number generator, often used in the context of generating random instructions or tasks","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"INSTRUCTION REFINEMENT FLOW","type":"TECHNOLOGY","description":"Instruction Refinement Flow is a process that involves a Suggester-Editor pair to increase the complexity of generated instructions","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"TOOL USE","type":"CONCEPT","description":"Tool use refers to the task of enabling models to interact with external tools or services via APIs","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"API DESCRIPTION","type":"DOCUMENT","description":"An API description is a document that provides details about an API, including its name, description, parameters, and usage","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"LIBRARY RECONSTRUCTION","type":"TECHNOLOGY","description":"Library Reconstruction is a scenario in the Content Transformation Flow where a list of APIs is synthesized from a random seed","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"NUTRITIONAL PROFILES","type":"CONCEPT","description":"Nutritional profiles refer to the detailed information about the nutritional content of food items, such as calorie count, protein, and fat","source_id":"427e98b00e49b6a8f8649054122dd45b"},{"name":"PARAPHRASING AGENT","type":"","description":"","source_id":"427e98b00e49b6a8f8649054122dd45b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NATASCHA VAN DER ZWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Natascha van der Zwan is a researcher who identifies three distinct research streams that have approached financialization<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"FINANCIALIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Financialization is a broad concept that encompasses the increasing social impact and interconnection of financial discourses, markets, actors, and institutions<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"UNIVERSITY OF IOWA\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The University of Iowa is the location where an event took place from April 6-8, 2017<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">The American Anthropological Association is an organization that manages the registration and abstract submission process for the SEA 2017 Annual Meeting<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d0\">EVENT<\/data>      <data key=\"d1\">The SEA 2017 Annual Meeting is an event held at the University of Iowa from April 6-8, 2017, with an abstract deadline of December 1, 2016<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SUGGESTER-EDITOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Suggester-Editor is a duo that increases the complexity of generated instructions by providing suggestions and edits<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"AGENTINSTRUCT FLOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct Flow is a process that enables models to interact with external tools or services via APIs<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Content Transformation Flow is a process that synthesizes a list of APIs from a random seed, such as source code snippets or an API description<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"API RETRIEVAL AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The API Retrieval Agent is a system that iteratively searches for similar code to expand an API list<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"LLM\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">LLM stands for Large Language Model, which is used in various tasks such as debating, content transformation, and API synthesis<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"VIEW ALL FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">View All Food Items is an API that enables clients to obtain a detailed list of food items, complete with nutritional profiles<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"SEARCH FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Search Food Items is an API that allows clients to search for food items by name and retrieve a list of matching items<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"IOWA CITY\">      <data key=\"d0\">LOCATION<\/data>      <data key=\"d1\">Iowa City is the location where the SEA 2017 Annual Meeting was held<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"DECEMBER 1, 2016\">      <data key=\"d0\">DATE<\/data>      <data key=\"d1\">December 1, 2016, is the abstract submission deadline for the SEA 2017 Annual Meeting<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"FINANCE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Finance refers to the management of money, including activities such as investing, borrowing, lending, budgeting, and saving<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"RANDOM SEED\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">A random seed is a value or set of values used to initialize a random number generator, often used in the context of generating random instructions or tasks<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Instruction Refinement Flow is a process that involves a Suggester-Editor pair to increase the complexity of generated instructions<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"TOOL USE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Tool use refers to the task of enabling models to interact with external tools or services via APIs<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"API DESCRIPTION\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">An API description is a document that provides details about an API, including its name, description, parameters, and usage<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"LIBRARY RECONSTRUCTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Library Reconstruction is a scenario in the Content Transformation Flow where a list of APIs is synthesized from a random seed<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"NUTRITIONAL PROFILES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Nutritional profiles refer to the detailed information about the nutritional content of food items, such as calorie count, protein, and fat<\/data>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <node id=\"PARAPHRASING AGENT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/node>    <edge source=\"NATASCHA VAN DER ZWAN\" target=\"FINANCIALIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Natascha van der Zwan identifies three distinct research streams that have approached financialization<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"FINANCIALIZATION\" target=\"FINANCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Finance is a broader concept that encompasses financialization, which includes the increasing social impact and interconnection of financial discourses, markets, actors, and institutions<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"UNIVERSITY OF IOWA\" target=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The SEA 2017 Annual Meeting was held at the University of Iowa from April 6-8, 2017<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"AMERICAN ANTHROPOLOGICAL ASSOCIATION\" target=\"SEA 2017 ANNUAL MEETING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The American Anthropological Association manages the registration and abstract submission process for the SEA 2017 Annual Meeting<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEA 2017 ANNUAL MEETING\" target=\"IOWA CITY\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Iowa City is the location where the SEA 2017 Annual Meeting was held<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SEA 2017 ANNUAL MEETING\" target=\"DECEMBER 1, 2016\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">December 1, 2016, is the abstract submission deadline for the SEA 2017 Annual Meeting<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"SUGGESTER-EDITOR\" target=\"INSTRUCTION REFINEMENT FLOW\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Instruction Refinement Flow involves a Suggester-Editor pair to increase the complexity of generated instructions<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"CONTENT TRANSFORMATION FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AgentInstruct Flow involves enabling models to interact with external tools or services, which is part of the Content Transformation Flow<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT FLOW\" target=\"TOOL USE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tool use is a task in the AgentInstruct Flow that involves enabling models to interact with external tools or services via APIs<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"API RETRIEVAL AGENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Content Transformation Flow uses the API Retrieval Agent to iteratively search for similar code to expand the API list<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"LLM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Content Transformation Flow uses LLMs to hypothesize other APIs present in the library<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"API DESCRIPTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">An API description can be used as a random seed in the Content Transformation Flow to synthesize a list of APIs<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"CONTENT TRANSFORMATION FLOW\" target=\"LIBRARY RECONSTRUCTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Library Reconstruction is a scenario in the Content Transformation Flow where a list of APIs is synthesized from a random seed<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"VIEW ALL FOOD ITEMS\" target=\"SEARCH FOOD ITEMS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">View All Food Items and Search Food Items are related APIs in the same library<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"VIEW ALL FOOD ITEMS\" target=\"NUTRITIONAL PROFILES\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">View All Food Items API provides a detailed list of food items, complete with nutritional profiles<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>    <edge source=\"RANDOM SEED\" target=\"PARAPHRASING AGENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Paraphrasing Agent uses a random seed to create text modification tasks<\/data>      <data key=\"d5\">427e98b00e49b6a8f8649054122dd45b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0922646b93a124514ce2a267d961d229","chunk":" Remaining APIs from the Library\n[\n{\n\"name\": \"Search Food Items\",\n\"description\": \"Allows clients to search for food items by name and retrieve a list of\nmatching items ...\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The name or partial name of the food item to search for.\"\n},\n\"limit\": {\n\"type\": \"number\",\n\"description\": \"Optional parameter to limit the number of search results\nreturned.\"\n}\n},\n\"required\": [\n\"query\"\n]\n}\n},\n10{\n\"name\": \"Get Food Item Details\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Create Meal Plan\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Update Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Track User Meal\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get Dietary Recommendations\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Add New Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Delete Food Item\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n},\n{\n\"name\": \"Get User Nutritional Stats\",\n\"description\": \"...\",\n\"parameters\": {\n...\n}\n}\n]\nSeed Instruction Creation Flow The seed instruction creation flow, consume the list of\nAPIs and employs variety of agents to create several tasks of the following types:\n1. Tasks that require the use of a single API:\n11(a) Tasks where the input supplies all necessary parameters.\n(b) Tasks where the input includes superfluous parameters.\n(c) Tasks where the input is missing some required parameters.\n2. Tasks that necessitate the use of multiple APIs:\n(a) Tasks where the input provides all necessary parameters.\n(b) Tasks where the input is missing some required parameters.\n3. Tasks that require a single API which is not listed among the available APIs.\n4.Tasks that require multiple APIs but lack some of the essential APIs in the provided\nlist.\nRefinement Flow The goal of the Refinement flow is to increase the complexity, for e.g.,\nlooking at the task and the conversation and to suggest refinements to increase the number\nof steps required to solve the task.\nThe following figure provides an example of a multi-turn conversation created by the Agent-\nInstruct flow.\nEXAMPLE: Instruction Data Created by AgentInstruct]\nSystem Message\nAs an AI assistant, your role is to assist users in achieving their desired outcomes. You have\nthe capability to utilize a variety of APIs, and at each step of the process, you are required\nto produce a markdown output.\nAt any given moment, you have two choices to proceed:\nChoice 1: Initiate an API Call If you choose to call an API, you must generate a makdown\nincluding the \"name\" of the API and the necessary \"parameters\" for the call. After creating\nthe response, you must wait for the system to execute the API with the provided parameters\nand return the results to you. The markdown you output should follow this structure:\nACTION_TYPE : API_CALL\nACTION: name(parameter_name=value, another_parameter_name=value, ....)\nChoice 2: Conclude the Process There are times when you may decide to end the process,\neither because a) you have successfully obtained the answer\nb) you have determined that the appropriate APIs are not available.\nc) you discovered that user did not provide values for some required parameter\nIn case you successfully obtained the answer use FINAL_ANSWER as the action name\nwith only one parameter being the final answer. In case you decided to stop due to other\nreasons, use FAILED as the action with one parameter describing the reason you decided to\nstop. Overall you should follow this structure:\nACTION_TYPE : STOP\nACTION: [ACTION_NAME]([VALUE])\nBelow is a list of APIs that you have access to:\n[\nAPIs from the Library construction example both the seed and the\ngenerated ones\n]\nUSER:\nI want to start a new diet plan for the next week. My goal is to consume around 1500\ncalories per day, and I prefer vegetarian meals. I\u2019ll be eating three meals a day. Can you\ncreate a meal plan for me? Also, I\u2019d like to track what I eat every day and get some new\nfood recommendations. At the end of the week, I want to see my nutritional summary. Oh,\nand I found this great recipe for a Quinoa Salad that I want to add to your database. I also\nnoticed that the calorie count for \u2018Chana Masala\u2019 seems off; it should be lower. And please\nremove \u2019Butter Chicken\u2019 from my list; I\u2019ve stopped eating it.\n12ASSISTANT:\nTo assist you with your request, I will perform the following actions:\n1. Create a meal plan based on your dietary preferences (vegetarian) and caloric goal (1500\ncalories per day) for three meals a day.\n2. Provide you with a way to track your daily meals.\n3. Offer you new food recommendations.\n4. Generate a nutritional summary at the end of the week. 5. Add the Quinoa Salad recipe\nto the database.\n6. Update the nutritional information for \u2019Chana Masala\u2019.\n7. Remove \u2019Butter Chicken\u2019 from the database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"","chunk_id":"0922646b93a124514ce2a267d961d229","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"SEARCH FOOD ITEMS","type":"API","description":"Search Food Items is an API that allows clients to search for food items by name and retrieve a list of matching items. It requires a query parameter and optionally accepts a limit parameter to restrict the number of search results returned","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET FOOD ITEM DETAILS","type":"API","description":"Get Food Item Details is an API that retrieves detailed information about a specific food item","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"CREATE MEAL PLAN","type":"API","description":"Create Meal Plan is an API that allows clients to create a meal plan based on specified parameters such as dietary preferences, caloric goals, and the number of meals per day","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"UPDATE FOOD ITEM","type":"API","description":"Update Food Item is an API that allows clients to update the details of an existing food item","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"TRACK USER MEAL","type":"API","description":"Track User Meal is an API that allows clients to track the meals consumed by a user","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET DIETARY RECOMMENDATIONS","type":"API","description":"Get Dietary Recommendations is an API that provides dietary recommendations based on user preferences and nutritional needs","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"ADD NEW FOOD ITEM","type":"API","description":"Add New Food Item is an API that allows clients to add a new food item to the database","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"DELETE FOOD ITEM","type":"API","description":"Delete Food Item is an API that allows clients to remove a food item from the database","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"GET USER NUTRITIONAL STATS","type":"API","description":"Get User Nutritional Stats is an API that retrieves the nutritional statistics of a user","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"SEED INSTRUCTION CREATION FLOW","type":"PROCESS","description":"The Seed Instruction Creation Flow is a process that consumes a list of APIs and employs various agents to create several types of tasks, including those requiring single or multiple APIs, and tasks with missing or superfluous parameters","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"REFINEMENT FLOW","type":"PROCESS","description":"The Refinement Flow is a process aimed at increasing the complexity of tasks by suggesting refinements to increase the number of steps required to solve the task","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"AGENT-INSTRUCT","type":"SYSTEM","description":"Agent-Instruct is a system that creates multi-turn conversations to assist users in achieving their desired outcomes by utilizing a variety of APIs and producing markdown outputs at each step","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"USER","type":"PERSON","description":"The User is an individual who interacts with the AI assistant to achieve specific goals such as creating a meal plan, tracking meals, getting food recommendations, and updating food item details","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"ASSISTANT","type":"SYSTEM","description":"The Assistant is an AI system designed to help users achieve their desired outcomes by utilizing various APIs and following a structured process to provide responses and actions","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"CHANA MASALA","type":"FOOD ITEM","description":"Chana Masala is a food item whose calorie count the user wants to update to a lower value","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"BUTTER CHICKEN","type":"FOOD ITEM","description":"Butter Chicken is a food item that the user wants to remove from the database","source_id":"0922646b93a124514ce2a267d961d229"},{"name":"QUINOA SALAD","type":"","description":"","source_id":"0922646b93a124514ce2a267d961d229"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"SEARCH FOOD ITEMS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Search Food Items is an API that allows clients to search for food items by name and retrieve a list of matching items. It requires a query parameter and optionally accepts a limit parameter to restrict the number of search results returned<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET FOOD ITEM DETAILS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Get Food Item Details is an API that retrieves detailed information about a specific food item<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"CREATE MEAL PLAN\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Create Meal Plan is an API that allows clients to create a meal plan based on specified parameters such as dietary preferences, caloric goals, and the number of meals per day<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"UPDATE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Update Food Item is an API that allows clients to update the details of an existing food item<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"TRACK USER MEAL\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Track User Meal is an API that allows clients to track the meals consumed by a user<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET DIETARY RECOMMENDATIONS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Get Dietary Recommendations is an API that provides dietary recommendations based on user preferences and nutritional needs<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"ADD NEW FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Add New Food Item is an API that allows clients to add a new food item to the database<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"DELETE FOOD ITEM\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Delete Food Item is an API that allows clients to remove a food item from the database<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"GET USER NUTRITIONAL STATS\">      <data key=\"d0\">API<\/data>      <data key=\"d1\">Get User Nutritional Stats is an API that retrieves the nutritional statistics of a user<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"SEED INSTRUCTION CREATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Seed Instruction Creation Flow is a process that consumes a list of APIs and employs various agents to create several types of tasks, including those requiring single or multiple APIs, and tasks with missing or superfluous parameters<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"REFINEMENT FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">The Refinement Flow is a process aimed at increasing the complexity of tasks by suggesting refinements to increase the number of steps required to solve the task<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"AGENT-INSTRUCT\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">Agent-Instruct is a system that creates multi-turn conversations to assist users in achieving their desired outcomes by utilizing a variety of APIs and producing markdown outputs at each step<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"USER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">The User is an individual who interacts with the AI assistant to achieve specific goals such as creating a meal plan, tracking meals, getting food recommendations, and updating food item details<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"ASSISTANT\">      <data key=\"d0\">SYSTEM<\/data>      <data key=\"d1\">The Assistant is an AI system designed to help users achieve their desired outcomes by utilizing various APIs and following a structured process to provide responses and actions<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"CHANA MASALA\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">Chana Masala is a food item whose calorie count the user wants to update to a lower value<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"BUTTER CHICKEN\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">Butter Chicken is a food item that the user wants to remove from the database<\/data>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <node id=\"QUINOA SALAD\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0922646b93a124514ce2a267d961d229<\/data>    <\/node>    <edge source=\"SEARCH FOOD ITEMS\" target=\"GET FOOD ITEM DETAILS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Search Food Items can be used to find food items, and Get Food Item Details can then be used to retrieve detailed information about those items<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"TRACK USER MEAL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Create Meal Plan can be used to create a meal plan, and Track User Meal can be used to track the meals consumed according to that plan<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"GET DIETARY RECOMMENDATIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Create Meal Plan can be used to create a meal plan, and Get Dietary Recommendations can provide additional dietary suggestions<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"CREATE MEAL PLAN\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Create Meal Plan API to create a meal plan for the User<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"ADD NEW FOOD ITEM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Add New Food Item can be used to add new food items to the database, and Update Food Item can be used to modify the details of these items<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"DELETE FOOD ITEM\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Delete Food Item can be used to remove food items from the database, and Update Food Item can be used to modify the details of existing items<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"UPDATE FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Update Food Item API to update the nutritional information for Chana Masala<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"GET USER NUTRITIONAL STATS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Get User Nutritional Stats can be used to retrieve nutritional statistics, which can be informed by the data collected through Track User Meal<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"TRACK USER MEAL\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Track User Meal API to help the User track their daily meals<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET DIETARY RECOMMENDATIONS\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Get Dietary Recommendations API to offer new food recommendations to the User<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"ADD NEW FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Add New Food Item API to add the Quinoa Salad recipe to the database<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"DELETE FOOD ITEM\" target=\"ASSISTANT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Assistant uses the Delete Food Item API to remove Butter Chicken from the database<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"GET USER NUTRITIONAL STATS\" target=\"ASSISTANT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The Assistant uses the Get User Nutritional Stats API to generate a nutritional summary for the User at the end of the week<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"SEED INSTRUCTION CREATION FLOW\" target=\"REFINEMENT FLOW\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Seed Instruction Creation Flow generates initial tasks, and the Refinement Flow increases the complexity of these tasks<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"AGENT-INSTRUCT\" target=\"ASSISTANT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Agent-Instruct is a system that helps the Assistant create multi-turn conversations to assist users<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"ASSISTANT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The User interacts with the Assistant to achieve specific goals such as creating a meal plan, tracking meals, getting food recommendations, and updating food item details<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"QUINOA SALAD\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to add Quinoa Salad to the database<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"CHANA MASALA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to update the calorie count for Chana Masala to a lower value<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>    <edge source=\"USER\" target=\"BUTTER CHICKEN\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The User wants to remove Butter Chicken from the database<\/data>      <data key=\"d5\">0922646b93a124514ce2a267d961d229<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"09cb89de3b77d765983cff25b7d74a1a","chunk":" database.\nLet\u2019s start by creating a vegetarian meal plan for you with a caloric goal of 1500 calories\nper day and three meals a day.\n\u2018 \u2018 \u2018markdown\nACTION_TYPE : API_CALL\nACTION: Create Meal Plan(dietary_preferences=[\"vegetarian \"] ,\ncaloric_goal=1500, num_meals=3)\n\u2018 \u2018 \u2018\nUSER:\n{\n\"status\": \"success\",\n\"meal_plan\": {\n\"day_1\": {\n\"breakfast\": {\n\"food_items\": [\"Oatmeal with Fruits\", \"Almond Milk\"],\n\"total_calories\": 350\n},\n\"lunch\": {\n\"food_items\": [\"Chickpea Salad\", \"Whole Wheat Bread\"],\n\"total_calories\": 500\n},\n\"dinner\": {\n\"food_items\": [\"Mixed Vegetable Stir Fry\", \"Brown Rice\"],\n\"total_calories\": 650\n}\n},\n\"day_2\": {\n...\n},\n...\n}\n}\nASSISTANT:\nYour meal plan for the next week has been successfully created. Here\u2019s a brief overview:\n- **Day 1**: Oatmeal with fruits and almond milk for breakfast, chickpea salad with whole\nwheat bread for lunch, and mixed vegetable stir fry with brown rice for dinner.\n- **Day 2**: .... ....\nNext, let\u2019s proceed to add the Quinoa Salad recipe to the database. Please provide the\nnutritional information for the Quinoa Salad so I can proceed with the addition.\nAs for updating the \u2019Chana Masala\u2019 and removing \u2019Butter Chicken\u2019, I will need\ntheir unique identifiers (food_id) to make those changes. If you have those identi-\nfiers, please provide them. If not, I can search for these items in the database to find\nthe correct identifiers. Would you like me to search for \u2019Chana Masala\u2019 and \u2019Butter Chicken\u2019?\nUSER:\n...(truncated)\n133 Orca-3\n3.1 Dataset Description\nWe implemented an AgentInstruct Flow for 17 different capabilities as described in Table 1.\nWe created a collection of approximately 22 million instructions aimed at teaching the\naforementioned skills. We have used unstructured text and code files sampled from\nKnowledgePile[ 7], AutoMathText[ 38], a subset of openstax and a subset of apache-2.0\nlicensed source code files from [ 4]. The dataset covers variety of skills, as detailed in Table 1.\nUsing unstructured content as seeds for instruction data generation has several benefits.\nFirst, there is abundance of such data enabling the generation of large-scale and diverse\ninstruction data. Additionally, it enables us to avoid using any benchmark-specific data as\nseeds and hence focus on optimizing for a capability, not for a specific benchmark.\nIn addition to the 22 million instructions, we have incorporated approximately 3.8 million\npaired instructions sourced from Orca-1[ 21], Orca-2[ 18], Orca-Math[ 19] and samples from\nother publicly available sources such as [ 5,37,10,30]. We refer to this data as Orca-2.5-\ndataset.\nThe culmination of these datasets results in approximately 25.8 million paired instructions,\nall of which are incorporated into the training of Orca-3. Furthermore, we have trained a\nseparate model, referred to as Orca-2.5, using the 3.8 million instructions (Orca-2.5-dataset).\nThe purpose of this is to compare and evaluate the impact of the 22 million instructions\ncurated through AgentInstruct.\n3.2 Training Details\nWe use the 25.8 million paired instructions described earlier to finetune Mistral-7b-v0.1.\nWe choose this model because the it makes the weights publicly available for the base\n(no-instruction-tuned) version, with a permissive license allowing easy redistribution. We\nrefer to the finetuned model ( Mistral-7b-v0.1 finetuned on AgentInstruct dataset) as Orca-3.\nEach pair in the dataset undergoes a tokenization process using the Mistral tokenizer,\nensuring a maximum sequence length of 8192 with packing. To guarantee that the training\nloss is calculated based only on the response conditioned on the prompt, label masking is\napplied. Weight decay was set at 0.1\nThe finetuneing used 19 NVIDIA A100 nodes, or 152 NVIDIA A100 GPUs, each with a\nbatch size of 10. We used AdamW optimizer with an initial learning rate of 8e-6 and a a\ncosine learning rate schedule. We also used a linear learning rate warm-up during the initial\n500 steps. The model was trained for three epochs and the training process concluded after\napproximately 200 hours.\n4 Evaluation Results\n4.1 Orca-Bench\nThe Orca-Bench dataset serves as a held-out test set, consisting of 100 samples from each of\nthe 17 skills for which data was curated using AgentInstruct, except for the Open Domain\nQuestion Answering (ODQA) category, where we created two test sets. The first subset,\nreferred to as ODQA, consists of 100 questions originated from the initial seed instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by","chunk_id":"09cb89de3b77d765983cff25b7d74a1a","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"VEGETARIAN MEAL PLAN","type":"PLAN","description":"A meal plan designed for vegetarians with a caloric goal of 1500 calories per day, consisting of three meals a day","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"CALORIC GOAL","type":"METRIC","description":"The target number of calories to be consumed per day, set at 1500 calories for the meal plan","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"OATMEAL WITH FRUITS","type":"FOOD ITEM","description":"A breakfast food item included in the vegetarian meal plan, consisting of oatmeal and fruits","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ALMOND MILK","type":"FOOD ITEM","description":"A breakfast beverage included in the vegetarian meal plan, made from almonds","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"CHICKPEA SALAD","type":"FOOD ITEM","description":"A lunch food item included in the vegetarian meal plan, consisting of chickpeas and other salad ingredients","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"WHOLE WHEAT BREAD","type":"FOOD ITEM","description":"A lunch food item included in the vegetarian meal plan, made from whole wheat","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MIXED VEGETABLE STIR FRY","type":"FOOD ITEM","description":"A dinner food item included in the vegetarian meal plan, consisting of various stir-fried vegetables","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"BROWN RICE","type":"FOOD ITEM","description":"A dinner food item included in the vegetarian meal plan, made from whole grain rice","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"QUINOA SALAD","type":"FOOD ITEM","description":"A food item that the user wants to add to the database, requiring nutritional information for addition","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"CHANA MASALA","type":"FOOD ITEM","description":"A food item that the user wants to update in the database, requiring its unique identifier","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"BUTTER CHICKEN","type":"FOOD ITEM","description":"A food item that the user wants to remove from the database, requiring its unique identifier","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"A system used to create a collection of instructions aimed at teaching various skills, implemented for 17 different capabilities","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-3","type":"MODEL","description":"A model trained using the AgentInstruct dataset, consisting of approximately 25.8 million paired instructions","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"KNOWLEDGEPILE","type":"DATASET","description":"A source of unstructured text and code files used as seeds for instruction data generation in the AgentInstruct system","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"AUTOMATHTEXT","type":"DATASET","description":"A source of unstructured text and code files used as seeds for instruction data generation in the AgentInstruct system","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-1","type":"DATASET","description":"A dataset consisting of paired instructions used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-2","type":"DATASET","description":"A dataset consisting of paired instructions used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-MATH","type":"DATASET","description":"A dataset consisting of paired instructions used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-2.5","type":"MODEL","description":"A model trained using approximately 3.8 million paired instructions, used to compare and evaluate the impact of the 22 million instructions curated through AgentInstruct","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MISTRAL-7B-V0.1","type":"MODEL","description":"The base model finetuned using the AgentInstruct dataset to create Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"NVIDIA A100","type":"HARDWARE","description":"The type of GPU used in the training of Orca-3, with 152 GPUs used in total","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ADAMW OPTIMIZER","type":"TECHNOLOGY","description":"The optimizer used in the training of Orca-3, with an initial learning rate of 8e-6","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-BENCH","type":"DATASET","description":"A held-out test set used to evaluate the performance of models trained with the AgentInstruct dataset, consisting of 100 samples from each of the 17 skills","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"GPT-4","type":"MODEL","description":"A model used as a baseline for scoring the performance of models evaluated with the Orca-Bench dataset","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DATABASE","type":"","description":"","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 2","type":"TIME PERIOD","description":"The second day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 3","type":"TIME PERIOD","description":"The third day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 4","type":"TIME PERIOD","description":"The fourth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 5","type":"TIME PERIOD","description":"The fifth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 6","type":"TIME PERIOD","description":"The sixth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 7","type":"TIME PERIOD","description":"The seventh day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"NUTRITIONAL INFORMATION","type":"METRIC","description":"The data required to add the Quinoa Salad recipe to the database, including details like calories, protein, fat, etc.","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"FOOD_ID","type":"METRIC","description":"The unique identifier required to update or remove food items like Chana Masala and Butter Chicken from the database","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"OPENSTAX","type":"DATASET","description":"A source of unstructured text used as seeds for instruction data generation in the AgentInstruct system","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"APACHE-2.0 LICENSED SOURCE CODE","type":"DATASET","description":"A subset of source code files used as seeds for instruction data generation in the AgentInstruct system","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"ORCA-2.5-DATASET","type":"DATASET","description":"A dataset consisting of approximately 3.8 million paired instructions used to train the Orca-2.5 model","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TOKENIZATION","type":"TECHNOLOGY","description":"The process applied to each pair in the dataset using the Mistral tokenizer, ensuring a maximum sequence length of 8192 with packing","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"LABEL MASKING","type":"TECHNOLOGY","description":"A technique used during training to ensure that the training loss is calculated based only on the response conditioned on the prompt","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"WEIGHT DECAY","type":"METRIC","description":"A parameter set at 0.1 during the training of Orca-3 to prevent overfitting","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"BATCH SIZE","type":"METRIC","description":"The number of training examples utilized in one iteration, set at 10 for each of the 152 NVIDIA A100 GPUs used in the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"INITIAL LEARNING RATE","type":"METRIC","description":"The starting learning rate for the AdamW optimizer, set at 8e-6 during the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"COSINE LEARNING RATE SCHEDULE","type":"TECHNOLOGY","description":"A learning rate schedule used during the training of Orca-3","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"LINEAR LEARNING RATE WARM-UP","type":"TECHNOLOGY","description":"A technique used during the initial 500 steps of training Orca-3 to gradually increase the learning rate","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"EPOCH","type":"METRIC","description":"A full pass through the training dataset, with Orca-3 being trained for three epochs","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"TRAINING HOURS","type":"METRIC","description":"The total time taken to train Orca-3, approximately 200 hours","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"OPEN DOMAIN QUESTION ANSWERING (ODQA)","type":"TECHNOLOGY","description":"A category in the Orca-Bench dataset consisting of 100 questions originated from the initial seed instruction phase","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"COMPLEX ODQA","type":"TECHNOLOGY","description":"A subset of the ODQA category in the Orca-Bench dataset, including more intricate questions developed during the refinement phase","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"MULTI-TURN INTERACTION","type":"TECHNOLOGY","description":"A type of interaction in the Orca-Bench dataset involving multiple exchanges","source_id":"09cb89de3b77d765983cff25b7d74a1a"},{"name":"DAY 1","type":"","description":"","source_id":"09cb89de3b77d765983cff25b7d74a1a"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"VEGETARIAN MEAL PLAN\">      <data key=\"d0\">PLAN<\/data>      <data key=\"d1\">A meal plan designed for vegetarians with a caloric goal of 1500 calories per day, consisting of three meals a day<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"CALORIC GOAL\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The target number of calories to be consumed per day, set at 1500 calories for the meal plan<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"OATMEAL WITH FRUITS\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A breakfast food item included in the vegetarian meal plan, consisting of oatmeal and fruits<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ALMOND MILK\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A breakfast beverage included in the vegetarian meal plan, made from almonds<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"CHICKPEA SALAD\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A lunch food item included in the vegetarian meal plan, consisting of chickpeas and other salad ingredients<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"WHOLE WHEAT BREAD\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A lunch food item included in the vegetarian meal plan, made from whole wheat<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MIXED VEGETABLE STIR FRY\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A dinner food item included in the vegetarian meal plan, consisting of various stir-fried vegetables<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"BROWN RICE\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A dinner food item included in the vegetarian meal plan, made from whole grain rice<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"QUINOA SALAD\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A food item that the user wants to add to the database, requiring nutritional information for addition<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"CHANA MASALA\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A food item that the user wants to update in the database, requiring its unique identifier<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"BUTTER CHICKEN\">      <data key=\"d0\">FOOD ITEM<\/data>      <data key=\"d1\">A food item that the user wants to remove from the database, requiring its unique identifier<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A system used to create a collection of instructions aimed at teaching various skills, implemented for 17 different capabilities<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model trained using the AgentInstruct dataset, consisting of approximately 25.8 million paired instructions<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"KNOWLEDGEPILE\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A source of unstructured text and code files used as seeds for instruction data generation in the AgentInstruct system<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"AUTOMATHTEXT\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A source of unstructured text and code files used as seeds for instruction data generation in the AgentInstruct system<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-1\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of paired instructions used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-2\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of paired instructions used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-MATH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of paired instructions used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model trained using approximately 3.8 million paired instructions, used to compare and evaluate the impact of the 22 million instructions curated through AgentInstruct<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MISTRAL-7B-V0.1\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">The base model finetuned using the AgentInstruct dataset to create Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"NVIDIA A100\">      <data key=\"d0\">HARDWARE<\/data>      <data key=\"d1\">The type of GPU used in the training of Orca-3, with 152 GPUs used in total<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ADAMW OPTIMIZER\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The optimizer used in the training of Orca-3, with an initial learning rate of 8e-6<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A held-out test set used to evaluate the performance of models trained with the AgentInstruct dataset, consisting of 100 samples from each of the 17 skills<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">A model used as a baseline for scoring the performance of models evaluated with the Orca-Bench dataset<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DATABASE\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 2\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The second day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 3\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The third day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 4\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The fourth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 5\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The fifth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 6\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The sixth day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 7\">      <data key=\"d0\">TIME PERIOD<\/data>      <data key=\"d1\">The seventh day in the vegetarian meal plan, consisting of three meals: breakfast, lunch, and dinner<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"NUTRITIONAL INFORMATION\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The data required to add the Quinoa Salad recipe to the database, including details like calories, protein, fat, etc.<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"FOOD_ID\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The unique identifier required to update or remove food items like Chana Masala and Butter Chicken from the database<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"OPENSTAX\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A source of unstructured text used as seeds for instruction data generation in the AgentInstruct system<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"APACHE-2.0 LICENSED SOURCE CODE\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A subset of source code files used as seeds for instruction data generation in the AgentInstruct system<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"ORCA-2.5-DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">A dataset consisting of approximately 3.8 million paired instructions used to train the Orca-2.5 model<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TOKENIZATION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The process applied to each pair in the dataset using the Mistral tokenizer, ensuring a maximum sequence length of 8192 with packing<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"LABEL MASKING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A technique used during training to ensure that the training loss is calculated based only on the response conditioned on the prompt<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"WEIGHT DECAY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A parameter set at 0.1 during the training of Orca-3 to prevent overfitting<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"BATCH SIZE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The number of training examples utilized in one iteration, set at 10 for each of the 152 NVIDIA A100 GPUs used in the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"INITIAL LEARNING RATE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The starting learning rate for the AdamW optimizer, set at 8e-6 during the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"COSINE LEARNING RATE SCHEDULE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A learning rate schedule used during the training of Orca-3<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"LINEAR LEARNING RATE WARM-UP\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A technique used during the initial 500 steps of training Orca-3 to gradually increase the learning rate<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"EPOCH\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">A full pass through the training dataset, with Orca-3 being trained for three epochs<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"TRAINING HOURS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">The total time taken to train Orca-3, approximately 200 hours<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"OPEN DOMAIN QUESTION ANSWERING (ODQA)\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A category in the Orca-Bench dataset consisting of 100 questions originated from the initial seed instruction phase<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"COMPLEX ODQA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A subset of the ODQA category in the Orca-Bench dataset, including more intricate questions developed during the refinement phase<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"MULTI-TURN INTERACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">A type of interaction in the Orca-Bench dataset involving multiple exchanges<\/data>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <node id=\"DAY 1\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/node>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"CALORIC GOAL\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The vegetarian meal plan is designed to meet a caloric goal of 1500 calories per day<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"OATMEAL WITH FRUITS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Oatmeal with fruits is included in the vegetarian meal plan for breakfast<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"ALMOND MILK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Almond milk is included in the vegetarian meal plan for breakfast<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"CHICKPEA SALAD\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Chickpea salad is included in the vegetarian meal plan for lunch<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"WHOLE WHEAT BREAD\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Whole wheat bread is included in the vegetarian meal plan for lunch<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"MIXED VEGETABLE STIR FRY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mixed vegetable stir fry is included in the vegetarian meal plan for dinner<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"BROWN RICE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Brown rice is included in the vegetarian meal plan for dinner<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 1\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 1 is the first day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 2\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 2 is the second day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 3 is the third day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 4\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 4 is the fourth day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 5\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 5 is the fifth day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 6\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 6 is the sixth day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"VEGETARIAN MEAL PLAN\" target=\"DAY 7\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Day 7 is the seventh day in the vegetarian meal plan<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"QUINOA SALAD\" target=\"DATABASE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user wants to add the Quinoa Salad recipe to the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"QUINOA SALAD\" target=\"NUTRITIONAL INFORMATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Nutritional information is required to add the Quinoa Salad recipe to the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"CHANA MASALA\" target=\"DATABASE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user wants to update the Chana Masala recipe in the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"CHANA MASALA\" target=\"FOOD_ID\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The unique identifier (food_id) is required to update the Chana Masala recipe in the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"BUTTER CHICKEN\" target=\"DATABASE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The user wants to remove the Butter Chicken recipe from the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"BUTTER CHICKEN\" target=\"FOOD_ID\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The unique identifier (food_id) is required to remove the Butter Chicken recipe from the database<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"ORCA-3\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-3 was trained using the AgentInstruct dataset<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"KNOWLEDGEPILE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">KnowledgePile is a source of unstructured text and code files used in the AgentInstruct system<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"AUTOMATHTEXT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AutoMathText is a source of unstructured text and code files used in the AgentInstruct system<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"OPENSTAX\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Openstax is a source of unstructured text used in the AgentInstruct system<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"APACHE-2.0 LICENSED SOURCE CODE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Apache-2.0 licensed source code is a subset of source code files used in the AgentInstruct system<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-1\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-1 is one of the datasets used in the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-2\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-2 is one of the datasets used in the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-MATH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-Math is one of the datasets used in the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-2.5\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-2.5 is a model used to compare and evaluate the impact of the 22 million instructions curated through AgentInstruct on Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-V0.1\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Mistral-7b-v0.1 is the base model finetuned to create Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"NVIDIA A100\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">NVIDIA A100 GPUs were used in the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ADAMW OPTIMIZER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AdamW optimizer was used in the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA-BENCH\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Orca-Bench is a dataset used to evaluate the performance of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TOKENIZATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Tokenization is applied to each pair in the dataset using the Mistral tokenizer during the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LABEL MASKING\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Label masking is used during the training of Orca-3 to ensure that the training loss is calculated based only on the response conditioned on the prompt<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"WEIGHT DECAY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Weight decay was set at 0.1 during the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BATCH SIZE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">A batch size of 10 was used for each of the 152 NVIDIA A100 GPUs during the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"INITIAL LEARNING RATE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The initial learning rate for the AdamW optimizer was set at 8e-6 during the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"COSINE LEARNING RATE SCHEDULE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">A cosine learning rate schedule was used during the training of Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"LINEAR LEARNING RATE WARM-UP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">A linear learning rate warm-up was used during the initial 500 steps of training Orca-3<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"EPOCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 was trained for three epochs<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TRAINING HOURS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The training of Orca-3 took approximately 200 hours<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-2.5\" target=\"ORCA-2.5-DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-2.5-dataset is used to train the Orca-2.5 model<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a baseline for scoring the performance of models evaluated with the Orca-Bench dataset<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"OPEN DOMAIN QUESTION ANSWERING (ODQA)\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">ODQA is a category in the Orca-Bench dataset<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"COMPLEX ODQA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Complex ODQA is a subset of the ODQA category in the Orca-Bench dataset<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"MULTI-TURN INTERACTION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Multi-turn interaction is a type of interaction in the Orca-Bench dataset<\/data>      <data key=\"d5\">09cb89de3b77d765983cff25b7d74a1a<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bd4eb9459bc29b4c2da4658914fd4635","chunk":" instruction\nphase. The second subset, termed Complex ODQA, includes more intricate questions\ndeveloped during the refinement phase.\nWe evaluated the performance of all baselines using the Orca-Bench dataset. These were\nscored relative to GPT-4 on a scale ranging from 0 to 10. It is worth noting that some of\nthe entries within Orca-Bench involve multiple exchanges. To illustrate, let a multi-turn\ninteraction in Orca-Bench be denoted by the sequence (system message, user 1, assistant,\nuser 2, assistant, ...), where each turn is crafted by GPT4 (teacher). For every user iinput, we\ngenerateacorresponding studentresponse, which isconditionedontheprecedingconversation\nhistory as established by the teacher. We then evaluate the student\u2019s generated response\n14Figure 4: Performance Comparison between Baselines and Orca3 Checkpoints. Scores are\nrelative to GPT-4 with the outer circle denoting GPT-4\u2019s score of 10.\nagainst the original teacher\u2019s response, rating each on a scale from 0 to 10. To calculate a\nstudent\u2019s overall score, we sum the student\u2019s individual scores and divide this total by the\nsum of the teacher\u2019s scores. This ratio is then multiplied by 10 to normalize the student\u2019s\nfinal score to a 0 to 10 scale.\nAgentInstruct\u2019s objective is to synthesize a large and diverse corpus of data with varying\ndegrees of difficulty. An efficacious execution of this strategy should yield a dataset against\nwhich baseline models like Orca-2.5, Mistral-Instruct-7b, and ChatGPT score substantially\nbelow 10, demonstrating their relative inferiority to GPT-4\u2014a model that is designated as\nthe benchmark with a score of 10. The performance comparison, as depicted in Figure 4,\nillustrates the comparative analysis between baseline models and Orca-3 . This figure shows\nthe notable enhancement in a broad spectrum of capabilities during post-training, enabled\nby the AgentInstruct data.\nTable 2 encapsulates the average (macro) scores across all assessed dimensions. On average,\nincluding orca-3 after each training epoch, the inclusion of AgentInstruct data has led to\na performance augmentation of 33.94% over the Orca 2.5 baseline and an enhancement of\n14.92% over Mistral-Instruct-7B.\n4.2 Benchmark Results\nWe evaluate Orca-3 against 5 baseline models including Orca-2.5, Mistral-7B-Instruct-v0.3,\nLLAMA3-8B-Instruct, GPT-3.5-turbo and GPT-4 on the following benchmarks:\n15Model Orca-Bench Score\nOrca-2.5 7.13\nMistral-Instruct-7B 8.31\nChatGPT 8.13\nOrca-3 (checkpoint epoch 1) 9.35\nOrca-3 (checkpoint epoch 2) 9.49\nOrca-3 9.55\nTable 2: Average Performance of Different Models on Orca-Bench. Scores are computed on\na scale of 0 to 10; 10 being the score of GPT4.\nModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-\nInstructLLAMA3-\n8B\ninstructGPT-3.5-\nturboGPT-4\nAGIEval 56.80 (+40%) 42.71 40.52 47.17 50.91 61.99\nMMLU 69.95 (+19%) 60.34 58.61 63.44 68.26 67.07\nARC 92.47 (+12%) 86.39 82.72 85.74 92.0 93.35\nBBH 61.83 (+38%) 48.63 44.71 54.97 54.17 76.06\nGPQA 28.12 (-4%) 27.68 29.46 28.12 27.9 33.93\nDROP 71.14 (+22%) 65.19 58.12 68.44 67.15 67.36\nGSM8K 83.09 (+54%) 74.3 54.06 77.48 78.1\u221786.88\nFOFO 84.01 (+12%) 66.19 75.3 79.35 76.92 87.45\nIFEval 49.54 (+2%) 45.29 48.61 - 58.6 79.3\nMT-Bench 8.20 (+9%) 7.15 7.53 7.99 8.01 9.04\nAlpacaEval 24.80 (+45%) 13.47 17.1 22.9 22.7 55\nInfoBench 84.30 (+4%) 79.6 81 - 86.7 89.4\nEQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8","chunk_id":"bd4eb9459bc29b4c2da4658914fd4635","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"COMPLEX ODQA","type":"DATASET","description":"Complex ODQA is a subset of questions developed during the refinement phase, which includes more intricate questions","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-BENCH","type":"DATASET","description":"Orca-Bench is a dataset used to evaluate the performance of various baseline models, scored relative to GPT-4 on a scale from 0 to 10","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a model used as a benchmark with a score of 10 in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is a strategy aimed at synthesizing a large and diverse corpus of data with varying degrees of difficulty","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-2.5","type":"MODEL","description":"Orca-2.5 is a baseline model evaluated on the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"MISTRAL-INSTRUCT-7B","type":"MODEL","description":"Mistral-Instruct-7B is a baseline model evaluated on the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"CHATGPT","type":"MODEL","description":"ChatGPT is a baseline model evaluated on the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a model evaluated on the Orca-Bench dataset, showing notable enhancement in capabilities during post-training","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"LLAMA3-8B-INSTRUCT","type":"MODEL","description":"LLAMA3-8B-Instruct is a baseline model evaluated on the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a baseline model evaluated on the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"MODEL"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"ARC","type":"BENCHMARK","description":"ARC is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"BBH","type":"BENCHMARK","description":"BBH is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"GPQA","type":"BENCHMARK","description":"GPQA is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"DROP","type":"BENCHMARK","description":"DROP is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"FOFO","type":"BENCHMARK","description":"FOFO is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"IFEVAL","type":"BENCHMARK","description":"IFEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InfoBench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"EQBENCH","type":"BENCHMARK","description":"EQBench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"METRIC-V2","type":"BENCHMARK","description":"Metric-v2 is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"METRIC-V1","type":"BENCHMARK","description":"Metric-v1 is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines","source_id":"bd4eb9459bc29b4c2da4658914fd4635","entity_type":"BENCHMARK"},{"name":"INSTRUCTION PHASE","type":"PHASE","description":"The instruction phase is a phase during which questions are developed for the dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"REFINEMENT PHASE","type":"PHASE","description":"The refinement phase is a phase during which more intricate questions are developed for the dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"SYSTEM MESSAGE","type":"MESSAGE","description":"A system message is part of a multi-turn interaction in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"USER MESSAGE","type":"MESSAGE","description":"A user message is part of a multi-turn interaction in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ASSISTANT MESSAGE","type":"MESSAGE","description":"An assistant message is part of a multi-turn interaction in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"TEACHER","type":"ROLE","description":"The teacher is GPT-4, which crafts each turn in a multi-turn interaction in the Orca-Bench dataset","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"STUDENT","type":"ROLE","description":"The student generates responses conditioned on the preceding conversation history as established by the teacher","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-3 CHECKPOINT","type":"MODEL","description":"Orca-3 Checkpoint is a specific version of the Orca-3 model evaluated at different epochs","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-3 (CHECKPOINT EPOCH 1)","type":"MODEL","description":"Orca-3 (checkpoint epoch 1) is a specific version of the Orca-3 model evaluated at the first epoch","source_id":"bd4eb9459bc29b4c2da4658914fd4635"},{"name":"ORCA-3 (CHECKPOINT EPOCH 2)","type":"MODEL","description":"Orca-3 (checkpoint epoch 2) is a specific version of the Orca-3 model evaluated at the second epoch","source_id":"bd4eb9459bc29b4c2da4658914fd4635"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"COMPLEX ODQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Complex ODQA is a subset of questions developed during the refinement phase, which includes more intricate questions<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-BENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Orca-Bench is a dataset used to evaluate the performance of various baseline models, scored relative to GPT-4 on a scale from 0 to 10<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a model used as a benchmark with a score of 10 in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is a strategy aimed at synthesizing a large and diverse corpus of data with varying degrees of difficulty<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5 is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-Instruct-7B is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"CHATGPT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">ChatGPT is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a model evaluated on the Orca-Bench dataset, showing notable enhancement in capabilities during post-training<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA3-8B-Instruct is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">ARC is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GPQA is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROP is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FOFO is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">IFEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InfoBench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"EQBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">EQBench is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"METRIC-V2\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Metric-v2 is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"METRIC-V1\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Metric-v1 is a benchmark used to evaluate the performance of various models, including Orca-3 and other baselines<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"INSTRUCTION PHASE\">      <data key=\"d0\">PHASE<\/data>      <data key=\"d1\">The instruction phase is a phase during which questions are developed for the dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"REFINEMENT PHASE\">      <data key=\"d0\">PHASE<\/data>      <data key=\"d1\">The refinement phase is a phase during which more intricate questions are developed for the dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"SYSTEM MESSAGE\">      <data key=\"d0\">MESSAGE<\/data>      <data key=\"d1\">A system message is part of a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"USER MESSAGE\">      <data key=\"d0\">MESSAGE<\/data>      <data key=\"d1\">A user message is part of a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ASSISTANT MESSAGE\">      <data key=\"d0\">MESSAGE<\/data>      <data key=\"d1\">An assistant message is part of a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"TEACHER\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">The teacher is GPT-4, which crafts each turn in a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"STUDENT\">      <data key=\"d0\">ROLE<\/data>      <data key=\"d1\">The student generates responses conditioned on the preceding conversation history as established by the teacher<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-3 CHECKPOINT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 Checkpoint is a specific version of the Orca-3 model evaluated at different epochs<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-3 (CHECKPOINT EPOCH 1)\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 (checkpoint epoch 1) is a specific version of the Orca-3 model evaluated at the first epoch<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <node id=\"ORCA-3 (CHECKPOINT EPOCH 2)\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 (checkpoint epoch 2) is a specific version of the Orca-3 model evaluated at the second epoch<\/data>      <data key=\"d2\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/node>    <edge source=\"COMPLEX ODQA\" target=\"ORCA-BENCH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Complex ODQA is a subset of questions included in the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"GPT-4\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">GPT-4 is used as a benchmark model with a score of 10 in the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"ORCA-2.5\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-2.5 is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Mistral-Instruct-7B is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"CHATGPT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">ChatGPT is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"ORCA-3\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Orca-3 is a model evaluated on the Orca-Bench dataset, showing notable enhancement in capabilities during post-training<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">LLAMA3-8B-Instruct is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-BENCH\" target=\"GPT-3.5-TURBO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">GPT-3.5-turbo is a baseline model evaluated on the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"ORCA-3\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AgentInstruct data has led to a performance augmentation of 33.94% over the Orca 2.5 baseline and an enhancement of 14.92% over Mistral-Instruct-7B<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGIEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the AGIEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MMLU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the MMLU benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ARC\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the ARC benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"BBH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the BBH benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPQA\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the GPQA benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"DROP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the DROP benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the GSM8K benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"FOFO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the FOFO benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"IFEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the IFEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MT-BENCH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the MT-Bench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ALPACAEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the AlpacaEval benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"INFOBENCH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the InfoBench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"EQBENCH\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the EQBench benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"METRIC-V2\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the Metric-v2 benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"METRIC-V1\">      <data key=\"d4\">2.0<\/data>      <data key=\"d5\">Orca-3 is evaluated on the Metric-v1 benchmark<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"INSTRUCTION PHASE\" target=\"REFINEMENT PHASE\">      <data key=\"d4\">6.0<\/data>      <data key=\"d5\">The instruction phase precedes the refinement phase in the development of questions for the dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"SYSTEM MESSAGE\" target=\"USER MESSAGE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">System messages and user messages are part of a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"USER MESSAGE\" target=\"ASSISTANT MESSAGE\">      <data key=\"d4\">7.0<\/data>      <data key=\"d5\">User messages and assistant messages are part of a multi-turn interaction in the Orca-Bench dataset<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>    <edge source=\"TEACHER\" target=\"STUDENT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">The teacher (GPT-4) crafts each turn in a multi-turn interaction, and the student generates responses conditioned on the preceding conversation history<\/data>      <data key=\"d6\">bd4eb9459bc29b4c2da4658914fd4635<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"86f77e15d41cbd0cb33f635ccb2cb66b","chunk":"EQBench\nMetric-v2 91.36(+4%) 88.03 87.75 88.67 88.95 93.32\nMetric-v1 50.28 (+28%) 38.8 39.27 42.13 42.05 55.98\nTable 3: Performance of Orca-3 and other baseline models on all the benchmarks. Note:\nGPT-3.5-turbo scores for GSM8K are taken from [ 1]. We show in (+x%) the relative\nimprovement over Mistral-7b-Instruct.\n\u2022AGIEval : AGIEval [ 39] is a human-centric benchmark that evaluates a model\u2019s\nabilities in tasks pertinent to human-cognition and problem-solving. It evaluates\nhow well models perform in answering questions from human-centric standardized\nexams such as SAT, LSAT and math competitions.\n\u2022MMLU : Massive Multitask Language Understanding (MMLU) [ 9] benchmark\nmeasures a model\u2019s multitask understanding. The benchmark includes approximately\n16000 multiple choice questions covering a wide range of 57 academic subjects such\nas maths, philosphy, medicine, psychology, computer-science, law etc. testing both\ngeneral and specialized knowledge of the model being tested.\n\u2022ARC: The AI2 Reasoning Challenge (ARC) [ 2] benchmark, developed by AllenAI,\nmeasures the reasoning, commonsense knowledge and deep comprehension abilities\nof language models. The test set contains 3548 multiple-choice questions that are\ndivided into 2 sets : Easy(2376) and Challenge(1172).\n\u2022BBH: Big Bench Hard [ 31] consists of a set of 23 tasks selected from the broader\nBig-Bench benchmark spanning a wide array of academic subjects requiring complex,\nmulti-step reasoning.\n\u2022GPQA: Graduate-level Google-Proof Q&A [ 27] is a challenging benchmark of 448\nhigh-quality and extremely difficult multiple-choice questions created by domain\nexperts(pursuing PhDs in their domains) in biology, chemistry and physics.\n\u2022DROP: Discrete Reasoning over Paragraphs [ 6] is a Reading Comprehension bench-\nmark requiring the models to resolve references in questions and perform discrete\noperations over them such as sorting, counting, addition etc.\n16\u2022GSM8K : Grade School Math 8K [ 3] is a dataset of high quality diverse grade\nschool math word problems. The test split of the dataset consists of 1.32K problems\nrequiring between 2 and 8 steps to solve primarily involving sequence of elementary\ncalculations using basic arithmetic operations.\n\u2022FoFo: Format Following [ 34] is a benchmark that evaluates a model\u2019s ability to\nfollow complex, domain-specific formats. The benchmark tests format following on\na diverse range of real-world formats and instructions from domains like Healthcare,\nFinance, Marketing etc. created using AI-Human collaboration.\n\u2022IFEval: Instruction-Following Evaluation [ 40] is a benchmark measuring a model\u2019s\nability to follow natural language instructions using a set of 500 prompts covering\n25 types of \u2019verifiable instructions\u2019 where each prompt can contain one or more of\nthese instructions.\n\u2022MT-Bench : MT-Bench [ 16] benchmark is specifically designed to assess the com-\npetence of chat assistants in multi-turn conversations using GPT-4 as the evaluator.\n\u2022AlpacaEval : AlpacaEval [ 14] is a benchmark specifically designed for chat-based\nlanguage models to assess their abilities in the context of instruction-following tasks.\nIt is a single-turn benchmark consisting of 805 instructions representative of user\ninteractions on Alpaca web demo.\n\u2022InFoBench : The InFoBench [ 25] benchmark evaluates models instruction fol-\nlowing capability using a new metric called Decomposed Requirements Following\nRatio(DRFR). DRFR breaks complex instructions down into simpler criteria and\nfacilitates analysis of an LLM\u2019s compliance to these decomposed tasks in detail.\nThe benchmark has 500 diverse instructions and 2250 decomposed questions across\nmultiple constraint categories.\n\u2022EQBench : This Emotional Intelligence benchmark [ 23] evaluates aspects of emo-\ntional intelligence in language models. It tests models capabilities to comprehend\nintricate emotions and social interactions by providing a conversation between char-\nacters and then asking the model to predict intensity of emotional states of those\ncharacters. The authors discovered a strong correlation (r=0.97) between EQ-Bench\nand comprehensive multi-domain benchmarks like MMLU.\nThe results for all the baselines on each benchmark are given in table 3. All of the evaluations\nfor Orca-3 and other baselines was done in a zero-shot setting unless mentioned otherwise in\nthe text.\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified in Appendix B.\n4.3 Evaluation: Reading Comprehension\nReading comprehension is a crucial capability for LLMs. It is arguably even more important\nfor Small Language Models (SLMs), as they are better suited as reasoning engines than mere\nretrieval systems. Through targeted training with AgentInstruct, we observe substantial\nimprovement in Mistral\u2019s reading comprehension capabilities (Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math","chunk_id":"86f77e15d41cbd0cb33f635ccb2cb66b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"EQBENCH","type":"BENCHMARK","description":"EQBench is an Emotional Intelligence benchmark that evaluates aspects of emotional intelligence in language models by testing their capabilities to comprehend intricate emotions and social interactions","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"METRIC-V2","type":"METRIC","description":"Metric-v2 is a performance metric used to evaluate models, showing a 4% improvement over previous metrics","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"METRIC-V1","type":"METRIC","description":"Metric-v1 is a performance metric used to evaluate models, showing a 28% improvement over previous metrics","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a model evaluated on various benchmarks, showing performance improvements over other baseline models","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a model whose scores for GSM8K are referenced in the text","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GSM8K","type":"BENCHMARK","description":"Grade School Math 8K (GSM8K) is a dataset of high-quality diverse grade school math word problems requiring between 2 and 8 steps to solve","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a human-centric benchmark that evaluates a model\u2019s abilities in tasks pertinent to human cognition and problem-solving, including standardized exams like SAT and LSAT","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MMLU","type":"BENCHMARK","description":"Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model\u2019s multitask understanding across 57 academic subjects with approximately 16000 multiple-choice questions","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ARC","type":"BENCHMARK","description":"The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI to measure reasoning, commonsense knowledge, and deep comprehension abilities of language models","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"BBH","type":"BENCHMARK","description":"Big Bench Hard (BBH) is a set of 23 tasks from the broader Big-Bench benchmark requiring complex, multi-step reasoning","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GPQA","type":"BENCHMARK","description":"Graduate-level Google-Proof Q&A (GPQA) is a challenging benchmark of 448 high-quality multiple-choice questions created by domain experts in biology, chemistry, and physics","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"DROP","type":"BENCHMARK","description":"Discrete Reasoning over Paragraphs (DROP) is a Reading Comprehension benchmark requiring models to resolve references in questions and perform discrete operations like sorting and counting","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"FOFO","type":"BENCHMARK","description":"Format Following (FoFo) is a benchmark that evaluates a model\u2019s ability to follow complex, domain-specific formats across various real-world domains","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"IFEVAL","type":"BENCHMARK","description":"Instruction-Following Evaluation (IFEval) is a benchmark measuring a model\u2019s ability to follow natural language instructions using a set of 500 prompts covering 25 types of verifiable instructions","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark designed to assess the competence of chat assistants in multi-turn conversations using GPT-4 as the evaluator","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark for chat-based language models to assess their abilities in instruction-following tasks, consisting of 805 instructions","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InFoBench is a benchmark that evaluates models' instruction-following capability using the Decomposed Requirements Following Ratio (DRFR) metric","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7b-Instruct is a model used as a baseline for comparison in various benchmarks","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"LSAT","type":"EXAM","description":"The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its difficulty","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is a targeted training method used to improve reading comprehension capabilities in models","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"APPENDIX E","type":"DOCUMENT","description":"Appendix E contains more details about the baselines used in the evaluations","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"APPENDIX B","type":"DOCUMENT","description":"Appendix B specifies the types of tasks\/benchmarks and the corresponding method used to extract answers and generate metrics","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ORCA 2.5","type":"MODEL","description":"Orca 2.5 is a model used as a baseline for comparison in reading comprehension capabilities","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a model used as a benchmark for evaluating the performance of a 7B model in reading comprehension sections of the LSAT","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"ALLENAI","type":"ORGANIZATION","description":"AllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"SAT","type":"EXAM","description":"The SAT is a standardized test used for college admissions, included in the AGIEval benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MATH COMPETITIONS","type":"EXAM","description":"Math competitions are included in the AGIEval benchmark to evaluate problem-solving abilities","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"PHILOSOPHY","type":"SUBJECT","description":"Philosophy is one of the 57 academic subjects covered in the MMLU benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MEDICINE","type":"SUBJECT","description":"Medicine is one of the 57 academic subjects covered in the MMLU benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"PSYCHOLOGY","type":"SUBJECT","description":"Psychology is one of the 57 academic subjects covered in the MMLU benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"COMPUTER-SCIENCE","type":"SUBJECT","description":"Computer Science is one of the 57 academic subjects covered in the MMLU benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"LAW","type":"SUBJECT","description":"Law is one of the 57 academic subjects covered in the MMLU benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"BIOLOGY","type":"SUBJECT","description":"Biology is one of the subjects in the GPQA benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"CHEMISTRY","type":"SUBJECT","description":"Chemistry is one of the subjects in the GPQA benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"PHYSICS","type":"SUBJECT","description":"Physics is one of the subjects in the GPQA benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"HEALTHCARE","type":"DOMAIN","description":"Healthcare is one of the domains tested in the FoFo benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"FINANCE","type":"DOMAIN","description":"Finance is one of the domains tested in the FoFo benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"MARKETING","type":"DOMAIN","description":"Marketing is one of the domains tested in the FoFo benchmark","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"DRFR","type":"METRIC","description":"Decomposed Requirements Following Ratio (DRFR) is a metric used in the InFoBench benchmark to evaluate instruction-following capability","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"},{"name":"TABLE 3","type":"","description":"","source_id":"86f77e15d41cbd0cb33f635ccb2cb66b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"EQBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">EQBench is an Emotional Intelligence benchmark that evaluates aspects of emotional intelligence in language models by testing their capabilities to comprehend intricate emotions and social interactions<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"METRIC-V2\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Metric-v2 is a performance metric used to evaluate models, showing a 4% improvement over previous metrics<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"METRIC-V1\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Metric-v1 is a performance metric used to evaluate models, showing a 28% improvement over previous metrics<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a model evaluated on various benchmarks, showing performance improvements over other baseline models<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a model whose scores for GSM8K are referenced in the text<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Grade School Math 8K (GSM8K) is a dataset of high-quality diverse grade school math word problems requiring between 2 and 8 steps to solve<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a human-centric benchmark that evaluates a model&#8217;s abilities in tasks pertinent to human cognition and problem-solving, including standardized exams like SAT and LSAT<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Massive Multitask Language Understanding (MMLU) is a benchmark that measures a model&#8217;s multitask understanding across 57 academic subjects with approximately 16000 multiple-choice questions<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ARC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">The AI2 Reasoning Challenge (ARC) is a benchmark developed by AllenAI to measure reasoning, commonsense knowledge, and deep comprehension abilities of language models<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"BBH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Big Bench Hard (BBH) is a set of 23 tasks from the broader Big-Bench benchmark requiring complex, multi-step reasoning<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GPQA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Graduate-level Google-Proof Q&amp;A (GPQA) is a challenging benchmark of 448 high-quality multiple-choice questions created by domain experts in biology, chemistry, and physics<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Discrete Reasoning over Paragraphs (DROP) is a Reading Comprehension benchmark requiring models to resolve references in questions and perform discrete operations like sorting and counting<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Format Following (FoFo) is a benchmark that evaluates a model&#8217;s ability to follow complex, domain-specific formats across various real-world domains<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Instruction-Following Evaluation (IFEval) is a benchmark measuring a model&#8217;s ability to follow natural language instructions using a set of 500 prompts covering 25 types of verifiable instructions<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark designed to assess the competence of chat assistants in multi-turn conversations using GPT-4 as the evaluator<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark for chat-based language models to assess their abilities in instruction-following tasks, consisting of 805 instructions<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InFoBench is a benchmark that evaluates models' instruction-following capability using the Decomposed Requirements Following Ratio (DRFR) metric<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7b-Instruct is a model used as a baseline for comparison in various benchmarks<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"LSAT\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Test (LSAT) is a standardized test used for law school admissions, known for its difficulty<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is a targeted training method used to improve reading comprehension capabilities in models<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"APPENDIX E\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Appendix E contains more details about the baselines used in the evaluations<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"APPENDIX B\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Appendix B specifies the types of tasks\/benchmarks and the corresponding method used to extract answers and generate metrics<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ORCA 2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca 2.5 is a model used as a baseline for comparison in reading comprehension capabilities<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a model used as a benchmark for evaluating the performance of a 7B model in reading comprehension sections of the LSAT<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"ALLENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">AllenAI is the organization that developed the AI2 Reasoning Challenge (ARC) benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"SAT\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The SAT is a standardized test used for college admissions, included in the AGIEval benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MATH COMPETITIONS\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">Math competitions are included in the AGIEval benchmark to evaluate problem-solving abilities<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"PHILOSOPHY\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Philosophy is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MEDICINE\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Medicine is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"PSYCHOLOGY\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Psychology is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"COMPUTER-SCIENCE\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Computer Science is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"LAW\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Law is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"BIOLOGY\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Biology is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"CHEMISTRY\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Chemistry is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"PHYSICS\">      <data key=\"d0\">SUBJECT<\/data>      <data key=\"d1\">Physics is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"HEALTHCARE\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Healthcare is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"FINANCE\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Finance is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"MARKETING\">      <data key=\"d0\">DOMAIN<\/data>      <data key=\"d1\">Marketing is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"DRFR\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Decomposed Requirements Following Ratio (DRFR) is a metric used in the InFoBench benchmark to evaluate instruction-following capability<\/data>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <node id=\"TABLE 3\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/node>    <edge source=\"EQBENCH\" target=\"MMLU\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">EQBench has a strong correlation (r=0.97) with comprehensive multi-domain benchmarks like MMLU<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 shows performance improvements over Mistral-7b-Instruct in various benchmarks<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GSM8K\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Orca-3's performance on GSM8K is compared with other models<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AgentInstruct is used to improve Orca-3's reading comprehension capabilities<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"TABLE 3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Table 3 shows the performance of Orca-3 on various benchmarks<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"APPENDIX E\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Appendix E contains more details about the baselines used in the evaluations of Orca-3<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"APPENDIX B\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Appendix B specifies the types of tasks\/benchmarks and the corresponding method used to extract answers and generate metrics for Orca-3<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"ORCA 2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3 shows an 18% improvement over Orca 2.5 in reading comprehension capabilities<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ORCA-3\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3's performance in reading comprehension is elevated to match that of GPT-4<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"GSM8K\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">GPT-3.5-turbo's scores for GSM8K are referenced<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"SAT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AGIEval includes the SAT as part of its human-centric benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"LSAT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AGIEval includes the LSAT as part of its human-centric benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"AGIEVAL\" target=\"MATH COMPETITIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AGIEval includes math competitions as part of its human-centric benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"PHILOSOPHY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Philosophy is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"MEDICINE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Medicine is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"PSYCHOLOGY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Psychology is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"COMPUTER-SCIENCE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Computer Science is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"LAW\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Law is one of the 57 academic subjects covered in the MMLU benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"ARC\" target=\"ALLENAI\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">AllenAI developed the AI2 Reasoning Challenge (ARC) benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"BIOLOGY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Biology is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"CHEMISTRY\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Chemistry is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"GPQA\" target=\"PHYSICS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Physics is one of the subjects in the GPQA benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"HEALTHCARE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Healthcare is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"FINANCE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Finance is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"MARKETING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Marketing is one of the domains tested in the FoFo benchmark<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"DRFR\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">InFoBench uses the Decomposed Requirements Following Ratio (DRFR) metric to evaluate instruction-following capability<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AgentInstruct is used to improve Mistral-7b-Instruct's reading comprehension capabilities<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"TABLE 3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Table 3 shows the performance of Mistral-7b-Instruct on various benchmarks<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>    <edge source=\"LSAT\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">AgentInstruct helps models perform well on the reading comprehension sections of the LSAT<\/data>      <data key=\"d5\">86f77e15d41cbd0cb33f635ccb2cb66b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"bb87f82e6a9f1d4da6480ec78a0e3701","chunk":"Table ??)\u2014showcasing an 18%\nimprovement over Orca 2.5 and a 21% gain relative to Mistral-Instruct-7b. Furthermore, by\nleveraging this data-driven approach, we have elevated the performance of a 7B model to\nmatch that of GPT-4 on the reading comprehension sections of the Law School Admission\nTests (LSATs), which are considered difficult for human test-takers.\n4.4 Evaluation: Math\nAssessing the reasoning capabilities of AI models can be effectively accomplished through\nmath problem solving. While SLMs have shown considerable improvement in elementary\nmath, their performance typically falters with more complex high school and college-level\nmathematics. Math problems are generated by the Open Domain Question Answering\nand Multiple-Choice Questions Flows. With AgentInstruct, we have managed to enhance\nMistral\u2019s proficiency across a spectrum of difficulties, ranging from elementary to college-level\nmath (Table 5. This has led to a signficant performance boost, with improvements ranging\n17Model Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval lsat-rc75.84\n(+21%,+20%)62.45 63.2 63.57 72.86\nAGIEval sat-en87.38\n(+13%,+15%)77.18 75.73 82.04 82.52\nAGIEval\ngaokao-english87.25\n(+13%,+17%)77.45 74.84 83.01 87.25\nAGIEval lsat-lr63.14\n(+45%,+36%)43.53 46.27 54.9 68.82\nDROP71.14\n(+9%,+22%)65.19 58.12 67.15 67.36\nAverage76.95\n(+18%,+21%)65.16 63.63 70.13 75.76\nTable 4: Performance of models on reading comprehension based sub-tasks and benchmarks.\nThe figures (x%, y%) adjacent to the Orca-3 results signify the percentage of improvement\ncompared to Orca 2.5 and Mistral-7B-Instruct, respectively.\nModel Orca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructGPT-3.5-\nturboGPT-4\nAGIEval math42.90\n(+73%,+168%)24.8 16.0 38.0 57.9\nAGIEval sat-math80.91\n(+34%,+50%)60.45 54.09 67.73 90.0\nBBH multistep\n-arithmetic-two66.80\n(+1418%,+882%)4.4 6.8 46.4 77.2\nMMLU abstract\nalgebra55.00\n(+129%,+104%)24.0 27.0 47.0 70.0\nMMLU college\nmathematics44.00\n(+63%,+44%)30.0 34.0 39.0 62.0\nMMLU high-school\nmathematics66.67\n(+41%,+94%)47.41 34.44 57.04 66.67\nGSM8K83.09\n(+12%,+54%)74.3 54.06 78.1\u221786.88\nTable 5: Performance scores of models on Math benchmarks. Note: GPT-3.5-turbo accuracy\nscores reported for GSM8K are taken from Phi3 paper[ 1]. The figures (x%, y%) adjacent\nto the Orca-3 results signify the percentage of improvement compared to Orca 2.5 and\nMistral-7B-Instruct, respectively.\nfrom 44% to 168% on various popular mathematical benchmarks. It should be emphasized\nthat the objective of Generative Teaching is to teach a skill than generating data to meet a\nspecific benchmark. The effectiveness of AgentInstruct for Generative Teaching is evidenced\nby marked enhancements across a variety of mathematical datasets.\n4.5 Evaluation: Format Following\nFollowing formatting guidelines is essential for language models to be applicable in real-world\nsituations. In all AgentInstruct flows, we ensure that format-following is taught for each\nparticular scenario, by synthesizing, through agents, several formatting guidelines. By doing\nso, we are able to significantly improve (11.5%) Mistral\u2019s ability to follow formats, surpassing\neven the capabilities of Gemini Pro.\n18Model FoFo\nOpen-sourceOrca-3-7B 84.01 (+26.92%,+11.5%)\nOrca-2.5-7B 66.19\nMistral-7B-Instruct 75.3\nClosed-sourceGPT-3.5-turbo 76.92\nGemini Pro 80.25\u2217\nGPT-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive","chunk_id":"bb87f82e6a9f1d4da6480ec78a0e3701","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA 2.5","type":"TECHNOLOGY","description":"Orca 2.5 is a model used as a baseline for performance comparison in various benchmarks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"MISTRAL-INSTRUCT-7B","type":"TECHNOLOGY","description":"Mistral-Instruct-7B is a model used as a baseline for performance comparison in various benchmarks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a model used as a baseline for performance comparison in various benchmarks, known for its high performance in reading comprehension and math","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"LSAT","type":"EXAM","description":"The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty in reading comprehension sections","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"EXAM"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is a system that enhances the proficiency of models like Mistral across various difficulties in math, from elementary to college-level","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"OPEN DOMAIN QUESTION ANSWERING","type":"TECHNOLOGY","description":"Open Domain Question Answering is a method used to generate math problems for assessing AI models","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"MULTIPLE-CHOICE QUESTIONS FLOWS","type":"TECHNOLOGY","description":"Multiple-Choice Questions Flows is a method used to generate math problems for assessing AI models","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"ORCA-3-7B","type":"TECHNOLOGY","description":"Orca-3-7B is a model that has shown significant improvements over Orca 2.5 and Mistral-7B-Instruct in various benchmarks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"GPT-3.5-TURBO","type":"TECHNOLOGY","description":"GPT-3.5-turbo is a model used as a baseline for performance comparison in various benchmarks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"AGIEVAL","type":"BENCHMARK","description":"AGIEval is a benchmark used to evaluate the performance of models on various tasks, including reading comprehension and math","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"DROOP","type":"BENCHMARK","description":"DROOP is a benchmark used to evaluate the performance of models on reading comprehension tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"BBH MULTISTEP-ARITHMETIC-TWO","type":"BENCHMARK","description":"BBH multistep-arithmetic-two is a benchmark used to evaluate the performance of models on multi-step arithmetic problems","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"MMLU","type":"BENCHMARK","description":"MMLU is a benchmark used to evaluate the performance of models on various mathematical tasks, including abstract algebra and college-level mathematics","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"GSM8K","type":"BENCHMARK","description":"GSM8K is a benchmark used to evaluate the performance of models on math problems","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"GENERATIVE TEACHING","type":"TECHNOLOGY","description":"Generative Teaching is a method aimed at teaching skills rather than generating data to meet specific benchmarks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"FORMAT FOLLOWING","type":"TECHNOLOGY","description":"Format Following is a capability of language models to adhere to specific formatting guidelines, essential for real-world applications","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"GEMINI PRO","type":"TECHNOLOGY","description":"Gemini Pro is a model used as a baseline for performance comparison in format-following tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"TECHNOLOGY"},{"name":"FOFO","type":"BENCHMARK","description":"FoFo is a benchmark used to evaluate the performance of models on format-following tasks","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701","entity_type":"BENCHMARK"},{"name":"LSAT-RC","type":"BENCHMARK","description":"LSAT-RC is a reading comprehension sub-task of the LSAT used to evaluate model performance","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"SAT-EN","type":"BENCHMARK","description":"SAT-EN is an English sub-task of the SAT used to evaluate model performance","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"GAOKAO-ENGLISH","type":"BENCHMARK","description":"Gaokao-English is an English sub-task of the Gaokao used to evaluate model performance","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"LSAT-LR","type":"BENCHMARK","description":"LSAT-LR is a logical reasoning sub-task of the LSAT used to evaluate model performance","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"SAT-MATH","type":"BENCHMARK","description":"SAT-Math is a math sub-task of the SAT used to evaluate model performance","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"ABSTRACT ALGEBRA","type":"BENCHMARK","description":"Abstract Algebra is a sub-task of the MMLU used to evaluate model performance in abstract algebra","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"COLLEGE MATHEMATICS","type":"BENCHMARK","description":"College Mathematics is a sub-task of the MMLU used to evaluate model performance in college-level mathematics","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"HIGH-SCHOOL MATHEMATICS","type":"BENCHMARK","description":"High-School Mathematics is a sub-task of the MMLU used to evaluate model performance in high school-level mathematics","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"SAT","type":"","description":"","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"},{"name":"GAOKAO","type":"","description":"","source_id":"bb87f82e6a9f1d4da6480ec78a0e3701"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA 2.5\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca 2.5 is a model used as a baseline for performance comparison in various benchmarks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MISTRAL-INSTRUCT-7B\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Mistral-Instruct-7B is a model used as a baseline for performance comparison in various benchmarks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a model used as a baseline for performance comparison in various benchmarks, known for its high performance in reading comprehension and math<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LSAT\">      <data key=\"d0\">EXAM<\/data>      <data key=\"d1\">The Law School Admission Tests (LSATs) are standardized tests used for law school admissions, known for their difficulty in reading comprehension sections<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">EXAM<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is a system that enhances the proficiency of models like Mistral across various difficulties in math, from elementary to college-level<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Open Domain Question Answering is a method used to generate math problems for assessing AI models<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"MULTIPLE-CHOICE QUESTIONS FLOWS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Multiple-Choice Questions Flows is a method used to generate math problems for assessing AI models<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ORCA-3-7B\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca-3-7B is a model that has shown significant improvements over Orca 2.5 and Mistral-7B-Instruct in various benchmarks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-3.5-turbo is a model used as a baseline for performance comparison in various benchmarks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"AGIEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AGIEval is a benchmark used to evaluate the performance of models on various tasks, including reading comprehension and math<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"DROOP\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">DROOP is a benchmark used to evaluate the performance of models on reading comprehension tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"BBH MULTISTEP-ARITHMETIC-TWO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">BBH multistep-arithmetic-two is a benchmark used to evaluate the performance of models on multi-step arithmetic problems<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"MMLU\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MMLU is a benchmark used to evaluate the performance of models on various mathematical tasks, including abstract algebra and college-level mathematics<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">GSM8K is a benchmark used to evaluate the performance of models on math problems<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Generative Teaching is a method aimed at teaching skills rather than generating data to meet specific benchmarks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FORMAT FOLLOWING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Format Following is a capability of language models to adhere to specific formatting guidelines, essential for real-world applications<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GEMINI PRO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Gemini Pro is a model used as a baseline for performance comparison in format-following tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FoFo is a benchmark used to evaluate the performance of models on format-following tasks<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>      <data key=\"d3\">BENCHMARK<\/data>    <\/node>    <node id=\"LSAT-RC\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">LSAT-RC is a reading comprehension sub-task of the LSAT used to evaluate model performance<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"SAT-EN\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">SAT-EN is an English sub-task of the SAT used to evaluate model performance<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"GAOKAO-ENGLISH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Gaokao-English is an English sub-task of the Gaokao used to evaluate model performance<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"LSAT-LR\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">LSAT-LR is a logical reasoning sub-task of the LSAT used to evaluate model performance<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"SAT-MATH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">SAT-Math is a math sub-task of the SAT used to evaluate model performance<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"ABSTRACT ALGEBRA\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Abstract Algebra is a sub-task of the MMLU used to evaluate model performance in abstract algebra<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"COLLEGE MATHEMATICS\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">College Mathematics is a sub-task of the MMLU used to evaluate model performance in college-level mathematics<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"HIGH-SCHOOL MATHEMATICS\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">High-School Mathematics is a sub-task of the MMLU used to evaluate model performance in high school-level mathematics<\/data>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"SAT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <node id=\"GAOKAO\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/node>    <edge source=\"ORCA 2.5\" target=\"ORCA-3-7B\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Orca-3-7B has shown significant improvements over Orca 2.5 in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"ORCA-3-7B\">      <data key=\"d4\">18.0<\/data>      <data key=\"d5\">Orca-3-7B has shown significant improvements over Mistral-Instruct-7B in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">AgentInstruct has enhanced Mistral's proficiency across various difficulties in math<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MISTRAL-INSTRUCT-7B\" target=\"FORMAT FOLLOWING\">      <data key=\"d4\">16.0<\/data>      <data key=\"d5\">Format Following has significantly improved Mistral's ability to follow formats<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"ORCA-3-7B\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Orca-3-7B has been compared to GPT-4 in various benchmarks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"AGIEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">AGIEval is used to evaluate the performance of GPT-4 on various tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"DROOP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">DROOP is used to evaluate the performance of GPT-4 on reading comprehension tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"BBH MULTISTEP-ARITHMETIC-TWO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">BBH multistep-arithmetic-two is used to evaluate the performance of GPT-4 on multi-step arithmetic problems<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"MMLU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">MMLU is used to evaluate the performance of GPT-4 on various mathematical tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"GSM8K\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">GSM8K is used to evaluate the performance of GPT-4 on math problems<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"LSAT\" target=\"LSAT-RC\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LSAT-RC is a reading comprehension sub-task of the LSAT<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"LSAT\" target=\"LSAT-LR\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">LSAT-LR is a logical reasoning sub-task of the LSAT<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"OPEN DOMAIN QUESTION ANSWERING\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">AgentInstruct uses math problems generated by Open Domain Question Answering<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"MULTIPLE-CHOICE QUESTIONS FLOWS\">      <data key=\"d4\">12.0<\/data>      <data key=\"d5\">AgentInstruct uses math problems generated by Multiple-Choice Questions Flows<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"AGIEVAL\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">AGIEval is used to evaluate the performance of Orca-3-7B on various tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"DROOP\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">DROOP is used to evaluate the performance of Orca-3-7B on reading comprehension tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"BBH MULTISTEP-ARITHMETIC-TWO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">BBH multistep-arithmetic-two is used to evaluate the performance of Orca-3-7B on multi-step arithmetic problems<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MMLU\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">MMLU is used to evaluate the performance of Orca-3-7B on various mathematical tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"GSM8K\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">GSM8K is used to evaluate the performance of Orca-3-7B on math problems<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"FOFO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">FoFo is used to evaluate the performance of Orca-3-7B on format-following tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"ABSTRACT ALGEBRA\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Abstract Algebra is a sub-task of the MMLU<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"COLLEGE MATHEMATICS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">College Mathematics is a sub-task of the MMLU<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"MMLU\" target=\"HIGH-SCHOOL MATHEMATICS\">      <data key=\"d4\">1.0<\/data>      <data key=\"d5\">High-School Mathematics is a sub-task of the MMLU<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"FORMAT FOLLOWING\" target=\"GEMINI PRO\">      <data key=\"d4\">14.0<\/data>      <data key=\"d5\">Format Following has surpassed the capabilities of Gemini Pro<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GEMINI PRO\" target=\"FOFO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">FoFo is used to evaluate the performance of Gemini Pro on format-following tasks<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"SAT-EN\" target=\"SAT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">SAT-EN is an English sub-task of the SAT<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"GAOKAO-ENGLISH\" target=\"GAOKAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Gaokao-English is an English sub-task of the Gaokao<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>    <edge source=\"SAT-MATH\" target=\"SAT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">SAT-Math is a math sub-task of the SAT<\/data>      <data key=\"d6\">bb87f82e6a9f1d4da6480ec78a0e3701<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"8ee9617c145e19fa95f1f9349bfbe69b","chunk":"-4 87.45\nTable 6: Performance of Orca-3-7B model and other open and closed-source baselines on\nFoFo benchmark. The figures (x%, y%) adjacent to the Orca-3 results signify the percentage\nof improvement compared to Orca 2.5 and Mistral-7B-Instruct, respectively. Note: The\nscores for Gemini Pro are taken from the original paper [34]\n4.6 Evaluation: Abstractive Summarization\nSummarization is an important capability for Language Models, with many models achieving\nhigh quality summarization performance, yet struggling with hallucination. We assessed\nsummarization ability using two key metrics: hallucinations and quality. For this purpose,\nwe utilized GPT4 as our evaluator. The prompts utilized in these evaluations can be found\nin Appendix B. We used the following benchmarks for evaluating summarization abilities:\n\u2022ACI-Bench: The Ambient Clinical Intelligence Benchmark (ACI-Bench) [ 32] is a\ndataset designed for benchmarking automatic report generation from doctor-patient\nconversations. The test set comprises 120 data points.\n\u2022InstruSum: A dataset [ 15] for evaluating the generation capabilities LLMs for\ninstruction-controllable summarization. It consists of 100 datapoints.\n\u2022Orca-Sum: A newly created benchmark to evaluate LLMs\u2019 ability to follow summa-\nrization and grounded data transformation instructions. To construct this test set,\nwe sampled data from 45 summarization datasets collected from Hugging Face across\nmultiple domains such as news, conversations, science, health, social, e-mails, code,\netc. for a total of 458 datapoints. We randomly collected, up to 1000 datapoints\nwhich then we carefully deduplicated to avoid overlapping with the training set. We\nthen used GPT-4 to generate a set of 40 prompts for each dataset out of each we\nrandomly sampled one for each selected datapoint. The prompts are dataset-specific\nand focus on summarization, grounding, and data transformation. For instance, a\nprompt may ask the model to generate a TikTok video out of a scientific paper or\na legal contract from a Wikipedia page. This allows us to measure not only the\nquality of the response but also hallucination in a challenging scenario, as the model\nis forced to move between formats and domains.\nThe results are presented in Table 7. With the AgentInstruct approach, we successfully\nachieved a reduction in hallucinations by 31.34%, while attaining a quality level comparable\nto GPT4 (Teacher).\n4.7 Evaluation: RAG\nThe RAG (Retrieval Augmented Generation) skill significantly boosts the capacity of Lan-\nguage Models to generate informed, contextually precise responses, hence upgrading their\noverall performance and usefulness. It is arguably more effective to test the RAG proficiency\nof language models in areas where the models have limited knowledge. For this study, we\nselected MIRAGE[ 35], a benchmark that focuses on answering medical questions by referring\nto information retrieved from a medical corpus. Since the medical domain is not typically a\nprimary focus of the models evaluated in this study, MIRAGE provides an effective platform\nfor assessing their RAG capabilities. Additionally, AgentInstruct RAG data used generic,\nnon medical data seed, enabling us to test how well can the skill (RAG) be applied to new\ndomains.\n19ModelOrca-3\n-7BOrca-2.5\n-7BMistral-\n7B-InstructLLAMA3-\n8B-instructGPT-3.5-\nturboGPT-4\nHallucination Rate (%) - Smaller is better\nAll (micro) 21.09\n(-26.12%,\n-31.34%)28.55 30.72 34.22 21.13 15.07\nOrca-Sum 28.17 36.84 39.61 38.43 28.60 21.66\nInstruSum 9.00 12.00 17.00 25.00 12.00 1.00\nACI-Bench 4.20 10.83 8.30 25.83 1.70 1.70\nQuality Score (1-10) - Higher is better\nAll (micro) 9.14\n(+7.91%,\n+3.28%)8.47 8.85 9.00 8.69 9.08\nOrca-Sum 8.95 8.27 8.61 8.90 8.32 8.61\nInstruSum 9.17 8.30 9.16 9.21 9.27 9.31\nACI-Bench 9.72 9.39 9.48 9.23 9.60 9.70\nTable 7: Hallucination rates and quality scores evaluated by GPT4. The figures (x%, y%)\nadjacent to the Orca-3 results signify the percentage of improvement compared to Orca v2.5\nand Mistral-7B-Instruct, respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(","chunk_id":"8ee9617c145e19fa95f1f9349bfbe69b","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ORCA-3-7B","type":"MODEL","description":"Orca-3-7B is a language model evaluated on the FoFo benchmark, showing improvements over Orca 2.5 and Mistral-7B-Instruct","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"FOFO BENCHMARK","type":"BENCHMARK","description":"FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA 2.5","type":"MODEL","description":"Orca 2.5 is a previous version of the Orca language model, used as a baseline for comparison with Orca-3-7B","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MISTRAL-7B-INSTRUCT","type":"MODEL","description":"Mistral-7B-Instruct is a language model used as a baseline for comparison with Orca-3-7B","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"GEMINI PRO","type":"MODEL","description":"Gemini Pro is a language model whose scores are referenced from the original paper","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a language model used as an evaluator for summarization ability and other tasks","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a language model used for comparison in various evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ABSTRACTIVE SUMMARIZATION","type":"TASK","description":"Abstractive Summarization is a task for language models to generate summaries, evaluated using metrics like hallucinations and quality","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"HALLUCINATIONS","type":"METRIC","description":"Hallucinations is a metric used to assess the accuracy of generated summaries by language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"QUALITY","type":"METRIC","description":"Quality is a metric used to assess the overall quality of generated summaries by language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ACI-BENCH","type":"BENCHMARK","description":"ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"INSTRUSUM","type":"BENCHMARK","description":"InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA-SUM","type":"BENCHMARK","description":"Orca-Sum is a newly created benchmark to evaluate language models' ability to follow summarization and grounded data transformation instructions","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"AGENTINSTRUCT","type":"APPROACH","description":"AgentInstruct is an approach that successfully reduced hallucinations by 31.34% while attaining a quality level comparable to GPT-4","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"RAG","type":"SKILL","description":"RAG (Retrieval Augmented Generation) is a skill that boosts the capacity of language models to generate informed, contextually precise responses","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MIRAGE","type":"BENCHMARK","description":"MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MMLU-MED","type":"DATASET","description":"MMLU-Med is a dataset used in the MIRAGE benchmark for evaluating medical question answering","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MEDQA-US","type":"DATASET","description":"MedQA-US is a dataset used in the MIRAGE benchmark for evaluating medical question answering","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MEDMCQA","type":"DATASET","description":"MedMCQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"PUBMEDQA","type":"DATASET","description":"PubMedQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"BIOASQ","type":"DATASET","description":"BioASQ is a dataset used in the MIRAGE benchmark for evaluating medical question answering","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"LLAMA3-8B-INSTRUCT","type":"MODEL","description":"LLAMA3-8B-instruct is a language model used for comparison in various evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"MODEL","type":"MODEL","description":"Model is a general term referring to the various language models evaluated in the text","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"TABLE 6","type":"DOCUMENT","description":"Table 6 presents the performance of the Orca-3-7B model and other open and closed-source baselines on the FoFo benchmark","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"TABLE 7","type":"DOCUMENT","description":"Table 7 presents hallucination rates and quality scores evaluated by GPT-4 for various models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"APPENDIX B","type":"DOCUMENT","description":"Appendix B contains the prompts utilized in the evaluations of summarization abilities","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"APPENDIX E","type":"DOCUMENT","description":"Appendix E contains more details about the baselines used in the evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"EVALUATION","type":"TASK","description":"Evaluation is the process of assessing the performance of language models on various benchmarks and tasks","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"PERFORMANCE","type":"METRIC","description":"Performance is a general term referring to how well the language models perform on various benchmarks and tasks","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"IMPROVEMENT","type":"METRIC","description":"Improvement is a metric indicating the percentage of enhancement in performance compared to previous models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"HUGGING FACE","type":"ORGANIZATION","description":"Hugging Face is a platform from which data was sampled to create the Orca-Sum benchmark","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"DATASET","type":"DATASET","description":"Dataset is a general term referring to the various datasets used for evaluating language models","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"PROMPT","type":"TASK","description":"Prompt is a specific instruction given to a language model to generate a response","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"TIKTOK VIDEO","type":"DOCUMENT","description":"TikTok video is an example of a format that a language model may be asked to generate from a scientific paper","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"SCIENTIFIC PAPER","type":"DOCUMENT","description":"Scientific paper is an example of a source document that a language model may be asked to transform into another format","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"LEGAL CONTRACT","type":"DOCUMENT","description":"Legal contract is an example of a format that a language model may be asked to generate from a Wikipedia page","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"WIKIPEDIA PAGE","type":"DOCUMENT","description":"Wikipedia page is an example of a source document that a language model may be asked to transform into another format","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"CO-T","type":"TASK","description":"CoT (Chain of Thought) is a method used by GPT-4 for reasoning through complex problems","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"RAG DATA","type":"DATASET","description":"RAG data refers to the data used in the Retrieval Augmented Generation skill evaluations","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"NON-MEDICAL DATA","type":"DATASET","description":"Non-medical data is a type of data seed used in the AgentInstruct RAG evaluations to test the skill in new domains","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA-3","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"ORCA-2.5","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"},{"name":"TASK","type":"","description":"","source_id":"8ee9617c145e19fa95f1f9349bfbe69b"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ORCA-3-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3-7B is a language model evaluated on the FoFo benchmark, showing improvements over Orca 2.5 and Mistral-7B-Instruct<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"FOFO BENCHMARK\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FoFo benchmark is used to evaluate the performance of language models, including Orca-3-7B<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA 2.5\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca 2.5 is a previous version of the Orca language model, used as a baseline for comparison with Orca-3-7B<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct is a language model used as a baseline for comparison with Orca-3-7B<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"GEMINI PRO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Gemini Pro is a language model whose scores are referenced from the original paper<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a language model used as an evaluator for summarization ability and other tasks<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a language model used for comparison in various evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Abstractive Summarization is a task for language models to generate summaries, evaluated using metrics like hallucinations and quality<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"HALLUCINATIONS\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Hallucinations is a metric used to assess the accuracy of generated summaries by language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"QUALITY\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Quality is a metric used to assess the overall quality of generated summaries by language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ACI-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">ACI-Bench is a dataset designed for benchmarking automatic report generation from doctor-patient conversations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"INSTRUSUM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InstruSum is a dataset for evaluating the generation capabilities of language models for instruction-controllable summarization<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA-SUM\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">Orca-Sum is a newly created benchmark to evaluate language models' ability to follow summarization and grounded data transformation instructions<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">APPROACH<\/data>      <data key=\"d1\">AgentInstruct is an approach that successfully reduced hallucinations by 31.34% while attaining a quality level comparable to GPT-4<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">SKILL<\/data>      <data key=\"d1\">RAG (Retrieval Augmented Generation) is a skill that boosts the capacity of language models to generate informed, contextually precise responses<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MIRAGE\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MIRAGE is a benchmark focusing on answering medical questions by referring to information retrieved from a medical corpus<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MMLU-MED\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MMLU-Med is a dataset used in the MIRAGE benchmark for evaluating medical question answering<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MEDQA-US\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MedQA-US is a dataset used in the MIRAGE benchmark for evaluating medical question answering<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MEDMCQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MedMCQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"PUBMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">PubMedQA is a dataset used in the MIRAGE benchmark for evaluating medical question answering<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"BIOASQ\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">BioASQ is a dataset used in the MIRAGE benchmark for evaluating medical question answering<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"LLAMA3-8B-INSTRUCT\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">LLAMA3-8B-instruct is a language model used for comparison in various evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Model is a general term referring to the various language models evaluated in the text<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"TABLE 6\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Table 6 presents the performance of the Orca-3-7B model and other open and closed-source baselines on the FoFo benchmark<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"TABLE 7\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Table 7 presents hallucination rates and quality scores evaluated by GPT-4 for various models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"APPENDIX B\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Appendix B contains the prompts utilized in the evaluations of summarization abilities<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"APPENDIX E\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Appendix E contains more details about the baselines used in the evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"EVALUATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Evaluation is the process of assessing the performance of language models on various benchmarks and tasks<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"PERFORMANCE\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Performance is a general term referring to how well the language models perform on various benchmarks and tasks<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"IMPROVEMENT\">      <data key=\"d0\">METRIC<\/data>      <data key=\"d1\">Improvement is a metric indicating the percentage of enhancement in performance compared to previous models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"HUGGING FACE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Hugging Face is a platform from which data was sampled to create the Orca-Sum benchmark<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Dataset is a general term referring to the various datasets used for evaluating language models<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"PROMPT\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Prompt is a specific instruction given to a language model to generate a response<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"TIKTOK VIDEO\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">TikTok video is an example of a format that a language model may be asked to generate from a scientific paper<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"SCIENTIFIC PAPER\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Scientific paper is an example of a source document that a language model may be asked to transform into another format<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"LEGAL CONTRACT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Legal contract is an example of a format that a language model may be asked to generate from a Wikipedia page<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"WIKIPEDIA PAGE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Wikipedia page is an example of a source document that a language model may be asked to transform into another format<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"CO-T\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">CoT (Chain of Thought) is a method used by GPT-4 for reasoning through complex problems<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"RAG DATA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">RAG data refers to the data used in the Retrieval Augmented Generation skill evaluations<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"NON-MEDICAL DATA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">Non-medical data is a type of data seed used in the AgentInstruct RAG evaluations to test the skill in new domains<\/data>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"ORCA-2.5\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <node id=\"TASK\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/node>    <edge source=\"ORCA-3-7B\" target=\"FOFO BENCHMARK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B was evaluated on the FoFo benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"ORCA 2.5\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3-7B shows improvements over Orca 2.5<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MISTRAL-7B-INSTRUCT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-3-7B shows improvements over Mistral-7B-Instruct<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"TABLE 6\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B's performance is presented in Table 6<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"TABLE 7\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B's hallucination rates and quality scores are presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B was evaluated on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"PERFORMANCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B's performance was assessed on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"IMPROVEMENT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B showed improvements over Orca 2.5 and Mistral-7B-Instruct<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"FOFO BENCHMARK\" target=\"GEMINI PRO\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Scores for Gemini Pro on the FoFo benchmark are taken from the original paper<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"TABLE 6\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct's performance is presented in Table 6<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"TABLE 7\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct's hallucination rates and quality scores are presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct was evaluated on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"PERFORMANCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct's performance was assessed on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT\" target=\"IMPROVEMENT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct's performance was used as a baseline for improvement<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GEMINI PRO\" target=\"TABLE 6\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Gemini Pro's scores are referenced in Table 6<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GEMINI PRO\" target=\"EVALUATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Gemini Pro was evaluated on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GEMINI PRO\" target=\"PERFORMANCE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Gemini Pro's performance was assessed on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"ABSTRACTIVE SUMMARIZATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-4 was used as an evaluator for abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"TABLE 7\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 evaluated hallucination rates and quality scores presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"APPENDIX B\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-4 used prompts found in Appendix B for evaluations<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"APPENDIX E\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">GPT-4's evaluations reference baselines detailed in Appendix E<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"EVALUATION\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">GPT-4 was used as an evaluator in various tasks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"PROMPT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 used specific prompts for generating responses<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"CO-T\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 uses Chain of Thought (CoT) for reasoning through complex problems<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"TABLE 7\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-3.5-turbo's performance is presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-3.5-turbo was used for comparison in various evaluations<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"HALLUCINATIONS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Hallucinations is a metric used to assess the accuracy of abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"QUALITY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Quality is a metric used to assess the overall quality of abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"ACI-BENCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">ACI-Bench is used for evaluating abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"INSTRUSUM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">InstruSum is used for evaluating abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"ORCA-SUM\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca-Sum is used for evaluating abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ABSTRACTIVE SUMMARIZATION\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">AgentInstruct approach was used to reduce hallucinations in abstractive summarization<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ACI-BENCH\" target=\"DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">ACI-Bench is a dataset used for evaluating summarization abilities<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"INSTRUSUM\" target=\"DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">InstruSum is a dataset used for evaluating summarization abilities<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-SUM\" target=\"HUGGING FACE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Data for the Orca-Sum benchmark was sampled from Hugging Face<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"ORCA-SUM\" target=\"DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-Sum is a dataset used for evaluating summarization abilities<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"AGENTINSTRUCT\" target=\"NON-MEDICAL DATA\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">AgentInstruct RAG data used non-medical data seeds for evaluations<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"RAG\" target=\"MIRAGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">RAG skill was evaluated using the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"RAG\" target=\"RAG DATA\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">RAG data is used in the evaluations of the Retrieval Augmented Generation skill<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MMLU-MED\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MMLU-Med is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MEDQA-US\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MedQA-US is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"MEDMCQA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MedMCQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"PUBMEDQA\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">PubMedQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"BIOASQ\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">BioASQ is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MIRAGE\" target=\"DATASET\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">MIRAGE is a dataset used for evaluating RAG capabilities<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MMLU-MED\" target=\"DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MMLU-Med is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MEDQA-US\" target=\"DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MedQA-US is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"MEDMCQA\" target=\"DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MedMCQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PUBMEDQA\" target=\"DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">PubMedQA is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"BIOASQ\" target=\"DATASET\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">BioASQ is a dataset used in the MIRAGE benchmark<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"LLAMA3-8B-INSTRUCT\" target=\"TABLE 7\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LLAMA3-8B-instruct's performance is presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"LLAMA3-8B-INSTRUCT\" target=\"EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LLAMA3-8B-instruct was evaluated on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"LLAMA3-8B-INSTRUCT\" target=\"PERFORMANCE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">LLAMA3-8B-instruct's performance was assessed on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"TABLE 6\" target=\"ORCA-3\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3's performance is presented in Table 6<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"TABLE 6\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca 2.5's performance is presented in Table 6<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"TABLE 7\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca 2.5's hallucination rates and quality scores are presented in Table 7<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"EVALUATION\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca 2.5 was evaluated on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PERFORMANCE\" target=\"ORCA-2.5\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca 2.5's performance was assessed on various benchmarks<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"IMPROVEMENT\" target=\"ORCA-2.5\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Orca 2.5's performance was used as a baseline for improvement<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"TASK\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Prompts are used to instruct language models to generate responses<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"TIKTOK VIDEO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">A prompt may ask the model to generate a TikTok video from a scientific paper<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"SCIENTIFIC PAPER\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">A prompt may ask the model to generate a TikTok video from a scientific paper<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"LEGAL CONTRACT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">A prompt may ask the model to generate a legal contract from a Wikipedia page<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>    <edge source=\"PROMPT\" target=\"WIKIPEDIA PAGE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">A prompt may ask the model to generate a legal contract from a Wikipedia page<\/data>      <data key=\"d5\">8ee9617c145e19fa95f1f9349bfbe69b<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"ab04427ae0415a1c812a35cf8d3ee1a2","chunk":" respectively.\nMIRAGE Datasets\nMMLU-\nMedMedQA-\nUSMedMCQA PubMedQA BioASQ Avg.\nGPT-4\n(0613)CoT 89.44 83.97 69.88 39.6 84.3 73.44\nRAG87.24 82.8 66.65 70.6 92.56 79.97\nGPT-3.5-turbo\n(0613)CoT 72.91 65.04 55.25 36 74.27 60.69\nRAG75.48 66.61 58.04 67.4 90.29 71.57\nOrca-2.5-7BCoT 63.91 51.37 43.65 29.6 71.04 51.92\nRAG53.72 37.08 39.23 19 69.09 43.62\nMistral-7B-\nInstruct-v0.1CoT 50.96 42.73 34.9 27.6 47.57 40.75\nRAG54.64 35.35 43.41 30.2 68.77 46.47\nOrca-3-7BCoT 71.35 55.38 51.33 27.8 75.24 56.22\nRAG71.17\n(+30.25%)51.85\n(+46.68%)57.95\n(+33.49%)58.2\n(+92.71%)82.2\n(+19.52%)64.27\n(+38.30%)\nTable 8: Evaluation results of RAG skill on MIRAGE. The figures (x%) adjacent to the Orca-\n3 results signify the percentage of improvement compared to Mistral-7B-Instruct respectively.\nCoT shows the performance of the same models when answering directly without using RAG\nWe use the same retrieval mechanism across all models on MIRAGE [ 35], using MedRAG,\nthe corpus accompanying the benchmark. This involves using the same retrieval function\nand the same number of retrieved documents for all models. As all models are presented\nwith the same set of retrieved documents, the comparison accurately reflects the ability of\ndifferent models to incorporate retrieved results into their responses.\nTable 8 shows the results of all models on MIRAGE with and without leveraging RAG1.\nOverall, we observe that\n\u2022Models with a deeper understanding of the task (CoT scores) tend to have higher\nRAG scores. If we restrict our focus to only RAG performance, applying post-\ntraining, we\u2019ve managed to enhance Mistral\u2019s performance by an average of 38.30%.\n1Results of GPT-4 and GPT-3.5-Turbo are from [35].\n20\u2022Of the five datasets in MIRAGE, PubMedQA arguably offers the most effective\ntestbed for assessing models ability to do RAG. In PubMedQA, all models have\nlimited prior knowledge, and the retrieved context provides essential information, as\ndemonstrated by GPT4\u2019s performance leap. All Mistral fine-tunes exhibit similar\nperformance, but only Orca-3 (Mistral trained with AgentInstruct RAG flow data)\nshows a substantial improvement, resulting in a relative improvement of 92.71% over\nMistral-Instruct.\n5 Limitations\nAgentInstruct reduces human expertise required for data generation significantly and enables\ncreating of high-quality synthetic data at scale. However, this is till an early step in this\ndirection and could suffer from many limitations associated with synthetic data generation,\nincluding but not limited to:\nExtensibility: Creating the agentic flows for different skills depends on human effort for\nthe construction of the flows. Future work should consider how to automate the construction\nof the agentic flow from the user specification.\nAccuracy: Synthetic data may not perfectly replicate the complexity and nuances of real-\nworld data, leading to potential inaccuracies. Additional work is needed to better assess the\nquality of the data.\nCost: Generating synthetic data with multiple agents using LLMs and tools can be resource-\nintensive.\nBias:If the original seed data used to generate synthetic data contains biases, these biases\ncan be reflected and even amplified in the synthetic data.\nValidation : It can be difficult to validate synthetic data to ensure it accurately represents\nthe desired scenarios.\nDependency on Seed Data : The quality of synthetic data is dependent on the quality of\nthe real data used as seeds. Poor quality input data could result in poor quality synthetic\ndata.\nOrca-3 is fine-tuned with the AgentInstruct data based on the Mistral model family, and\nretains many of its limitations, as well as the common limitations of other large language\nmodels and limitations originating from its training process, including:\nData Biases: Large language models, trained on extensive data, can inadvertently carry\nbiases present in the source data. Consequently, the models may generate outputs that could\nbe potentially biased or unfair.\nLack of Transparency: Due to the complexity and size, large language models can act\nas \u201cblack boxes\u201d, making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from","chunk_id":"ab04427ae0415a1c812a35cf8d3ee1a2","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"MMLU","type":"DATASET","description":"MMLU is one of the datasets included in the MIRAGE Datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MEDMEDQA","type":"DATASET","description":"MedMedQA is one of the datasets included in the MIRAGE Datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"USMEDMCQA","type":"DATASET","description":"USMedMCQA is one of the datasets included in the MIRAGE Datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"PUBMEDQA","type":"DATASET","description":"PubMedQA is one of the datasets included in the MIRAGE Datasets and is considered an effective testbed for assessing models' ability to do RAG","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"BIOASQ","type":"DATASET","description":"BioASQ is one of the datasets included in the MIRAGE Datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"GPT-4","type":"MODEL","description":"GPT-4 is a model evaluated on the MIRAGE Datasets, showing high performance in both CoT and RAG tasks","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"GPT-3.5-TURBO","type":"MODEL","description":"GPT-3.5-turbo is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-2.5-7B","type":"MODEL","description":"Orca-2.5-7B is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MISTRAL-7B-INSTRUCT-V0.1","type":"MODEL","description":"Mistral-7B-Instruct-v0.1 is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"ORCA-3-7B","type":"MODEL","description":"Orca-3-7B is a model evaluated on the MIRAGE Datasets, showing significant improvement in RAG tasks compared to Mistral-7B-Instruct","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"RAG","type":"TECHNOLOGY","description":"RAG (Retrieval-Augmented Generation) is a technique used to enhance model performance by incorporating retrieved documents into their responses","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MEDRAG","type":"TECHNOLOGY","description":"MedRAG is the retrieval mechanism used across all models on the MIRAGE Datasets","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is a method that reduces human expertise required for data generation and enables the creation of high-quality synthetic data at scale","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"AZURE","type":"ORGANIZATION","description":"Azure is recommended for reviewing transparency notes related to large language models","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"MIRAGE DATASETS","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"FALDOR","type":"PERSON","description":"Faldor is an author who has worked on open-endedness and AI-GAs","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"LEHMAN","type":"PERSON","description":"Lehman is an author who has worked on open-endedness and AI-GAs","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"STANLEY","type":"PERSON","description":"Stanley is an author who has worked on open-endedness and AI-GAs","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"WANG","type":"PERSON","description":"Wang is an author who has worked on open-endedness and AI-GAs","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"},{"name":"COT","type":"","description":"","source_id":"ab04427ae0415a1c812a35cf8d3ee1a2"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"MMLU\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MMLU is one of the datasets included in the MIRAGE Datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MEDMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">MedMedQA is one of the datasets included in the MIRAGE Datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"USMEDMCQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">USMedMCQA is one of the datasets included in the MIRAGE Datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"PUBMEDQA\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">PubMedQA is one of the datasets included in the MIRAGE Datasets and is considered an effective testbed for assessing models' ability to do RAG<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"BIOASQ\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">BioASQ is one of the datasets included in the MIRAGE Datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-4 is a model evaluated on the MIRAGE Datasets, showing high performance in both CoT and RAG tasks<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"GPT-3.5-TURBO\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">GPT-3.5-turbo is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-2.5-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-2.5-7B is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MISTRAL-7B-INSTRUCT-V0.1\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Mistral-7B-Instruct-v0.1 is a model evaluated on the MIRAGE Datasets, showing performance in both CoT and RAG tasks<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"ORCA-3-7B\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3-7B is a model evaluated on the MIRAGE Datasets, showing significant improvement in RAG tasks compared to Mistral-7B-Instruct<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"RAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">RAG (Retrieval-Augmented Generation) is a technique used to enhance model performance by incorporating retrieved documents into their responses<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MEDRAG\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MedRAG is the retrieval mechanism used across all models on the MIRAGE Datasets<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is a method that reduces human expertise required for data generation and enables the creation of high-quality synthetic data at scale<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"AZURE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Azure is recommended for reviewing transparency notes related to large language models<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"MIRAGE DATASETS\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"FALDOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Faldor is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"LEHMAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lehman is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"STANLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Stanley is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is an author who has worked on open-endedness and AI-GAs<\/data>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <node id=\"COT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/node>    <edge source=\"MMLU\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">MMLU is part of the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MEDMEDQA\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">MedMedQA is part of the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"USMEDMCQA\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">USMedMCQA is part of the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"PUBMEDQA\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">PubMedQA is part of the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"BIOASQ\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">BioASQ is part of the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is evaluated on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"AZURE\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Azure is recommended for reviewing transparency notes related to GPT-4<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"COT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">CoT shows the performance of GPT-4 when answering directly without using RAG<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-3.5-turbo is evaluated on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"AZURE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Azure is recommended for reviewing transparency notes related to GPT-3.5-turbo<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"GPT-3.5-TURBO\" target=\"COT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">CoT shows the performance of GPT-3.5-turbo when answering directly without using RAG<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-2.5-7B\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-2.5-7B is evaluated on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-2.5-7B\" target=\"COT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">CoT shows the performance of Orca-2.5-7B when answering directly without using RAG<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT-V0.1\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Mistral-7B-Instruct-v0.1 is evaluated on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MISTRAL-7B-INSTRUCT-V0.1\" target=\"COT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">CoT shows the performance of Mistral-7B-Instruct-v0.1 when answering directly without using RAG<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is evaluated on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"AGENTINSTRUCT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Orca-3-7B is fine-tuned with the AgentInstruct data<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"ORCA-3-7B\" target=\"COT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">CoT shows the performance of Orca-3-7B when answering directly without using RAG<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"RAG\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">RAG is used to enhance model performance on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MEDRAG\" target=\"MIRAGE DATASETS\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">MedRAG is the retrieval mechanism used across all models on the MIRAGE Datasets<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"MIRAGE DATASETS\" target=\"META AGENT SEARCH\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses the MIRAGE Datasets to evaluate the performance of various models<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"LEHMAN\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Faldor and Lehman have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"STANLEY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Faldor and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"FALDOR\" target=\"WANG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Faldor and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"STANLEY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Lehman and Stanley have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"LEHMAN\" target=\"WANG\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Lehman and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>    <edge source=\"STANLEY\" target=\"WANG\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Stanley and Wang have both worked on open-endedness and AI-GAs<\/data>      <data key=\"d5\">ab04427ae0415a1c812a35cf8d3ee1a2<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"dd9a46950237e49ef9b1c7ef08e08d42","chunk":" making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information2.\nContent Harms: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from government and technology leaders around content harms\nfor AI technologies in future. We value and acknowledge the important role that research\nand open source community can play in this direction.\nHallucination: It is important to be aware and cautious not to entirely rely on a given\nlanguage model for critical decisions or information that might have deep impact as it is\nnot obvious how to prevent these models from fabricating content. Moreover, it is not clear\nwhether small models may be more susceptible to hallucination in ungrounded generation\nuse cases due to their smaller sizes and hence reduced memorization capacities. This is an\n2https:\/\/learn.microsoft.com\/en-us\/legal\/cognitive-services\/openai\/\ntransparency-note\n21active research topic and we hope there will be more rigorous measurement, understanding\nand mitigations around this topic.\nPotential for Misuse: Without suitable safeguards, there is a risk that these models could\nbe maliciously used for generating disinformation or harmful content.\nData Distribution: Orca-3\u2019s performance is likely to correlate strongly with the distribution\nof the tuning data. This correlation might limit its accuracy in areas underrepresented in\nthe training dataset.\n6 Conclusions\nThe AgentInstruct approach to Generative Teaching offers a promising solution to the\nchallenge of generating large amount of diverse and high-quality data for model post-training.\nThis method stands out by using agentic flows for synthetic data generation, thus addressing\nkey concerns associated with the use of synthetic data in model training, such as the lack of\ndiversity and the need for intensive human curation and intervention during the data creation\nprocess. By leveraging an agentic framework, AgentInstruct can generate tailored datasets\ncomprising both prompts and responses from unstructured data sources, facilitating the\npost-training of models and teaching them variety of skills. The efficacy of this approach is\nexemplifiedbythesubstantialimprovementobserved intheOrca-3 model, which, post-trained\nwith a 25M pair dataset generated by AgentInstruct, showcased a notable performance gain\nacross multiple benchmarks. We believe using agentic flows for creating synthetic data can\nshow significant value for all stages of model training, including pre-training, post-training\nand domain\/task specialization. The ability to use unstructured content to generate diverse\nand high-quality instruction data given any specifications could pave the way for creating\n(semi) automated pipelines using synthetic data for model customization (using domain\nspecific content as seeds) and continual improvement (generating higher quality data than\nthe base model with agentic flows).\nReferences\n[1]Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl,\nAlon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Qin Cai, Martin Cai, Caio\nC\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen,\nYen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,\nMatthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao,\nAmit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J.\nHewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,\nDongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\nYunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi\nLin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,\nBrandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin,\nMarko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied,\nAdil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla,\nXia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang,\nRachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report","chunk_id":"dd9a46950237e49ef9b1c7ef08e08d42","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"AZURE","type":"ORGANIZATION","description":"Azure is a cloud computing service created by Microsoft, which provides various services including transparency notes related to AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CONTENT HARMS","type":"CONCEPT","description":"Content harms refer to the various types of harmful content that large language models can generate, necessitating awareness and preventive actions","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CONTENT MODERATION SERVICES","type":"SERVICE","description":"Content moderation services are provided by different companies and institutions to help prevent content harms caused by large language models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GOVERNMENT","type":"ORGANIZATION","description":"Government entities are expected to provide better regulations and standards around content harms for AI technologies in the future","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"TECHNOLOGY LEADERS","type":"GROUP","description":"Technology leaders are expected to contribute to better regulations and standards around content harms for AI technologies in the future","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RESEARCH COMMUNITY","type":"GROUP","description":"The research community plays an important role in addressing content harms and improving AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"OPEN SOURCE COMMUNITY","type":"GROUP","description":"The open source community plays an important role in addressing content harms and improving AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HALLUCINATION","type":"CONCEPT","description":"Hallucination refers to the phenomenon where language models fabricate content, making it unreliable for critical decisions or information","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ORCA-3","type":"MODEL","description":"Orca-3 is a language model whose performance is likely to correlate strongly with the distribution of the tuning data","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AGENTINSTRUCT","type":"TECHNOLOGY","description":"AgentInstruct is an approach to Generative Teaching that uses agentic flows for synthetic data generation to improve model post-training","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SYNTHETIC DATA","type":"DATA","description":"Synthetic data is artificially generated data used in model training, which can lack diversity and require intensive human curation","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AGENTIC FLOWS","type":"TECHNOLOGY","description":"Agentic flows are used in AgentInstruct to generate tailored datasets from unstructured data sources for model post-training","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"25M PAIR DATASET","type":"DATA","description":"A 25M pair dataset generated by AgentInstruct was used to post-train the Orca-3 model, resulting in a notable performance gain","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MARAH ABDIN","type":"PERSON","description":"Marah Abdin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SAM ADE JACOBS","type":"PERSON","description":"Sam Ade Jacobs is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMMAR AHMAD AWAN","type":"PERSON","description":"Ammar Ahmad Awan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JYOTI ANEJA","type":"PERSON","description":"Jyoti Aneja is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HANY AWADALLA","type":"PERSON","description":"Hany Awadalla is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NGUYEN BACH","type":"PERSON","description":"Nguyen Bach is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIT BAHREE","type":"PERSON","description":"Amit Bahree is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ARASH BAKHTIARI","type":"PERSON","description":"Arash Bakhtiari is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JIANMIN BAO","type":"PERSON","description":"Jianmin Bao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HARKIRAT BEHL","type":"PERSON","description":"Harkirat Behl is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ALON BENHAIM","type":"PERSON","description":"Alon Benhaim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MISHA BILENKO","type":"PERSON","description":"Misha Bilenko is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JOHAN BJORCK","type":"PERSON","description":"Johan Bjorck is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"S\u00c9BASTIEN BUBECK","type":"PERSON","description":"S\u00e9bastien Bubeck is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"QIN CAI","type":"PERSON","description":"Qin Cai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MARTIN CAI","type":"PERSON","description":"Martin Cai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CAIO C\u00c9SAR TEODORO MENDES","type":"PERSON","description":"Caio C\u00e9sar Teodoro Mendes is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"VISHRAV CHAUDHARY","type":"PERSON","description":"Vishrav Chaudhary is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONG CHEN","type":"PERSON","description":"Dong Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONGDONG CHEN","type":"PERSON","description":"Dongdong Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YEN-CHUN CHEN","type":"PERSON","description":"Yen-Chun Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YI-LING CHEN","type":"PERSON","description":"Yi-Ling Chen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PARUL CHOPRA","type":"PERSON","description":"Parul Chopra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIYANG DAI","type":"PERSON","description":"Xiyang Dai is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ALLIE DEL GIORNO","type":"PERSON","description":"Allie Del Giorno is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GUSTAVO DE ROSA","type":"PERSON","description":"Gustavo de Rosa is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MATTHEW DIXON","type":"PERSON","description":"Matthew Dixon is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RONEN ELDAN","type":"PERSON","description":"Ronen Eldan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"VICTOR FRAGOSO","type":"PERSON","description":"Victor Fragoso is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DAN ITER","type":"PERSON","description":"Dan Iter is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MEI GAO","type":"PERSON","description":"Mei Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MIN GAO","type":"PERSON","description":"Min Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIT GARG","type":"PERSON","description":"Amit Garg is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ABHISHEK GOSWAMI","type":"PERSON","description":"Abhishek Goswami is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SURIYA GUNASEKAR","type":"PERSON","description":"Suriya Gunasekar is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"EMMAN HAIDER","type":"PERSON","description":"Emman Haider is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JUNHENG HAO","type":"PERSON","description":"Junheng Hao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RUSSELL J. HEWETT","type":"PERSON","description":"Russell J. Hewett is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JAMIE HUYNH","type":"PERSON","description":"Jamie Huynh is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MOJAN JAVAHERIPI","type":"PERSON","description":"Mojan Javaheripi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIN JIN","type":"PERSON","description":"Xin Jin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PIERO KAUFFMANN","type":"PERSON","description":"Piero Kauffmann is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NIKOS KARAMPATZIAKIS","type":"PERSON","description":"Nikos Karampatziakis is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DONGWOO KIM","type":"PERSON","description":"Dongwoo Kim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MAHOUD KHADEMI","type":"PERSON","description":"Mahoud Khademi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LEV KURILENKO","type":"PERSON","description":"Lev Kurilenko is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JAMES R. LEE","type":"PERSON","description":"James R. Lee is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YIN TAT LEE","type":"PERSON","description":"Yin Tat Lee is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YUANZHI LI","type":"PERSON","description":"Yuanzhi Li is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YUNSHENG LI","type":"PERSON","description":"Yunsheng Li is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHEN LIANG","type":"PERSON","description":"Chen Liang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LARS LIDEN","type":"PERSON","description":"Lars Liden is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CE LIU","type":"PERSON","description":"Ce Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MENGCHEN LIU","type":"PERSON","description":"Mengchen Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"WEISHUNG LIU","type":"PERSON","description":"Weishung Liu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ERIC LIN","type":"PERSON","description":"Eric Lin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ZEQI LIN","type":"PERSON","description":"Zeqi Lin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHONG LUO","type":"PERSON","description":"Chong Luo is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PIYUSH MADAN","type":"PERSON","description":"Piyush Madan is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MATT MAZZOLA","type":"PERSON","description":"Matt Mazzola is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HARDIK MODI","type":"PERSON","description":"Hardik Modi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ANH NGUYEN","type":"PERSON","description":"Anh Nguyen is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BRANDON NORICK","type":"PERSON","description":"Brandon Norick is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BARUN PATRA","type":"PERSON","description":"Barun Patra is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DANIEL PEREZ-BECKER","type":"PERSON","description":"Daniel Perez-Becker is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"THOMAS PORTET","type":"PERSON","description":"Thomas Portet is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"REID PRYZANT","type":"PERSON","description":"Reid Pryzant is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HEYANG QIN","type":"PERSON","description":"Heyang Qin is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MARKO RADMILAC","type":"PERSON","description":"Marko Radmilac is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SAMBUDHA ROY","type":"PERSON","description":"Sambudha Roy is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"OLATUNJI RUWASE","type":"PERSON","description":"Olatunji Ruwase is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"OLLI SAARIKIVI","type":"PERSON","description":"Olli Saarikivi is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AMIN SAIED","type":"PERSON","description":"Amin Saied is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ADIL SALIM","type":"PERSON","description":"Adil Salim is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MICHAEL SANTACROCE","type":"PERSON","description":"Michael Santacroce is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SHITAL SHAH","type":"PERSON","description":"Shital Shah is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"NING SHANG","type":"PERSON","description":"Ning Shang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HITESHI SHARMA","type":"PERSON","description":"Hiteshi Sharma is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SWADHEEN SHUKLA","type":"PERSON","description":"Swadheen Shukla is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIA SONG","type":"PERSON","description":"Xia Song is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MASAHIRO TANAKA","type":"PERSON","description":"Masahiro Tanaka is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"ANDREA TUPINI","type":"PERSON","description":"Andrea Tupini is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"XIN WANG","type":"PERSON","description":"Xin Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"LIJUAN WANG","type":"PERSON","description":"Lijuan Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CHUNYU WANG","type":"PERSON","description":"Chunyu Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"YU WANG","type":"PERSON","description":"Yu Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"RACHEL WARD","type":"PERSON","description":"Rachel Ward is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GUANHUA WANG","type":"PERSON","description":"Guanhua Wang is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PHILIPP WITTE","type":"PERSON","description":"Philipp Witte is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"HAIPING WU","type":"PERSON","description":"Haiping Wu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MICHAEL WYATT","type":"PERSON","description":"Michael Wyatt is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BIN XIAO","type":"PERSON","description":"Bin Xiao is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is one of the authors of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"JIAHANG XU","type":"PERSON","description":"","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"TRANSPARENCY NOTES","type":"DOCUMENT","description":"Transparency notes from Azure provide information about the rationale behind specific outputs or decisions made by AI models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"TECHNOLOGY"},{"name":"DISINFORMATION","type":"CONCEPT","description":"Disinformation refers to false information that is spread deliberately to deceive people, which can be generated by large language models without suitable safeguards","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"DATA DISTRIBUTION","type":"CONCEPT","description":"Data distribution refers to the way in which data is spread across different categories or areas, affecting the performance of models like Orca-3","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"GENERATIVE TEACHING","type":"CONCEPT","description":"Generative Teaching is an approach that uses methods like AgentInstruct to generate large amounts of diverse and high-quality data for model post-training","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"UNSTRUCTURED DATA","type":"DATA","description":"Unstructured data refers to information that does not have a pre-defined data model, which can be used by AgentInstruct to generate tailored datasets","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PHI-3 TECHNICAL REPORT","type":"DOCUMENT","description":"The Phi-3 technical report is a document authored by multiple researchers, detailing the technical aspects of the Phi-3 model","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MICROSOFT","type":"ORGANIZATION","description":"Microsoft is the company behind Azure, which provides various AI-related services including transparency notes","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"AI TECHNOLOGIES","type":"CONCEPT","description":"AI technologies refer to the various applications and systems that use artificial intelligence to perform tasks, which can be subject to content harms and hallucinations","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"BENCHMARKS","type":"CONCEPT","description":"Benchmarks are standards or points of reference against which things may be compared or assessed, used to measure the performance of models like Orca-3","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"REGULATIONS","type":"CONCEPT","description":"Regulations refer to rules or directives made and maintained by an authority, which are hoped to be improved by government and technology leaders for AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"STANDARDS","type":"CONCEPT","description":"Standards refer to a level of quality or attainment, which are hoped to be improved by government and technology leaders for AI technologies","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MITIGATIONS","type":"CONCEPT","description":"Mitigations refer to actions taken to reduce the severity, seriousness, or painfulness of something, such as the hallucination issue in AI models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MEASUREMENT","type":"CONCEPT","description":"Measurement refers to the process of obtaining the magnitude of a quantity, which is important for understanding and mitigating issues like hallucination in AI models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"POST-TRAINING","type":"CONCEPT","description":"Post-training refers to the process of further training a model after its initial training, often using additional data to improve its performance","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"PRE-TRAINING","type":"CONCEPT","description":"Pre-training refers to the initial phase of training a model on a large dataset before fine-tuning it for specific tasks","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"DOMAIN\/TASK SPECIALIZATION","type":"CONCEPT","description":"Domain\/task specialization refers to the process of customizing a model to perform well in specific areas or tasks","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"CONTINUAL IMPROVEMENT","type":"CONCEPT","description":"Continual improvement refers to the ongoing effort to improve products, services, or processes, such as using agentic flows to generate higher quality data for models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"SEMI-AUTOMATED PIPELINES","type":"TECHNOLOGY","description":"Semi-automated pipelines refer to partially automated processes that use synthetic data for model customization and continual improvement","source_id":"dd9a46950237e49ef9b1c7ef08e08d42"},{"name":"MODEL CUSTOMIZATION","type":"CONCEPT","description":"Model customization refers to the process of tailoring a model to meet specific requirements or perform specific tasks","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"DOMAIN SPECIFIC CONTENT","type":"DATA","description":"Domain specific content refers to information that is relevant to a particular field or area, used as seeds for generating synthetic data in model customization","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"CITATIONS","type":"DOCUMENT","description":"Citations refer to references to other works or documents, such as those listed in the references section of the Phi-3 technical report","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DOCUMENT"},{"name":"APPENDIX E","type":"DOCUMENT","description":"Appendix E is a section in a document that provides additional details, such as more information about baselines in the provided text","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DOCUMENT"},{"name":"ACTIVE RESEARCH TOPIC","type":"CONCEPT","description":"Active research topic refers to an area of study that is currently being investigated, such as the measurement, understanding, and mitigations of hallucination in AI models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"HARMFUL CONTENT","type":"CONCEPT","description":"Harmful content refers to information that can cause damage or distress, which can be generated by large language models without suitable safeguards","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"SAFEGUARDS","type":"CONCEPT","description":"Safeguards refer to measures taken to protect against possible dangers or risks, such as the misuse of large language models to generate disinformation or harmful content","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"TUNING DATA","type":"DATA","description":"Tuning data refers to the dataset used to fine-tune a model, which can affect its performance and accuracy","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"PERFORMANCE GAIN","type":"CONCEPT","description":"Performance gain refers to the improvement in a model's performance, such as the substantial improvement observed in the Orca-3 model post-trained with a 25M pair dataset","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"MULTIPLE BENCHMARKS","type":"CONCEPT","description":"Multiple benchmarks refer to various standards or points of reference used to measure the performance of models like Orca-3","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"UNSTRUCTURED DATA SOURCES","type":"DATA","description":"Unstructured data sources refer to information that does not have a pre-defined data model, which can be used by AgentInstruct to generate tailored datasets","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"PROMPTS","type":"DATA","description":"Prompts refer to the initial inputs or questions given to a model to generate responses, used in the datasets generated by AgentInstruct","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"RESPONSES","type":"DATA","description":"Responses refer to the outputs or answers generated by a model in reaction to prompts, used in the datasets generated by AgentInstruct","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"UNSTRUCTURED CONTENT","type":"DATA","description":"Unstructured content refers to information that does not have a pre-defined data model, which can be used to generate diverse and high-quality instruction data","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"INSTRUCTION DATA","type":"DATA","description":"Instruction data refers to the information used to teach or train a model, which can be generated from unstructured content using agentic flows","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"MODEL TRAINING","type":"CONCEPT","description":"Model training refers to the process of teaching a model to perform tasks by feeding it data and adjusting its parameters","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"DATA CREATION PROCESS","type":"CONCEPT","description":"Data creation process refers to the method of generating data for training models, which can involve human curation and intervention","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"HUMAN CURATION","type":"CONCEPT","description":"Human curation refers to the process of manually selecting and organizing data, which is often required in the data creation process for model training","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"INTERVENTION","type":"CONCEPT","description":"Intervention refers to the involvement of humans in the data creation process to ensure the quality and relevance of the data","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"DIVERSE ANSWERS","type":"DATA","description":"Diverse answers refer to a variety of responses generated by a model, which can help explore potential solutions and improve model performance","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"},{"name":"POTENTIAL SOLUTIONS","type":"CONCEPT","description":"Potential solutions refer to possible answers or methods for solving a problem, which can be explored using diverse answers generated by models","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"CONCEPT"},{"name":"BASE MODEL","type":"MODEL","description":"Base model refers to the initial version of a model before it undergoes further training or customization","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"MODEL"},{"name":"HIGHER QUALITY DATA","type":"DATA","description":"Higher quality data refers to information that is more accurate, relevant, and useful for training models, which can be generated using agentic flows","source_id":"dd9a46950237e49ef9b1c7ef08e08d42","entity_type":"DATA"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"AZURE\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Azure is a cloud computing service created by Microsoft, which provides various services including transparency notes related to AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CONTENT HARMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Content harms refer to the various types of harmful content that large language models can generate, necessitating awareness and preventive actions<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CONTENT MODERATION SERVICES\">      <data key=\"d0\">SERVICE<\/data>      <data key=\"d1\">Content moderation services are provided by different companies and institutions to help prevent content harms caused by large language models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GOVERNMENT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Government entities are expected to provide better regulations and standards around content harms for AI technologies in the future<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"TECHNOLOGY LEADERS\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">Technology leaders are expected to contribute to better regulations and standards around content harms for AI technologies in the future<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RESEARCH COMMUNITY\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">The research community plays an important role in addressing content harms and improving AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"OPEN SOURCE COMMUNITY\">      <data key=\"d0\">GROUP<\/data>      <data key=\"d1\">The open source community plays an important role in addressing content harms and improving AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HALLUCINATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Hallucination refers to the phenomenon where language models fabricate content, making it unreliable for critical decisions or information<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ORCA-3\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Orca-3 is a language model whose performance is likely to correlate strongly with the distribution of the tuning data<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AGENTINSTRUCT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">AgentInstruct is an approach to Generative Teaching that uses agentic flows for synthetic data generation to improve model post-training<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SYNTHETIC DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Synthetic data is artificially generated data used in model training, which can lack diversity and require intensive human curation<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AGENTIC FLOWS\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Agentic flows are used in AgentInstruct to generate tailored datasets from unstructured data sources for model post-training<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"25M PAIR DATASET\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">A 25M pair dataset generated by AgentInstruct was used to post-train the Orca-3 model, resulting in a notable performance gain<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MARAH ABDIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marah Abdin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SAM ADE JACOBS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sam Ade Jacobs is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMMAR AHMAD AWAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ammar Ahmad Awan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JYOTI ANEJA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jyoti Aneja is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HANY AWADALLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hany Awadalla is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NGUYEN BACH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nguyen Bach is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIT BAHREE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Bahree is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ARASH BAKHTIARI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arash Bakhtiari is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JIANMIN BAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianmin Bao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HARKIRAT BEHL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harkirat Behl is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ALON BENHAIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alon Benhaim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MISHA BILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Misha Bilenko is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JOHAN BJORCK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Johan Bjorck is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"S&#201;BASTIEN BUBECK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">S&#233;bastien Bubeck is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"QIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qin Cai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MARTIN CAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Martin Cai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CAIO C&#201;SAR TEODORO MENDES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caio C&#233;sar Teodoro Mendes is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"VISHRAV CHAUDHARY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vishrav Chaudhary is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dong Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONGDONG CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongdong Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YEN-CHUN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yen-Chun Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YI-LING CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi-Ling Chen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PARUL CHOPRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Parul Chopra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIYANG DAI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiyang Dai is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ALLIE DEL GIORNO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Allie Del Giorno is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GUSTAVO DE ROSA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gustavo de Rosa is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MATTHEW DIXON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Dixon is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RONEN ELDAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ronen Eldan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"VICTOR FRAGOSO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Fragoso is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DAN ITER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Iter is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MEI GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mei Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MIN GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Min Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIT GARG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amit Garg is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ABHISHEK GOSWAMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abhishek Goswami is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SURIYA GUNASEKAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Suriya Gunasekar is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"EMMAN HAIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Emman Haider is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JUNHENG HAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Junheng Hao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RUSSELL J. HEWETT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Russell J. Hewett is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JAMIE HUYNH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jamie Huynh is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MOJAN JAVAHERIPI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mojan Javaheripi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIN JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Jin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PIERO KAUFFMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piero Kauffmann is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NIKOS KARAMPATZIAKIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nikos Karampatziakis is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DONGWOO KIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dongwoo Kim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MAHOUD KHADEMI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mahoud Khademi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LEV KURILENKO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lev Kurilenko is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JAMES R. LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James R. Lee is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YIN TAT LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yin Tat Lee is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YUANZHI LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yuanzhi Li is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YUNSHENG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunsheng Li is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHEN LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Liang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LARS LIDEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lars Liden is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ce Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MENGCHEN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mengchen Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"WEISHUNG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weishung Liu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ERIC LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Lin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ZEQI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zeqi Lin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHONG LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chong Luo is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PIYUSH MADAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Piyush Madan is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MATT MAZZOLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Mazzola is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HARDIK MODI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hardik Modi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ANH NGUYEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anh Nguyen is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BRANDON NORICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Brandon Norick is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BARUN PATRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Barun Patra is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DANIEL PEREZ-BECKER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel Perez-Becker is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"THOMAS PORTET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Portet is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"REID PRYZANT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reid Pryzant is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HEYANG QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heyang Qin is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MARKO RADMILAC\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marko Radmilac is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SAMBUDHA ROY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sambudha Roy is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"OLATUNJI RUWASE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olatunji Ruwase is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"OLLI SAARIKIVI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Olli Saarikivi is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AMIN SAIED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amin Saied is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ADIL SALIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adil Salim is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MICHAEL SANTACROCE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Santacroce is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SHITAL SHAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shital Shah is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"NING SHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Shang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HITESHI SHARMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hiteshi Sharma is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SWADHEEN SHUKLA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swadheen Shukla is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIA SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xia Song is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MASAHIRO TANAKA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Masahiro Tanaka is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"ANDREA TUPINI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrea Tupini is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"XIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"LIJUAN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lijuan Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CHUNYU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyu Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"YU WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"RACHEL WARD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rachel Ward is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GUANHUA WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guanhua Wang is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PHILIPP WITTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Witte is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"HAIPING WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haiping Wu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MICHAEL WYATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Wyatt is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BIN XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bin Xiao is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is one of the authors of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"JIAHANG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\" \/>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"TRANSPARENCY NOTES\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Transparency notes from Azure provide information about the rationale behind specific outputs or decisions made by AI models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"DISINFORMATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Disinformation refers to false information that is spread deliberately to deceive people, which can be generated by large language models without suitable safeguards<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DATA DISTRIBUTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data distribution refers to the way in which data is spread across different categories or areas, affecting the performance of models like Orca-3<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"GENERATIVE TEACHING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Generative Teaching is an approach that uses methods like AgentInstruct to generate large amounts of diverse and high-quality data for model post-training<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"UNSTRUCTURED DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Unstructured data refers to information that does not have a pre-defined data model, which can be used by AgentInstruct to generate tailored datasets<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PHI-3 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Phi-3 technical report is a document authored by multiple researchers, detailing the technical aspects of the Phi-3 model<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MICROSOFT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">Microsoft is the company behind Azure, which provides various AI-related services including transparency notes<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"AI TECHNOLOGIES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">AI technologies refer to the various applications and systems that use artificial intelligence to perform tasks, which can be subject to content harms and hallucinations<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"BENCHMARKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Benchmarks are standards or points of reference against which things may be compared or assessed, used to measure the performance of models like Orca-3<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"REGULATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Regulations refer to rules or directives made and maintained by an authority, which are hoped to be improved by government and technology leaders for AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"STANDARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Standards refer to a level of quality or attainment, which are hoped to be improved by government and technology leaders for AI technologies<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MITIGATIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Mitigations refer to actions taken to reduce the severity, seriousness, or painfulness of something, such as the hallucination issue in AI models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MEASUREMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Measurement refers to the process of obtaining the magnitude of a quantity, which is important for understanding and mitigating issues like hallucination in AI models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"POST-TRAINING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Post-training refers to the process of further training a model after its initial training, often using additional data to improve its performance<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"PRE-TRAINING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Pre-training refers to the initial phase of training a model on a large dataset before fine-tuning it for specific tasks<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"DOMAIN\/TASK SPECIALIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Domain\/task specialization refers to the process of customizing a model to perform well in specific areas or tasks<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"CONTINUAL IMPROVEMENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Continual improvement refers to the ongoing effort to improve products, services, or processes, such as using agentic flows to generate higher quality data for models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"SEMI-AUTOMATED PIPELINES\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Semi-automated pipelines refer to partially automated processes that use synthetic data for model customization and continual improvement<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>    <\/node>    <node id=\"MODEL CUSTOMIZATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Model customization refers to the process of tailoring a model to meet specific requirements or perform specific tasks<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DOMAIN SPECIFIC CONTENT\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Domain specific content refers to information that is relevant to a particular field or area, used as seeds for generating synthetic data in model customization<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"CITATIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Citations refer to references to other works or documents, such as those listed in the references section of the Phi-3 technical report<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"APPENDIX E\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">Appendix E is a section in a document that provides additional details, such as more information about baselines in the provided text<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"ACTIVE RESEARCH TOPIC\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Active research topic refers to an area of study that is currently being investigated, such as the measurement, understanding, and mitigations of hallucination in AI models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"HARMFUL CONTENT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Harmful content refers to information that can cause damage or distress, which can be generated by large language models without suitable safeguards<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"SAFEGUARDS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Safeguards refer to measures taken to protect against possible dangers or risks, such as the misuse of large language models to generate disinformation or harmful content<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"TUNING DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Tuning data refers to the dataset used to fine-tune a model, which can affect its performance and accuracy<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"PERFORMANCE GAIN\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Performance gain refers to the improvement in a model's performance, such as the substantial improvement observed in the Orca-3 model post-trained with a 25M pair dataset<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"MULTIPLE BENCHMARKS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Multiple benchmarks refer to various standards or points of reference used to measure the performance of models like Orca-3<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"UNSTRUCTURED DATA SOURCES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Unstructured data sources refer to information that does not have a pre-defined data model, which can be used by AgentInstruct to generate tailored datasets<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"PROMPTS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Prompts refer to the initial inputs or questions given to a model to generate responses, used in the datasets generated by AgentInstruct<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"RESPONSES\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Responses refer to the outputs or answers generated by a model in reaction to prompts, used in the datasets generated by AgentInstruct<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"UNSTRUCTURED CONTENT\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Unstructured content refers to information that does not have a pre-defined data model, which can be used to generate diverse and high-quality instruction data<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"INSTRUCTION DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Instruction data refers to the information used to teach or train a model, which can be generated from unstructured content using agentic flows<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"MODEL TRAINING\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Model training refers to the process of teaching a model to perform tasks by feeding it data and adjusting its parameters<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DATA CREATION PROCESS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Data creation process refers to the method of generating data for training models, which can involve human curation and intervention<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"HUMAN CURATION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Human curation refers to the process of manually selecting and organizing data, which is often required in the data creation process for model training<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"INTERVENTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Intervention refers to the involvement of humans in the data creation process to ensure the quality and relevance of the data<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"DIVERSE ANSWERS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Diverse answers refer to a variety of responses generated by a model, which can help explore potential solutions and improve model performance<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>    <node id=\"POTENTIAL SOLUTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Potential solutions refer to possible answers or methods for solving a problem, which can be explored using diverse answers generated by models<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">CONCEPT<\/data>    <\/node>    <node id=\"BASE MODEL\">      <data key=\"d0\">MODEL<\/data>      <data key=\"d1\">Base model refers to the initial version of a model before it undergoes further training or customization<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">MODEL<\/data>    <\/node>    <node id=\"HIGHER QUALITY DATA\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Higher quality data refers to information that is more accurate, relevant, and useful for training models, which can be generated using agentic flows<\/data>      <data key=\"d2\">dd9a46950237e49ef9b1c7ef08e08d42<\/data>      <data key=\"d3\">DATA<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"cc20c99cad8edecc66b82ac751ff7172","chunk":" Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable\nlanguage model locally on your phone, 2024. URL https:\/\/arxiv.org\/abs\/2404.14219 .\n[2]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv:1803.05457v1, 2018.\n[3]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXivpreprint arXiv:2110.14168, 2021.\n[4]CodeParrot. Github-code clean dataset, 2022. https:\/\/huggingface.co\/datasets\/\ncodeparrot\/github-code-clean [Accessed: (06\/15\/2024)].\n22[5]Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXivpreprint arXiv:2305.14233, 2023.\n[6]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. DROP: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In Proceedings ofthe2019Conference oftheNorthAmerican Chapter ofthe\nAssociation forComputational Linguistics: HumanLanguage Technologies, Volume1(Long\nandShortPapers), pages 2368\u20132378, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653\/v1\/N19-1246. URL https:\/\/aclanthology.org\/\nN19-1246 .\n[7]Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, and Dahua Lin.\nQuery of cc: Unearthing large scale domain-specific knowledge from public corpora. arXiv\npreprint arXiv:2401.14624, 2024.\n[8]Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. The false promise of imitating proprietary llms, 2023. URL https:\n\/\/arxiv.org\/abs\/2305.15717 .\n[9]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXivpreprint arXiv:2103.03874, 2021.\n[10]Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep\nDasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nCamels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL https:\n\/\/arxiv.org\/abs\/2311.10702 .\n[11]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\n[12]Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie\nLu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2309.00267 .\n[13]Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacae","chunk_id":"cc20c99cad8edecc66b82ac751ff7172","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"WANG","type":"PERSON","description":"Wang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PHILIPP WITTE","type":"PERSON","description":"Philipp Witte is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAIPING WU","type":"PERSON","description":"Haiping Wu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MICHAEL WYATT","type":"PERSON","description":"Michael Wyatt is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BIN XIAO","type":"PERSON","description":"Bin Xiao is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIAHANG XU","type":"PERSON","description":"Jiahang Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"WEIJIAN XU","type":"PERSON","description":"Weijian Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SONALI YADAV","type":"PERSON","description":"Sonali Yadav is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"FAN YANG","type":"PERSON","description":"Fan Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIANWEI YANG","type":"PERSON","description":"Jianwei Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZIYI YANG","type":"PERSON","description":"Ziyi Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YIFAN YANG","type":"PERSON","description":"Yifan Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DONGHAN YU","type":"PERSON","description":"Donghan Yu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LU YUAN","type":"PERSON","description":"Lu Yuan is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHENGRUIDONG ZHANG","type":"PERSON","description":"Chengruidong Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CYRIL ZHANG","type":"PERSON","description":"Cyril Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JIANWEN ZHANG","type":"PERSON","description":"Jianwen Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LI LYNA ZHANG","type":"PERSON","description":"Li Lyna Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YI ZHANG","type":"PERSON","description":"Yi Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUE ZHANG","type":"PERSON","description":"Yue Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUNAN ZHANG","type":"PERSON","description":"Yunan Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XIREN ZHOU","type":"PERSON","description":"Xiren Zhou is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PHI-3 TECHNICAL REPORT","type":"DOCUMENT","description":"The Phi-3 technical report is a document titled \"A highly capable language model locally on your phone\" published in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PETER CLARK","type":"PERSON","description":"Peter Clark is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ISAAC COWHEY","type":"PERSON","description":"Isaac Cowhey is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OREN ETZIONI","type":"PERSON","description":"Oren Etzioni is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TUSHAR KHOT","type":"PERSON","description":"Tushar Khot is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ASHISH SABHARWAL","type":"PERSON","description":"Ashish Sabharwal is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CARISSA SCHOENICK","type":"PERSON","description":"Carissa Schoenick is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"OYVIND TAFJORD","type":"PERSON","description":"Oyvind Tafjord is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THINK YOU HAVE SOLVED QUESTION ANSWERING? TRY ARC, THE AI2 REASONING CHALLENGE","type":"DOCUMENT","description":"The paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" was published on arXiv in 2018","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"KARL COBBE","type":"PERSON","description":"Karl Cobbe is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VINEET KOSARAJU","type":"PERSON","description":"Vineet Kosaraju is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MOHAMMAD BAVARIAN","type":"PERSON","description":"Mohammad Bavarian is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MARK CHEN","type":"PERSON","description":"Mark Chen is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HEEWOO JUN","type":"PERSON","description":"Heewoo Jun is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LUKASZ KAISER","type":"PERSON","description":"Lukasz Kaiser is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATTHIAS PLAPPERT","type":"PERSON","description":"Matthias Plappert is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JERRY TWOREK","type":"PERSON","description":"Jerry Tworek is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JACOB HILTON","type":"PERSON","description":"Jacob Hilton is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"REIICHIRO NAKANO","type":"PERSON","description":"Reiichiro Nakano is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS","type":"DOCUMENT","description":"The paper titled \"Training verifiers to solve math word problems\" was published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CODEPARROT","type":"ORGANIZATION","description":"CodeParrot is the organization that published the Github-code clean dataset in 2022","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GITHUB-CODE CLEAN DATASET","type":"DATASET","description":"The Github-code clean dataset is a dataset published by CodeParrot in 2022","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NING DING","type":"PERSON","description":"Ning Ding is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YULIN CHEN","type":"PERSON","description":"Yulin Chen is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOKAI XU","type":"PERSON","description":"Bokai Xu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHI ZHENG","type":"PERSON","description":"Zhi Zheng is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SHENGDING HU","type":"PERSON","description":"Shengding Hu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN LIU","type":"PERSON","description":"Zhiyuan Liu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MAOSONG SUN","type":"PERSON","description":"Maosong Sun is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BOWEN ZHOU","type":"PERSON","description":"Bowen Zhou is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ENHANCING CHAT LANGUAGE MODELS BY SCALING HIGH-QUALITY INSTRUCTIONAL CONVERSATIONS","type":"DOCUMENT","description":"The paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DHEERU DUA","type":"PERSON","description":"Dheeru Dua is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YIZHONG WANG","type":"PERSON","description":"Yizhong Wang is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PRADEEP DASIGI","type":"PERSON","description":"Pradeep Dasigi is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GABRIEL STANOVSKY","type":"PERSON","description":"Gabriel Stanovsky is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAMEER SINGH","type":"PERSON","description":"Sameer Singh is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATT GARDNER","type":"PERSON","description":"Matt Gardner is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DROP: A READING COMPREHENSION BENCHMARK REQUIRING DISCRETE REASONING OVER PARAGRAPHS","type":"DOCUMENT","description":"The paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" was published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHAOYE FEI","type":"PERSON","description":"Zhaoye Fei is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YUNFAN SHAO","type":"PERSON","description":"Yunfan Shao is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LINYANG LI","type":"PERSON","description":"Linyang Li is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ZHIYUAN ZENG","type":"PERSON","description":"Zhiyuan Zeng is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANG YAN","type":"PERSON","description":"Hang Yan is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XIPENG QIU","type":"PERSON","description":"Xipeng Qiu is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAHUA LIN","type":"PERSON","description":"Dahua Lin is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"QUERY OF CC: UNEARTHING LARGE SCALE DOMAIN-SPECIFIC KNOWLEDGE FROM PUBLIC CORPORA","type":"DOCUMENT","description":"The paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" was published on arXiv in 2024","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ARNAV GUDIBANDE","type":"PERSON","description":"Arnav Gudibande is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC WALLACE","type":"PERSON","description":"Eric Wallace is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHARLIE SNELL","type":"PERSON","description":"Charlie Snell is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XINYANG GENG","type":"PERSON","description":"Xinyang Geng is one","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAO LIU","type":"PERSON","description":"Hao Liu is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PIETER ABBEEL","type":"PERSON","description":"Pieter Abbeel is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SERGEY LEVINE","type":"PERSON","description":"Sergey Levine is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAWN SONG","type":"PERSON","description":"Dawn Song is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THE FALSE PROMISE OF IMITATING PROPRIETARY LLMS","type":"DOCUMENT","description":"The paper titled \"The false promise of imitating proprietary llms\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAN HENDRYCKS","type":"PERSON","description":"Dan Hendrycks is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"COLLIN BURNS","type":"PERSON","description":"Collin Burns is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAURAV KADAVATH","type":"PERSON","description":"Saurav Kadavath is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"AKUL ARORA","type":"PERSON","description":"Akul Arora is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"STEVEN BASART","type":"PERSON","description":"Steven Basart is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ERIC TANG","type":"PERSON","description":"Eric Tang is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JACOB STEINHARDT","type":"PERSON","description":"Jacob Steinhardt is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MEASURING MATHEMATICAL PROBLEM SOLVING WITH THE MATH DATASET","type":"DOCUMENT","description":"The paper titled \"Measuring mathematical problem solving with the math dataset\" was published on arXiv in 2021","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HAMISH IVISON","type":"PERSON","description":"Hamish Ivison is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VALENTINA PYATKIN","type":"PERSON","description":"Valentina Pyatkin is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NATHAN LAMBERT","type":"PERSON","description":"Nathan Lambert is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MATTHEW PETERS","type":"PERSON","description":"Matthew Peters is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JOEL JANG","type":"PERSON","description":"Joel Jang is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DAVID WADDEN","type":"PERSON","description":"David Wadden is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"NOAH A. SMITH","type":"PERSON","description":"Noah A. Smith is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"IZ BELTAGY","type":"PERSON","description":"Iz Beltagy is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANNANEH HAJISHIRZI","type":"PERSON","description":"Hannaneh Hajishirzi is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CAMELS IN A CHANGING CLIMATE: ENHANCING LM ADAPTATION WITH TULU 2","type":"DOCUMENT","description":"The paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ALBERT Q. JIANG","type":"PERSON","description":"Albert Q. Jiang is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ALEXANDRE SABLAYROLLES","type":"PERSON","description":"Alexandre Sablayrolles is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ARTHUR MENSCH","type":"PERSON","description":"Arthur Mensch is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CHRIS BAMFORD","type":"PERSON","description":"Chris Bamford is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DEVENDRA SINGH CHAPLOT","type":"PERSON","description":"Devendra Singh Chaplot is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DIEGO DE LAS CASAS","type":"PERSON","description":"Diego de las Casas is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"FLORIAN BRESSAND","type":"PERSON","description":"Florian Bressand is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GIANNA LENGYEL","type":"PERSON","description":"Gianna Lengyel is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GUILLAUME LAMPLE","type":"PERSON","description":"Guillaume Lample is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"LUCILE SAULNIER","type":"PERSON","description":"Lucile Saulnier is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"L\u00c9LIO RENARD LAVAUD","type":"PERSON","description":"L\u00e9lio Renard Lavaud is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MARIE-ANNE LACHAUX","type":"PERSON","description":"Marie-Anne Lachaux is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PIERRE STOCK","type":"PERSON","description":"Pierre Stock is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TEVEN LE SCAO","type":"PERSON","description":"Teven Le Scao is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THIBAUT LAVRIL","type":"PERSON","description":"Thibaut Lavril is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THOMAS WANG","type":"PERSON","description":"Thomas Wang is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TIMOTH\u00c9E LACROIX","type":"PERSON","description":"Timoth\u00e9e Lacroix is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"WILLIAM EL SAYED","type":"PERSON","description":"William El Sayed is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"MISTRAL 7B","type":"DOCUMENT","description":"The paper titled \"Mistral 7b\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HARRISON LEE","type":"PERSON","description":"Harrison Lee is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SAMRAT PHATALE","type":"PERSON","description":"Samrat Phatale is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HASSAN MANSOOR","type":"PERSON","description":"Hassan Mansoor is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"THOMAS MESNARD","type":"PERSON","description":"Thomas Mesnard is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"JOHAN FERRET","type":"PERSON","description":"Johan Ferret is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"KELLIE LU","type":"PERSON","description":"Kellie Lu is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"COLTON BISHOP","type":"PERSON","description":"Colton Bishop is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ETHAN HALL","type":"PERSON","description":"Ethan Hall is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"VICTOR CARBUNE","type":"PERSON","description":"Victor Carbune is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ABHINAV RASTOGI","type":"PERSON","description":"Abhinav Rastogi is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"SUSHANT PRAKASH","type":"PERSON","description":"Sushant Prakash is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK","type":"DOCUMENT","description":"The paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"GUOHAO LI","type":"PERSON","description":"Guohao Li is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HASAN ABED AL KADER HAMMOUD","type":"PERSON","description":"Hasan Abed Al Kader Hammoud is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"HANI ITANI","type":"PERSON","description":"Hani Itani is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"DMITRII KHIZBULLIN","type":"PERSON","description":"Dmitrii Khizbullin is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"BERNARD GHANEM","type":"PERSON","description":"Bernard Ghanem is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CAMEL: COMMUNICATIVE AGENTS FOR 'MIND' EXPLORATION OF LARGE LANGUAGE MODEL SOCIETY","type":"DOCUMENT","description":"The paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" was published on arXiv in 2023","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"XUECHEN LI","type":"PERSON","description":"Xuechen Li is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TIANYI ZHANG","type":"PERSON","description":"Tianyi Zhang is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"YANN DUBOIS","type":"PERSON","description":"Yann Dubois is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ROHAN TAORI","type":"PERSON","description":"Rohan Taori is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ISHAAN GULRAJANI","type":"PERSON","description":"Ishaan Gulrajani is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"CARLOS GUESTRIN","type":"PERSON","description":"Carlos Guestrin is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"TATSUNORI B. HASHIMOTO","type":"PERSON","description":"Tatsunori B. Hashimoto is one of the authors of the paper titled \"Alpaca\" published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"},{"name":"ALPACA","type":"DOCUMENT","description":"The paper titled \"Alpaca\" was published on arXiv","source_id":"cc20c99cad8edecc66b82ac751ff7172"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PHILIPP WITTE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Philipp Witte is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAIPING WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Haiping Wu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MICHAEL WYATT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michael Wyatt is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BIN XIAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bin Xiao is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIAHANG XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahang Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"WEIJIAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weijian Xu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SONALI YADAV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sonali Yadav is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"FAN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fan Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIANWEI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianwei Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZIYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ziyi Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YIFAN YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Yang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DONGHAN YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Donghan Yu is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LU YUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lu Yuan is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHENGRUIDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chengruidong Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CYRIL ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Cyril Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JIANWEN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianwen Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LI LYNA ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Lyna Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUE ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yue Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUNAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunan Zhang is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XIREN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiren Zhou is one of the authors of the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PHI-3 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The Phi-3 technical report is a document titled \"A highly capable language model locally on your phone\" published in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PETER CLARK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Peter Clark is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ISAAC COWHEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Isaac Cowhey is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OREN ETZIONI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oren Etzioni is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TUSHAR KHOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tushar Khot is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ASHISH SABHARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ashish Sabharwal is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CARISSA SCHOENICK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carissa Schoenick is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"OYVIND TAFJORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Oyvind Tafjord is one of the authors of the paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THINK YOU HAVE SOLVED QUESTION ANSWERING? TRY ARC, THE AI2 REASONING CHALLENGE\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Think you have solved question answering? try arc, the ai2 reasoning challenge\" was published on arXiv in 2018<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"KARL COBBE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Karl Cobbe is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VINEET KOSARAJU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Vineet Kosaraju is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MOHAMMAD BAVARIAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammad Bavarian is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MARK CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mark Chen is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HEEWOO JUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Heewoo Jun is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LUKASZ KAISER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lukasz Kaiser is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATTHIAS PLAPPERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthias Plappert is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JERRY TWOREK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jerry Tworek is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JACOB HILTON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Hilton is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"REIICHIRO NAKANO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Reiichiro Nakano is one of the authors of the paper titled \"Training verifiers to solve math word problems\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TRAINING VERIFIERS TO SOLVE MATH WORD PROBLEMS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Training verifiers to solve math word problems\" was published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CODEPARROT\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">CodeParrot is the organization that published the Github-code clean dataset in 2022<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GITHUB-CODE CLEAN DATASET\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">The Github-code clean dataset is a dataset published by CodeParrot in 2022<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NING DING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ning Ding is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YULIN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yulin Chen is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOKAI XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bokai Xu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhi Zheng is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SHENGDING HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shengding Hu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Liu is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MAOSONG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Maosong Sun is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BOWEN ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bowen Zhou is one of the authors of the paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ENHANCING CHAT LANGUAGE MODELS BY SCALING HIGH-QUALITY INSTRUCTIONAL CONVERSATIONS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Enhancing chat language models by scaling high-quality instructional conversations\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DHEERU DUA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dheeru Dua is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YIZHONG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yizhong Wang is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PRADEEP DASIGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pradeep Dasigi is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GABRIEL STANOVSKY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gabriel Stanovsky is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAMEER SINGH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sameer Singh is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATT GARDNER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matt Gardner is one of the authors of the paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DROP: A READING COMPREHENSION BENCHMARK REQUIRING DISCRETE REASONING OVER PARAGRAPHS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\" was published in the Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHAOYE FEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhaoye Fei is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YUNFAN SHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yunfan Shao is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LINYANG LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Linyang Li is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ZHIYUAN ZENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyuan Zeng is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANG YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hang Yan is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XIPENG QIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xipeng Qiu is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAHUA LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dahua Lin is one of the authors of the paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"QUERY OF CC: UNEARTHING LARGE SCALE DOMAIN-SPECIFIC KNOWLEDGE FROM PUBLIC CORPORA\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Query of cc: Unearthing large scale domain-specific knowledge from public corpora\" was published on arXiv in 2024<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ARNAV GUDIBANDE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arnav Gudibande is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC WALLACE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Wallace is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHARLIE SNELL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Charlie Snell is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XINYANG GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyang Geng is one<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAO LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hao Liu is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PIETER ABBEEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pieter Abbeel is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SERGEY LEVINE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sergey Levine is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAWN SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dawn Song is one of the authors of the paper titled \"The false promise of imitating proprietary llms\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THE FALSE PROMISE OF IMITATING PROPRIETARY LLMS\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"The false promise of imitating proprietary llms\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAN HENDRYCKS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dan Hendrycks is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"COLLIN BURNS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Collin Burns is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAURAV KADAVATH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Saurav Kadavath is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"AKUL ARORA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Akul Arora is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"STEVEN BASART\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Steven Basart is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ERIC TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Eric Tang is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JACOB STEINHARDT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jacob Steinhardt is one of the authors of the paper titled \"Measuring mathematical problem solving with the math dataset\" published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MEASURING MATHEMATICAL PROBLEM SOLVING WITH THE MATH DATASET\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Measuring mathematical problem solving with the math dataset\" was published on arXiv in 2021<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HAMISH IVISON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamish Ivison is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VALENTINA PYATKIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Valentina Pyatkin is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NATHAN LAMBERT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Lambert is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MATTHEW PETERS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Matthew Peters is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JOEL JANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joel Jang is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DAVID WADDEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">David Wadden is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"NOAH A. SMITH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Noah A. Smith is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"IZ BELTAGY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Iz Beltagy is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANNANEH HAJISHIRZI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hannaneh Hajishirzi is one of the authors of the paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CAMELS IN A CHANGING CLIMATE: ENHANCING LM ADAPTATION WITH TULU 2\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ALBERT Q. JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Albert Q. Jiang is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ALEXANDRE SABLAYROLLES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexandre Sablayrolles is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ARTHUR MENSCH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arthur Mensch is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CHRIS BAMFORD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chris Bamford is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DEVENDRA SINGH CHAPLOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Devendra Singh Chaplot is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DIEGO DE LAS CASAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Diego de las Casas is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"FLORIAN BRESSAND\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Florian Bressand is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GIANNA LENGYEL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gianna Lengyel is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GUILLAUME LAMPLE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guillaume Lample is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"LUCILE SAULNIER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lucile Saulnier is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"L&#201;LIO RENARD LAVAUD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">L&#233;lio Renard Lavaud is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MARIE-ANNE LACHAUX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Marie-Anne Lachaux is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PIERRE STOCK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pierre Stock is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TEVEN LE SCAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Teven Le Scao is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THIBAUT LAVRIL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thibaut Lavril is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THOMAS WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Wang is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TIMOTH&#201;E LACROIX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Timoth&#233;e Lacroix is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"WILLIAM EL SAYED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">William El Sayed is one of the authors of the paper titled \"Mistral 7b\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"MISTRAL 7B\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Mistral 7b\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HARRISON LEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Harrison Lee is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SAMRAT PHATALE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samrat Phatale is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HASSAN MANSOOR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hassan Mansoor is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"THOMAS MESNARD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Mesnard is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"JOHAN FERRET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Johan Ferret is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"KELLIE LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kellie Lu is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"COLTON BISHOP\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Colton Bishop is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ETHAN HALL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ethan Hall is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"VICTOR CARBUNE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Victor Carbune is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ABHINAV RASTOGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Abhinav Rastogi is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"SUSHANT PRAKASH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sushant Prakash is one of the authors of the paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"RLAIF: SCALING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH AI FEEDBACK\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"GUOHAO LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guohao Li is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HASAN ABED AL KADER HAMMOUD\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hasan Abed Al Kader Hammoud is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"HANI ITANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hani Itani is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"DMITRII KHIZBULLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dmitrii Khizbullin is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"BERNARD GHANEM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernard Ghanem is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CAMEL: COMMUNICATIVE AGENTS FOR 'MIND' EXPLORATION OF LARGE LANGUAGE MODEL SOCIETY\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" was published on arXiv in 2023<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"XUECHEN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuechen Li is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TIANYI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianyi Zhang is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"YANN DUBOIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yann Dubois is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ROHAN TAORI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rohan Taori is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ISHAAN GULRAJANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ishaan Gulrajani is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"CARLOS GUESTRIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carlos Guestrin is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"TATSUNORI B. HASHIMOTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tatsunori B. Hashimoto is one of the authors of the paper titled \"Alpaca\" published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <node id=\"ALPACA\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The paper titled \"Alpaca\" was published on arXiv<\/data>      <data key=\"d2\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/node>    <edge source=\"WANG\" target=\"PHILIPP WITTE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang and Philipp Witte co-authored the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"WANG\" target=\"HAIPING WU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang and Haiping Wu co-authored the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"WANG\" target=\"MICHAEL WYATT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang and Michael Wyatt co-authored the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"WANG\" target=\"BIN XIAO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang and Bin Xiao co-authored the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>    <edge source=\"WANG\" target=\"CAN XU\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Wang and Can Xu co-authored the Phi-3 technical report: A highly capable language model locally on your phone, 2024<\/data>      <data key=\"d5\">cc20c99cad8edecc66b82ac751ff7172<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"3d1f6634f93f8a4c296dc8df7e59859e","chunk":"rii Khizbullin, and Bernard\nGhanem. Camel: Communicative agents for \"mind\" exploration of large language model society,\n2023. URL https:\/\/arxiv.org\/abs\/2303.17760 .\n[14]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, andTatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-following\nmodels. https:\/\/github.com\/tatsu-lab\/alpaca_eval , 2023.\n[15]Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei\nLiu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and\nevaluation capabilities of large language models for instruction controllable summarization,\n2023. URL https:\/\/arxiv.org\/abs\/2311.09184 .\n[16]Lm-sys. Mt-Bench, 2023. URL https:\/\/huggingface.co\/spaces\/lmsys\/mt-bench\/tree\/\ncf27f9f9da48f72169bce3c3e784d24347d1e833\/data\/mt_bench\/model_answer .\n[17]Daniel van Strien Loubna Ben Allal, Anton Lozhkov. Cosmopedia: how to create large-scale\nsynthetic data for pre-training, 2024. URL https:\/\/huggingface.co\/blog\/cosmopedia .\n[18]Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj\nAgarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi,\nGuoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching\nsmall language models how to reason, 2023. URL https:\/\/arxiv.org\/abs\/2311.11045 .\n[19]ArindamMitra, HamedKhanpour, CorbyRosset, andAhmedAwadallah. Orca-math: Unlocking\nthe potential of slms in grade school math. arXivpreprint arXiv:2402.14830, 2024.\n[20]Subhabrata Mukherjee and Ahmed Awadallah. Xtremedistil: Multi-stage distillation for massive\nmultilingual models, 2020.\n23[21]Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv\npreprint arXiv:2306.02707, 2023.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23]Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models,\n2024. URL https:\/\/arxiv.org\/abs\/2312.06281 .\n[24]Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4, 2023. URL https:\/\/arxiv.org\/abs\/2304.03277 .\n[25]YiweiQin, KaiqiangSong, YebowenHu, Wenlin Yao, SangwooCho, XiaoyangWang, Xuansheng\nWu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in\nlarge language models, 2024. URL https:\/\/arxiv.org\/abs\/2401.03601 .\n[26]Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to\nmaster 16000+ real-world apis, 2023.\n[27]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023. URL https:\/\/arxiv.org\/abs\/2311.12022 .\n[28]Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and\nTengyang Xie. Direct nash optimization: Teaching language models to self-improve with general\npreferences, 2024. URL https:\/\/arxiv.org\/abs\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S.","chunk_id":"3d1f6634f93f8a4c296dc8df7e59859e","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"RII KHIZBULLIN","type":"PERSON","description":"Rii Khizbullin is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"BERNARD GHANEM","type":"PERSON","description":"Bernard Ghanem is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CAMEL","type":"TECHNOLOGY","description":"Camel is a system described as Communicative agents for 'mind' exploration of large language model society, published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"XUECHEN LI","type":"PERSON","description":"Xuechen Li is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"TIANYI ZHANG","type":"PERSON","description":"Tianyi Zhang is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YANN DUBOIS","type":"PERSON","description":"Yann Dubois is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ROHAN TAORI","type":"PERSON","description":"Rohan Taori is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ISHAAN GULRAJANI","type":"PERSON","description":"Ishaan Gulrajani is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CARLOS GUESTRIN","type":"PERSON","description":"Carlos Guestrin is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PERCY LIANG","type":"PERSON","description":"Percy Liang is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"TATSUNORI B. HASHIMOTO","type":"PERSON","description":"Tatsunori B. Hashimoto is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ALPACAEVAL","type":"TECHNOLOGY","description":"Alpacaeval is an automatic evaluator of instruction-following models, published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"YIXIN LIU","type":"PERSON","description":"Yixin Liu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ALEXANDER R. FABBRI","type":"PERSON","description":"Alexander R. Fabbri is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"JIAWEN CHEN","type":"PERSON","description":"Jiawen Chen is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YILUN ZHAO","type":"PERSON","description":"Yilun Zhao is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SIMENG HAN","type":"PERSON","description":"Simeng Han is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHAFIQ JOTY","type":"PERSON","description":"Shafiq Joty is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PENGFEI LIU","type":"PERSON","description":"Pengfei Liu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023\nPengfei Liu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"DRAGOMIR RADEV","type":"PERSON","description":"Dragomir Radev is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CHIEN-SHENG WU","type":"PERSON","description":"Chien-Sheng Wu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ARMAN COHAN","type":"PERSON","description":"Arman Cohan is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"MT-BENCH","type":"TECHNOLOGY","description":"MT-Bench is a system mentioned in the text, published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"LM-SYS","type":"ORGANIZATION","description":"LM-Sys is the organization that published MT-Bench in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"ORGANIZATION"},{"name":"DANIEL VAN STRIEN","type":"PERSON","description":"Daniel van Strien is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LOUBNA BEN ALLAL","type":"PERSON","description":"Loubna Ben Allal is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANTON LOZHKOV","type":"PERSON","description":"Anton Lozhkov is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"COSMOPEDIA","type":"TECHNOLOGY","description":"Cosmopedia is a system for creating large-scale synthetic data for pre-training, published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"ARINDAM MITRA","type":"PERSON","description":"Arindam Mitra is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LUCIANO DEL CORRO","type":"PERSON","description":"Luciano Del Corro is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHWETI MAHAJAN","type":"PERSON","description":"Shweti Mahajan is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANDRES CODAS","type":"PERSON","description":"Andres Codas is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CLARISSE SIMOES","type":"PERSON","description":"Clarisse Simoes is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SAHAJ AGARWAL","type":"PERSON","description":"Sahaj Agarwal is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XUXI CHEN","type":"PERSON","description":"Xuxi Chen is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ANASTASIA RAZDAIBIEDINA","type":"PERSON","description":"Anastasia Razdaibiedina is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ERIK JONES","type":"PERSON","description":"Erik Jones is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KRITI AGGARWAL","type":"PERSON","description":"Kriti Aggarwal is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"HAMID PALANGI","type":"PERSON","description":"Hamid Palangi is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"GUOQING ZHENG","type":"PERSON","description":"Guoqing Zheng is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CORBY ROSSET","type":"PERSON","description":"Corby Rosset is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"HAMED KHANPOUR","type":"PERSON","description":"Hamed Khanpour is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"AHMED AWADALLAH","type":"PERSON","description":"Ahmed Awadallah is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ORCA 2","type":"TECHNOLOGY","description":"Orca 2 is a system for teaching small language models how to reason, published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"ORCA-MATH","type":"TECHNOLOGY","description":"Orca-Math is a system for unlocking the potential of small language models in grade school math, published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"SUBHABRATA MUKHERJEE","type":"PERSON","description":"Subhabrata Mukherjee is one of the authors of the paper titled \"Xtremedistil: Multi-stage distillation for massive multilingual models\" published in 2020","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XTREMEDISTIL","type":"TECHNOLOGY","description":"Xtremedistil is a multi-stage distillation process for massive multilingual models, published in 2020","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"GANESH JAWAHAR","type":"PERSON","description":"Ganesh Jawahar is one of the authors of the paper titled \"Orca: Progressive learning from complex explanation traces of GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"ORCA","type":"TECHNOLOGY","description":"Orca is a system for progressive learning from complex explanation traces of GPT-4, published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"OPENAI","type":"ORGANIZATION","description":"OpenAI is the organization that published the GPT-4 technical report in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"ORGANIZATION"},{"name":"GPT-4 TECHNICAL REPORT","type":"DOCUMENT","description":"The GPT-4 technical report is a document published by OpenAI in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"DOCUMENT"},{"name":"SAMUEL J. PAECH","type":"PERSON","description":"Samuel J. Paech is the author of the paper titled \"EQ-Bench: An emotional intelligence benchmark for large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"EQ-BENCH","type":"TECHNOLOGY","description":"EQ-Bench is an emotional intelligence benchmark for large language models, published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"BAOLIN PENG","type":"PERSON","description":"Baolin Peng is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"CHUNYUAN LI","type":"PERSON","description":"Chunyuan Li is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"PENGCHENG HE","type":"PERSON","description":"Pengcheng He is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"MICHEL GALLEY","type":"PERSON","description":"Michel Galley is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"JIANFENG GAO","type":"PERSON","description":"Jianfeng Gao is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"INSTRUCTION TUNING WITH GPT-4","type":"TECHNOLOGY","description":"Instruction tuning with GPT-4 is a method described in a paper published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"YIWEI QIN","type":"PERSON","description":"Yiwei Qin is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KAIQIANG SONG","type":"PERSON","description":"Kaiqiang Song is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YEBOWEN HU","type":"PERSON","description":"Yebowen Hu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"WENLIN YAO","type":"PERSON","description":"Wenlin Yao is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SANGWOO CHO","type":"PERSON","description":"Sangwoo Cho is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XIAOYANG WANG","type":"PERSON","description":"Xiaoyang Wang is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"XUANSHENG WU","type":"PERSON","description":"Xuansheng Wu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"FEI LIU","type":"PERSON","description":"Fei Liu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"DONG YU","type":"PERSON","description":"Dong Yu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"INFOBENCH","type":"TECHNOLOGY","description":"Infobench is a system for evaluating instruction following ability in large language models, published in 2024","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"TECHNOLOGY"},{"name":"YUJIA QIN","type":"PERSON","description":"Yujia Qin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"SHIHAO LIANG","type":"PERSON","description":"Shihao Liang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YINING YE","type":"PERSON","description":"Yining Ye is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"KUNLUN ZHU","type":"PERSON","description":"Kunlun Zhu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"LAN YAN","type":"PERSON","description":"Lan Yan is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YAXI LU","type":"PERSON","description":"Yaxi Lu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e","entity_type":"PERSON"},{"name":"YANKAI LIN","type":"PERSON","description":"Yankai Lin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"XIN CONG","type":"PERSON","description":"Xin Cong is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"XIANGRU TANG","type":"PERSON","description":"Xiangru Tang is one of the authors of the paper titled \"(\"entity\"","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"JIAHAO ZHANG","type":"PERSON","description":"Jiahao Zhang is one of the authors of a paper mentioned in the text","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"LINDSAY RONEY","type":"PERSON","description":"Lindsay Roney is one of the authors of a paper mentioned in the text","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"},{"name":"JOANNA C. S.","type":"PERSON","description":"Joanna C. S. is one of the authors of a paper mentioned in the text","source_id":"3d1f6634f93f8a4c296dc8df7e59859e"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"RII KHIZBULLIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rii Khizbullin is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"BERNARD GHANEM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Bernard Ghanem is one of the authors of the paper titled \"Camel: Communicative agents for 'mind' exploration of large language model society\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CAMEL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Camel is a system described as Communicative agents for 'mind' exploration of large language model society, published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"XUECHEN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuechen Li is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TIANYI ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianyi Zhang is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANN DUBOIS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yann Dubois is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ROHAN TAORI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Rohan Taori is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ISHAAN GULRAJANI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ishaan Gulrajani is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CARLOS GUESTRIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Carlos Guestrin is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PERCY LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Percy Liang is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"TATSUNORI B. HASHIMOTO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tatsunori B. Hashimoto is one of the authors of the paper titled \"Alpacaeval: An automatic evaluator of instruction-following models\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Alpacaeval is an automatic evaluator of instruction-following models, published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YIXIN LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yixin Liu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ALEXANDER R. FABBRI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alexander R. Fabbri is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIAWEN CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiawen Chen is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YILUN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yilun Zhao is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SIMENG HAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Simeng Han is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHAFIQ JOTY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shafiq Joty is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGFEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengfei Liu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023Pengfei Liu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DRAGOMIR RADEV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dragomir Radev is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHIEN-SHENG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chien-Sheng Wu is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ARMAN COHAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arman Cohan is one of the authors of the paper titled \"Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">MT-Bench is a system mentioned in the text, published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"LM-SYS\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">LM-Sys is the organization that published MT-Bench in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"DANIEL VAN STRIEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daniel van Strien is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LOUBNA BEN ALLAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Loubna Ben Allal is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANTON LOZHKOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anton Lozhkov is one of the authors of the paper titled \"Cosmopedia: how to create large-scale synthetic data for pre-training\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"COSMOPEDIA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Cosmopedia is a system for creating large-scale synthetic data for pre-training, published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ARINDAM MITRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Arindam Mitra is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LUCIANO DEL CORRO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Luciano Del Corro is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHWETI MAHAJAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shweti Mahajan is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANDRES CODAS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andres Codas is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CLARISSE SIMOES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Clarisse Simoes is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SAHAJ AGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sahaj Agarwal is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUXI CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuxi Chen is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ANASTASIA RAZDAIBIEDINA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Anastasia Razdaibiedina is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ERIK JONES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erik Jones is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KRITI AGGARWAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kriti Aggarwal is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAMID PALANGI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamid Palangi is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"GUOQING ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guoqing Zheng is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CORBY ROSSET\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Corby Rosset is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"HAMED KHANPOUR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hamed Khanpour is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"AHMED AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Awadallah is one of the authors of the paper titled \"Orca 2: Teaching small language models how to reason\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ORCA 2\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca 2 is a system for teaching small language models how to reason, published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"ORCA-MATH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca-Math is a system for unlocking the potential of small language models in grade school math, published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"SUBHABRATA MUKHERJEE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Subhabrata Mukherjee is one of the authors of the paper titled \"Xtremedistil: Multi-stage distillation for massive multilingual models\" published in 2020<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XTREMEDISTIL\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Xtremedistil is a multi-stage distillation process for massive multilingual models, published in 2020<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"GANESH JAWAHAR\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ganesh Jawahar is one of the authors of the paper titled \"Orca: Progressive learning from complex explanation traces of GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"ORCA\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Orca is a system for progressive learning from complex explanation traces of GPT-4, published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"OPENAI\">      <data key=\"d0\">ORGANIZATION<\/data>      <data key=\"d1\">OpenAI is the organization that published the GPT-4 technical report in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">ORGANIZATION<\/data>    <\/node>    <node id=\"GPT-4 TECHNICAL REPORT\">      <data key=\"d0\">DOCUMENT<\/data>      <data key=\"d1\">The GPT-4 technical report is a document published by OpenAI in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">DOCUMENT<\/data>    <\/node>    <node id=\"SAMUEL J. PAECH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Samuel J. Paech is the author of the paper titled \"EQ-Bench: An emotional intelligence benchmark for large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"EQ-BENCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">EQ-Bench is an emotional intelligence benchmark for large language models, published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"BAOLIN PENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Baolin Peng is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"CHUNYUAN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chunyuan Li is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"PENGCHENG HE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pengcheng He is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"MICHEL GALLEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Michel Galley is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JIANFENG GAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jianfeng Gao is one of the authors of the paper titled \"Instruction tuning with GPT-4\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INSTRUCTION TUNING WITH GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Instruction tuning with GPT-4 is a method described in a paper published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YIWEI QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiwei Qin is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KAIQIANG SONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kaiqiang Song is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YEBOWEN HU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yebowen Hu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"WENLIN YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenlin Yao is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SANGWOO CHO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sangwoo Cho is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XIAOYANG WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyang Wang is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"XUANSHENG WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xuansheng Wu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"FEI LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Fei Liu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"DONG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Dong Yu is one of the authors of the paper titled \"Infobench: Evaluating instruction following ability in large language models\" published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Infobench is a system for evaluating instruction following ability in large language models, published in 2024<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">TECHNOLOGY<\/data>    <\/node>    <node id=\"YUJIA QIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujia Qin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"SHIHAO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shihao Liang is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YINING YE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yining Ye is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"KUNLUN ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kunlun Zhu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"LAN YAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lan Yan is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YAXI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaxi Lu is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"YANKAI LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yankai Lin is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"XIN CONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xin Cong is one of the authors of the paper titled \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\" published in 2023<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"XIANGRU TANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiangru Tang is one of the authors of the paper titled \"(\"entity\"<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"JIAHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahao Zhang is one of the authors of a paper mentioned in the text<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"LINDSAY RONEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lindsay Roney is one of the authors of a paper mentioned in the text<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>    <node id=\"JOANNA C. S.\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joanna C. S. is one of the authors of a paper mentioned in the text<\/data>      <data key=\"d2\">3d1f6634f93f8a4c296dc8df7e59859e<\/data>    <\/node>  <\/graph><\/graphml>"}
{"id":"f4e98ee0b7fb42428f3312f29cb444dd","chunk":"\/2404.03715 .\n[29]Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson. The curse of recursion: Training on generated data makes models forget, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2305.17493 .\n[30]Mohammed Latif Siddiq, Jiahao Zhang, Lindsay Roney, and Joanna C. S. Santos.\nRe(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos at-\ntacks. In Proceedings ofthe46thInternational Conference onSoftware Engineering, NIER\nTrack(ICSE-NIER \u201924), 2024. doi: 10.1145\/3639476.3639757.\n[31]Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, AakankshaChowdhery, QuocVLe, EdHChi, DennyZhou, , andJasonWei. Challenging\nbig-bench tasks and whether chain-of-thought can solve them. arXivpreprint arXiv:2210.09261 ,\n2022.\n[32]Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen.\nAci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note\ngeneration, 2023.\n[33]Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger,\nand Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation,\n2023. URL https:\/\/arxiv.org\/abs\/2308.08155 .\n[34]Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and\nCaiming Xiong. Fofo: A benchmark to evaluate llms\u2019 format-following capability, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2402.18667 .\n[35]Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\ngeneration for medicine. arXivpreprint arXiv:2402.13178, 2024.\n[36]Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions,\n2023.\n[37]Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. arXivpreprint arXiv:2309.12284, 2023.\n24[38]Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Automathtext: Autonomous\ndata selection with language models for mathematical texts. arXivpreprint arXiv:2402.07625 ,\n2024.\n[39]Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels, 2023.\n[40]Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2311.07911 .\n25A Agentic Flows Details\nA.1 Reading Comprehension Flow\nReading Comprehension transformation agents :\n1.Argument Passage Generator: This agent is adept at creating passages that\narticulate arguments, which may occasionally contain logical inconsistencies.\n2.Debate Passage Generator: It specializes in crafting passages that mimic the\nstructure and content of debate transcripts.\n3.Conversation Passage Generator: This agent generates passages that depict\ndialogues.\n4.Meeting Transcript Generator: It is designed to produce meeting transcripts.\n5.Poem Generator: This agent generates poems.\n6.Satirical Passage Generator: It creates texts infused with satirical wit.\n7.Instructional Passage Generator: This agent generates passages resembling\ninstructional manuals.\n8.Long Text Generator: It extends the original text by incorporating additional\ninformation, thereby increasing its length.\n9.Identity Agent: A straightforward agent that replicates the input text verbatim.\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Literal Comprehension Question (Short Answer(or list)): a question that asks for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay","chunk_id":"f4e98ee0b7fb42428f3312f29cb444dd","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"ILIA SHUMAILOV","type":"PERSON","description":"Ilia Shumailov is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZAKHAR SHUMAYLOV","type":"PERSON","description":"Zakhar Shumaylov is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIREN ZHAO","type":"PERSON","description":"Yiren Zhao is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YARIN GAL","type":"PERSON","description":"Yarin Gal is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NICOLAS PAPERNOT","type":"PERSON","description":"Nicolas Papernot is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ROSS ANDERSON","type":"PERSON","description":"Ross Anderson is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MOHAMMED LATIF SIDDIQ","type":"PERSON","description":"Mohammed Latif Siddiq is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER \u201924) in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIAHAO ZHANG","type":"PERSON","description":"Jiahao Zhang is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER \u201924) in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LINDSAY RONEY","type":"PERSON","description":"Lindsay Roney is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER \u201924) in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JOANNA C. S. SANTOS","type":"PERSON","description":"Joanna C. S. Santos is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER \u201924) in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MIRAC SUZGUN","type":"PERSON","description":"Mirac Suzgun is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NATHAN SCALES","type":"PERSON","description":"Nathan Scales is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NATHANAEL SCH\u00c4RLI","type":"PERSON","description":"Nathanael Sch\u00e4rli is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SEBASTIAN GEHRMANN","type":"PERSON","description":"Sebastian Gehrmann is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YI TAY","type":"PERSON","description":"Yi Tay is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"HYUNG WON CHUNG","type":"PERSON","description":"Hyung Won Chung is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AAKANKSHA CHOWDHERY","type":"PERSON","description":"Aakanksha Chowdhery is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QUOC V LE","type":"PERSON","description":"Quoc V Le is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ED H CHI","type":"PERSON","description":"Ed H Chi is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DENNY ZHOU","type":"PERSON","description":"Denny Zhou is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022\nDenny Zhou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd","entity_type":"PERSON"},{"name":"JASON WEI","type":"PERSON","description":"Jason Wei is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEN WAI YIM","type":"PERSON","description":"Wen wai Yim is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YUJUAN FU","type":"PERSON","description":"Yujuan Fu is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ASMA BEN ABACHA","type":"PERSON","description":"Asma Ben Abacha is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NEAL SNIDER","type":"PERSON","description":"Neal Snider is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"THOMAS LIN","type":"PERSON","description":"Thomas Lin is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MELIHA YETISGEN","type":"PERSON","description":"Meliha Yetisgen is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QINGYUN WU","type":"PERSON","description":"Qingyun Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"GAGAN BANSAL","type":"PERSON","description":"Gagan Bansal is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIEYU ZHANG","type":"PERSON","description":"Jieyu Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIRAN WU","type":"PERSON","description":"Yiran Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"BEIBIN LI","type":"PERSON","description":"Beibin Li is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ERKANG ZHU","type":"PERSON","description":"Erkang Zhu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LI JIANG","type":"PERSON","description":"Li Jiang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XIAOYUN ZHANG","type":"PERSON","description":"Xiaoyun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SHAOKUN ZHANG","type":"PERSON","description":"Shaokun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIALE LIU","type":"PERSON","description":"Jiale Liu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AHMED HASSAN AWADALLAH","type":"PERSON","description":"Ahmed Hassan Awadallah is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RYEN W WHITE","type":"PERSON","description":"Ryen W White is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DOUG BURGER","type":"PERSON","description":"Doug Burger is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHI WANG","type":"PERSON","description":"Chi Wang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CONGYING XIA","type":"PERSON","description":"Congying Xia is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHEN XING","type":"PERSON","description":"Chen Xing is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIANGSHU DU","type":"PERSON","description":"Jiangshu Du is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XINYI YANG","type":"PERSON","description":"Xinyi Yang is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIHAO FENG","type":"PERSON","description":"Yihao Feng is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RAN XU","type":"PERSON","description":"Ran Xu is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WENPENG YIN","type":"PERSON","description":"Wenpeng Yin is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CAIMING XIONG","type":"PERSON","description":"Caiming Xiong is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms\u2019 format-following capability\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"GUANGZHI XIONG","type":"PERSON","description":"Guangzhi Xiong is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QIAO JIN","type":"PERSON","description":"Qiao Jin is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZHIYONG LU","type":"PERSON","description":"Zhiyong Lu is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AIDONG ZHANG","type":"PERSON","description":"Aidong Zhang is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CAN XU","type":"PERSON","description":"Can Xu is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"QINGFENG SUN","type":"PERSON","description":"Qingfeng Sun is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"KAI ZHENG","type":"PERSON","description":"Kai Zheng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"XIUBO GENG","type":"PERSON","description":"Xiubo Geng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"PU ZHAO","type":"PERSON","description":"Pu Zhao is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JIAZHAN FENG","type":"PERSON","description":"Jiazhan Feng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CHONGYANG TAO","type":"PERSON","description":"Chongyang Tao is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DAXIN JIANG","type":"PERSON","description":"Daxin Jiang is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LONGHUI YU","type":"PERSON","description":"Longhui Yu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEISEN JIANG","type":"PERSON","description":"Weisen Jiang is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"HAN SHI","type":"PERSON","description":"Han Shi is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JINCHENG YU","type":"PERSON","description":"Jincheng Yu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZHENGYING LIU","type":"PERSON","description":"Zhengying Liu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YU ZHANG","type":"PERSON","description":"Yu Zhang is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JAMES T KWOK","type":"PERSON","description":"James T Kwok is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ZHENGUO LI","type":"PERSON","description":"Zhenguo Li is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ADRIAN WELLER","type":"PERSON","description":"Adrian Weller is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEIYANG LIU","type":"PERSON","description":"Weiyang Liu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIFAN ZHANG","type":"PERSON","description":"Yifan Zhang is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIFAN LUO","type":"PERSON","description":"Yifan Luo is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ANDREW CHI-CHIH YAO","type":"PERSON","description":"Andrew Chi-Chih Yao is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WANJUN ZHONG","type":"PERSON","description":"Wanjun Zhong is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"RUIXIANG CUI","type":"PERSON","description":"Ruixiang Cui is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YIDUO GUO","type":"PERSON","description":"Yiduo Guo is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YAOBO LIANG","type":"PERSON","description":"Yaobo Liang is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SHUAI LU","type":"PERSON","description":"Shuai Lu is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YANLIN WANG","type":"PERSON","description":"Yanlin Wang is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"AMIN SAIED","type":"PERSON","description":"Amin Saied is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"WEIZHU CHEN","type":"PERSON","description":"Weizhu Chen is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NAN DUAN","type":"PERSON","description":"Nan Duan is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"JEFFREY ZHOU","type":"PERSON","description":"Jeffrey Zhou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"TIANJIAN LU","type":"PERSON","description":"Tianjian Lu is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SWAROOP MISHRA","type":"PERSON","description":"Swaroop Mishra is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SIDDHARTHA BRAHMA","type":"PERSON","description":"Siddhartha Brahma is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SUJOY BASU","type":"PERSON","description":"Sujoy Basu is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"YI LUAN","type":"PERSON","description":"Yi Luan is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LE HOU","type":"PERSON","description":"Le Hou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"ARGUMENT PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Argument Passage Generator is an agent adept at creating passages that articulate arguments, which may occasionally contain logical inconsistencies","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"DEBATE PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Debate Passage Generator is an agent that specializes in crafting passages that mimic the structure and content of debate transcripts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CONVERSATION PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Conversation Passage Generator is an agent that generates passages depicting dialogues","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"MEETING TRANSCRIPT GENERATOR","type":"TECHNOLOGY","description":"Meeting Transcript Generator is an agent designed to produce meeting transcripts","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"POEM GENERATOR","type":"TECHNOLOGY","description":"Poem Generator is an agent that generates poems","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"SATIRICAL PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Satirical Passage Generator is an agent that creates texts infused with satirical wit","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"INSTRUCTIONAL PASSAGE GENERATOR","type":"TECHNOLOGY","description":"Instructional Passage Generator is an agent that generates passages resembling instructional manuals","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LONG TEXT GENERATOR","type":"TECHNOLOGY","description":"Long Text Generator is an agent that extends the original text by incorporating additional information, thereby increasing its length","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"IDENTITY AGENT","type":"TECHNOLOGY","description":"Identity Agent is a straightforward agent that replicates the input text verbatim","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"LITERAL COMPREHENSION QUESTION","type":"TECHNOLOGY","description":"Literal Comprehension Question is a type of question that asks for specific details or facts clearly stated in the text","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"NUMERICAL DISCRETE REASONING","type":"TECHNOLOGY","description":"Numerical Discrete Reasoning is a type of question that requires the reader to use numerical reasoning over many facts from the text","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"CRITICAL COMPREHENSION QUESTION","type":"TECHNOLOGY","description":"Critical Comprehension Question is a type of question that constructs two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"},{"name":"EVALUATIVE COMPREHENSION QUESTION","type":"TECHNOLOGY","description":"Evaluative Comprehension Question is a type of question that requires the reader to assess and evaluate the content of the text","source_id":"f4e98ee0b7fb42428f3312f29cb444dd"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d6\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d5\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d3\" for=\"node\" attr.name=\"entity_type\" attr.type=\"string\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ILIA SHUMAILOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ilia Shumailov is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZAKHAR SHUMAYLOV\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zakhar Shumaylov is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIREN ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiren Zhao is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YARIN GAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yarin Gal is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NICOLAS PAPERNOT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nicolas Papernot is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ROSS ANDERSON\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ross Anderson is one of the authors of the paper titled \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MOHAMMED LATIF SIDDIQ\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mohammed Latif Siddiq is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIAHAO ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiahao Zhang is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LINDSAY RONEY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Lindsay Roney is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JOANNA C. S. SANTOS\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Joanna C. S. Santos is one of the authors of the paper titled \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MIRAC SUZGUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Mirac Suzgun is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NATHAN SCALES\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathan Scales is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NATHANAEL SCH&#196;RLI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nathanael Sch&#228;rli is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SEBASTIAN GEHRMANN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sebastian Gehrmann is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YI TAY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Tay is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"HYUNG WON CHUNG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Hyung Won Chung is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AAKANKSHA CHOWDHERY\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aakanksha Chowdhery is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QUOC V LE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Quoc V Le is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ED H CHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ed H Chi is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DENNY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Denny Zhou is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022Denny Zhou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>      <data key=\"d3\">PERSON<\/data>    <\/node>    <node id=\"JASON WEI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jason Wei is one of the authors of the paper titled \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEN WAI YIM\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wen wai Yim is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YUJUAN FU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yujuan Fu is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ASMA BEN ABACHA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Asma Ben Abacha is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NEAL SNIDER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Neal Snider is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"THOMAS LIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Thomas Lin is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MELIHA YETISGEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Meliha Yetisgen is one of the authors of the paper titled \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QINGYUN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingyun Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"GAGAN BANSAL\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Gagan Bansal is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIEYU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jieyu Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIRAN WU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiran Wu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"BEIBIN LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Beibin Li is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ERKANG ZHU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Erkang Zhu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LI JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Li Jiang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XIAOYUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiaoyun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SHAOKUN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shaokun Zhang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIALE LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiale Liu is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AHMED HASSAN AWADALLAH\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ahmed Hassan Awadallah is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RYEN W WHITE\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ryen W White is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DOUG BURGER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Doug Burger is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHI WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chi Wang is one of the authors of the paper titled \"Autogen: Enabling next-gen llm applications via multi-agent conversation\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CONGYING XIA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Congying Xia is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHEN XING\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chen Xing is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIANGSHU DU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiangshu Du is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XINYI YANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xinyi Yang is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIHAO FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yihao Feng is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ran Xu is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WENPENG YIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wenpeng Yin is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CAIMING XIONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Caiming Xiong is one of the authors of the paper titled \"Fofo: A benchmark to evaluate llms&#8217; format-following capability\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"GUANGZHI XIONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Guangzhi Xiong is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QIAO JIN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qiao Jin is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZHIYONG LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhiyong Lu is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AIDONG ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Aidong Zhang is one of the authors of the paper titled \"Benchmarking retrieval-augmented generation for medicine\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CAN XU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Can Xu is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"QINGFENG SUN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Qingfeng Sun is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"KAI ZHENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Kai Zheng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"XIUBO GENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Xiubo Geng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"PU ZHAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Pu Zhao is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JIAZHAN FENG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jiazhan Feng is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CHONGYANG TAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Chongyang Tao is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DAXIN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Daxin Jiang is one of the authors of the paper titled \"Wizardlm: Empowering large language models to follow complex instructions\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LONGHUI YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Longhui Yu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEISEN JIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weisen Jiang is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"HAN SHI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Han Shi is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JINCHENG YU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jincheng Yu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZHENGYING LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhengying Liu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YU ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yu Zhang is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JAMES T KWOK\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">James T Kwok is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ZHENGUO LI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Zhenguo Li is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ADRIAN WELLER\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Adrian Weller is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEIYANG LIU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weiyang Liu is one of the authors of the paper titled \"Metamath: Bootstrap your own mathematical questions for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIFAN ZHANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Zhang is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIFAN LUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yifan Luo is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ANDREW CHI-CHIH YAO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Andrew Chi-Chih Yao is one of the authors of the paper titled \"Automathtext: Autonomous data selection with language models for mathematical texts\" published in 2024<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WANJUN ZHONG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Wanjun Zhong is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"RUIXIANG CUI\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Ruixiang Cui is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YIDUO GUO\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yiduo Guo is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YAOBO LIANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yaobo Liang is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SHUAI LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Shuai Lu is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YANLIN WANG\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yanlin Wang is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"AMIN SAIED\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Amin Saied is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"WEIZHU CHEN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Weizhu Chen is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NAN DUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Nan Duan is one of the authors of the paper titled \"Agieval: A human-centric benchmark for evaluating foundation models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"JEFFREY ZHOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Jeffrey Zhou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"TIANJIAN LU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Tianjian Lu is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SWAROOP MISHRA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Swaroop Mishra is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SIDDHARTHA BRAHMA\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Siddhartha Brahma is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SUJOY BASU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Sujoy Basu is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"YI LUAN\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Yi Luan is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LE HOU\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Le Hou is one of the authors of the paper titled \"Instruction-following evaluation for large language models\" published in 2023<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"ARGUMENT PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Argument Passage Generator is an agent adept at creating passages that articulate arguments, which may occasionally contain logical inconsistencies<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"DEBATE PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Debate Passage Generator is an agent that specializes in crafting passages that mimic the structure and content of debate transcripts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CONVERSATION PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Conversation Passage Generator is an agent that generates passages depicting dialogues<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"MEETING TRANSCRIPT GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meeting Transcript Generator is an agent designed to produce meeting transcripts<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"POEM GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Poem Generator is an agent that generates poems<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"SATIRICAL PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Satirical Passage Generator is an agent that creates texts infused with satirical wit<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"INSTRUCTIONAL PASSAGE GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Instructional Passage Generator is an agent that generates passages resembling instructional manuals<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LONG TEXT GENERATOR\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Long Text Generator is an agent that extends the original text by incorporating additional information, thereby increasing its length<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"IDENTITY AGENT\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Identity Agent is a straightforward agent that replicates the input text verbatim<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"LITERAL COMPREHENSION QUESTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Literal Comprehension Question is a type of question that asks for specific details or facts clearly stated in the text<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Numerical Discrete Reasoning is a type of question that requires the reader to use numerical reasoning over many facts from the text<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Critical Comprehension Question is a type of question that constructs two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Evaluative Comprehension Question is a type of question that requires the reader to assess and evaluate the content of the text<\/data>      <data key=\"d2\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/node>    <edge source=\"ILIA SHUMAILOV\" target=\"ZAKHAR SHUMAYLOV\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ilia Shumailov and Zakhar Shumaylov co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ILIA SHUMAILOV\" target=\"YIREN ZHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ilia Shumailov and Yiren Zhao co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ILIA SHUMAILOV\" target=\"YARIN GAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ilia Shumailov and Yarin Gal co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ILIA SHUMAILOV\" target=\"NICOLAS PAPERNOT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ilia Shumailov and Nicolas Papernot co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ILIA SHUMAILOV\" target=\"ROSS ANDERSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Ilia Shumailov and Ross Anderson co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ZAKHAR SHUMAYLOV\" target=\"YIREN ZHAO\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zakhar Shumaylov and Yiren Zhao co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ZAKHAR SHUMAYLOV\" target=\"YARIN GAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zakhar Shumaylov and Yarin Gal co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ZAKHAR SHUMAYLOV\" target=\"NICOLAS PAPERNOT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zakhar Shumaylov and Nicolas Papernot co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"ZAKHAR SHUMAYLOV\" target=\"ROSS ANDERSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Zakhar Shumaylov and Ross Anderson co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"YIREN ZHAO\" target=\"YARIN GAL\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yiren Zhao and Yarin Gal co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"YIREN ZHAO\" target=\"NICOLAS PAPERNOT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yiren Zhao and Nicolas Papernot co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"YIREN ZHAO\" target=\"ROSS ANDERSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yiren Zhao and Ross Anderson co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"YARIN GAL\" target=\"NICOLAS PAPERNOT\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yarin Gal and Nicolas Papernot co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"YARIN GAL\" target=\"ROSS ANDERSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Yarin Gal and Ross Anderson co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NICOLAS PAPERNOT\" target=\"ROSS ANDERSON\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nicolas Papernot and Ross Anderson co-authored the paper \"The curse of recursion: Training on generated data makes models forget\" published in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MOHAMMED LATIF SIDDIQ\" target=\"JIAHAO ZHANG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mohammed Latif Siddiq and Jiahao Zhang co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MOHAMMED LATIF SIDDIQ\" target=\"LINDSAY RONEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mohammed Latif Siddiq and Lindsay Roney co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MOHAMMED LATIF SIDDIQ\" target=\"JOANNA C. S. SANTOS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mohammed Latif Siddiq and Joanna C. S. Santos co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"JIAHAO ZHANG\" target=\"LINDSAY RONEY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jiahao Zhang and Lindsay Roney co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"JIAHAO ZHANG\" target=\"JOANNA C. S. SANTOS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Jiahao Zhang and Joanna C. S. Santos co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"LINDSAY RONEY\" target=\"JOANNA C. S. SANTOS\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Lindsay Roney and Joanna C. S. Santos co-authored the paper \"Re(gex|dos)eval: Evaluating generated regular expressions and their proneness to dos attacks\" published in the Proceedings of the 46th International Conference on Software Engineering, NIER Track (ICSE-NIER &#8217;24) in 2024<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"NATHAN SCALES\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Nathan Scales co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"NATHANAEL SCH&#196;RLI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Nathanael Sch&#228;rli co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"SEBASTIAN GEHRMANN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Sebastian Gehrmann co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"YI TAY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Yi Tay co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"HYUNG WON CHUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Hyung Won Chung co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"AAKANKSHA CHOWDHERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Aakanksha Chowdhery co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"QUOC V LE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Quoc V Le co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"ED H CHI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Ed H Chi co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"DENNY ZHOU\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Denny Zhou co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"MIRAC SUZGUN\" target=\"JASON WEI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Mirac Suzgun and Jason Wei co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"NATHANAEL SCH&#196;RLI\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Nathanael Sch&#228;rli co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"SEBASTIAN GEHRMANN\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Sebastian Gehrmann co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"YI TAY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Yi Tay co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"HYUNG WON CHUNG\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Hyung Won Chung co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"AAKANKSHA CHOWDHERY\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Aakanksha Chowdhery co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>    <edge source=\"NATHAN SCALES\" target=\"QUOC V LE\">      <data key=\"d4\">8.0<\/data>      <data key=\"d5\">Nathan Scales and Quoc V Le co-authored the paper \"Challenging big-bench tasks and whether chain-of-thought can solve them\" published in 2022<\/data>      <data key=\"d6\">f4e98ee0b7fb42428f3312f29cb444dd<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"5819b66e04fd77fa705574edc49395bb","chunk":" for a\nspecific detail(s) or fact(s) clearly stated in the text.\n2.Numerical Discrete Reasoning (Reasoning): questions that require the reader to use\nnumerical reasoning over many facts from the text.\n3.Critical Comprehension Question (True\/False): construct two statements about the\npurpose or point of view that the reader must assess as true or false, with one being\ntrue and the other false.\n4.Evaluative Comprehension Question (Essay): an open-ended question that prompts\nan in-depth analysis of the text\u2019s theme or the effectiveness of an argument.\n5.Vocabulary and Language Use (Fill-in-the-Blank): a fill-in-the-blank question that\ntests understanding of a particular word or phrase used in the text.\n6.Relationship Comprehension Question (Matching): a matching question where\nrespondents pair items based on a specific criterion.\n7.Sequencing Events (Ordering): a series of events from the text arranged in the\ncorrect chronological order.\n8. Strengthen: identify information that would make the argument\u2019s conclusion more\nlikely to be true.\n9.Weaken: find evidence or an argument that would make the conclusion less likely to\nbe true.\n10.Assumption (Necessary Assumption): determine what must be true for the argument\nto hold.\n11. Flaw: point out a mistake in the argument\u2019s reasoning.\n12.Inference (Must Be True): Choose an option that logically follows from the informa-\ntion provided.\n13.Principle (Identify the Principle): Recognize the general rule or principle that\nunderlies the argument.\n14.Method of Reasoning (Describe the Argument): Describe how the argument is\nconstructed logically.\n15.Resolve the Paradox: Offer an explanation that reconciles seemingly contradictory\ninformation.\n26A.2 Text Modification Flow\nInstruction Taxonomy for Seed Instruction Generation Flow\n1.Paraphrasing: Rewriting text using different words and sentence structures while\nmaintaining the original meaning.\n2.Text Simplification: Making text easier to read and understand by using simpler\nwords and sentence structures, often for children or language learners.\n3.Text Expansion: Adding more information or detail to make text more comprehensive\nor to meet a certain word count.\n4.Text Translation: Converting text from one language to another while attempting\nto preserve the original meaning as closely as possible.\n5.Text Formatting: Altering the appearance of text to improve readability or for\nstylistic purposes.\n6.Sentiment Modification: Changing the tone of the text to alter its emotional impact,\nsuch as making a sentence sound more positive or negative.\n7.Text Annotation: Adding notes, comments, or explanations to a text, often for the\npurpose of analysis or to provide additional context.\n8.Keyword Replacement: Substituting specific words or phrases with synonyms or\nrelated terms.\n9. Text Removing: Redacting or removing content from text.\n10.Text Capitalization: Adjusting the case of letters in text, such as converting to\nuppercase, lowercase, title case, or sentence case, starting every sentence with a\nparticular letter, word.\n11.Text Styling: Applying styles like bold, italics, underline, etc., to emphasize certain\nparts of the text or for aesthetic purposes.\n12.Content Rewriting: Extensively modifying a text to produce a new version, which\ncould involve changing the perspective, style, or target audience.\n13.Data Normalization: Standardizing text to ensure consistency, such as converting\ndates and times to a standard format or unifying the spelling of words.\n14.Plagiarism Rewording: Altering text to avoid plagiarism, ensuring that the content\nis original.\n15.Code Switching: Alternating between languages or dialects within a text, often to\nreflect bilingual speakers\u2019 patterns or for creative writing.\n16.Text Obfuscation: Intentionally making text vague or harder to understand, some-\ntimes for security purposes (like masking personal data).\n17.Textual Entailment: Modifying a sentence or phrase to either entail or contradict\nanother sentence, often used in natural language processing tasks.\n18.Rewriting with vocabulary limitations: Rewriting the entire text or a piece of it\nwhile using a limited vocabulary. For example, all words should start with letter \u2019a\u2019,\nall n-th word should start with letter \u2019b\u2019, each sentence should start with a \u2019vowel\u2019,\netc.\nB Evaluation Details\nThe types of tasks\/benchmarks and the corresponding method used to extract answer and\ngenerate metrics is specified below:\n\u2022Multiple Choice Questions : All the models are evaluated in an open-ended\ngeneration setting with an empty system message We then use GPT-4 for extraction\nof the option selected by the model from model\u2019s response instead of regex based\nextraction done in [ 18]. The extracted prediction is matched with the ground truth\nto generate accuracy scores.\nThe system message used for the GPT-4 extractions is as follows:\n27MCQ GPT-4 Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\nquestion, answer options and a student\u2019s response. Your task is to parse the\noption student selected in their response as their final answer and return the\nalphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2.","chunk_id":"5819b66e04fd77fa705574edc49395bb","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"NUMERICAL DISCRETE REASONING","type":"QUESTION TYPE","description":"Numerical Discrete Reasoning questions require the reader to use numerical reasoning over many facts from the text","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CRITICAL COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Critical Comprehension Questions involve constructing two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATIVE COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Evaluative Comprehension Questions are open-ended questions that prompt an in-depth analysis of the text\u2019s theme or the effectiveness of an argument","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"VOCABULARY AND LANGUAGE USE","type":"QUESTION TYPE","description":"Vocabulary and Language Use questions are fill-in-the-blank questions that test understanding of a particular word or phrase used in the text","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"RELATIONSHIP COMPREHENSION QUESTION","type":"QUESTION TYPE","description":"Relationship Comprehension Questions are matching questions where respondents pair items based on a specific criterion","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SEQUENCING EVENTS","type":"QUESTION TYPE","description":"Sequencing Events questions involve arranging a series of events from the text in the correct chronological order","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"STRENGTHEN","type":"QUESTION TYPE","description":"Strengthen questions require identifying information that would make the argument\u2019s conclusion more likely to be true","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"WEAKEN","type":"QUESTION TYPE","description":"Weaken questions involve finding evidence or an argument that would make the conclusion less likely to be true","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"ASSUMPTION","type":"QUESTION TYPE","description":"Assumption questions require determining what must be true for the argument to hold","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"FLAW","type":"QUESTION TYPE","description":"Flaw questions involve pointing out a mistake in the argument\u2019s reasoning","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"INFERENCE","type":"QUESTION TYPE","description":"Inference questions require choosing an option that logically follows from the information provided","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PRINCIPLE","type":"QUESTION TYPE","description":"Principle questions involve recognizing the general rule or principle that underlies the argument","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"METHOD OF REASONING","type":"QUESTION TYPE","description":"Method of Reasoning questions involve describing how the argument is constructed logically","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"RESOLVE THE PARADOX","type":"QUESTION TYPE","description":"Resolve the Paradox questions involve offering an explanation that reconciles seemingly contradictory information","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PARAPHRASING","type":"TEXT MODIFICATION","description":"Paraphrasing involves rewriting text using different words and sentence structures while maintaining the original meaning","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT SIMPLIFICATION","type":"TEXT MODIFICATION","description":"Text Simplification involves making text easier to read and understand by using simpler words and sentence structures, often for children or language learners","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT EXPANSION","type":"TEXT MODIFICATION","description":"Text Expansion involves adding more information or detail to make text more comprehensive or to meet a certain word count","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT TRANSLATION","type":"TEXT MODIFICATION","description":"Text Translation involves converting text from one language to another while attempting to preserve the original meaning as closely as possible","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT FORMATTING","type":"TEXT MODIFICATION","description":"Text Formatting involves altering the appearance of text to improve readability or for stylistic purposes","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SENTIMENT MODIFICATION","type":"TEXT MODIFICATION","description":"Sentiment Modification involves changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT ANNOTATION","type":"TEXT MODIFICATION","description":"Text Annotation involves adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"KEYWORD REPLACEMENT","type":"TEXT MODIFICATION","description":"Keyword Replacement involves substituting specific words or phrases with synonyms or related terms","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT REMOVING","type":"TEXT MODIFICATION","description":"Text Removing involves redacting or removing content from text","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT CAPITALIZATION","type":"TEXT MODIFICATION","description":"Text Capitalization involves adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT STYLING","type":"TEXT MODIFICATION","description":"Text Styling involves applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CONTENT REWRITING","type":"TEXT MODIFICATION","description":"Content Rewriting involves extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"DATA NORMALIZATION","type":"TEXT MODIFICATION","description":"Data Normalization involves standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"PLAGIARISM REWORDING","type":"TEXT MODIFICATION","description":"Plagiarism Rewording involves altering text to avoid plagiarism, ensuring that the content is original","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"CODE SWITCHING","type":"TEXT MODIFICATION","description":"Code Switching involves alternating between languages or dialects within a text, often to reflect bilingual speakers\u2019 patterns or for creative writing","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT OBFUSCATION","type":"TEXT MODIFICATION","description":"Text Obfuscation involves intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXTUAL ENTAILMENT","type":"TEXT MODIFICATION","description":"Textual Entailment involves modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"REWRITING WITH VOCABULARY LIMITATIONS","type":"TEXT MODIFICATION","description":"Rewriting with Vocabulary Limitations involves rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"MULTIPLE CHOICE QUESTIONS","type":"EVALUATION METHOD","description":"Multiple Choice Questions are evaluated in an open-ended generation setting with an empty system message, and GPT-4 is used for extraction of the option selected by the model from the model\u2019s response","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is used for extracting the option selected by the model from the model\u2019s response in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"TEXT MODIFICATION FLOW","type":"PROCESS","description":"Text Modification Flow is a process that involves various techniques for modifying text to generate seed instructions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"INSTRUCTION TAXONOMY","type":"PROCESS","description":"Instruction Taxonomy is a classification system used for generating seed instructions in the Text Modification Flow","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"SEED INSTRUCTION GENERATION","type":"PROCESS","description":"Seed Instruction Generation is a process that involves creating initial instructions using various text modification techniques","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EVALUATION DETAILS","type":"PROCESS","description":"Evaluation Details specify the types of tasks and benchmarks used to extract answers and generate metrics","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"EXTRACTION SYSTEM MESSAGE","type":"TECHNOLOGY","description":"The Extraction System Message is a system message used by GPT-4 to extract the option selected by the model from the model\u2019s response in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"GPT-4 EXTRACTION","type":"TECHNOLOGY","description":"GPT-4 Extraction is the process of using GPT-4 to extract the option selected by the model from the model\u2019s response in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"STUDENT RESPONSE","type":"DATA","description":"Student Response is the answer provided by a student, which is parsed by GPT-4 to extract the selected option in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"QUESTION","type":"DATA","description":"Question is the prompt provided to the student, which is used along with the student\u2019s response to extract the selected option in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"},{"name":"ANSWER OPTIONS","type":"DATA","description":"Answer Options are the possible choices provided to the student, from which the selected option is extracted in the evaluation of Multiple Choice Questions","source_id":"5819b66e04fd77fa705574edc49395bb"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"NUMERICAL DISCRETE REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Numerical Discrete Reasoning questions require the reader to use numerical reasoning over many facts from the text<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Critical Comprehension Questions involve constructing two statements about the purpose or point of view that the reader must assess as true or false, with one being true and the other false<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATIVE COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Evaluative Comprehension Questions are open-ended questions that prompt an in-depth analysis of the text&#8217;s theme or the effectiveness of an argument<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"VOCABULARY AND LANGUAGE USE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Vocabulary and Language Use questions are fill-in-the-blank questions that test understanding of a particular word or phrase used in the text<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"RELATIONSHIP COMPREHENSION QUESTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Relationship Comprehension Questions are matching questions where respondents pair items based on a specific criterion<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SEQUENCING EVENTS\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Sequencing Events questions involve arranging a series of events from the text in the correct chronological order<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"STRENGTHEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Strengthen questions require identifying information that would make the argument&#8217;s conclusion more likely to be true<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"WEAKEN\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Weaken questions involve finding evidence or an argument that would make the conclusion less likely to be true<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"ASSUMPTION\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Assumption questions require determining what must be true for the argument to hold<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"FLAW\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Flaw questions involve pointing out a mistake in the argument&#8217;s reasoning<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"INFERENCE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Inference questions require choosing an option that logically follows from the information provided<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PRINCIPLE\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Principle questions involve recognizing the general rule or principle that underlies the argument<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"METHOD OF REASONING\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Method of Reasoning questions involve describing how the argument is constructed logically<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"RESOLVE THE PARADOX\">      <data key=\"d0\">QUESTION TYPE<\/data>      <data key=\"d1\">Resolve the Paradox questions involve offering an explanation that reconciles seemingly contradictory information<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PARAPHRASING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Paraphrasing involves rewriting text using different words and sentence structures while maintaining the original meaning<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT SIMPLIFICATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Simplification involves making text easier to read and understand by using simpler words and sentence structures, often for children or language learners<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT EXPANSION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Expansion involves adding more information or detail to make text more comprehensive or to meet a certain word count<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT TRANSLATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Translation involves converting text from one language to another while attempting to preserve the original meaning as closely as possible<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT FORMATTING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Formatting involves altering the appearance of text to improve readability or for stylistic purposes<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SENTIMENT MODIFICATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Sentiment Modification involves changing the tone of the text to alter its emotional impact, such as making a sentence sound more positive or negative<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT ANNOTATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Annotation involves adding notes, comments, or explanations to a text, often for the purpose of analysis or to provide additional context<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"KEYWORD REPLACEMENT\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Keyword Replacement involves substituting specific words or phrases with synonyms or related terms<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT REMOVING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Removing involves redacting or removing content from text<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT CAPITALIZATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Capitalization involves adjusting the case of letters in text, such as converting to uppercase, lowercase, title case, or sentence case<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT STYLING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Styling involves applying styles like bold, italics, underline, etc., to emphasize certain parts of the text or for aesthetic purposes<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CONTENT REWRITING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Content Rewriting involves extensively modifying a text to produce a new version, which could involve changing the perspective, style, or target audience<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"DATA NORMALIZATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Data Normalization involves standardizing text to ensure consistency, such as converting dates and times to a standard format or unifying the spelling of words<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"PLAGIARISM REWORDING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Plagiarism Rewording involves altering text to avoid plagiarism, ensuring that the content is original<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"CODE SWITCHING\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Code Switching involves alternating between languages or dialects within a text, often to reflect bilingual speakers&#8217; patterns or for creative writing<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT OBFUSCATION\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Text Obfuscation involves intentionally making text vague or harder to understand, sometimes for security purposes like masking personal data<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXTUAL ENTAILMENT\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Textual Entailment involves modifying a sentence or phrase to either entail or contradict another sentence, often used in natural language processing tasks<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"REWRITING WITH VOCABULARY LIMITATIONS\">      <data key=\"d0\">TEXT MODIFICATION<\/data>      <data key=\"d1\">Rewriting with Vocabulary Limitations involves rewriting the entire text or a piece of it while using a limited vocabulary, such as all words starting with a specific letter<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"MULTIPLE CHOICE QUESTIONS\">      <data key=\"d0\">EVALUATION METHOD<\/data>      <data key=\"d1\">Multiple Choice Questions are evaluated in an open-ended generation setting with an empty system message, and GPT-4 is used for extraction of the option selected by the model from the model&#8217;s response<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is used for extracting the option selected by the model from the model&#8217;s response in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"TEXT MODIFICATION FLOW\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Text Modification Flow is a process that involves various techniques for modifying text to generate seed instructions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"INSTRUCTION TAXONOMY\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Instruction Taxonomy is a classification system used for generating seed instructions in the Text Modification Flow<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"SEED INSTRUCTION GENERATION\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Seed Instruction Generation is a process that involves creating initial instructions using various text modification techniques<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EVALUATION DETAILS\">      <data key=\"d0\">PROCESS<\/data>      <data key=\"d1\">Evaluation Details specify the types of tasks and benchmarks used to extract answers and generate metrics<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">The Extraction System Message is a system message used by GPT-4 to extract the option selected by the model from the model&#8217;s response in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"GPT-4 EXTRACTION\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 Extraction is the process of using GPT-4 to extract the option selected by the model from the model&#8217;s response in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"STUDENT RESPONSE\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Student Response is the answer provided by a student, which is parsed by GPT-4 to extract the selected option in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Question is the prompt provided to the student, which is used along with the student&#8217;s response to extract the selected option in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <node id=\"ANSWER OPTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Answer Options are the possible choices provided to the student, from which the selected option is extracted in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d2\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/node>    <edge source=\"NUMERICAL DISCRETE REASONING\" target=\"CRITICAL COMPREHENSION QUESTION\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both are types of questions used to assess comprehension and reasoning skills<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"EVALUATIVE COMPREHENSION QUESTION\" target=\"VOCABULARY AND LANGUAGE USE\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both are types of questions used to assess different aspects of text comprehension<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"RELATIONSHIP COMPREHENSION QUESTION\" target=\"SEQUENCING EVENTS\">      <data key=\"d3\">5.0<\/data>      <data key=\"d4\">Both are types of questions used to assess understanding of relationships and sequences in the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"STRENGTHEN\" target=\"WEAKEN\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are types of questions used to assess the strength of an argument<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"ASSUMPTION\" target=\"FLAW\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are types of questions used to assess the logical structure of an argument<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"INFERENCE\" target=\"PRINCIPLE\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are types of questions used to assess the underlying logic and principles in the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"METHOD OF REASONING\" target=\"RESOLVE THE PARADOX\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are types of questions used to assess the logical construction and resolution of contradictions in the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"PARAPHRASING\" target=\"TEXT SIMPLIFICATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Both are text modification techniques used to alter the text while maintaining its original meaning<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT EXPANSION\" target=\"TEXT TRANSLATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to add information or convert text while preserving its meaning<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT FORMATTING\" target=\"SENTIMENT MODIFICATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to alter the appearance or tone of the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT ANNOTATION\" target=\"KEYWORD REPLACEMENT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to add context or substitute specific words<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT REMOVING\" target=\"TEXT CAPITALIZATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to alter the content or case of the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT STYLING\" target=\"CONTENT REWRITING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to emphasize or extensively modify the text<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"DATA NORMALIZATION\" target=\"PLAGIARISM REWORDING\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to ensure consistency or originality<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"CODE SWITCHING\" target=\"TEXT OBFUSCATION\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to alternate languages or make text harder to understand<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXTUAL ENTAILMENT\" target=\"REWRITING WITH VOCABULARY LIMITATIONS\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Both are text modification techniques used to modify text with specific constraints<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"GPT-4\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used for extracting the option selected by the model in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"EVALUATION DETAILS\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Evaluation Details specify the method used to extract answers and generate metrics for Multiple Choice Questions<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"MULTIPLE CHOICE QUESTIONS\" target=\"GPT-4 EXTRACTION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 Extraction is used to extract the option selected by the model in the evaluation of Multiple Choice Questions<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The Extraction System Message is used by GPT-4 to extract the option selected by the model<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"STUDENT RESPONSE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 parses the Student Response to extract the selected option<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"TEXT MODIFICATION FLOW\" target=\"INSTRUCTION TAXONOMY\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Instruction Taxonomy is part of the Text Modification Flow process<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"INSTRUCTION TAXONOMY\" target=\"SEED INSTRUCTION GENERATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Instruction Taxonomy is used in the Seed Instruction Generation process<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"QUESTION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Question and Student Response are used together to extract the selected option<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"ANSWER OPTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">The Answer Options and Student Response are used together to extract the selected option<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"ANSWER OPTIONS\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The Answer Options are provided along with the Question to the student<\/data>      <data key=\"d5\">5819b66e04fd77fa705574edc49395bb<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"103d98395c393552cc954c89d4e59f50","chunk":"alphabet ID of that answer in the provided options. If the student gave multiple\nanswers return them as a list.\nUse the following format:\nParsed Student Answer: Final answer extracted from Student\u2019s response. This\nshould only be the alphabets representing the option the student chose.\nExample 1:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect, so is 2. 3 seems incorrect as well. I think 1 is the correct\nfinal answer.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: B\nExample 2:\nInput :\nQuestion:\nFind all c in Z3such that Z3[x]\/(x2+c)is a field.\nStudent Response :\nI think 0 is incorrect. 3 seems incorrect as well. I think 1 and 2 could be the\ncorrect final answers.\nOptions :\n[(A) 0, (B) 1, (C) 2, (D) 3 ]\nOutput:\nParsed Student Answer: [B,C]\n\u2022Exact Match\/Span Extraction Problems : For tasks with math based questions\nlike GSM8K and problems where a ground-truth answer value is given (like DROP),\nwe prompt the models being evaluated to generate the answer and use GPT-4\nto extract the exact answer and also match it with the ground-truth provided to\nproduce a final verdict of whether the model\u2019s answer was \u2019Correct\u2019 or \u2019Incorrect\u2019.\nWe use a specific system message for maths based questions, and another for all the\nother exact match\/span extraction problems, both of which are provided below.\nMaths GPT-4 Extraction System Message\nAs an expert Math teacher, your role is to evaluate a student\u2019s answer to a\nword problem. The problem is accompanied by a correct solution provided by\nthe problem setter. It is important to remember that there may be various\nmethods to solve a word problem, so the student\u2019s steps might not always align\nwith those in the problem setter\u2019s solution. However, the final answer, typically\na number, should be unique and match the problem setter\u2019s answer.\nUse the following format:\nError Analysis:\n28In one sentence, extract the final answer from the problem setter\u2019s solution and\ncompare it with the student\u2019s answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\nGeneral Extraction System Message\nYou are an Evaluator Assistant. You support the exam evaluator by parsing\nstudent responses. You are an unbiased Evaluator and do not rely on your\nknowledge but stick to the user provided context. You are provided with the\ncorrect answer and a student\u2019s response. Your task is to parse the answer from\nstudent\u2019s response and then match it with the correct answer. If the student\u2019s\nfinal answer matches the correct answer provided, output a \u2019Correct\u2019, else an\n\u2019Incorrect\u2019.\nPlease rely strictly on the correct answer given in the context only.\nUse the following format:\nError Analysis:\nInonesentence, extractthefinalanswerfromthestudent\u2019ssolutionandcompare\nit with the correct answer. Do they match?\nFinal Verdict:\nCorrect\/Incorrect\n\u2022EQBench : For EQBench, we prompt the models to generate the emotion scores\ngiven the conversation in the prompt and then use GPT-4 to extract the scores\ngenerated by the model for each emotion in the prompt. The metric scores are\ngenerated using both the version 1 and 2 implementations described in the EQBench\npaper and the creators\u2019 github repository. The scoring calculation is calibrated such\nthat a score of 0 corresponds to answering randomly, and a 100 would denote perfect\nalignment with the reference answer. The system message used for extraction of\nemotion scores from evaluated model\u2019s response using GPT-4 is given below:\nEQBench GPT-4 Extraction System Message\nYou are a helpful assistant. You will be given a student agent response which\nwill consist of possible emotions and a score from 0-10 for each of those emotions,\nfollowed by a step by step critique and then revised scores in the following\nformat, First pass scores:\nEmotion1: <score>\nEmotion2: <score>\nEmotion3: <score>\nEmotion4: <score>\nCritique: <your critique here>\nRevised scores:\nEmotion1: <revised score>\nEmotion2: <revised score>\nEmotion3: <revised score>\nEmotion4: <revised score>\n[End of answer]\nRemember: zero is a valid score as well.\nYou will also be provided with the Emotions. Your task is to parse the Revised\nscores for each of the emotions from the student agent response. Return the\nrevised scores in the student agent response for the emotions in the following\nformat:\n\"Emotion1\" : \"Score\",\n\"Emotion2\" : \"Score\",\n\"Emotion3\" : \"Score\",\n29\"Emotion4\" : \"Score\"\nFor example:\nInput\nStudent Agent Response:\nFirst pass scores:\nResigned: 8\nAngry: 2\nHopeful: 4\nEmbarrassed: 9\nCritique:\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHope","chunk_id":"103d98395c393552cc954c89d4e59f50","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1200,"entities":[{"name":"PARSED STUDENT ANSWER","type":"CONCEPT","description":"Parsed Student Answer is the final answer extracted from the student's response, represented by the alphabet(s) corresponding to the option(s) the student chose","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EXACT MATCH\/SPAN EXTRACTION PROBLEMS","type":"CONCEPT","description":"Exact Match\/Span Extraction Problems are tasks where the model generates an answer and matches it with a ground-truth answer to determine correctness","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"GSM8K","type":"DATASET","description":"GSM8K is a dataset used for math-based questions where models are evaluated on their ability to generate correct answers","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"DROP","type":"DATASET","description":"DROP is a dataset where models are evaluated on their ability to generate correct answers for given problems","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE","type":"INSTRUCTION","description":"Maths GPT-4 Extraction System Message is a specific system message used to evaluate a student's answer to a math word problem by comparing it with the problem setter's solution","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"GENERAL EXTRACTION SYSTEM MESSAGE","type":"INSTRUCTION","description":"General Extraction System Message is a system message used to parse student responses and match them with the correct answer provided in the context","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EQBENCH","type":"DATASET","description":"EQBench is a dataset used to evaluate models on their ability to generate emotion scores based on conversations","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE","type":"INSTRUCTION","description":"EQBench GPT-4 Extraction System Message is a system message used to extract emotion scores from a model's response in the EQBench dataset","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"STUDENT","type":"PERSON","description":"The student is the individual whose responses are being evaluated and parsed","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"QUESTION","type":"CONCEPT","description":"The question is the problem or prompt given to the student to solve or respond to","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"STUDENT RESPONSE","type":"CONCEPT","description":"The student response is the answer or explanation provided by the student in response to the question","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"OPTIONS","type":"CONCEPT","description":"Options are the multiple-choice answers provided to the student for selection","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"FINAL ANSWER","type":"CONCEPT","description":"The final answer is the student's selected option(s) after considering all possibilities","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"META AGENT SEARCH","type":"TECHNOLOGY","description":"Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"BASELINES","type":"CONCEPT","description":"Baselines are the initial seeds used in Meta Agent Search to discover better-performing agents","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"MODEL","type":"CONCEPT","description":"The model is the AI system being evaluated for its ability to generate correct answers or emotion scores","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"MODEL'S ANSWER","type":"CONCEPT","description":"The model's answer is the response generated by the AI model being evaluated","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"GROUND-TRUTH ANSWER","type":"CONCEPT","description":"The ground-truth answer is the correct answer provided by the problem setter for comparison with the model's answer","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"FINAL VERDICT","type":"CONCEPT","description":"The final verdict is the determination of whether the student's or model's answer is correct or incorrect","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"EMOTION","type":"CONCEPT","description":"Emotion is a feeling or reaction that is scored in the EQBench dataset","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"SCORE","type":"CONCEPT","description":"The score is the numeric value assigned to an emotion in the EQBench dataset","source_id":"103d98395c393552cc954c89d4e59f50"},{"name":"CRITIQUE","type":"CONCEPT","description":"The critique is the step-by-step analysis provided to explain the revised scores in the EQBench dataset","source_id":"103d98395c393552cc954c89d4e59f50"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"PARSED STUDENT ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Parsed Student Answer is the final answer extracted from the student's response, represented by the alphabet(s) corresponding to the option(s) the student chose<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Exact Match\/Span Extraction Problems are tasks where the model generates an answer and matches it with a ground-truth answer to determine correctness<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"GSM8K\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">GSM8K is a dataset used for math-based questions where models are evaluated on their ability to generate correct answers<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"DROP\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">DROP is a dataset where models are evaluated on their ability to generate correct answers for given problems<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">Maths GPT-4 Extraction System Message is a specific system message used to evaluate a student's answer to a math word problem by comparing it with the problem setter's solution<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">General Extraction System Message is a system message used to parse student responses and match them with the correct answer provided in the context<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EQBENCH\">      <data key=\"d0\">DATASET<\/data>      <data key=\"d1\">EQBench is a dataset used to evaluate models on their ability to generate emotion scores based on conversations<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d0\">INSTRUCTION<\/data>      <data key=\"d1\">EQBench GPT-4 Extraction System Message is a system message used to extract emotion scores from a model's response in the EQBench dataset<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"STUDENT\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">The student is the individual whose responses are being evaluated and parsed<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"QUESTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The question is the problem or prompt given to the student to solve or respond to<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"STUDENT RESPONSE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The student response is the answer or explanation provided by the student in response to the question<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"OPTIONS\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Options are the multiple-choice answers provided to the student for selection<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"FINAL ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The final answer is the student's selected option(s) after considering all possibilities<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"META AGENT SEARCH\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">Meta Agent Search is a method that uses all baselines as initial seeds in the archive to progressively discover agents that perform better than state-of-the-art hand-designed baselines<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"BASELINES\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Baselines are the initial seeds used in Meta Agent Search to discover better-performing agents<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"MODEL\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The model is the AI system being evaluated for its ability to generate correct answers or emotion scores<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"MODEL'S ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The model's answer is the response generated by the AI model being evaluated<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"GROUND-TRUTH ANSWER\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The ground-truth answer is the correct answer provided by the problem setter for comparison with the model's answer<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"FINAL VERDICT\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The final verdict is the determination of whether the student's or model's answer is correct or incorrect<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"EMOTION\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">Emotion is a feeling or reaction that is scored in the EQBench dataset<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"SCORE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The score is the numeric value assigned to an emotion in the EQBench dataset<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <node id=\"CRITIQUE\">      <data key=\"d0\">CONCEPT<\/data>      <data key=\"d1\">The critique is the step-by-step analysis provided to explain the revised scores in the EQBench dataset<\/data>      <data key=\"d2\">103d98395c393552cc954c89d4e59f50<\/data>    <\/node>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\" target=\"GSM8K\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GSM8K is a dataset used for Exact Match\/Span Extraction Problems involving math-based questions<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\" target=\"DROP\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">DROP is a dataset used for Exact Match\/Span Extraction Problems where a ground-truth answer is provided<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\" target=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Maths GPT-4 Extraction System Message is used for evaluating math-based Exact Match\/Span Extraction Problems<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\" target=\"GENERAL EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">General Extraction System Message is used for evaluating non-math Exact Match\/Span Extraction Problems<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EXACT MATCH\/SPAN EXTRACTION PROBLEMS\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model is evaluated on its ability to generate correct answers in Exact Match\/Span Extraction Problems<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GSM8K\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model is evaluated on its ability to generate correct answers in the GSM8K dataset<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"DROP\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model is evaluated on its ability to generate correct answers in the DROP dataset<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"MATHS GPT-4 EXTRACTION SYSTEM MESSAGE\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model's answers to math problems are evaluated using the Maths GPT-4 Extraction System Message<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GENERAL EXTRACTION SYSTEM MESSAGE\" target=\"MODEL\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">The model's answers to non-math problems are evaluated using the General Extraction System Message<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EQBENCH\" target=\"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">EQBench GPT-4 Extraction System Message is used to extract emotion scores from responses in the EQBench dataset<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EQBENCH\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model is evaluated on its ability to generate emotion scores in the EQBench dataset<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EQBENCH GPT-4 EXTRACTION SYSTEM MESSAGE\" target=\"MODEL\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The model's emotion scores are extracted using the EQBench GPT-4 Extraction System Message<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"STUDENT\" target=\"STUDENT RESPONSE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The student provides the student response to the question<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"QUESTION\" target=\"STUDENT RESPONSE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The student response is given in reply to the question<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"STUDENT RESPONSE\" target=\"FINAL ANSWER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The final answer is extracted from the student response<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"OPTIONS\" target=\"FINAL ANSWER\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The final answer is selected from the provided options<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"META AGENT SEARCH\" target=\"BASELINES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Meta Agent Search uses baselines as initial seeds to discover better-performing agents<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"MODEL\" target=\"MODEL'S ANSWER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The model generates the model's answer<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"MODEL'S ANSWER\" target=\"GROUND-TRUTH ANSWER\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The model's answer is compared with the ground-truth answer to determine correctness<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"GROUND-TRUTH ANSWER\" target=\"FINAL VERDICT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">The final verdict is based on the comparison between the model's answer and the ground-truth answer<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"EMOTION\" target=\"SCORE\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Each emotion is assigned a score in the EQBench dataset<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>    <edge source=\"SCORE\" target=\"CRITIQUE\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">The critique provides an explanation for the revised scores<\/data>      <data key=\"d5\">103d98395c393552cc954c89d4e59f50<\/data>    <\/edge>  <\/graph><\/graphml>"}
{"id":"0cf2e43f324fa4175b9b00b90e5e90ba","chunk":":\nElliot is likely to feel resigned because he has just confessed his feelings to\nAlex, knowing that Alex is already in a relationship. He might feel a bit\nangry at himself for putting himself in this situation. There is a slight sense of\nhopefulness in his confession, hoping that Alex might reciprocate his feelings.\nHe is also likely to feel embarrassed for putting Alex in an awkward position.\nRevised scores:\nResigned: 7\nAngry: 3\nHopeful: 5\nEmbarrassed: 8\nEmotions:\n1. Resigned, 2. Angry, 3. Hopeful, 4. Embarrassed\nOutput\n\"Resigned\" : 7,\n\"Angry\" : 3,\n\"Hopeful\" : 5,\n\"Embarrassed\" : 8\n\u2022Open-Ended Generation : These are the tasks where model is prompted to\ngenerate an answer to an open-ended question, but a ground-truth to match the\nanswer is not available. The metric calculation method for the benchmarks in this\ncategory are provided below:\n\u2013FOFO: For this benchmark the evaluation is done using a judge, GPT-4(version\n0613). We use the judge system message provided in the original paper of the\nbenchmark [ 34]. GPT-4 is used to give a format correctness score between 0\nand 1, 1 meaning the model\u2019s response strictly follows the format specified in\nthe prompt and 0 otherwise. The final score is measured as the percentage of\ntimes the model being evaluated followed the format specified in the prompt\nstrictly.\n\u2013IFEval: IFEval benchmark requires checking if the model response follows the\nverifiable instructions given in the prompt. For this we use the code provided\nby the authors [40].\n\u2013MT-Bench : MT-Bench benchmark consists of a first-turn query and a second-\nturn query independent of the evaluated model\u2019s response. The benchmark\nemploys GPT-4 to judge each turn\u2019s response and provide a score from 1 to 10.\nThe average score over all interactions is reported. System message and prompt\ntemplate used is the one provided by the creators [16].\n\u2013AlpacaEval : In this benchmark we measure win-rates, i.e. the number of times\na powerful LLM (GPT-4-turbo version 0613 in our case) prefers the outputs of\nthe evaluated model over a reference answer [14].\n\u2013InfoBench : InfoBench is also evaluated using GPT-4 (version 1106-preview) as\nthe judge determining if the model response follows the decomposed instruction\nand we use the implementation provided by the creators of the benchmark [ 25].\n30Hallucination Judge Example\nYou will be given a summary instruction and a generated summary.\nYour task to decide if there is any hallucination in the generated\nsummary.\nUser Message:\n{{place summary task here}}\nGenerated Summary:\n{{place response here}}\n=========================\nGo through each section in the generated summary, do the following:\n- Extract relevant facts from the article that can be used to verify\nthe correctness of the summary\n- Decide if any section contains hallucination or not.\nAt the end output a JSON with the format:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part from the summary in\nlist format; if no, leave this empty.\"}\nUse the format:\nAnalysis:\nsection 1:\nwrite the part of the summary\nrelevant segments:\nextract relevant segments from the article\njudgement:\ndecide if the section of the summary is supported by the article\nrepeat this for all sections\n....\nFinal verdict:\n{\"hallucination_detected\": \"yes\/no\", \"hallucinated_span\": \"If yes,\nthe exact span of every hallucinated text part in list format; if no,\nleave this empty.\"}\nFigure 5: Prompt template used for hallucination detection in Text Summarization.\nB.1 Summarization Quality and Hallucination Evaluation\nWe use GPT-4 with the following prompts for evaluating quality and hallucination in\nsummarization:\n31Quality Judge Example\nPlease act as an impartial judge and evaluate the quality of the\nresponse provided by an AI assistant to the user instruction\ndisplayed below.\nYour evaluation should assess the following criteria:\n- Instruction Adherence: Does the response correctly follow the user\ninstruction?\n- Content Grounding: Is the answer grounded in the instruction\nwithout introducing new content beyond what is already present?\nPenalize hallucinations.\n- Overall Quality: Assess the clarity, coherence, and completeness\nof the response.\nBegin your evaluation with a short explanation highlighting the pros\nand cons of the answer. Be as objective as possible. After providing\nyour explanation, rate the overall quality of the response on a scale\nof 1 to 10 using this format:\n\"Rating: [[rating]]\" (e.g., \"Rating: [[5]]\").\nUser Instruction:\n{{place instruction here}}\nAssistant\u2019s Response:\n[The Start of Assistant\u2019s Answer]\n{{place response here}}\n[The End of Assistant\u2019s Answer]\nFigure 6: Prompt template for evaluation of summary quality.\n32","chunk_id":"0cf2e43f324fa4175b9b00b90e5e90ba","document_ids":["c5a70861b9987da7046fe2110eb3616c"],"n_tokens":1085,"entities":[{"name":"ALEX","type":"PERSON","description":"Alex is a person who is already in a relationship and has been confessed to by Elliot, putting Alex in an awkward position","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"RESIGNED","type":"EMOTION","description":"Resigned is an emotion felt by Elliot, scored 7, indicating a strong sense of acceptance of the situation","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ANGRY","type":"EMOTION","description":"Angry is an emotion felt by Elliot, scored 3, indicating a mild sense of frustration or self-directed anger","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HOPEFUL","type":"EMOTION","description":"Hopeful is an emotion felt by Elliot, scored 5, indicating a moderate sense of optimism that Alex might reciprocate his feelings","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMBARRASSED","type":"EMOTION","description":"Embarrassed is an emotion felt by Elliot, scored 8, indicating a strong sense of discomfort for putting Alex in an awkward position","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"OPEN-ENDED GENERATION","type":"TASK","description":"Open-Ended Generation is a task where a model is prompted to generate an answer to an open-ended question without a ground-truth to match the answer","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"FOFO","type":"BENCHMARK","description":"FOFO is a benchmark for evaluating format correctness of model responses using GPT-4 as a judge, scoring between 0 and 1","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"IFEVAL","type":"BENCHMARK","description":"IFEval is a benchmark that checks if the model response follows verifiable instructions given in the prompt","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"MT-BENCH","type":"BENCHMARK","description":"MT-Bench is a benchmark that evaluates model responses to first-turn and second-turn queries, scoring each turn from 1 to 10 using GPT-4","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ALPACAEVAL","type":"BENCHMARK","description":"AlpacaEval is a benchmark that measures win-rates by comparing model outputs to a reference answer, judged by GPT-4-turbo","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"INFOBENCH","type":"BENCHMARK","description":"InfoBench is a benchmark evaluated using GPT-4 to determine if the model response follows decomposed instructions","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HALLUCINATION JUDGE","type":"TASK","description":"Hallucination Judge is a task where a judge decides if there is any hallucination in a generated summary by comparing it to relevant facts from the article","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"QUALITY JUDGE","type":"TASK","description":"Quality Judge is a task where a judge evaluates the quality of a response based on instruction adherence, content grounding, and overall quality, scoring from 1 to 10","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"GPT-4","type":"TECHNOLOGY","description":"GPT-4 is a large language model used as a judge in various benchmarks and tasks to evaluate model responses","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"GPT-4-TURBO","type":"TECHNOLOGY","description":"GPT-4-turbo is a version of GPT-4 used in the AlpacaEval benchmark to judge model outputs","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"VERSION 0613","type":"VERSION","description":"Version 0613 is a specific version of GPT-4 used in the FOFO and AlpacaEval benchmarks","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"VERSION 1106-PREVIEW","type":"VERSION","description":"Version 1106-preview is a specific version of GPT-4 used in the InfoBench benchmark","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"ELLIOT","type":"","description":"","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"EMOTIONS","type":"DATA","description":"Emotions are the different feelings experienced by Elliot, including Resigned, Angry, Hopeful, and Embarrassed","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"SUMMARY INSTRUCTION","type":"TASK","description":"Summary Instruction is a task where a judge evaluates the correctness of a generated summary by comparing it to relevant facts from the article","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"GENERATED SUMMARY","type":"TASK","description":"Generated Summary is a task where a summary is generated and then evaluated for correctness and hallucination","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"PROMPT TEMPLATE","type":"TASK","description":"Prompt Template is a predefined format used for evaluating tasks such as hallucination detection and summary quality","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"TEXT SUMMARIZATION","type":"TASK","description":"Text Summarization is a task where a summary of a given text is generated and evaluated for quality and hallucination","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"SUMMARIZATION QUALITY","type":"TASK","description":"Summarization Quality is a task where the quality of a generated summary is evaluated based on criteria like instruction adherence, content grounding, and overall quality","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"HALLUCINATION EVALUATION","type":"TASK","description":"Hallucination Evaluation is a task where a generated summary is checked for hallucinations by comparing it to relevant facts from the article","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"QUALITY EVALUATION","type":"TASK","description":"Quality Evaluation is a task where the quality of a generated response is evaluated based on criteria like instruction adherence, content grounding, and overall quality","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"},{"name":"REVISED SCORES","type":"","description":"","source_id":"0cf2e43f324fa4175b9b00b90e5e90ba"}],"entity_graph":"<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"ALEX\">      <data key=\"d0\">PERSON<\/data>      <data key=\"d1\">Alex is a person who is already in a relationship and has been confessed to by Elliot, putting Alex in an awkward position<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"RESIGNED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Resigned is an emotion felt by Elliot, scored 7, indicating a strong sense of acceptance of the situation<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ANGRY\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Angry is an emotion felt by Elliot, scored 3, indicating a mild sense of frustration or self-directed anger<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HOPEFUL\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Hopeful is an emotion felt by Elliot, scored 5, indicating a moderate sense of optimism that Alex might reciprocate his feelings<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMBARRASSED\">      <data key=\"d0\">EMOTION<\/data>      <data key=\"d1\">Embarrassed is an emotion felt by Elliot, scored 8, indicating a strong sense of discomfort for putting Alex in an awkward position<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"OPEN-ENDED GENERATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Open-Ended Generation is a task where a model is prompted to generate an answer to an open-ended question without a ground-truth to match the answer<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"FOFO\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">FOFO is a benchmark for evaluating format correctness of model responses using GPT-4 as a judge, scoring between 0 and 1<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"IFEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">IFEval is a benchmark that checks if the model response follows verifiable instructions given in the prompt<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"MT-BENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">MT-Bench is a benchmark that evaluates model responses to first-turn and second-turn queries, scoring each turn from 1 to 10 using GPT-4<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ALPACAEVAL\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">AlpacaEval is a benchmark that measures win-rates by comparing model outputs to a reference answer, judged by GPT-4-turbo<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"INFOBENCH\">      <data key=\"d0\">BENCHMARK<\/data>      <data key=\"d1\">InfoBench is a benchmark evaluated using GPT-4 to determine if the model response follows decomposed instructions<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HALLUCINATION JUDGE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Hallucination Judge is a task where a judge decides if there is any hallucination in a generated summary by comparing it to relevant facts from the article<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"QUALITY JUDGE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Quality Judge is a task where a judge evaluates the quality of a response based on instruction adherence, content grounding, and overall quality, scoring from 1 to 10<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"GPT-4\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4 is a large language model used as a judge in various benchmarks and tasks to evaluate model responses<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"GPT-4-TURBO\">      <data key=\"d0\">TECHNOLOGY<\/data>      <data key=\"d1\">GPT-4-turbo is a version of GPT-4 used in the AlpacaEval benchmark to judge model outputs<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"VERSION 0613\">      <data key=\"d0\">VERSION<\/data>      <data key=\"d1\">Version 0613 is a specific version of GPT-4 used in the FOFO and AlpacaEval benchmarks<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"VERSION 1106-PREVIEW\">      <data key=\"d0\">VERSION<\/data>      <data key=\"d1\">Version 1106-preview is a specific version of GPT-4 used in the InfoBench benchmark<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"ELLIOT\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"EMOTIONS\">      <data key=\"d0\">DATA<\/data>      <data key=\"d1\">Emotions are the different feelings experienced by Elliot, including Resigned, Angry, Hopeful, and Embarrassed<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"SUMMARY INSTRUCTION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Summary Instruction is a task where a judge evaluates the correctness of a generated summary by comparing it to relevant facts from the article<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"GENERATED SUMMARY\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Generated Summary is a task where a summary is generated and then evaluated for correctness and hallucination<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"PROMPT TEMPLATE\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Prompt Template is a predefined format used for evaluating tasks such as hallucination detection and summary quality<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"TEXT SUMMARIZATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Text Summarization is a task where a summary of a given text is generated and evaluated for quality and hallucination<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"SUMMARIZATION QUALITY\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Summarization Quality is a task where the quality of a generated summary is evaluated based on criteria like instruction adherence, content grounding, and overall quality<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"HALLUCINATION EVALUATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Hallucination Evaluation is a task where a generated summary is checked for hallucinations by comparing it to relevant facts from the article<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"QUALITY EVALUATION\">      <data key=\"d0\">TASK<\/data>      <data key=\"d1\">Quality Evaluation is a task where the quality of a generated response is evaluated based on criteria like instruction adherence, content grounding, and overall quality<\/data>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <node id=\"REVISED SCORES\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/node>    <edge source=\"RESIGNED\" target=\"ELLIOT\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Elliot feels resigned about the situation, scored 7<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"RESIGNED\" target=\"EMOTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Emotions include the feeling of being Resigned<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ANGRY\" target=\"ELLIOT\">      <data key=\"d3\">6.0<\/data>      <data key=\"d4\">Elliot feels angry at himself for putting himself in this situation, scored 3<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ANGRY\" target=\"REVISED SCORES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Revised scores include a score of 3 for the emotion Angry<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ANGRY\" target=\"EMOTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Emotions include the feeling of being Angry<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HOPEFUL\" target=\"ELLIOT\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Elliot feels hopeful that Alex might reciprocate his feelings, scored 5<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HOPEFUL\" target=\"REVISED SCORES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Revised scores include a score of 5 for the emotion Hopeful<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HOPEFUL\" target=\"EMOTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Emotions include the feeling of being Hopeful<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMBARRASSED\" target=\"ELLIOT\">      <data key=\"d3\">9.0<\/data>      <data key=\"d4\">Elliot feels embarrassed for putting Alex in an awkward position, scored 8<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMBARRASSED\" target=\"REVISED SCORES\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Revised scores include a score of 8 for the emotion Embarrassed<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"EMBARRASSED\" target=\"EMOTIONS\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Emotions include the feeling of being Embarrassed<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"FOFO\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">FOFO is a benchmark used in the Open-Ended Generation task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"IFEVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">IFEval is a benchmark used in the Open-Ended Generation task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"MT-BENCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">MT-Bench is a benchmark used in the Open-Ended Generation task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"ALPACAEVAL\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">AlpacaEval is a benchmark used in the Open-Ended Generation task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"OPEN-ENDED GENERATION\" target=\"INFOBENCH\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">InfoBench is a benchmark used in the Open-Ended Generation task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the FOFO benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"FOFO\" target=\"VERSION 0613\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Version 0613 of GPT-4 is used in the FOFO benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"IFEVAL\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the IFEval benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"MT-BENCH\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the MT-Bench benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ALPACAEVAL\" target=\"GPT-4-TURBO\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4-turbo is used as a judge in the AlpacaEval benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"ALPACAEVAL\" target=\"VERSION 0613\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Version 0613 of GPT-4-turbo is used in the AlpacaEval benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the InfoBench benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"INFOBENCH\" target=\"VERSION 1106-PREVIEW\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">Version 1106-preview of GPT-4 is used in the InfoBench benchmark<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"HALLUCINATION JUDGE\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the Hallucination Judge task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"QUALITY JUDGE\" target=\"GPT-4\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in the Quality Judge task<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"HALLUCINATION EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in Hallucination Evaluation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"GPT-4\" target=\"QUALITY EVALUATION\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">GPT-4 is used as a judge in Quality Evaluation<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"SUMMARY INSTRUCTION\" target=\"GENERATED SUMMARY\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Summary Instruction involves evaluating a Generated Summary for correctness<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"SUMMARY INSTRUCTION\" target=\"HALLUCINATION EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Summary Instruction involves evaluating a Generated Summary for hallucinations<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"GENERATED SUMMARY\" target=\"HALLUCINATION EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Generated Summary is evaluated for hallucinations<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"GENERATED SUMMARY\" target=\"QUALITY EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Generated Summary is evaluated for quality<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"PROMPT TEMPLATE\" target=\"HALLUCINATION EVALUATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Prompt Template is used for evaluating hallucinations in a Generated Summary<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"PROMPT TEMPLATE\" target=\"QUALITY EVALUATION\">      <data key=\"d3\">7.0<\/data>      <data key=\"d4\">Prompt Template is used for evaluating the quality of a Generated Summary<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION\" target=\"HALLUCINATION EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Text Summarization involves evaluating the generated summary for hallucinations<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"TEXT SUMMARIZATION\" target=\"QUALITY EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Text Summarization involves evaluating the generated summary for quality<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>    <edge source=\"SUMMARIZATION QUALITY\" target=\"QUALITY EVALUATION\">      <data key=\"d3\">8.0<\/data>      <data key=\"d4\">Summarization Quality involves evaluating the quality of a generated summary<\/data>      <data key=\"d5\">0cf2e43f324fa4175b9b00b90e5e90ba<\/data>    <\/edge>  <\/graph><\/graphml>"}
