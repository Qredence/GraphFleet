
-Goal-
Given a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.
Next, report all relationships among the identified entities.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- entity_name: Name of the entity, capitalized
- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.
- entity_description: Comprehensive description of the entity's attributes and activities
Format each entity as ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.
For each pair of related entities, extract the following information:
- source_entity: name of the source entity, as identified in step 1
- target_entity: name of the target entity, as identified in step 1
- relationship_description: explanation as to why you think the source entity and the target entity are related to each other
- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity
Format each relationship as ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)

3. Return output in The primary language of the provided text is "English." as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

4. If you have to translate into The primary language of the provided text is "English.", just translate the descriptions, nothing else!

5. When finished, output {completion_delimiter}.

-Examples-
######################

Example 1:

text:
 prefers the outputs of
the evaluated model over a reference answer [14].
–InfoBench : InfoBench is also evaluated using GPT-4 (version 1106-preview) as
the judge determining if the model response follows the decomposed instruction
and we use the implementation provided by the creators of the benchmark [ 25].
30Hallucination Judge Example
You will be given a summary instruction and a generated summary.
Your task to decide if there is any hallucination in the generated
summary.
User Message:
{{place summary task here}}
Generated Summary:
{{place response here}}
=========================
Go through each section in the generated summary, do the following:
- Extract relevant facts from the article that can be used to verify
the correctness of the summary
- Decide if any section contains hallucination or not.
At the end output a JSON with the format:
{"hallucination_detected": "yes/no", "hallucinated_span": "If yes,
the exact span
------------------------
output:
("entity"{tuple_delimiter}INFOBENCH{tuple_delimiter}TOOL/BENCHMARK{tuple_delimiter}InfoBench is a benchmark evaluated using GPT-4 to determine if the model response follows the decomposed instruction)
{record_delimiter}
("entity"{tuple_delimiter}GPT-4{tuple_delimiter}MODEL{tuple_delimiter}GPT-4 is a version of OpenAI's language model used to evaluate InfoBench)
{record_delimiter}
("entity"{tuple_delimiter}HALLUCINATION JUDGE{tuple_delimiter}TOOL/PROCESS{tuple_delimiter}A process where a judge determines if there is any hallucination in a generated summary)
{record_delimiter}
("relationship"{tuple_delimiter}INFOBENCH{tuple_delimiter}GPT-4{tuple_delimiter}InfoBench is evaluated using GPT-4 to determine if the model response follows the decomposed instruction{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}HALLUCINATION JUDGE{tuple_delimiter}GPT-4{tuple_delimiter}GPT-4 is used in the Hallucination Judge process to evaluate generated summaries{tuple_delimiter}7)
{completion_delimiter}
#############################


Example 2:

text:
 , 21(5):88–92.
Koesten, L., Gregory, K., Groth, P., and Simperl, E. (2021). Talking datasets–understanding data
sensemaking behaviours. International journal of human-computer studies , 146:102562.
Kuratov, Y ., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search
of needles in a 11m haystack: Recurrent memory finds what llms miss.
LangChain (2024). Langchain graphs. https://python .langchain .com/docs/use cases/graph/.
Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via
incorporating query relevance and transfer learning with transformer models. In Advances in
Artificial Intelligence: 
------------------------
output:
("entity"{tuple_delimiter}KOESTEN, L.{tuple_delimiter}PERSON{tuple_delimiter}Koesten, L. is an author of the paper "Talking datasets–understanding data sensemaking behaviours")
{record_delimiter}
("entity"{tuple_delimiter}GREGORY, K.{tuple_delimiter}PERSON{tuple_delimiter}Gregory, K. is an author of the paper "Talking datasets–understanding data sensemaking behaviours")
{record_delimiter}
("entity"{tuple_delimiter}GROTH, P.{tuple_delimiter}PERSON{tuple_delimiter}Groth, P. is an author of the paper "Talking datasets–understanding data sensemaking behaviours")
{record_delimiter}
("entity"{tuple_delimiter}SIMPERL, E.{tuple_delimiter}PERSON{tuple_delimiter}Simperl, E. is an author of the paper "Talking datasets–understanding data sensemaking behaviours")
{record_delimiter}
("entity"{tuple_delimiter}INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES{tuple_delimiter}PUBLICATION{tuple_delimiter}The journal where the paper "Talking datasets–understanding data sensemaking behaviours" was published")
{record_delimiter}
("entity"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}PERSON{tuple_delimiter}Kuratov, Y. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}BULATOV, A.{tuple_delimiter}PERSON{tuple_delimiter}Bulatov, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}PERSON{tuple_delimiter}Anokhin, P. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}SOROKIN, D.{tuple_delimiter}PERSON{tuple_delimiter}Sorokin, D. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}SOROKIN, A.{tuple_delimiter}PERSON{tuple_delimiter}Sorokin, A. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}BURTSEV, M.{tuple_delimiter}PERSON{tuple_delimiter}Burtsev, M. is an author of the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss")
{record_delimiter}
("entity"{tuple_delimiter}LANGCHAIN{tuple_delimiter}ORGANIZATION{tuple_delimiter}LangChain is the organization behind the LangChain graphs project)
{record_delimiter}
("entity"{tuple_delimiter}LASKAR, M. T. R.{tuple_delimiter}PERSON{tuple_delimiter}Laskar, M. T. R. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models")
{record_delimiter}
("entity"{tuple_delimiter}HOQUE, E.{tuple_delimiter}PERSON{tuple_delimiter}Hoque, E. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models")
{record_delimiter}
("entity"{tuple_delimiter}HUANG, J.{tuple_delimiter}PERSON{tuple_delimiter}Huang, J. is an author of the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models")
{record_delimiter}
("entity"{tuple_delimiter}ADVANCES IN ARTIFICIAL INTELLIGENCE{tuple_delimiter}PUBLICATION{tuple_delimiter}The conference where the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models" was presented)
{record_delimiter}
("relationship"{tuple_delimiter}KOESTEN, L.{tuple_delimiter}GREGORY, K.{tuple_delimiter}Koesten, L. and Gregory, K. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KOESTEN, L.{tuple_delimiter}GROTH, P.{tuple_delimiter}Koesten, L. and Groth, P. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KOESTEN, L.{tuple_delimiter}SIMPERL, E.{tuple_delimiter}Koesten, L. and Simperl, E. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}GREGORY, K.{tuple_delimiter}GROTH, P.{tuple_delimiter}Gregory, K. and Groth, P. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}GREGORY, K.{tuple_delimiter}SIMPERL, E.{tuple_delimiter}Gregory, K. and Simperl, E. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}GROTH, P.{tuple_delimiter}SIMPERL, E.{tuple_delimiter}Groth, P. and Simperl, E. co-authored the paper "Talking datasets–understanding data sensemaking behaviours"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}BULATOV, A.{tuple_delimiter}Kuratov, Y. and Bulatov, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}Kuratov, Y. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}SOROKIN, D.{tuple_delimiter}Kuratov, Y. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}SOROKIN, A.{tuple_delimiter}Kuratov, Y. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}KURATOV, Y.{tuple_delimiter}BURTSEV, M.{tuple_delimiter}Kuratov, Y. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}BULATOV, A.{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}Bulatov, A. and Anokhin, P. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}BULATOV, A.{tuple_delimiter}SOROKIN, D.{tuple_delimiter}Bulatov, A. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}BULATOV, A.{tuple_delimiter}SOROKIN, A.{tuple_delimiter}Bulatov, A. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}BULATOV, A.{tuple_delimiter}BURTSEV, M.{tuple_delimiter}Bulatov, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}SOROKIN, D.{tuple_delimiter}Anokhin, P. and Sorokin, D. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}SOROKIN, A.{tuple_delimiter}Anokhin, P. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}ANOKHIN, P.{tuple_delimiter}BURTSEV, M.{tuple_delimiter}Anokhin, P. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}SOROKIN, D.{tuple_delimiter}SOROKIN, A.{tuple_delimiter}Sorokin, D. and Sorokin, A. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}SOROKIN, D.{tuple_delimiter}BURTSEV, M.{tuple_delimiter}Sorokin, D. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}SOROKIN, A.{tuple_delimiter}BURTSEV, M.{tuple_delimiter}Sorokin, A. and Burtsev, M. co-authored the paper "In search of needles in a 11m haystack: Recurrent memory finds what llms miss"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}LASKAR, M. T. R.{tuple_delimiter}HOQUE, E.{tuple_delimiter}Laskar, M. T. R. and Hoque, E. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}LASKAR, M. T. R.{tuple_delimiter}HUANG, J.{tuple_delimiter}Laskar, M. T. R. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}HOQUE, E.{tuple_delimiter}HUANG, J.{tuple_delimiter}Hoque, E. and Huang, J. co-authored the paper "Query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models"{tuple_delimiter}8)
{completion_delimiter}
#############################



-Real Data-
######################
text: {input_text}
######################
output:
