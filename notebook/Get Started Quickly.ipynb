{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started quickly \n",
    "First make sure to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/Qredence/GraphFleet.git\n",
    "\n",
    "\n",
    "cd GraphFleet\n",
    "poetry shell\n",
    "poetry install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/Qredence/GraphFleet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "### poetry shell\n",
    "## poetry install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Your Azure OpenAI API key\n",
    "export GRAPHRAG_API_KEY=\"\"\n",
    "\n",
    "# The base URL for your Azure OpenAI API endpoint\n",
    "export GRAPHRAG_API_BASE=\"\"\n",
    "\n",
    "# The API version you're using (e.g., \"2024-02-15-preview\")\n",
    "export GRAPHRAG_API_VERSION=\"\"\n",
    "\n",
    "# The name of the language model you're using (e.g., \"gpt-4\")\n",
    "export GRAPHRAG_LLM_MODEL=\"\"\n",
    "\n",
    "# The deployment name for your language model in Azure\n",
    "export GRAPHRAG_DEPLOYMENT_NAME=\"\"\n",
    "\n",
    "# The name of the embedding model you're using (e.g., \"text-embedding-ada-002\")\n",
    "export GRAPHRAG_EMBEDDING_MODEL=\"\"\n",
    "\n",
    "# Note: Replace the empty strings with your actual values before using GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the existing cell content with:\n",
    "! python -m graphrag.index --init --root ../graphfleet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your GraphRAG Pipeline\n",
    "\n",
    "This notebook guides you through configuring your GraphRAG pipeline using either OpenAI or Azure OpenAI.\n",
    "\n",
    "### 1. Environment Variables and Settings Files\n",
    "\n",
    "GraphRAG relies on two crucial files for configuration:\n",
    "\n",
    "- **.env:** This file stores environment variables. The most important one is GRAPHRAG_API_KEY, which holds your API key for either OpenAI or Azure OpenAI.\n",
    "- **settings.yaml:** This file contains settings that fine-tune the behavior of the GraphRAG pipeline.\n",
    "\n",
    "Here's a breakdown of how to configure each file for OpenAI and Azure OpenAI:\n",
    "\n",
    "### 2. OpenAI Configuration\n",
    "\n",
    "1. **Update .env:**\n",
    "   - Open the .env file located in your ./graphfleet directory.\n",
    "   - Find the line GRAPHRAG_API_KEY=<API_KEY>.\n",
    "   - Replace <API_KEY> with your actual OpenAI API key.\n",
    "\n",
    "2. **(Optional) Customize settings.yaml:**\n",
    "   - Open the settings.yaml file in the same directory.\n",
    "   - You can customize various aspects of the pipeline here, like which language model to use or how many results to return. Refer to the [configuration documentation](link-to-configuration-docs) for detailed options.\n",
    "\n",
    "### 3. Azure OpenAI Configuration\n",
    "\n",
    "1. **Update .env:**\n",
    "   - Open the .env file.\n",
    "   - Set the GRAPHRAG_API_KEY to your Azure OpenAI API key.\n",
    "\n",
    "2. **Configure settings.yaml:**\n",
    "   - Open the settings.yaml file.\n",
    "   - Search for the llm configuration section. You'll find two: one for chat and one for embeddings.\n",
    "   - **Chat Endpoint Example:**\n",
    "     ```yaml\n",
    "     llm:\n",
    "       type: azure_openai_chat \n",
    "       api_base: https://<your-instance>.openai.azure.com \n",
    "       api_version: your version  # Adjust if needed\n",
    "       deployment_name: <your-azure-model-deployment-name> \n",
    "     ```\n",
    "\n",
    "   - **Embeddings Endpoint Example:** \n",
    "     ```yaml\n",
    "     llm:\n",
    "       type: azure_openai_embedding\n",
    "       api_base: https://<your-instance>.openai.azure.com \n",
    "       api_version: your version  # Adjust if needed\n",
    "       deployment_name: <your-azure-model-deployment-name> \n",
    "     ```\n",
    "\n",
    "   - **Replace the placeholders:**\n",
    "     - <your-instance>: Your Azure OpenAI instance name.\n",
    "     - <your-azure-model-deployment-name>: The deployment name of your Azure OpenAI model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto generate prompts for your specific data index :\n",
    "This command does the following:\n",
    "- Runs the prompt_tune module of GraphRAG\n",
    "- Uses the configuration file settings.yaml in the ./graphfleet directory\n",
    "- Sets the root directory to ./graphfleet\n",
    "- Disables entity type generation with the --no-entity-types flag\n",
    "- Specifies the output directory for the generated prompts as ./graphfleet/prompts\n",
    "\n",
    "\n",
    "### This step is important because it customizes the prompts based on your specific data index, which can improve the relevance and effectiveness of your queries later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m graphrag.prompt_tune \\\n",
    "    --config ../graphfleet/settings.yaml \\\n",
    "    --root ../graphfleet \\\n",
    "    --no-entity-types \\\n",
    "    --output ../graphfleet/prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Your Data:\n",
    " Now, let's index your data to make it searchable. This is the final step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m graphrag.index \\\n",
    "    --verbose \\\n",
    "    --root ../graphfleet \\\n",
    "    --config ../graphfleet/settings.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing in Progress!\n",
    "\n",
    "Running the indexing pipeline might take a while ‚Äì don't worry, that's normal! ‚è≥ \n",
    "!\n",
    "**Factors that influence indexing time:**\n",
    "\n",
    "* **Size of your data:**  Larger datasets naturally take longer to process.\n",
    "* **Model selection:** Different models have varying processing speeds.\n",
    "* **Text chunk size:** This setting (configurable in your `.env` file) impacts how the data is broken down and indexed.\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "Once the indexing process is complete, you'll find a new folder in your project directory:\n",
    "\n",
    "   `./graphfleet/output/<timestamp>/artifacts` \n",
    "\n",
    "Inside this folder, you'll see a collection of `parquet` files. These files contain your indexed data, ready for GraphRAG to use! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Query! üöÄ\n",
    "\n",
    "Now that your data is indexed, the real fun begins: **asking questions!**  \n",
    "\n",
    "Let's explore how to use GraphRAG's query engine to extract insights from your dataset. \n",
    "\n",
    "### Global Search: Uncovering High-Level Themes\n",
    "\n",
    "Use global search to get a bird's-eye view of the main ideas in your data:\n",
    "\n",
    "## Explanation:\n",
    "\n",
    "python -m graphrag.query: Runs the GraphRAG query engine.\n",
    "--root ./graphfleet: Specifies the root directory of your GraphRAG project.\n",
    "--method global: Tells GraphRAG to perform a global search across all your data.\n",
    "\"What are the top themes in this story?\": Your natural language query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m graphrag.query \\\n",
    "--root ../graphfleet \\\n",
    "--method global \\\n",
    "--streaming \\\n",
    "\"Language Agent Tree Search?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "--method local: Instructs GraphRAG to focus on a specific part of your data relevant to the query.\n",
    "\"Who is Scrooge, and what are his main relationships?\": This query focuses on a character (Scrooge) and their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m graphrag.query \\\n",
    "--root ../graphfleet \\\n",
    "--method global \\\n",
    "\"How to write the multi-agent system that incorporates the tree search algorithm? and GraphRAG?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment! üß™\n",
    "\n",
    "Go ahead and ask your own questions! Try different query types, phrasings, and explore the power of GraphRAG to unlock insights from your indexed data.\n",
    "\n",
    "Now check the [local_search_notebook.py](local_search_notebook.ipynb) file to see how to use the local search engine and how to generate questions !\n",
    "Same for [global_search_notebook.ipynb](global_search_notebook.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphfleet-bVb82vZ5-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
